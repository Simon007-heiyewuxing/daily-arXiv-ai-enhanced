<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文提出并公开了KLOCR（1亿实例训练的韩英双语OCR基线）和KOCRBench（韩语VQA数据集），证明OCR增强能显著提升VLM在双语VQA任务的表现，并分析了提示方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在多语言场景下处理包含文本的图像（如韩语）时表现有限，缺少强大的双语OCR支持与相应的评测数据集，故需研究OCR增强的生成方法以提升多语言VQA性能。

Method: 作者训练了一个名为KLOCR的双语OCR基线，使用约1亿（100M）数据实例，并在此基础上将OCR功能整合到视觉-语言模型中；同时构建了韩语VQA基准KOCRBench，比较和分析了不同提示（prompting）方法的效果，并在多种开源与商用模型上进行了广泛实验。

Result: 实验结果显示，提取到的OCR文本能显著提升多种模型（开源与商用）在VQA任务上的表现；并提供了KLOCR模型、代码与KOCRBench数据集供社区使用。

Conclusion: 该论文表明在VLM中加入OCR能力能显著提升韩英双语的VQA表现，并通过训练与发布KLOCR基线模型及构建KOCRBench数据集推动该领域研究。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [2] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 用Oracle排序器替代奖励模型，配合GRPO_rank排序损失，Oracle-RLAIF能更高效地用AI反馈优化视频-语言模型，提升视频理解性能并降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统SFT加人类偏好强化学习成本高，且训练奖励模型昂贵；RLAIF使用AI裁判降低成本但依赖专门训练的奖励模型仍受限，因此需要更通用、节省数据和成本的替代方法。

Method: 提出Oracle-RLAIF框架：使用Oracle ranker对候选回复进行排序替代标量评分；提出GRPO_rank，一种基于Group Relative Policy Optimization的排序感知损失，直接优化序数反馈并计算排名感知优势；在多个视频理解基准上用该框架对VLM进行微调并评估性能。

Result: 实验显示Oracle-RLAIF在多个视频理解测试集上持续优于采用现有微调方法的主流VLM，证明用排序器替代奖励模型并采用GRPO_rank能带来更好的对齐效果和数据效率。

Conclusion: Oracle-RLAIF通过用一个通用的Oracle排序器取代训练好的奖励模型，并结合基于排序的GRPO_rank损失函数，实现了以排序反馈直接优化VLM的能力，从而在视频理解基准上优于现有方法，证明了用AI作为裁判的性价比与灵活性。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [3] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR在物理仿真中直接学习视觉到动作的策略，利用pixel-as-ray提供稳健全局引导，并通过专家蒸馏+强化学习提高样本效率，最终实现比两阶段方法更逼真、更物理合理的单目视频人体运动重建。


<details>
  <summary>Details</summary>
Motivation: 现有单目人体动作重建多依赖运动学估计，缺乏物理约束导致不真实；而传统物理后处理的两阶段方法会造成误差累积。作者旨在设计一个端到端的物理感知视觉控制框架，以同时满足视觉对齐和物理合理性。

Method: 提出pixel-as-ray策略，将2D关键点提升为3D空间射线并转换到全局坐标，作为策略输入，与来自预训练编码器的局部视觉特征结合，提供软性全局定位与局部细节。使用从mocap训练的专家策略蒸馏以缓解强化学习的样本效率问题，再用基于物理的奖励对视觉条件策略进行精炼。

Result: 在多种场景下，PhysHMR在视觉精度和物理真实度上均优于先前方法，生成高保真且物理合理的运动。

Conclusion: 本论文提出了一种统一的从单目视频重建物理合理人体运动的方法PhysHMR，通过在物理仿真环境中直接学习视觉到动作的策略，避免了传统先运动学估计再物理后处理的两阶段误差累积，从而提升了重建质量。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [4] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 当人与机器基线性能相近时，融合最有效；用PAR识别可获益的场景并通过智能选人实现优于机器与传统融合的系统性能。


<details>
  <summary>Details</summary>
Motivation: 理解在面部识别中何种条件下将人类审查与高性能算法结合能提升整体决策准确性，从而为智能使用AI提供实证指南。

Method: 收集不同水平（专家和非专家）识别者的数据，比较人-人和人-机融合的准确率，应用PAR分析基线能力差异对协作收益的影响；使用图论寻找最佳全人类合作结构；实施智能人机融合策略（挑选最能提升机器的个体）并与其他融合策略比较。

Result: PAR成功预测了融合收益，发现“关键融合区”很大：即使人在准确率低于机器时，融合仍能改善系统表现。智能挑选个体的融合优于仅用机器或无差别融合，并更好地缓解低表现个体的负面影响。全人系统的最佳表现接近智能人机协作的平均水平，但智能人机协作在减少低表现影响方面更优。

Conclusion: 人机协作可显著提升面部识别准确性，前提是协作者之间基线准确率相近；基于Proximal Accuracy Rule (PAR)可预测融合收益，并能识别“关键融合区”以智能选择能提升机器性能的人员。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 首个面向生成视频模型的不确定性量化框架（含校准度量、黑盒分解方法S-QUBED与基准数据集），通过潜在空间条件生成实现aleatoric与epistemic分解，并在基准数据集上展示良好校准与与准确率的负相关性。


<details>
  <summary>Details</summary>
Motivation: 生成视频模型在文本到视频任务中广泛应用，但存在虚构（hallucination）问题，缺乏对模型预测可信度的量化手段，存在安全隐患，因此需要专门的UQ方法。

Method: 提出三个核心组件：1) 基于稳健秩相关估计的校准评估度量，无需强假设；2) 一种黑盒UQ方法S-QUBED，通过在潜在空间中建模条件生成，将不确定性分解为aleatoric与epistemic；3) 构建用于基准测试的视频UQ数据集。

Result: 实验证明S-QUBED能计算出校准的总不确定性估计，与任务准确率负相关，并能有效区分并估计aleatoric与epistemic不确定性成分。

Conclusion: 该论文提出了首个用于生成视频模型不确定性量化（UQ）的框架，能够将总不确定性分解为不可约随机性（aleatoric）与知识欠缺（epistemic），并在实验中验证了方法的校准效果与与任务准确率的负相关性。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: PEO通过在不训练新模型的前提下优化提示词嵌入，用三项损失兼顾美学、对齐与提示保留，从而提升预训练扩散模型在简单提示下的图像质量，且表现优于或等同于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成在面对简单或未经过修饰的提示时往往无法生成高美学质量图像；手工撰写复杂提示或训练额外模块成本高且不便普适化。需要一种训练免费且能在不同骨干模型上使用的方法来自动提升简单提示的生成质量。

Method: 基于预训练的文本到图像扩散模型，针对输入的简单提示词对其文本嵌入进行优化。设计了三部分损失：提高生成图像的美学质量（aesthetic loss）、确保生成图像与优化后文本嵌入一致（alignment loss）、以及保持与原始提示最小偏差的提示保留项（prompt preservation loss）。该方法无需额外训练参数，并且与骨干模型无关。

Result: 实验中PEO在定量指标和主观评价上均超过或至少达到现有先进的文本到图像生成与提示适配方法的性能，证明其在提升视觉美学与保持提示语义方面有效。

Conclusion: 该论文提出了一种无需训练、可与任意预训练扩散模型配合的提示词嵌入优化方法（PEO），通过直接优化文本嵌入在单一简单提示下提升生成图像的美学质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [7] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 提出一种可移动的无标记多摄像系统（背部轻量装置+8台外部摄像+头显两目），通过ego-exo跟踪流水线在接近真实场景中生成高精度3D手部姿态标注，提升了数据集的环境多样性与标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有的第一视角手部追踪数据集多在受控环境下采集，导致环境多样性不足、模型泛化能力差；因此需要一种能在接近真实世界的自由移动条件下获取高精度3D手部与物体交互数据的方法。

Method: 使用轻量级背部携带的采集装置结合八台外部摄像机和佩戴式Meta Quest 3头显的两台第一视角摄像头，构建一个ego-exo（自顶+外部）跟踪流水线，通过多视角同步图像和几何约束生成高质量的3D手部姿态标注并对标注质量进行严格评估。

Result: 构建并发布了一个包含同步多视角图像与精确3D手部姿态标注的数据集，实验证明该方法在保持环境真实感的同时，显著提高了3D标注的准确性，从而缩小了环境真实度与标注精确度间的权衡。

Conclusion: 该论文提出了一种混合摄像系统，实现了在接近真实场景下对手部与物体的精准3D追踪，弥补了现有数据集多在受控实验室环境采集的不足，提升了环境多样性与标注精度之间的平衡。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [8] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 引入利用人体关键点的输入感知稀疏注意力和专门的蒸馏损失，将多步扩散视频模型蒸馏为少步实时学生模型，在提升唇动与手部动作真实性的同时实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频生成方法因大量去噪步骤和昂贵的注意力机制导致推理缓慢，难以满足实时应用需求；直接应用现有扩散蒸馏方法会导致视频质量下降，因此需要新的蒸馏策略以在速度与质量间取得平衡。

Method: 作者将多步扩散视频模型蒸馏为少步学生模型，关键技术包括：1）输入感知稀疏注意力（Input-aware sparse attention），利用输入的人体关键点对应关系，引导注意力聚焦于面部、手部和上半身等相关区域，减少冗余计算并加强时序部位对应；2）输入感知蒸馏损失（Input-aware distillation loss），专门针对唇动同步和手部动作进行设计以提升视觉细节；两者结合用于训练学生模型以在少步采样下保持高质量。

Result: 通过将稀疏注意力与输入感知蒸馏损失相结合，作者实现了在实时帧率下的音频驱动视频合成，相比现有音频驱动及输入驱动方法在视觉质量（包括唇形同步和手部动作真实性）和推理效率上均有提升；论文还通过大量消融和定量/定性实验验证了设计的有效性。

Conclusion: 本文提出了一种面向实时合成讲者视频的扩散模型蒸馏方法，通过在学生模型中引入输入人体姿态条件的稀疏注意力和输入感知蒸馏损失，实现在保证或提升视觉质量的同时大幅减少推理步骤，从而达到实时性能。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [9] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出FunLoRA：一种功能化的秩1 LoRA作为动态条件，用于持续适应生成模型，避免遗忘并只需用当前任务数据训练；在flow-matching模型上优于扩散模型SOTA，且更高效。


<details>
  <summary>Details</summary>
Motivation: 传统增量训练依赖在合成数据上重训练以缓解遗忘，但这会导致训练时间爆增和长期性能退化，因为合成数据不如真实数据丰富。为此设计更高表达力且参数高效的调控机制以避免遗忘。

Method: 提出Functional LoRA（FunLoRA），使用仅含秩1矩阵但通过特定函数将参数化矩阵的功能秩提升，从而作为动态条件机制注入到生成模型；在基于flow-matching的从零训练模型上，作为参数高效微调（PEFT）方法训练，只需当前任务数据，无需重训练或大量合成数据。

Result: 在大量实验中，FunLoRA在flow-matching模型上超过基于扩散模型的先前SOTA，取得更高的分类准确率，同时显著降低内存开销和采样时间。

Conclusion: FunLoRA通过功能化的低秩适配避免灾难性遗忘，仅在当前任务数据上训练即可，实现了持续适应生成模型的目标。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [10] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出面向美国交通灯与标志的双视场序列鲁棒框架，三层防御栈+序列投票，在多源实车数据上显著提升mAP并降低攻击成功率与高风险误分类。


<details>
  <summary>Details</summary>
Motivation: 现有工作多忽视时序连续性、多视场感知以及同时对抗数字与自然退化的鲁棒性，而这些在自动驾驶场景中会直接影响导航与安全。

Method: 构建包含aiMotive、Udacity、Waymo与德州自录视频的多源数据集，提取中长序列RGB并在四类ODD（高速、夜间、雨天、城市）下进行时序对齐；提出双视场（中远距离）输入并在模型前后端应用特征压缩、防御性蒸馏与熵基异常检测，最后通过序列投票融合提升决策一致性。

Result: 在真实异常检测应用实验中，统一防御栈达成79.8mAP，将攻击成功率降至18.2%，比YOLOv8/YOLOv9/BEVFormer表现更好，并将高风险误分类降低至32%；同时验证了物理转移性（recapture probes）。

Conclusion: 本文提出了一个双视场、保序列的鲁棒性框架，通过特征压缩、对抗性蒸馏和基于熵的异常检测组成三层防御栈，并结合序列级投票以增强稳定性，在多源实车数据集上显著提高了对交通灯与交通标志的识别鲁棒性。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [11] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO 通过迭代优化噪声扰动，实现了在流匹配生成模型上进行高效、稳定的强化学习，从而提升人类对齐与图像质量。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型在文本到图像生成上表现优异，但其确定性导致难以直接应用强化学习以优化视觉质量与人类对齐。现有通过随机噪声扰动引入随机性的做法低效且不稳定，因此需要更有效的扰动优化方法。

Method: 提出基于迭代候选扰动搜索的算法：对一组候选噪声扰动进行解码生成图像，用奖励函数打分，并以此更新噪声分布以向高奖励区域移动；循环执行直到收敛或达到资源上限。

Result: 在实验中，Smart-GRPO 在奖励优化速度与最终奖励值上均优于基线随机扰动方法，同时生成图像的视觉质量（可能通过FID或人工评估衡量）也得到提升，表明该方法在实用性和效果上均具优势。

Conclusion: Smart-GRPO 是一个针对流匹配(flow-matching)生成模型中强化学习问题的创新方法，通过优化噪声扰动分布实现有效的策略搜索，弥补了确定性流模型在强化学习中应用的不足。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [12] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter用Transformer生成密集几何一致的高斯初始化，结合贡献剪枝和几何监督，能从稀疏自由视图快速、稳健地重建高质量表面，优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有高斯点云重建方法依赖密集已校准视图，面对自由稀疏图像时重建表面质量差且易过拟合，需要一种能在稀疏视图下稳定、高效地重建细致几何的方法。

Method: 使用大规模Transformer编码多视图图像，采用自分裂高斯头生成密集且几何一致的高斯场景初始化；通过基于贡献的剪枝清除局部浮点器；在快速优化阶段结合可微相机参数、深度和多视图特征监督以减轻对有限视图的过拟合。

Result: 在DTU和Replica数据集上，FSFSplatter在表面重建质量和新视图合成上优于当前最先进方法，显示出更好的几何一致性和抗过拟合能力。

Conclusion: FSFSplatter提出了一种针对自由稀疏视图的快速表面重建方法，通过密集高斯初始化、相机参数估计和几何增强的场景优化来解决重建过拟合和局部浮点器问题。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [13] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC结合意图预测与视觉先验，通过混合注意力机制和440小时数据集提升文本驱动动作生成的精度、可控性与意图理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对动作执行因果逻辑与人类意图的刻画，且纯语言条件不足以指定精细时空细节，需引入意图建模与视觉先验以提升精度与可控性。

Method: 提出统一框架MoGIC：联合优化多模态条件的动作生成与意图预测；引入自适应作用域的注意力混合机制用于局部对齐；整合视觉先验进行视觉条件生成。并构建Mo440H作为训练与评估基准。

Result: 在HumanML3D和Mo440H上微调后，FID分别降低38.6%和34.6%；使用轻量文本头在动作描述任务上超过基于大语言模型的方法；支持意图预测与视觉条件生成，提升可控性与意图理解。

Conclusion: MoGIC通过联合意图建模与视觉先验，改善了文本到动作生成的因果理解与控制性；在大规模多数据集上微调后在FID和描述任务上显著优于现有方法。

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [14] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出基于语义与运动先验的运动自适应控制点分配与样条轨迹参数化方法，解决稀疏控制的静态冗余与动态不足问题，提升动态3D重建质量与效率。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法按几何分配控制点，导致静态冗余与动态区域表示不足，无法有效表达时变场景且计算资源浪费。需将控制点分布与运动复杂度匹配以提高效率与重建质量。

Method: 建立patch-token-node对应关系后，基于视觉基金会模型提取语义与运动先验，进行运动自适应压缩：通过迭代体素化与运动倾向评分调整控制点分布；使用2D跟踪初始化样条轨迹参数化，替换MLP变形场以表示时序演化。

Result: 实验表明该方法在重建质量和计算效率方面显著优于最先进方法，特别是在动态区域细节恢复和优化稳定性上有明显提升。

Conclusion: 该文提出运动自适应控制点分配框架，通过语义与运动先验将控制点密度与运动复杂度对齐，实现对动态区域的高密度表示与静态背景的压缩。使用样条曲线参数化轨迹替代MLP变形场，提升了运动平滑性与优化稳定性。实验显示在重建质量和效率上优于现有方法。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [15] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: Net2Net融合无监督DIP与预训练DRUNet，通过RED正则化将二者结合，提升真实噪声去除的泛化性与性能，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统手工先验在受控环境外表现欠佳，深度学习方法虽强但依赖大量标注且泛化性有限。为此提出将无监督适配能力与预训练模型的泛化能力结合，以应对真实世界噪声的复杂性和可变性，尤其是在训练数据有限的情况下。

Method: 方法将每张输入图像用无监督的DIP自适应拟合其独特噪声特性，同时引入在大规模数据上预训练的DRUNet作为外部去噪器，通过RED框架将DRUNet的先验作为正则项结合到DIP的优化过程中，从而实现联合优化与约束。

Result: 文中在基准数据集上的大量实验表明Net2Net在真实噪声去除任务上优于若干对比方法，特别是在不同噪声类型与有限训练数据场景下表现更佳。

Conclusion: 本文提出的Net2Net通过联合无监督的DIP（深图像先验）与有监督的预训练DRUNet并利用RED（通过去噪正则化）融合两者优点，旨在提升真实噪声去除效果。作者声称该方法在不同噪声模式和有限训练数据场景下具有更好泛化性和性能。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [16] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出Retrv-R1，用信息压缩+细节检查和检索定制的激活+课程奖励RL，使R1式逐步推理在多模态检索中既高效又准确，达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 直接将DeepSeek-R1的方法用于检索任务存在两大问题：高计算开销（多候选逐步推理导致token膨胀）与直接RL训练不稳定且结果欠佳，需设计专门模块与训练范式解决。

Method: 主要方法包括：1) 设计信息压缩模块与细节检查机制，减少推理候选的token消耗并保留关键细节；2) 采用检索定制的合成CoT数据进行激活训练；3) 以新的课程奖励进行强化学习微调，实现稳定且高效的检索优化。

Result: 在多项基准与任务上，Retrv-R1达到了SOTA性能，显著提升了检索准确性、计算效率，并表现出良好的泛化能力（文中以实验数据支撑）。

Conclusion: Retrv-R1通过引入信息压缩模块和分级训练范式，将R1风格的逐步推理方法高效地应用到多模态检索任务，从而在性能、效率和泛化能力上实现SOTA。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [17] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+是一个训练自由的测试时自适应框架，通过动态缓存与贝叶斯融合同时调整似然与先验，能高效提升VLM在识别与检测任务下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在实际分布偏移下性能下降；现有TTA方法要么依赖昂贵的反向传播，不利于实时应用，要么只调整似然而忽视先验，导致适配不充分。需一种高效、训练自由且同时调整似然和先验的TTA方法，适用于识别与检测。

Method: 提出BCA+框架：动态缓存存储并更新类嵌入、空间尺度和基于历史预测的类先验；将适配建模为贝叶斯推断，缓存预测由动态似然（特征与尺度相似性）与演化先验组合；引入不确定性引导融合以决定初始VLM输出与缓存预测的权重；整个过程无需反向传播与额外训练。

Result: 在多项识别与检测基准上的大量实验显示，BCA+在准确率/检测性能上均达到或超越现有最先进方法，同时保持高效性（无训练、无反向传播），适配过程中能够纠正语义理解与上下文置信度错误。

Conclusion: BCA+在不需训练和反向传播的前提下，通过引入动态缓存、联合更新类嵌入、尺度和自适应先验，并以贝叶斯推断融合初始VLM输出与缓存预测，实现对识别与检测任务的鲁棒测试时自适应，从而在分布偏移下显著提升性能。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [18] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: HGCD-BT通过半监督层次聚类融合对比学习，实现对已知与未知脑肿瘤类别的可靠发现与分类，在两个病理成像数据集上显著优于现有GCD方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法只能识别训练时出现的固定类别，无法发现训练集中缺失的新肿瘤类型；需要兼顾无监督特征学习与有监督先验并反映肿瘤分类的层次结构。

Method: 在对比学习基础上引入半监督层次聚类损失，结合层次聚类与对比学习以处理标签缺失与层级类别结构。

Result: 在OpenSRH补丁级任务上比最先进GCD方法提升约28%准确率，且在Digital Brain Tumor Atlas的整片图像任务上证明了方法的跨模态泛化能力。

Conclusion: 提出HGCD-BT，有效识别已知与未知脑肿瘤类别，并在两个数据集上均表现优异。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [19] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: AdaRD-Key是一种训练-free、查询驱动的关键帧采样算法，结合相关性评分与log-det多样性项，带有相关性感知门控以在弱对齐情形下切换为多样性模式，能实时为VLM提供信息丰富且非冗余的帧，显著提升长视频理解效果。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在长视频理解中常用均匀采样或固定排斥窗口的关键帧选择，容易错过关键短时刻或对查询不敏感；一些方法注重视觉多样性但忽视与查询的相关性。本工作旨在同时兼顾查询相关性与多样性，提升长视频问答和理解的准确性。

Method: 方法将查询条件的相关性评分与对数行列式（log-determinant）多样性项结合，形成RD-MV目标；采用轻量的相关性感知门控机制，在检测到查询与视频对齐弱时自动切换为仅多样性模式，从而覆盖更多信息；无需训练、实时运行且可插入现有VLM管线。

Result: 在LongVideoBench和Video-MME数据集上，AdaRD-Key在长视频理解任务中实现了SOTA性能提升，特别在长片段与信息密集的视频上效果明显。方法实时高效（单GPU可实时运行），并提供开源代码。

Conclusion: 该论文提出了一种训练-free的关键帧采样模块AdaRD-Key，通过统一的相关性-多样性最大体积（RD-MV）目标在查询驱动的长视频理解中选取信息丰富且非冗余的帧，尤其适用于长时序视频并在多个基准上表现优异。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [20] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 论文通过构建标注数据集与评估框架，采用三种提示策略，系统分析了VLMs解答rebus谜题时的推理质量与失败模式，指出模型在视觉组合方面有优势，但在缺失元素与文化语义理解上存在显著不足，且提示策略会改变模型的认知与表现。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态任务上表现优异，但在复杂的横向思维任务（如rebus）上存在显著困难，且其失败模式与内部推理过程尚未被系统研究，促使作者进行深入的可解释性研究。

Method: 构建并标注了221个跨六类认知类别的rebus数据集，提出将推理质量与答案正确性分离的评估框架；比较三种提示策略以诱导不同类型的解释过程，并对模型在各类别上的表现与解释质量进行定量与定性分析。

Result: 发现模型在视觉构图推理上表现较好，但在缺席元素解读与文化符号理解方面存在根本性弱点；不同提示策略显著影响模型的认知路径与解题效果，说明可解释性应与性能评估同等重要。

Conclusion: 该论文通过可解释性分析揭示了视觉-语言模型（VLMs）在解决填字式谜题（rebus puzzles）时的认知模式与局限，强调了解释性在评估模型性能中的重要性。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [21] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 提出一种合成文本移除基准：用对象感知放置与视觉语言模型生成内容，将文本渲染到复杂背景以保证干净真值，改进评估与泛化，数据集公开。


<details>
  <summary>Details</summary>
Motivation: 现有场景文本移除数据集存在人工编辑造成的真值伪影、背景过于简单以及评估指标不能反映生成质量，导致模型泛化能力和评估结果不可信。

Method: 通过对象感知放置和视觉-语言模型生成文本内容，将文本渲染到复杂背景上，保证干净的真值（ground truth），并构建更具挑战性的测试样本。

Result: 生成的数据集（可在Hugging Face获得）提供了更真实复杂的文本场景，便于评估和训练能泛化到非场景文本域的文本移除模型。

Conclusion: 本文提出了一个用于文本移除的新合成数据集，解决了现有基准的标注伪影和简单背景问题，从而提升跨域泛化与评估准确性。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [22] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 论文通过模态token、MoCA和QueryREPA对DETR的object queries进行模态对齐，实现轻量且实用的多模态医学影像目标检测性能提升。


<details>
  <summary>Details</summary>
Motivation: 混合医学模态（如CXR、CT、MRI）具有统计分布和表示空间差异，单一检测器难以同时适配多种模态，故通过表示对齐使不同模态的特征进入共享空间以缓解性能下降。

Method: 提出两部分方法：1) 定义轻量的模态token（基于文本嵌入）并通过Multimodality Context Attention (MoCA) 将模态上下文注入object queries；2) 引入QueryREPA预训练阶段，使用任务相关的对比目标和模态均衡批次，使queries与模态token对齐。

Result: 在多模态联合训练场景下，MoCA+QueryREPA在不同模态上均能稳定提升AP，且不改变检测器架构、带来极小延迟，具有实用性。

Conclusion: 该论文通过对DETR风格的object queries进行模态对齐，有效提升了在混合医学影像模态下的目标检测性能。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [23] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: MaskCD通过屏蔽图像头构造对比样本并结合对比解码，有效减少LVLM的视觉-语言幻觉且保持模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM容易产生与输入视觉/文本信息相矛盾的“幻觉”，既有基于对比解码的方法难以构造合适的对比样本，基于注意力操控的方法稳定性差，故提出一种更稳健的对比样本构造策略。

Method: 在推理阶段通过屏蔽模型的图像头（image heads）生成对比样本，随后采用对比解码（contrastive decoding）选择更可信的输出；该方法避免了手工构造对比样本或对注意力权重进行敏感操作。

Result: 在多个幻觉评估基准（CHAIR、POPE、AMBER、MME）上，MaskCD显著降低了幻觉发生率，同时在保留模型原有视觉-语言理解能力方面表现良好；在LLaVA-1.5-7b与Qwen-VL-7b上均有效。

Conclusion: 该论文提出MaskCD，通过屏蔽LVLMs的“image heads”构造对比样本并结合对比解码来抑制幻觉现象。实验在LLaVA-1.5-7b与Qwen-VL-7b上并在CHAIR、POPE、AMBER、MME等基准上验证，结果表明MaskCD在降低幻觉同时保持模型一般能力。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [24] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: VERNIER是一款开源相位处理软件，利用伪周期图案和局部相位阈值算法，在显微尺度下提供对物体六自由度的快速、鲁棒、纳米级定位与微弧度转角测量，适配不同应用的图案与倍镜选择。


<details>
  <summary>Details</summary>
Motivation: 现有在微小尺度（纳米与微弧度）下的六自由度位姿测量方案稀少且难以同时兼顾量程与分辨率，作者以设计特定标记图案并通过相位处理来填补显微镜级别高精度位移/角度测量的需求。

Method: 引入相位处理流程：利用伪周期图案获取局部相位，通过相位基于的局部阈值算法定位和解码，结合周期框架的相位展开实现亚像素甚至纳米级精度；支持多种编码模式（周期与编码结合）以兼顾大量程和高分辨率。

Result: 开发并开源了VERNIER软件，实验与仿真结果表明：在噪声、失焦与部分遮挡条件下仍能保持鲁棒性；通过不同图案设计可在厘米量程到纳米分辨率间折衷；提供了针对显微镜放大倍数和图案选择的实用指南。

Conclusion: 该论文提出并开源了基于伪周期模式的相位处理软件VERNIER，实现了在噪声、失焦和遮挡情况下对物体六自由度位姿的快速可靠测量，适用于纳米级位移和微弧度角度测量，并给出不同图案和显微镜倍镜选择的指导。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [25] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 提出将多模态医学图像视为顺序帧并通过PreWeightNet、ThresholdNet、EffiWeightNet和CMIM实现质量驱动的K到N模态生成，改善融合质量与模态一致性，实验验证优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床上需要灵活从现有模态重建缺失模态，面临模态间贡献异质性、融合质量控制与多输出模态身份一致性三大挑战；借鉴SAM2的序列帧范式和临床逐步整合信息的工作流程，提出按质量选择与渐进增强的方案。

Method: 将多模态数据视为顺序帧，设计三协作模块：PreWeightNet用于全局贡献评估，ThresholdNet用于自适应过滤噪声信息，EffiWeightNet用于计算每模态-任务的自适应权重；并引入CMIM利用视-语言建模施加生成图像与目标模态描述之间的因果一致性约束。

Result: 在多项基准上，Med-K2N比现有最先进方法有显著提升（论文宣称显著边际），并公开了源码。

Conclusion: 本文提出Med-K2N，通过顺序帧与质量驱动选择机制实现K到N模态生成，显著提升多模态医学图像合成性能。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [26] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: 提出了ELMF4EggQ——一个融合图像、形状和重量的多模态集成学习框架，并公开了186枚鸡蛋数据集；方法利用预训练CNN提取图像特征、PCA、SMOTE和分类器集成；多模态方法在蛋级和新鲜度分类上分别达86.57%和70.83%的准确率。


<details>
  <summary>Details</summary>
Motivation: 动机是实现准确、无损、可自动化的鸡蛋内部质量检测，以保证食品安全和生产效率，同时减少人工破壳检测的成本与浪费，并填补利用仅外部特征预测内部质量的研究空白与数据集缺失。

Method: 方法包括：采集186枚棕色鸡蛋的外部图像、形状参数和重量；用实验室测量（蛋黄指数、Haugh单位）获得内部质量标签；使用预训练CNN（ResNet152、DenseNet169、ResNet152V2）提取图像深度特征；PCA降维；使用SMOTE进行数据增强；对图像特征与结构特征融合后用多种机器学习分类器训练，并通过投票集成最优分类器预测。

Result: 结果表明：多模态融合显著优于仅图像或仅表格特征的基线方法；集成投票机制提升了准确性；具体性能为蛋级别分类准确率86.57%，新鲜度预测准确率70.83%。数据与代码已开源。

Conclusion: 该论文提出了一个利用外部非侵入性特征（图像、形状、重量）进行鸡蛋内部质量评估的集成学习框架ELMF4EggQ，并公开了一个带标注的棕色蛋数据集。实验显示，多模态融合方法优于单一模态，分类准确率在蛋级别分类上达86.57%，在新鲜度预测上达70.83%。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [27] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: Patch-ioner把零样本图像描述从全图转为patch级，利用密集语义特征（如DINO）聚合patch生成任意区域描述，在多项无监督区域描述任务上取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本图像描述器仅使用全图全局表征，无法进行区域级或细粒度描述；通过转向patch级表征可实现更灵活的区域描述且无需区域级配对数据。

Method: 将patch作为原子描述单元，利用密集视觉表征（如DINO）得到语义丰富的patch特征，按需聚合patch特征并在文本对齐空间进行解码以生成区域描述；分析了使潜在（latent）零样本描述器在该框架下可行的关键要素。

Result: 在零样本密集描述、区域集合描述以及新引入的trace描述任务上均优于其他基线和现有最先进方法，表明基于patch的语义表示在可扩展描述生成中的有效性，并确认采用DINO等能生成有意义密集特征的骨干网络是成功的关键。

Conclusion: 该论文提出了Patch-ioner框架，将零样本图像描述从全图级别转为基于patch（图像块）的细粒度描述，实现无区域级监督下的任意区域描述。

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [28] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 无监督、训练免费地利用InternImage特征+K-Means聚类和logits置信度阈值，即可在语义分割中有效检测未知/异常对象，成绩优于多种基线。


<details>
  <summary>Details</summary>
Motivation: 目标是解决自动驾驶等安全关键场景中语义分割对未知对象检测的需求，探究在没有外部异常样本或额外监督情况下，是否能利用基础模型的泛化特性来区分ID与OoD区域。

Method: 方法为训练免费：使用InternImage主干提取特征，结合K-Means聚类对特征进行分组，并对原始解码器logits施加置信度阈值以过滤高置信的ID像素，最终将低置信且属于特定聚类的像素判定为OoD。

Result: 在RoadAnomaly基准上实现50.02 AP，在ADE-OoD基准上实现48.77 AP（使用InternImage-L），超过了若干有监督和无监督基线，表明方法在不同数据集上都有竞争力。

Conclusion: 该论文表明，大型视觉基础模型（如InternImage）在微调用于语义分割后，能够在无需外部异常监督的情况下，凭借其特征表示和简单的聚类策略识别出分布外（OoD）区域，从而提升未知对象检测性能。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [29] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: HoloV通过对不同空间裁剪自适应分配剪枝预算，避免语义重复保留，显著提升MLLM在高剪枝率下的推理效率与性能保持。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的token剪枝倾向于保留语义相似的token，导致在高剪枝比下表示崩塌和性能显著下降，需要一种能保证全局语境代表性的剪枝策略。

Method: HoloV在视觉token剪枝时对不同空间裁剪区域自适应分配剪枝预算，优先保留能覆盖全局语境的token，避免只保留局部显著特征；该方法为即插即用框架，可与现有MLLM集成用于推理加速。

Result: 在多个任务和模型上，HoloV在激进剪枝下优于SOTA方法。例如：LLaVA1.5剪去88.9%视觉token后仍保留95.8%的原始性能，表现出更优的效率-精度权衡。

Conclusion: HoloV通过整体视角和自适应预算分配，显著缓解了注意力优先剪枝在高剪枝比下的语义重复保留问题，从而在多任务、多模型和高剪枝率条件下保持更好的性能。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [30] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 提出CAW：通过置信感知加权的KL损失与对抗输入上的特征对齐正则化，提升CLIP类零样本模型的对抗鲁棒性与泛化，同时减小内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型如CLIP虽具备强零样本能力，但对对抗攻击高度脆弱；现有对抗微调方法在提高鲁棒性的同时常牺牲泛化或资源消耗高，故需一种在不损失泛化能力并节省内存的情况下提升零样本鲁棒性的策略。

Method: 提出了Confidence-Aware Weighting (CAW)，包括(1) Confidence-Aware loss：用基于置信度的缩放因子加权干净与对抗预测之间的KL散度，以优先处理不确定的对抗样本；(2) 特征对齐正则化：在对抗输入上最小化冻结的预训练图像编码器与微调后编码器特征之间的距离，保持语义一致性。

Result: 在TinyImageNet和另外14个数据集上的大量实验表明，CAW在强攻击（如AutoAttack）下，相比PMG-AFT和TGA-ZSR等近期方法表现更好，同时内存占用更少，能同时提升干净与鲁棒精度。

Conclusion: CAW通过置信感知损失和特征对齐正则化双管齐下，有效提升了CLIP类视觉-语言模型的零样本鲁棒性，同时保持或提升了干净样本精度和泛化能力。

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [31] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: LVLM在超声多模态心血管风险预测上有前景，但需域适应、校准与多源数据融合方能达到临床可用性。


<details>
  <summary>Details</summary>
Motivation: 当前颈动脉斑块风险评估需整合影像与多源临床信息，传统模型缺乏透明可解释性；探索LVLMs能否以更直观可解释方式完成临床级风险分层。

Method: 构建模拟临床诊断的访谈式多轮问题序列，比较多种开源LVLM（通用与医学微调型），并通过LoRA对LLaVa-NeXT-Vicuna进行超声域微调，结合文本化的结构化表格数据进行多模态融合，评估分类与可解释性指标。

Result: 零样本下多数LVLM难以准确识别影像模态与解剖结构，风险分类性能普遍较差。经LoRA微调后，LLaVa-NeXT-Vicuna在卒中风险分层上显著提升；将结构化表格转为文本并联合超声影像进一步提升特异性与平衡准确率，达到与同数据集上的CNN基线相当的性能。

Conclusion: LVLMs在超声影像与临床/生物标志物融合用于颈动脉斑块风险评估上显示出潜力，但在零样本情况下存在显著局限性，需域自适应与多模态数据整合以提高性能与可解释性。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [32] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出FDA-VAE，通过对称潜在分布与Y形双向训练实现可解释的共享/独立特征分离，在更少参数下提升多相CE-MRI合成性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度自编码器的多相CE-MRI合成方法参数效率低且训练策略可解释性不足，亟需一种轻量且具有明确特征分离机制的方法。

Method: 提出了一种轻量级的特征解耦VAE（FDA-VAE），将输入与目标图像编码为关于标准正态分布对称的两个潜在分布，以分离共享与独立特征；引入Y形双向训练策略增强特征分离的可解释性。

Result: 实验表明，与现有端到端自编码器合成方法相比，FDA-VAE显著减少模型参数与推理时间，同时有效提升合成质量。

Conclusion: FDA-VAE通过对称潜在分布与Y形双向训练策略，有效分离多相CE-MRI中的共享与独立特征，从而在减少参数与推理时间的同时提高合成质量，验证了其轻量高效且具有可解释性的优势。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [33] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: LPG-Bench提供了针对长提示的T2I基准数据与人工标注；提出TIT零样本评估框架（TIT-Score、TIT-Score-LLM），在与人工偏好对齐上优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在短提示下表现良好但在长而详细提示上理解与保持一致性较差，且现有自动评估指标与人工偏好一致性低，需要针对长提示场景的基准与评估方法。

Method: 构建200条平均超过250字的长提示，使用13个主流T2I模型生成2600张图像并进行人工排序标注；提出TIT框架，通过将生成图像由LMM生成描述并与原提示比较，实现零样本的T2I一致性评估；给出两种实现：基于得分的TIT-Score与基于大模型的TIT-Score-LLM。

Result: 通过LPG-Bench的评测发现现有指标（如CLIP-score、LMM-score）在长提示任务上与人工判断一致性差；TIT方法显著提升与人工判断的一致性，TIT-Score-LLM在配对准确率上比最强基线高出7.31%（绝对值）。

Conclusion: 本文提出了LPG-Bench基准与TIT评估方法，针对长提示文本的图像生成对齐问题提供了系统性评测与更符合人工判断的自动指标。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [34] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 通过3DEditVerse数据集和3DEditFormer模型，实现在无需精确3D掩码的情况下对3D资产进行局部、结构保留且跨视图一致的高质量编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法存在速度慢、几何畸变、依赖人工精确3D掩码等问题，需要同时满足跨视图一致性、结构保真和细粒度可控性，因此需要新的大规模数据和结构保留的模型设计。

Method: 在数据方面，构建了3DEditVerse大规模配对编辑数据集（116,309训练对，1,500测试对），结合姿态驱动的几何编辑与基础模型引导的外观编辑以保证编辑局部性和多视角一致性；在模型方面，提出3DEditFormer，一种引入双重引导注意力和时序自适应门控的条件Transformer，用于将图像到3D生成同步解耦可编辑区域与保留结构，避免依赖精确3D掩码。

Result: 在大规模实验中，相较于最先进基线方法，本框架在定量指标和定性视觉效果上均有显著提升，展示了更精确、一致且无需辅助3D掩码的编辑能力。数据集与代码将公开。

Conclusion: 本文提出了一个实用且可扩展的3D编辑框架，通过大规模数据集和结构保留的条件Transformer，有效提升了局部3D几何与外观编辑的多视角一致性与精细可控性。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [35] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 通过模拟云覆盖并将雷达数据与光学影像融合，同时在解码器末端注入NDI，论文在有无云两种条件下均提升了地物语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 多数Sentinel-2数据集为无云影像，难以反映热带多云地区的真实情况；同时，深度网络下采样导致空间/光谱细节丢失，影响分割精度。

Method: 开发云注入算法以生成带云的Sentinel-2图像，使用Sentinel-1雷达数据与光学数据进行融合；在现有深度分割网络（如U-Net和DeepLabV3）中，在解码器的最后层轻量注入NDI特征。

Result: 在DFC2020数据集上，NDI注入在无云条件下提升了U-Net 1.99%、DeepLabV3 2.78%；在云覆盖条件下，加入Sentinel-1雷达数据相比仅用光学数据显著提高了各模型的分割性能。

Conclusion: 本文提出了两项针对卫星影像语义分割的改进：使用云注入算法模拟真实云覆盖以评估云对模型的影响；以及在解码器末端注入归一化差值指数（NDI）以减轻编码器下采样导致的细节丢失。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [36] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: 提出PocketSR：通过LiteED、在线退火剪枝与多层蒸馏，将生成模型的高质量RealSR带到边缘设备，146M参数、4K@0.8s、性能接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型生成模型的RealSR效果好但计算量大、延迟高，不适合边缘设备部署，因而需要一种既保持生成能力又极其轻量化的单步模型。

Method: 提出LiteED以替代原始SD中的VAE（参数减少97.5%），使用在线退火剪枝将生成先验从重模块迁移到轻模块，并结合多层特征蒸馏来补偿剪枝带来的知识损失。

Result: PocketSR模型（146M参数）处理4K图像仅需0.8s，相比先前方法显著加速，同时在单步与多步RealSR基线上达到或接近最先进性能。

Conclusion: PocketSR通过设计轻量化的VAE替代结构（LiteED）、U-Net的在线退火剪枝及多层特征蒸馏，在保持生成质量的同时显著降低计算量，实现了边缘设备可用的单步RealSR解决方案。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [37] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 提出MEve提示集并系统研究何时何处插入多事件条件，发现早期去噪步骤与块级层是控制事件切换的关键。


<details>
  <summary>Details</summary>
Motivation: 当前多事件T2V生成方法缺乏对事件迁移内在因素的检视，作者旨在找出何时（时间/去噪步）和何处（模型层）多事件提示能有效控制事件过渡。

Method: 作者构建了MEve自我策划提示集用于多事件评估，对两类代表性模型（OpenSora与CogVideoX）进行系统实验，分析在不同去噪步骤与不同层级插入多事件条件的效果差异。

Result: 实验表明：在去噪早期介入多事件条件显著有利于正确呈现事件序列；在模型的特定块级层插入条件也能影响事件边界和连贯性；两种模型家族在响应多事件提示的位置与敏感度存在差异。

Conclusion: 该论文表明在多事件文本到视频生成中，事件切换的可控性关键取决于在扩散去噪过程的早期步骤和模型的特定层（block-wise layers）进行干预。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [38] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut 用 EfficientNetV2-S + 迁移学习 + 强增强 + 类别加权损失，针对 FER2013 实现 62.8% acc / 0.590 macro-F1，强调可复现性与不平衡意识。


<details>
  <summary>Details</summary>
Motivation: 解决 FER 中由于遮挡、光照、姿态变化、细微类内差异与类别不平衡导致的识别困难，提供一种轻量、高效且可复现的方案。

Method: 对 FER2013 图像进行标准化、分层划分与增强，使用 EfficientNetV2-S 作为骨干、轻量化分类头微调，并采用类别加权损失处理数据不平衡。

Result: 在 FER2013 上取得 62.8% 准确率和 macro F1=0.590，表现与常规 CNN 基线相当，展示了有效架构与不平衡处理的实际价值。

Conclusion: InsideOut 提出了一套基于 EfficientNetV2-S 的可复现 FER 框架，通过迁移学习、强数据增强和面向不平衡的优化在 FER2013 数据集上达到有竞争力的性能。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [39] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 研究表明训练目标的离散/连续性质和条件信息丰富度显著影响视觉生成模型的组合泛化，在MaskGIT中用连续JEPA辅助松弛离散损失可提升性能。


<details>
  <summary>Details</summary>
Motivation: 理解哪些机制促进或抑制视觉生成模型的组合泛化，以便设计更能生成已知概念新组合的模型。

Method: 通过一系列受控实验比较不同设计选择对图像和视频生成中组合泛化的影响，重点考察训练目标的离散/连续性质及条件信息程度，并在MaskGIT上加入辅助连续JEPA损失进行验证。

Result: 发现两个关键影响因素（分布离散/连续、训练时条件信息程度），并实验证明在MaskGIT中加入连续JEPA辅助目标能够改善组合泛化。

Conclusion: 本文结论是：训练目标的分布形式（离散或连续）和训练期间条件信息对构成概念的可获得程度是影响视觉生成模型组合泛化能力的两个关键因素。将MaskGIT的离散损失用辅助的连续JEPA目标放松，可以在离散模型中提升组合泛化性能。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [40] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出一种在LDM潜在空间中通过轨迹位移采样实现的不可学习样本生成方法，兼顾高保真与对抗性，在多个数据集与攻击场景下显著提升不可学习性与视觉不可察觉性。


<details>
  <summary>Details</summary>
Motivation: 现有基于像素空间的图像中毒方法可感知性强，容易产生噪声和伪影，难以在保护数据隐私与知识产权时兼顾可视不可感知性与对抗性。因此提出在扩散模型的潜在空间中设计不可学习样本以提高隐蔽性与防护效果。

Method: 在潜在扩散模型框架下，通过在反演与去噪过程中修改去噪轨迹的起始潜变量进行轨迹位移采样（trajectory-shifted sampling）。该方法不直接在像素空间添加噪声，而是在Latent Diffusion Models(LDMs)的潜在空间操作，交替执行反演与去噪步骤以生成对抗性但视觉不可察觉的样本。

Result: 在四个基准数据集上验证，对抗当前最先进的反演攻击，实验显示在感知指标（PSNR、SSIM、FID）上可实现约8%–10%的改进，在五种对抗设置上的平均鲁棒性提升约10%，表明方法在保持视觉质量的同时显著抵抗模型反演与个性化训练。

Conclusion: 本文提出了一种在扩散模型潜在空间中进行图像扰动的模型驱动策略，通过交替去噪与反演并调整去噪轨迹起点，使生成的图像在视觉上接近原图但对下游生成模型的反演与个性化训练保持不可学习性，从而为未经授权的模型适配提供不可察觉的防护。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [41] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 几何约束使特征更有几何细节但不利于位姿反演，视觉-only 特征更通用，需进一步研究如何有效结合几何信息。


<details>
  <summary>Details</summary>
Motivation: 探讨将几何信息融入预训练视觉语义特征（几何约束）是否能提升在蒸射场（radiance fields）蒸馏后的语义表示和下游空间任务表现，特别是位姿估计和语义定位。

Method: 提出了名为 SPINE 的框架：先用蒸馏语义进行粗反演以获取初始位姿，再用基于光度的优化进行精细反演；比较了几何约束与视觉-only 特征在几项任务上的表现，包括特征细节、语义定位和位姿反演准确性。

Result: 实验表明：几何约束特征在结构细节上更丰富，但在语义物体定位上无显著提升；在位姿反演任务中反而导致精度下降。提出的 SPINE 框架能在无初值情况下进行辐射场反演，但仍未受益于几何约束特征。

Conclusion: 几何约束的语义特征在细节上更丰富，但在语义定位和辐射场位姿反演任务上并不优于视觉-only 特征；因此视觉-only 特征在更多下游任务上更通用。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [42] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete通过投影点云条件化和目标感知掩码在双分支扩散网络中融合几何信息与图像线索，实现更几何一致、视觉质量高的参考驱动图像补全。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型的方法仅依赖扩散先验、缺乏显式几何线索（如相机位姿或深度），导致当目标与参考视角差异大时生成内容常常错位或不合理，因此需要将几何信息整合到补全过程以提升几何一致性。

Method: 构建了双分支扩散模型：一支从被遮挡的目标图像合成缺失区域，另一支从投影点云提取几何特征；两分支通过跨分支自注意力联合建模。并在扩散过程中以投影点云作为条件输入，同时引入目标感知掩码—将目标视角投影到参考以检测并屏蔽参考中目标不可见但参考可见的区域，训练时引导模型聚焦有用线索。

Result: 在多个基准上，GeoComplete在PSNR上比最先进方法提升约17.1，显著提高了几何准确性，同时保持高视觉质量。

Conclusion: 本文提出了GeoComplete，通过显式3D结构引导增强参考驱动图像补全的几何一致性，在目标视角与参考视角差异较大时尤为有效。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [43] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 提出通过分离的视觉/音频描述（HVGC）和双塔+双重交叉注意力（BridgeDiT+DCA）来解决T2SV中的模态干扰与交互问题，实验表明方法有效并达SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前T2SV任务面临文本作为单一共享条件导致的模态干扰和缺乏有效跨模态交互机制的问题，作者希望通过解耦文本条件与改进交互模块来提升生成质量与同步性。

Method: 首先通过HVGC生成分离的视频与音频描述以避免条件干扰；在此基础上提出BridgeDiT双塔扩散Transformer，利用双重交叉注意力（DCA）在视觉与听觉特征间建立对称、双向的信息桥梁以实现语义与时序同步。

Result: 在三个基准数据集上进行广泛实验并辅以人类评估，结果在大多数指标上超过现有方法；消融研究证明了HVGC与DCA设计的有效性。

Conclusion: 该论文提出了HVGC框架和BridgeDiT模型，有效解决了文本条件下视频与音频之间的模态干扰与跨模态交互问题，实验与人类评估显示在多数指标上达到了SOTA水平。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [44] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR将视觉皮层分为结构与语义两级分支，分别提取对应特征并通过扩散模型融合，显著提升了复杂场景下的脑影像图像重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确重建复杂视觉刺激，原因在于自然场景低级特征异质性与高级特征语义纠缠。受视觉皮层分层表示理论启发，提出将不同处理路径分离来改善重建。

Method: 提出HAVIR模型：1) Structural Generator从空间处理体素提取结构信息并将其转换为潜在扩散先验；2) Semantic Extractor从语义处理体素提取并转换为CLIP嵌入；3) 利用Versatile Diffusion融合两个分支以合成最终图像。

Result: 实验表明HAVIR在结构性与语义性重建上均有提升，能够在复杂场景中恢复更准确的细节与语义信息，量化指标和主观评估均优于现有方法。

Conclusion: HAVIR通过将视觉皮层划分为两级区域并分别提取结构与语义特征，实现了更精细的脑影像到图像重建，从而提升了复杂场景下的重建质量并优于现有模型。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [45] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV通过先预测actor与object轨迹再条件化生成视频，去除了对密集mask标注的依赖，支持目标对象与轨迹的直观控制，并在新基准上表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂动态交互，且对密集mask标注的需求限制了实际应用。利用掩码作为控制信号有助于提升生成质量，但手工标注成本高，故提出无需稠密mask的可控方法。

Method: 采用解耦的两阶段流水线：第一阶段预测.actor和object的运动轨迹（可由动作描述或空间位置信息引导）；第二阶段以预测的轨迹为条件生成视频。

Result: 在新构建的两套基准（涵盖人-物交互和机器人操作）上进行大量实验，Mask2IV在视觉真实感和可控性上均优于现有基线方法。

Conclusion: Mask2IV提出了一个针对交互场景视频生成的两阶段框架，通过先预测行为者与物体的轨迹再生成视频，避免了对密集mask标注的依赖，并提供目标对象与轨迹的可控设置。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [46] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出结合拓扑（Reeb图）与马尔可夫模型的MRG以高效生成保留PoLs的时空轨迹，实验证明在多城市数据集上能在保真度和效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界中需要高保真的轨迹模拟以支持城市规划、流行病学与交通管理，但要在保真度、数据效率与计算效率之间取得平衡。作者提出MRG以利用拓扑结构捕捉PoLs并以概率化方式生成可扩展的模拟。

Method: 构建概率拓扑模型，将Reeb图（或类似拓扑结构）与马尔可夫过程结合，在节点/段上建模个体与群体的迁移/停留分布，从而在保有日常一致性的同时引入变异性，以生成未来轨迹。

Result: 在Urban Anomalies数据集（亚特兰大与柏林子集）上，使用Jensen-Shannon Divergence在群体与个体指标上评估，MRG在保持高保真度的同时在数据与计算效率上表现优异，显示出良好的可扩展性与跨城市泛化能力。

Conclusion: 该文提出了Markovian Reeb Graphs（MRG），用于从基线数据中学习并生成保留Patterns of Life（PoLs）的时空轨迹，兼顾个体与群体层面的移动结构。

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [47] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: SpineMed（含 SpineMed-450k 与 SpineBench）弥补了脊柱多模态椎体级别数据与评测空白，能提升模型在细粒度临床推理与决策支持上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前脊柱疾病诊断需跨 X 光、CT、MRI 在椎体级别进行复杂推理，但缺乏椎体级、可追溯的多模态指令数据集与标准化基准，限制了 AI 在临床决策中的实用性。

Method: 以临床医生参与的流水线构建 SpineMed-450k，来源包括教科书、指南、公开数据集和约1000例匿名医院病例，采用两阶段大模型生成（草稿与修订）保证指令实例质量与可追溯性。

Result: 在 SpineBench 基准上，多数先进 LVLM 在细粒度椎体级推理上存在系统性弱点；在 SpineMed-450k 上微调的模型在定位、病变评估与手术规划等任务上均显著提升，且临床医生评价其输出具有诊断清晰性与实用性。

Conclusion: SpineMed 提供了面向椎体级别推理的首个大规模多模态数据集和评估框架，能显著改善模型在细粒度脊柱诊断与规划任务上的表现。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [48] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出UniShield：一个感知-检测多智能体系统，动态选择并融合专家检测器，实现跨域伪造图像检测与定位，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为领域专用，泛化能力差，难以应对多样化的伪造图像（如DeepFake、AI生成、文档篡改等），且缺乏统一自适应框架。

Method: 引入感知智能体负责图像特征分析与检测模型选择；检测智能体整合多领域专家检测器，输出可解释的检测与定位报告。系统通过动态模型路由和专家融合来提高泛化能力。

Result: 在广泛实验中，UniShield在多个数据集和任务上优于现有统一方法和领域专用检测器，展示了更好的跨域性能和实用性。

Conclusion: UniShield通过多智能体架构实现了跨领域伪造图像检测与定位，兼具适应性和可扩展性，为实际应用提供了更通用的解决方案。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [49] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR通过生成式重光照采样训练光照条件NeRF并用双分支编码漫反射与镜面成分，实现了无需每次优化的高效任意环境重光照，且在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在可重光照三维重建中对每种光照需要反复优化或进行昂贵光传输模拟的问题，提供高效、可泛化的前向重光照能力。

Method: 先用生成式重光照模型在多种环境光下采样物体外观，构建训练集；再训练一个光照条件NeRF，并采用双分支结构分别编码全局漫反射效果和高频镜面高光；训练完成后可前向推理生成任意环境光照下的外观。

Result: 在TensoIR和Stanford-ORB数据集上，ROGR在大多数指标上超越现有最先进方法，并在真实物体采集中表现出色。

Conclusion: ROGR通过生成式重光照模型与光照条件NeRF相结合，实现了在任意环境光照下对目标物体的快速可重光照3D外观重建。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [50] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出将动态体积提示与内容自适应裁剪结合的训练策略，结合nnInteractive权重初始化，在单GPU上高效训练交互式3D分割模型，并在竞赛中取得有竞争力的指标。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型要么缺乏体积感知，要么交互能力有限，且在单GPU上对顺序细化反馈学习的计算成本高，需新的训练策略来兼顾交互性与体积信息并提高训练效率。

Method: 通过在训练阶段模拟真实用户交互（动态提示生成）并对编码器输入进行基于内容的自适应裁剪，优化图像编码器的计算利用；并基于公开的nnInteractive权重进行网络初始化以加速训练。

Result: 在Foundation Models for Interactive 3D Biomedical Image Segmentation竞赛中表现良好：平均最终Dice 0.6385，标准化表面距离（NSD）0.6614，AUC指标Dice 2.4799，NSD 2.5671。

Conclusion: 本文提出了一种结合动态体积提示生成与内容自适应裁剪的训练策略，用于提高交互式3D生物医学图像分割模型在有限GPU资源下的训练效率与体积感知能力。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [51] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: 提出PQGAN：将产品量化融入VQGAN，系统分析后显著提升重建与生成质量，并能无缝用于扩散模型以加速或提高分辨率。


<details>
  <summary>Details</summary>
Motivation: 经典的产品量化在可扩展向量编码方面有优势，但在高保真图像生成中的潜力尚未被充分挖掘。作者希望通过PQ获得更好的重建质量与更高效的离散潜变量表示，从而提升生成性能或生成效率。

Method: 在VQGAN结构中用PQ替换或扩展向量量化，系统分析编码维度、码本大小与子空间因子化（subspace factorisation）之间的相互作用，并在此基础上选取最优超参数；将训练好的PQ编码器无缝集成到预训练扩散模型以加速或提高分辨率。

Result: 在若干指标上显著优于现有基线：PSNR达到37dB（先前约为27dB），FID/LPIPS/CMMD可最多降低96%。此外发现VQ与PQ在增加嵌入维度时呈相反性能趋势，并提供了指导PQ超参数选择的经验规律。

Conclusion: PQGAN通过将产品量化（PQ）引入VQ框架，显著提升了自编码器在高保真图像重建与生成任务中的量化性能。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [52] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: Memory Forcing通过混合训练、链式前向训练、点到帧检索与增量3D重建，结合几何索引空间记忆，引导模型在探索与重访间切换记忆策略，显著提升自回归视频扩散模型的长期空间一致性与生成质量。


<details>
  <summary>Details</summary>
Motivation: 在受限计算预算和有限上下文窗口下，模型需要在生成新场景（需要时间记忆）和保证重访时的空间一致性（需要空间记忆）之间取得平衡，避免仅时间记忆导致长程空间不一致或过度依赖弱空间上下文而损害新场景生成质量。

Method: 提出了一套训练框架和几种技术组合：Hybrid Training划分不同的玩法场景以引导记忆使用；Chained Forward Training进行链式模型滚动预测以增强姿态多样性并促使模型依赖空间记忆；Point-to-Frame Retrieval通过将当前可见点映射回源帧高效检索历史信息；Incremental 3D Reconstruction维护和更新显式3D缓存以支持空间一致性。整体架构基于几何索引的空间记忆并与自回归视频扩散模型配合。

Result: 在多样环境中广泛实验表明，Memory Forcing在长时程空间一致性和生成质量上均优于对比方法，同时保持对长序列的计算效率。

Conclusion: Memory Forcing有效解决了自回归视频扩散模型在有限上下文下的时空记忆权衡问题，通过混合训练、链式前向训练、点到帧检索和增量三维重建四项关键技术，引导模型在探索阶段依赖时间记忆、在重访时利用空间记忆，从而在保持生成质量的同时显著提高长程空间一致性。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [53] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: MonSTeR提出了首个运动-场景-文本三模态检索模型，通过统一潜在空间融合单模态与跨模态表征，有效捕捉高阶模态依赖，在检索性能和人类一致性上均优于基线，并具备有意义的零样本下游能力。


<details>
  <summary>Details</summary>
Motivation: 人类动作受意图驱动，且动作能否发生取决于周围场景支持性。现有工作未提供用于评估骨骼运动、意图文本与场景语义三者对齐的检索工具，因此需要一个能够同时建模三模态关系的检索框架来评估与利用这种对齐。

Method: 模型设计上，MonSTeR借鉴高阶关系建模思想，融合单模态与跨模态表示以构建统一潜在空间；训练上可能使用对比学习或多任务损失来对齐运动、场景与文本；检索时在该潜在空间中计算相似度以支持灵活检索，并通过消融或对比实验验证融合策略优于仅用单模态表示的基线。

Result: 在多项检索任务中，MonSTeR在指标上优于仅使用单模态表征的三模态模型；用户研究表明检索得分与人类偏好一致；模型的潜在空间在零样本场景对象放置与运动描述生成任务上表现出可迁移性；并公开了代码与预训练模型。

Conclusion: MonSTeR成功提出首个面向运动-场景-文本三模态检索的统一模型，通过构建统一潜在空间并结合单模态与跨模态表征，有效捕捉模态间高阶依赖，从而在检索任务上优于仅依赖单模态表征的三模态基线，并在用户研究中展示其检索分数与人类偏好一致。模型在零样本场景对象放置与运动生成（Motion Captioning）等下游任务上展现了可迁移性。

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [54] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 一种无需训练、能插入任意网络的测试时防御：对输入做小平移扰动、对齐聚合特征并还原，利用随机共振“以噪制噪”在不丢信息的情况下显著恢复被对抗攻击破坏的性能。


<details>
  <summary>Details</summary>
Motivation: 现有防御通常通过滤波或平滑来减弱对抗噪声，但会造成信息损失；因此作者希望以最小的信息破坏实现鲁棒性，采用随机共振的思想用受控的随机变换提高信号可辨识性，从而抵抗对抗扰动。

Method: 对输入图像施加微小的随机平移变换，提取每个变换后的特征嵌入并进行对齐（映射回参考坐标系），对齐后聚合这些特征，然后根据封闭形式的公式将聚合结果映射回原图像输出；该流程无需修改网络、无需训练或微调，并可插入各种已有网络作为测试时处理模块。

Result: 在图像分类上相较于干净数据的性能损失最多恢复到68.1%；在立体匹配恢复71.9%；在光流任务恢复29.2%，并在多种对抗攻击下达到或接近最先进的防御效果，同时首次将通用测试时防御扩展到密集预测任务。

Conclusion: 提出了一种训练无关、架构无关、攻击无关的测试时防御方法，通过对输入图像施加小的平移扰动、对齐变换后的特征嵌入并聚合再映射回参考图像，从而在不丢失信息的前提下利用随机共振“以噪制噪”提高模型对对抗攻击的鲁棒性。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [55] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 提出Mixer：基于超球面随机嵌入和双分支模块的随机化神经网络，用于同时捕捉通道内与通道间纹理关系，通过新的优化目标增强纹理表示，在若干纹理基准上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 动机在于现有随机化神经网络在纹理识别中虽表现良好，但主要聚焦于改进跨信息预测（cross-information prediction），而未在随机化网络整体架构上做出显著创新。作者旨在通过改进嵌入与结构，增强对纹理内部及通道间关系的建模能力，从而提升表现。

Method: 方法核心包括：1) 使用高维超球面随机嵌入对输入进行投影，保证表示的角度不变性与能量归一；2) 设计双分支学习模块，一支侧重捕捉通道内（intra-channel）特征，另一支侧重通道间（inter-channel）交互；3) 提出新的优化目标（未在摘要中具体给出），用于融合两支路的表示以获得更丰富的纹理描述；整体为随机化网络架构，不强调端到端训练而利用随机投影与浅层可训练模块。

Result: 论文宣称在多个具有不同特性的纯纹理基准上取得有趣（即显著或稳健）结果，但摘要未给出具体指标、对比方法或提升幅度。代码将在发表后开源。

Conclusion: 该论文提出了一种名为Mixer的随机化神经网络，用于纹理表示学习，通过球面随机嵌入和双分支学习模块捕捉通道内外关系，并通过新的优化问题构建丰富的纹理表示。实验在若干纯纹理基准上展示了性能改进。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [56] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 通过显式的RULER坐标标记和均衡的I-MRoPE空间编码，本文有效解决了VLM在不同分辨率下的patch-to-pixel映射问题，显著提升GUI grounding的泛化与准确性。


<details>
  <summary>Details</summary>
Motivation: 动机是当前VLMs在高分辨率或未见过的显示器上，因隐式patch-to-pixel映射失败导致GUI grounding性能急剧下降，需要更稳健的坐标表示与位置编码。

Method: 方法包括引入RULER tokens作为显式坐标参考，以及提出Interleaved MRoPE对宽高维度进行均衡编码，从而减少基于视觉特征直接生成坐标的隐式映射负担。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上，所提方法稳定提升了grounding准确率，尤其是在高分辨率界面上提升显著，展示了更可靠的跨分辨率GUI自动化能力。

Conclusion: 本文提出的RULER和I-MRoPE针对GUI grounding中的坐标映射与位置编码问题，通过显式坐标标记和改进的空间编码，提高了高分辨率界面上的精度和泛化能力。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [57] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出LEAML：通过caption蒸馏正则化的伪QA生成与选择性神经元更新，实现对无标签图像的标签高效适配，显著优于常规微调。


<details>
  <summary>Details</summary>
Motivation: MLLM在专业领域（如医学影像）受限于标注稀缺导致OOD任务表现差，需一种标签高效的适配方法。

Method: 基于一个用caption蒸馏正则化的QA生成器，为无标签图像生成领域相关的伪问答对；在蒸馏过程中仅选择与问答最相关的神经元进行更新。

Result: 在胃肠镜和体育VQA数据集上，LEAML在极少监督下均优于标准微调，证明了伪QA+选择性参数更新的有效性。

Conclusion: LEAML在标注稀缺的专业领域通过伪QA生成与选择性神经元更新，有效提升了模型的域适应能力，优于常规微调。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [58] [A New Normalization Form for Limited Distinct Attributes](https://arxiv.org/abs/2510.02865)
*Niko S. Snell,Rayen C. Lee*

Main category: cs.DB

TL;DR: LDNF通过强制将非受限离散属性约束为有限取值来补充传统范式，旨在减少异常与提高查询准确性。


<details>
  <summary>Details</summary>
Motivation: 许多关系数据库中存在属性取值集合本应有限但没有明确约束的情况，导致数据异常和查询不准确，目前的范式未覆盖这一类问题。

Method: 通过定义非受限离散属性的识别准则，并引入约束和映射机制将其映射为受限离散属性，从而在关系模式上施加取值限制。

Result: 提出了LDNF的形式化定义与方法框架，说明其可与现有范式并用以提升数据完整性，但未给出具体实证或性能评估。

Conclusion: LDNF提出了一种通过限制属性取值集合来减少数据异常和冗余的规范化方法，作为现有范式的补充。

Abstract: In modern databases, the practice of data normalization continues to be
important in improving data integrity, minimizing redundancies, and eliminating
anomalies. However, since its inception and consequent improvements, there have
been no attempts to document a method which constrains the values of attributes
capable of only possessing a limited quantity of values. These non-limited
distinct attributes pose a problem throughout many relational databases as they
have the potential to cause data anomalies and query inaccuracies. Thus, a new
database normalization method, Limited Distinct Normal Form (LDNF), is
necessary in order to improve upon the currently established data normalization
process. In brief, LDNF is a method which turns non-limited distinct attributes
into limited distinct attributes by forcing the attributes to conform to a
limited quantity of values. Utilizing LDNF in tandem with existing normal forms
fulfills a need in normalization that is otherwise not present when only using
current methods. A formal approach to LDNF is therefore proposed.

</details>
