<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Milestone Determination for Autonomous Railway Operation](https://arxiv.org/abs/2510.06229)
*Josh Hunter,John McDermid,Simon Burton,Poppy Fynes,Mia Dempster*

Main category: cs.CV

TL;DR: 通过里程碑驱动、路段特定的序列数据构建和规则化模型训练，简化轨道视觉系统的学习目标，提高安全性和可解释性，适用于受控铁路自动化场景。


<details>
  <summary>Details</summary>
Motivation: 传统轨道视觉数据稀缺且缺乏时空连贯性，现有替代数据缺乏现实性或泛化性。通过关注路段相关的关键提示（里程碑），可以生成更贴近实际运行逻辑的序列数据，简化模型任务并提高可靠性。

Method: 构建路由驱动的序列化数据集，利用规则化里程碑标注来划分关键决策点；采用基于规则的简单模型（或将规则与学习模块混合）在受控场景下训练视觉代理，强调时序上下文与里程碑触发机制，而非通用物体检测。

Result: 方法在减少对大量标注和复杂动态对象识别的依赖同时，提升了在受控路线下的决策准确性与可解释性；在合成或有限真实数据上验证显示里程碑驱动模型在关键决策点的可靠性优于通用识别基线。

Conclusion: 本论文提出了基于里程碑驱动的轨道视觉数据生成与学习框架，主张通过路段特定、语境相关的线性里程碑提示替代通用目标识别，从而降低数据需求并提升决策可解释性与安全性。

Abstract: In the field of railway automation, one of the key challenges has been the
development of effective computer vision systems due to the limited
availability of high-quality, sequential data. Traditional datasets are
restricted in scope, lacking the spatio temporal context necessary for
real-time decision-making, while alternative solutions introduce issues related
to realism and applicability. By focusing on route-specific, contextually
relevant cues, we can generate rich, sequential datasets that align more
closely with real-world operational logic. The concept of milestone
determination allows for the development of targeted, rule-based models that
simplify the learning process by eliminating the need for generalized
recognition of dynamic components, focusing instead on the critical decision
points along a route. We argue that this approach provides a practical
framework for training vision agents in controlled, predictable environments,
facilitating safer and more efficient machine learning systems for railway
automation.

</details>


### [2] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: 作者构建了电影剧本数据集CML-Dataset，提出CML-Bench评估剧本的对话连贯性、角色一致性与情节合理性，并通过CML-Instruction提升LLM生成剧本质量，结果与人类评价一致。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在结构化文本生成上表现优异，但在捕捉电影剧本所需的情感深度与角色一致性方面存在不足，需有针对性的评测与生成策略。

Method: 构建CML-Dataset（summary, content）对；分析真实剧本的多镜头连贯性与叙事结构，定义三维评估指标（DC, CC, PR）；基于此设计CML-Bench并提出CML-Instruction用于指导LLM生成更具电影感的剧本。

Result: CML-Bench能区分人类与模型生成剧本的质量差异；使用CML-Instruction的LLM在多项指标（DC, CC, PR）上均有提升，且与人类偏好一致。

Conclusion: 本文指出当前大语言模型在生成电影剧本时缺乏“灵魂”，即情感深度与细腻叙事，提出并验证了一套数据集、评测基准与指令以提升脚本质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [3] [User to Video: A Model for Spammer Detection Inspired by Video Classification Technology](https://arxiv.org/abs/2510.06233)
*Haoyang Zhang,Zhou Yang,Yucai Pang*

Main category: cs.CV

TL;DR: 把用户行为‘影像化’并用视频分类方法检测水军，在微博和推特数据上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 将用户行为数据类比为视频帧以利用视频分类技术进行恶意账号识别

Method: 提出user2pixel和behavior2image算法进行像素化与帧生成，利用表示学习做低秩稠密向量化，结合切割和扩散算法完善图像，构建时间序列视频并用视频分类器识别水军。

Result: 提出UVSD框架：user2pixel将用户及其立场映射为像素，behavior2image把行为子空间转为帧图像并用切割扩散完善，基于时间序列构建行为视频并用视频分类模型检测水军

Conclusion: 行为可视化与视频分类结合能有效提升水军检测性能，UVSD在公开数据集上表现优于现有最优方法

Abstract: This article is inspired by video classification technology. If the user
behavior subspace is viewed as a frame image, consecutive frame images are
viewed as a video. Following this novel idea, a model for spammer detection
based on user videoization, called UVSD, is proposed. Firstly, a user2piexl
algorithm for user pixelization is proposed. Considering the adversarial
behavior of user stances, the user is viewed as a pixel, and the stance is
quantified as the pixel's RGB. Secondly, a behavior2image algorithm is proposed
for transforming user behavior subspace into frame images. Low-rank dense
vectorization of subspace user relations is performed using representation
learning, while cutting and diffusion algorithms are introduced to complete the
frame imageization. Finally, user behavior videos are constructed based on
temporal features. Subsequently, a video classification algorithm is combined
to identify the spammers. Experiments using publicly available datasets, i.e.,
WEIBO and TWITTER, show an advantage of the UVSD model over state-of-the-art
methods.

</details>


### [4] [Uncertainty Quantification In Surface Landmines and UXO Classification Using MC Dropout](https://arxiv.org/abs/2510.06238)
*Sagar Lekhak,Emmett J. Ientilucci,Dimah Dera,Susmita Ghosh*

Main category: cs.CV

TL;DR: 在ResNet-50中加入MC Dropout以量化不确定性，能在仿真数据上识别噪声或对抗攻击下的不可靠预测，提示需要更鲁棒的地雷检测模型。


<details>
  <summary>Details</summary>
Motivation: Improve reliability of deep learning detection of landmines/UXOs by quantifying uncertainty to avoid missed/misclassified detections under noise/adversarial attacks.

Method: Fine-tune ResNet-50; incorporate MC Dropout at inference via multiple stochastic forward passes to estimate epistemic uncertainty; evaluate on clean, adversarially perturbed, and noisy simulated images.

Result: Proposed MC Dropout integrated with fine-tuned ResNet-50; on simulated dataset it quantifies epistemic uncertainty and flags unreliable predictions under clean, noisy, and adversarial conditions.

Conclusion: 不确定性量化（MC Dropout）可为地雷/未爆弹检测提供额外可靠性指标，有助于识别在噪声或对抗条件下的高风险预测；仍需更鲁棒方法和在真实数据上的验证。

Abstract: Detecting surface landmines and unexploded ordnances (UXOs) using deep
learning has shown promise in humanitarian demining. However, deterministic
neural networks can be vulnerable to noisy conditions and adversarial attacks,
leading to missed detection or misclassification. This study introduces the
idea of uncertainty quantification through Monte Carlo (MC) Dropout, integrated
into a fine-tuned ResNet-50 architecture for surface landmine and UXO
classification, which was tested on a simulated dataset. Integrating the MC
Dropout approach helps quantify epistemic uncertainty, providing an additional
metric for prediction reliability, which could be helpful to make more informed
decisions in demining operations. Experimental results on clean, adversarially
perturbed, and noisy test images demonstrate the model's ability to flag
unreliable predictions under challenging conditions. This proof-of-concept
study highlights the need for uncertainty quantification in demining, raises
awareness about the vulnerability of existing neural networks in demining to
adversarial threats, and emphasizes the importance of developing more robust
and reliable models for practical applications.

</details>


### [5] [multimodars: A Rust-powered toolkit for multi-modality cardiac image fusion and registration](https://arxiv.org/abs/2510.06241)
*Anselm W. Stark,Marc Ilic,Ali Mokhtari,Pooya Mohammadi Kazaj,Christoph Graeni,Isaac Shiri*

Main category: cs.CV

TL;DR: 提出了multimodars工具包，实现了确定性对齐算法、NumPy为中心的数据模型和Rust后端，结合冠状动脉血管内成像与CCTA以支持多状态分析（静息/用药、支架前后），提供高性能、可重复、易整合的管道，接受CSV/NumPy输入。


<details>
  <summary>Details</summary>
Motivation: 现有单一模态无法同时兼顾高分辨率和整段血管的三维背景；已有融合方法缺乏开源、面向多状态分析且兼顾确定性与高性能的工具。

Method: 通过确定性配准算法将血管内成像与CCTA融合，采用紧凑的NumPy数据模型并将性能关键部分用Rust实现以加速处理和保证可重复性，支持CSV/NumPy输入格式。

Result: 实现了一个名为multimodars的开源工具包，支持多模态多状态冠状动脉影像融合，具有确定性行为、高性能Rust后端和良好兼容性，适合可扩展的可重复实验。

Conclusion: multimodars填补了现有工具的空白，提供适用于多状态（rest/stress、pre-/post-stenting）分析的可重复、确定性、高性能融合工具，便于大规模实验和与AIVUS-CAA生成的数据兼容。

Abstract: Combining complementary imaging modalities is critical to build reliable 3D
coronary models: intravascular imaging gives sub-millimetre resolution but
limited whole-vessel context, while CCTA supplies 3D geometry but suffers from
limited spatial resolution and artefacts (e.g., blooming). Prior work
demonstrated intravascular/CCTA fusion, yet no open, flexible toolkit is
tailored for multi-state analysis (rest/stress, pre-/post-stenting) while
offering deterministic behaviour, high performance, and easy pipeline
integration. multimodars addresses this gap with deterministic alignment
algorithms, a compact NumPy-centred data model, and an optimised Rust backend
suitable for scalable, reproducible experiments. The package accepts CSV/NumPy
inputs including data formats produced by the AIVUS-CAA software

</details>


### [6] [Does Physics Knowledge Emerge in Frontier Models?](https://arxiv.org/abs/2510.06251)
*Ieva Bagdonaviciute,Vibhav Vineet*

Main category: cs.CV

TL;DR: 当前VLMs的感知与物理推理能力未能结合成因果理解，导致在物理预测与反事实推理任务中表现欠佳且相互间相关性弱。


<details>
  <summary>Details</summary>
Motivation: 评估最先进视觉-语言模型(VLMs)在理解与预测物理动态方面的能力是否充足。

Method: 基准测试六个VLMs于CLEVRER、Physion、Physion++，并设计诊断子测验(对象、颜色、遮挡物感知与运动预测、空间关系等)以隔离感知与物理推理能力，比较它们与预测/反事实评估的相关性。

Result: 在三个物理仿真数据集(CLEVRER, Physion, Physion++)上基准测试六个前沿VLMs，设计诊断子测试将感知与物理推理分离，发现感知或物理推理表现强的模型未必在预测或反事实评估上表现更好。

Conclusion: 当前VLMs存在将感知与物理推理结合不足的问题，需要新的架构或方法更紧密地绑定感知与推理以获得可靠的因果物理理解。

Abstract: Leading Vision-Language Models (VLMs) show strong results in visual
perception and general reasoning, but their ability to understand and predict
physical dynamics remains unclear. We benchmark six frontier VLMs on three
physical simulation datasets - CLEVRER, Physion, and Physion++ - where the
evaluation tasks test whether a model can predict outcomes or hypothesize about
alternative situations. To probe deeper, we design diagnostic subtests that
isolate perception (objects, colors, occluders) from physics reasoning (motion
prediction, spatial relations). Intuitively, stronger diagnostic performance
should support higher evaluation accuracy. Yet our analysis reveals weak
correlations: models that excel at perception or physics reasoning do not
consistently perform better on predictive or counterfactual evaluation. This
counterintuitive gap exposes a central limitation of current VLMs: perceptual
and physics skills remain fragmented and fail to combine into causal
understanding, underscoring the need for architectures that bind perception and
reasoning more tightly.

</details>


### [7] [Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training](https://arxiv.org/abs/2510.06254)
*Xiaochen Zhao,Chengting Yu,Kairong Yu,Lei Liu,Aili Wang*

Main category: cs.CV

TL;DR: 把SNN中间层的放电率投影到轻量ANN分支，通过自蒸馏（仅用可靠的自生成知识）+基于速率的反向传播联合优化，降低训练复杂度并提升性能，在CIFAR和ImageNet上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于 surrogate gradient 的BPTT训练在性能和资源开销上落后于ANN，且随时间维度线性增长；动机是设计一种在有限资源下仍能高效训练高性能SNN的方法。

Method: 在SNN中引入轻量级ANN分支，将中间层脉冲率映射为ANN输入，用模型自生成的知识做自蒸馏，同时将教师信号分解为可靠/不可靠，训练过程中只利用可靠部分；联合采用速率基反向传播替代或减少BPTT。

Result: Proposes an enhanced self-distillation framework combined with rate-based backpropagation to train SNNs efficiently by projecting intermediate firing rates onto lightweight ANN branches and using decoupled reliable teacher signals to avoid harmful self-knowledge; reduces training complexity and achieves strong results on CIFAR/ImageNet benchmarks.

Conclusion: 在有限计算资源下，增强自蒸馏与基于速率的优化可减少BPTT开销并提升SNN性能，且通过筛选可靠教师信号避免低质量自学习带来负面影响。

Abstract: Spiking Neural Networks (SNNs) exhibit exceptional energy efficiency on
neuromorphic hardware due to their sparse activation patterns. However,
conventional training methods based on surrogate gradients and Backpropagation
Through Time (BPTT) not only lag behind Artificial Neural Networks (ANNs) in
performance, but also incur significant computational and memory overheads that
grow linearly with the temporal dimension. To enable high-performance SNN
training under limited computational resources, we propose an enhanced
self-distillation framework, jointly optimized with rate-based backpropagation.
Specifically, the firing rates of intermediate SNN layers are projected onto
lightweight ANN branches, and high-quality knowledge generated by the model
itself is used to optimize substructures through the ANN pathways. Unlike
traditional self-distillation paradigms, we observe that low-quality
self-generated knowledge may hinder convergence. To address this, we decouple
the teacher signal into reliable and unreliable components, ensuring that only
reliable knowledge is used to guide the optimization of the model. Extensive
experiments on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate that
our method reduces training complexity while achieving high-performance SNN
training. Our code is available at
https://github.com/Intelli-Chip-Lab/enhanced-self-distillation-framework-for-snn.

</details>


### [8] [Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis](https://arxiv.org/abs/2510.06260)
*Sher Khan,Raz Muhammad,Adil Hussain,Muhammad Sajjad,Muhammad Rashid*

Main category: cs.CV

TL;DR: 用异构CNN集成提高诊断稳健性，并将大语言模型直接嵌入流程生成结构化报告与患者友好教育，从检测到随访一体化。


<details>
  <summary>Details</summary>
Motivation: 当前皮肤癌检测存在观察者差异、肤色数据偏差和解释/沟通碎片化问题，需一个同时提升诊断可靠性和沟通可及性的统一系统。

Method: 1) 构建由不同架构CNN组成的异构集成，利用不确定性量化机制识别需复核的病例；2) 将大型语言模型嵌入诊断流程，将分类结果转化为结构化临床报告、诊疗推理和患者教育材料。

Result: 提出一种用于皮肤科诊断的统一框架，结合多样化的卷积神经网络集成与内嵌的大型语言模型，实现诊断可靠性提升与患者教育支持。

Conclusion: 该方法通过架构多样性与内置自然语言能力，降低误诊与偏见影响，同时改善临床沟通，有望推动可部署的皮肤科AI落地。

Abstract: Cutaneous malignancies demand early detection for favorable outcomes, yet
current diagnostics suffer from inter-observer variability and access
disparities. While AI shows promise, existing dermatological systems are
limited by homogeneous architectures, dataset biases across skin tones, and
fragmented approaches that treat natural language processing as separate
post-hoc explanations rather than integral to clinical decision-making. We
introduce a unified framework that fundamentally reimagines AI integration for
dermatological diagnostics through two synergistic innovations. First, a
purposefully heterogeneous ensemble of architecturally diverse convolutional
neural networks provides complementary diagnostic perspectives, with an
intrinsic uncertainty mechanism flagging discordant cases for specialist review
-- mimicking clinical best practices. Second, we embed large language model
capabilities directly into the diagnostic workflow, transforming classification
outputs into clinically meaningful assessments that simultaneously fulfill
medical documentation requirements and deliver patient-centered education. This
seamless integration generates structured reports featuring precise lesion
characterization, accessible diagnostic reasoning, and actionable monitoring
guidance -- empowering patients to recognize early warning signs between
visits. By addressing both diagnostic reliability and communication barriers
within a single cohesive system, our approach bridges the critical
translational gap that has prevented previous AI implementations from achieving
clinical impact. The framework represents a significant advancement toward
deployable dermatological AI that enhances diagnostic precision while actively
supporting the continuum of care from initial detection through patient
education, ultimately improving early intervention rates for skin lesions.

</details>


### [9] [Vision Transformer for Transient Noise Classification](https://arxiv.org/abs/2510.06273)
*Divyansh Srivastava,Andrzej Niedzielski*

Main category: cs.CV

TL;DR: 使用预训练的Vision Transformer（ViT-B/32）在扩展的Gravity Spy数据集（共24类）上训练，分类准确率92.26%，显示ViT在区分瞬态噪声方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 将Vision Transformer应用于LIGO的重力波干扰噪声分类，以提高对瞬态噪声（glitches）的识别能力，尤其是包含O3a运行中新增加的两类噪声。

Method: 采用预训练的ViT-B/32模型，在包含22类原始Gravity Spy类别和O3a新增2类共24类的合并数据集上进行微调训练，以对干扰噪声图像进行分类。

Result: 在结合Gravity Spy数据集与O3a新增两类的训练集上，预训练ViT-B/32模型达到了92.26%的分类效率。

Conclusion: Vision Transformer能有效提升LIGO瞬态噪声的分类性能，有助于提高重力波检测的准确性；不过需注意数据集扩展和模型泛化问题。

Abstract: Transient noise (glitches) in LIGO data hinders the detection of
gravitational waves (GW). The Gravity Spy project has categorized these noise
events into various classes. With the O3 run, there is the inclusion of two
additional noise classes and thus a need to train new models for effective
classification. We aim to classify glitches in LIGO data into 22 existing
classes from the first run plus 2 additional noise classes from O3a using the
Vision Transformer (ViT) model. We train a pre-trained Vision Transformer
(ViT-B/32) model on a combined dataset consisting of the Gravity Spy dataset
with the additional two classes from the LIGO O3a run. We achieve a
classification efficiency of 92.26%, demonstrating the potential of Vision
Transformer to improve the accuracy of gravitational wave detection by
effectively distinguishing transient noise.
  Key words: gravitational waves --vision transformer --machine learning

</details>


### [10] [General and Efficient Visual Goal-Conditioned Reinforcement Learning using Object-Agnostic Masks](https://arxiv.org/abs/2510.06277)
*Fahim Shahriar,Cheryl Wang,Alireza Azimi,Gautham Vasan,Hany Hamed Elanwar,A. Rupam Mahmood,Colin Bellinger*

Main category: cs.CV

TL;DR: 用目标掩码替代传统表示，可获得更快、更具泛化性的GCRL学习；在模拟和现实机器人上均表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有目标表示（目标图像、3D坐标、one-hot向量）在泛化、收敛速度或对特殊摄像头的需求上存在缺陷，掩码表示旨在提供更通用且易于处理的视觉目标信号。

Method: 使用目标掩码作为目标表示，结合从掩码生成的稠密奖励（避免依赖距离计算），在模拟中以真值掩码训练策略，并在真实机器人上通过预训练的开放词汇目标检测模型生成掩码以实现sim-to-real迁移。

Result: 在模拟中使用真值掩码训练，训练和测试（未见过物体）上达到99.9%到达准确率；还展示了高精度抓取任务实现（无需目标位置信息）以及在两种实体机器人上的从零开始学习与sim-to-real迁移成功。

Conclusion: 该论文提出基于掩码的目标表示方法，可提供与物体无关的视觉提示，从而提升目标条件强化学习（GCRL）的学习效率与泛化能力。

Abstract: Goal-conditioned reinforcement learning (GCRL) allows agents to learn diverse
objectives using a unified policy. The success of GCRL, however, is contingent
on the choice of goal representation. In this work, we propose a mask-based
goal representation system that provides object-agnostic visual cues to the
agent, enabling efficient learning and superior generalization. In contrast,
existing goal representation methods, such as target state images, 3D
coordinates, and one-hot vectors, face issues of poor generalization to unseen
objects, slow convergence, and the need for special cameras. Masks can be
processed to generate dense rewards without requiring error-prone distance
calculations. Learning with ground truth masks in simulation, we achieved 99.9%
reaching accuracy on training and unseen test objects. Our proposed method can
be utilized to perform pick-up tasks with high accuracy, without using any
positional information of the target. Moreover, we demonstrate learning from
scratch and sim-to-real transfer applications using two different physical
robots, utilizing pretrained open vocabulary object detection models for mask
generation.

</details>


### [11] [Improving the Spatial Resolution of GONG Solar Images to GST Quality Using Deep Learning](https://arxiv.org/abs/2510.06281)
*Chenyang Li,Qin Li,Haimin Wang,Bo Shen*

Main category: cs.CV

TL;DR: 使用Real-ESRGAN对齐训练GONG低分辨率Hα图像至BBSO/GST高分辨率水平，能显著恢复太阳小尺度结构，但受图像配准误差影响定量指标有限，未来将扩展数据集并改进配准。


<details>
  <summary>Details</summary>
Motivation: Improve spatial resolution of full-disk LR H-alpha images from GONG to resolve fine-scale solar features comparable to BBSO/GST HR images.

Method: GAN-based superresolution (Real-ESRGAN variant)

Result: Model recovers fine details in sunspot penumbrae, filaments, and fibrils; achieved MSE=467.15, RMSE=21.59, CC=0.7794; limited by slight misalignments between image pairs.

Conclusion: GAN模型能有效提升GONG全盘Hα图像的细节还原能力，但配准误差和数据规模限制性能，需要改进配准和扩充训练集以进一步提升重建质量。

Abstract: High-resolution (HR) solar imaging is crucial for capturing fine-scale
dynamic features such as filaments and fibrils. However, the spatial resolution
of the full-disk H$\alpha$ images is limited and insufficient to resolve these
small-scale structures. To address this, we propose a GAN-based superresolution
approach to enhance low-resolution (LR) full-disk H$\alpha$ images from the
Global Oscillation Network Group (GONG) to a quality comparable with HR
observations from the Big Bear Solar Observatory/Goode Solar Telescope
(BBSO/GST). We employ Real-ESRGAN with Residual-in-Residual Dense Blocks and a
relativistic discriminator. We carefully aligned GONG-GST pairs. The model
effectively recovers fine details within sunspot penumbrae and resolves fine
details in filaments and fibrils, achieving an average mean squared error (MSE)
of 467.15, root mean squared error (RMSE) of 21.59, and cross-correlation (CC)
of 0.7794. Slight misalignments between image pairs limit quantitative
performance, which we plan to address in future work alongside dataset
expansion to further improve reconstruction quality.

</details>


### [12] [ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations](https://arxiv.org/abs/2510.06292)
*Yike Wu,Yiwei Wang,Yujun Cai*

Main category: cs.CV

TL;DR: ChainMPQ是一种无训练、基于多视角问题与图文交错记忆的链式推理方法，能有效减缓LVLM的关系性幻觉并提升推理准确性。


<details>
  <summary>Details</summary>
Motivation: 关系性幻觉在LVLM的幻觉问题中占比最大但研究较少，需设计有效方法提升模型的关系推断能力。

Method: 在无需训练的前提下，依据问题提取主客体关键词并增强相应图像区域，构建围绕主语、宾语与关系的多视角问题序列，利用早期步骤累积的文本与视觉记忆作为后续推理的支持，从而形成图文交错的推理链。

Result: 在多种LVLM与基准测试上，ChainMPQ显著减少关系幻觉；消融实验验证了关键模块（主客体关键词提取、增强区域、多视角交错链）各自的重要性。

Conclusion: ChainMPQ能显著降低关系性幻觉，提高LVLM在关系推理任务的可靠性。

Abstract: While Large Vision-Language Models (LVLMs) achieve strong performance in
multimodal tasks, hallucinations continue to hinder their reliability. Among
the three categories of hallucinations, which include object, attribute, and
relation, relation hallucinations account for the largest proportion but have
received the least attention. To address this issue, we propose ChainMPQ
(Multi-Perspective Questions guided Interleaved Chain of Image and Text), a
training-free method that improves relational inference in LVLMs by utilizing
accumulated textual and visual memories. ChainMPQ first extracts subject and
object keywords from the question to enhance the corresponding image regions.
It then constructs multi-perspective questions that focus on the three core
components of a relationship: the subject, the object, and the relation that
links them. These questions are sequentially input to the model, with textual
and visual memories from earlier steps providing supporting context for
subsequent ones, thereby forming an interleaved chain of images and text that
guides progressive relational reasoning. Experiments on multiple LVLMs and
benchmarks show that ChainMPQ substantially reduces relation hallucinations,
while ablation studies further validate the effectiveness of its three core
modules.

</details>


### [13] [Efficient High-Resolution Image Editing with Hallucination-Aware Loss and Adaptive Tiling](https://arxiv.org/abs/2510.06295)
*Young D. Kwon,Abhinav Mehrotra,Malcolm Chadwick,Alberto Gil Ramos,Sourav Bhattacharya*

Main category: cs.CV

TL;DR: MobilePicasso enables efficient on-device 4K image editing using a three-stage pipeline (low-res editing with hallucination-aware loss, latent projection, adaptive tiling upscaling), improving quality and reducing hallucinations while drastically lowering latency and keeping memory overhead minimal.


<details>
  <summary>Details</summary>
Motivation: Enable efficient high-resolution (4K) image-to-image editing on resource-constrained mobile devices by reducing memory and computational cost while maintaining or improving image quality and reducing hallucinations.

Method: Three-stage pipeline: (1) perform editing at standard resolution using a hallucination-aware loss to avoid spurious content, (2) project edited latent back to pixel space via latent projection to avoid direct pixel-space computations, (3) upscale edited latent to high resolution with adaptive context-preserving tiling to maintain coherence and context while being memory efficient.

Result: MobilePicasso, a three-stage system: (i) edit at standard resolution with hallucination-aware loss, (ii) latent projection to pixel space, (iii) upscale edited latent to higher resolution via adaptive context-preserving tiling. User study shows 18-48% quality improvement and 14-51% reduction in hallucinations. Offers up to 55.8x speed-up with only ~9% extra runtime memory compared to prior work; on-device runtime can surpass server A100 GPU model.

Conclusion: MobilePicasso provides a practical solution for high-res image editing on mobile: better image quality, fewer hallucinations, much faster execution (even exceeding some server-side models) with only slight memory increase.

Abstract: High-resolution (4K) image-to-image synthesis has become increasingly
important for mobile applications. Existing diffusion models for image editing
face significant challenges, in terms of memory and image quality, when
deployed on resource-constrained devices. In this paper, we present
MobilePicasso, a novel system that enables efficient image editing at high
resolutions, while minimising computational cost and memory usage.
MobilePicasso comprises three stages: (i) performing image editing at a
standard resolution with hallucination-aware loss, (ii) applying latent
projection to overcome going to the pixel space, and (iii) upscaling the edited
image latent to a higher resolution with adaptive context-preserving tiling.
Our user study with 46 participants reveals that MobilePicasso not only
improves image quality by 18-48% but reduces hallucinations by 14-51% over
existing methods. MobilePicasso demonstrates significantly lower latency, e.g.,
up to 55.8$\times$ speed-up, yet with a small increase in runtime memory, e.g.,
a mere 9% increase over prior work. Surprisingly, the on-device runtime of
MobilePicasso is observed to be faster than a server-based high-resolution
image editing model running on an A100 GPU.

</details>


### [14] [RGBD Gaze Tracking Using Transformer for Feature Fusion](https://arxiv.org/abs/2510.06298)
*Tobias J. Bauer*

Main category: cs.CV

TL;DR: 基于RGBD与Transformer尝试注视角度估计并新建数据集；实验证明预训练GAN与Transformer并非总有利，MLP或去掉GAN能得到更低误差。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏深度信息或不适合注视角度估计，且RGBD与Transformer结合未被充分研究，因而探索这种组合及其替代方案以提高注视估计性能。

Method: 基于Lian等人架构，使用GAN用于深度去噪与头部姿态特征提取；在此基础上加入Transformer用于RGB与深度特征融合；对比不使用GAN、以及用MLP替换Transformer的变体，在上海TechGaze+、ETH-XGaze和自建OTH数据集上训练评估。

Result: 实现了基于RGBD图像的注视追踪系统，使用Transformer模块融合特征并比较不同架构与预训练模块的影响。创建了新数据集并在三个数据集上训练评估，结果显示去掉预训练GAN或替换Transformer为MLP在本任务中能显著改善误差。

Conclusion: 在本工作中，使用RGBD输入与Transformer的组合并未超越简化架构；去掉预训练GAN或用MLP替换Transformer能提升性能，表明模型设计与预训练策略对RGBD注视估计影响较大。

Abstract: Subject of this thesis is the implementation of an AI-based Gaze Tracking
system using RGBD images that contain both color (RGB) and depth (D)
information. To fuse the features extracted from the images, a module based on
the Transformer architecture is used. The combination of RGBD input images and
Transformers was chosen because it has not yet been investigated. Furthermore,
a new dataset is created for training the AI models as existing datasets either
do not contain depth information or only contain labels for Gaze Point
Estimation that are not suitable for the task of Gaze Angle Estimation. Various
model configurations are trained, validated and evaluated on a total of three
different datasets. The trained models are then to be used in a real-time
pipeline to estimate the gaze direction and thus the gaze point of a person in
front of a computer screen. The AI model architecture used in this thesis is
based on an earlier work by Lian et al. It uses a Generative Adversarial
Network (GAN) to simultaneously remove depth map artifacts and extract head
pose features. Lian et al. achieve a mean Euclidean error of 38.7mm on their
own dataset ShanghaiTechGaze+. In this thesis, a model architecture with a
Transformer module for feature fusion achieves a mean Euclidean error of 55.3mm
on the same dataset, but we show that using no pre-trained GAN module leads to
a mean Euclidean error of 30.1mm. Replacing the Transformer module with a
Multilayer Perceptron (MLP) improves the error to 26.9mm. These results are
coherent with the ones on the other two datasets. On the ETH-XGaze dataset, the
model with Transformer module achieves a mean angular error of 3.59{\deg} and
without Transformer module 3.26{\deg}, whereas the fundamentally different
model architecture used by the dataset authors Zhang et al. achieves a mean
angular error of 2.04{\deg}. On the OTH-Gaze-Estimation dataset created for...

</details>


### [15] [Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping](https://arxiv.org/abs/2510.06299)
*Tiago de Conto,John Armston,Ralph Dubayah*

Main category: cs.CV

TL;DR: 通过将GEDI激光雷达与多模态SAR结合并用改进的EfficientNetV2训练，作者实现了一个参数少、可扩展的深度学习框架，生成全球25 m分辨率、2015–2022年多时相的森林结构复杂度产品，性能高且带不确定性估计，并可通过迁移学习扩展到其他指标。


<details>
  <summary>Details</summary>
Motivation: GEDI提供高质量但空间稀疏的垂直结构观测，限制了高分辨率连续地图的生成；将GEDI与覆盖全球的SAR影像融合可弥补采样稀疏性，实现精细的全球森林结构监测。

Method: 基于约1.3亿条GEDI足迹，改进EfficientNetV2架构进行训练，输入多模态SAR数据（时间序列与多频率/极化信息），采用小参数量（<400k）模型并估计预测不确定性，利用迁移学习扩展到其他结构变量。

Result: 模型在全球范围内达到了高性能（R2=0.82），能保持细尺度空间格局，并提供校准的不确定性估计；生成了2015–2022年的多时相全球森林结构复杂度产品，且通过迁移学习能低成本预测其他结构变量。

Conclusion: 该论文成功提出并验证了一个可扩展的深度学习框架，将稀疏的GEDI激光雷达观测与多模态SAR数据融合，实现了全球25 m分辨率的森林结构复杂度连续制图，并能多时相生成2015–2022年的动态数据集。

Abstract: Forest structural complexity metrics integrate multiple canopy attributes
into a single value that reflects habitat quality and ecosystem function.
Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has
enabled mapping of structural complexity in temperate and tropical forests, but
its sparse sampling limits continuous high-resolution mapping. We present a
scalable, deep learning framework fusing GEDI observations with multimodal
Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25
m) wall-to-wall maps of forest structural complexity. Our adapted
EfficientNetV2 architecture, trained on over 130 million GEDI footprints,
achieves high performance (global R2 = 0.82) with fewer than 400,000
parameters, making it an accessible tool that enables researchers to process
datasets at any scale without requiring specialized computing infrastructure.
The model produces accurate predictions with calibrated uncertainty estimates
across biomes and time periods, preserving fine-scale spatial patterns. It has
been used to generate a global, multi-temporal dataset of forest structural
complexity from 2015 to 2022. Through transfer learning, this framework can be
extended to predict additional forest structural variables with minimal
computational cost. This approach supports continuous, multi-temporal
monitoring of global forest structural dynamics and provides tools for
biodiversity conservation and ecosystem management efforts in a changing
climate.

</details>


### [16] [Lumina-DiMOO: An Omni Diffusion Large Language Model for Multi-Modal Generation and Understanding](https://arxiv.org/abs/2510.06308)
*Yi Xin,Qi Qin,Siqi Luo,Kaiwen Zhu,Juncheng Yan,Yan Tai,Jiayi Lei,Yuewen Cao,Keqi Wang,Yibin Wang,Jinbin Bai,Qian Yu,Dengyang Jiang,Yuandong Pu,Haoxing Chen,Le Zhuo,Junjun He,Gen Luo,Tianbin Li,Ming Hu,Jin Ye,Shenglong Ye,Bo Zhang,Chang Xu,Wenhai Wang,Hongsheng Li,Guangtao Zhai,Tianfan Xue,Bin Fu,Xiaohong Liu,Yu Qiao,Yihao Liu*

Main category: cs.CV

TL;DR: 该论文提出Lumina-DiMOO，一种基于全离散扩散建模的多模态生成与理解基础模型，支持文本-图像、图像-图像生成及图像理解任务，在采样效率和性能上优于之前的自回归或混合模型，并开源代码与权重。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态模型多依赖自回归或混合AR-扩散方法，存在采样效率低或任务支持受限的问题，作者希望通过离散扩散实现更高效、通用的多模态生成与理解模型。

Method: 采用全离散扩散模型对不同模态的输入输出进行统一表征与生成，替代传统自回归或混合AR-扩散范式，从而提高采样效率并支持多种任务（text-to-image、image-to-image及图像理解）；具体实现细节应包括离散化表示、扩散与反向采样过程以及多模态条件机制。

Result: 在多个基准上达到或超过现有开源统一多模态模型的性能，同时在采样效率上优于AR或混合方法，并公开代码与检查点。

Conclusion: Lumina-DiMOO通过将全离散扩散用于统一多模态建模，既提升了采样效率又保持或提升了生成与理解性能，达到了多项基准的SOTA，并为社区提供了开源资源以推动相关研究。

Abstract: We introduce Lumina-DiMOO, an open-source foundational model for seamless
multi-modal generation and understanding. Lumina-DiMOO sets itself apart from
prior unified models by utilizing a fully discrete diffusion modeling to handle
inputs and outputs across various modalities. This innovative approach allows
Lumina-DiMOO to achieve higher sampling efficiency compared to previous
autoregressive (AR) or hybrid AR-Diffusion paradigms and adeptly support a
broad spectrum of multi-modal tasks, including text-to-image generation,
image-to-image generation (e.g., image editing, subject-driven generation, and
image inpainting, etc.), as well as image understanding. Lumina-DiMOO achieves
state-of-the-art performance on multiple benchmarks, surpassing existing
open-source unified multi-modal models. To foster further advancements in
multi-modal and discrete diffusion model research, we release our code and
checkpoints to the community. Project Page:
https://synbol.github.io/Lumina-DiMOO.

</details>


### [17] [TransFIRA: Transfer Learning for Face Image Recognizability Assessment](https://arxiv.org/abs/2510.06353)
*Allen Tu,Kartik Narayan,Joshua Gleason,Jennifer Xu,Matthew Meyn,Tom Goldstein,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出TransFIRA，一种轻量、无标注的识别性评估框架，通过在嵌入空间基于类中心相似度与角度分离定义识别性，并用于图像聚合与可解释性，实验证明在人脸与人体识别任务上效果优越且具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉质量度量不能预测在特定编码器下输入是否可被识别，现有FIQA方法依赖人为启发、标注或复杂生成模型，未与编码器决策边界对齐，因此需要一种轻量、无标注且与编码器几何一致的识别性评估方法。

Method: 在嵌入空间计算样本与类中心的相似度（CCS）及类中心间角度分离（CCAS），基于此定义构建识别性评分并用于样本加权/过滤的聚合策略；通过转移学习从已训练编码器的特征导出识别性指标，无需额外标注或生成模型；提供可视化工具来揭示图像退化与个体因素如何影响识别性。

Result: 在BRIAR与IJB-C上实现了最先进的验证准确率，识别性与真实识别性的相关性近乎翻倍；在跨数据集转移和人体识别任务上表现稳健，且提供了可解释性分析结果。

Conclusion: TransFIRA 将识别性直接绑定到编码器的决策几何：利用类中心相似度(CCS)与类中心角分离(CCAS)作为识别性标准，实现了无外部标签、无启发式、无需针对主干训练的高性能识别性评估，提升了验证准确率、相关性及可解释性，并扩展至人体识别。

Abstract: Face recognition in unconstrained environments such as surveillance, video,
and web imagery must contend with extreme variation in pose, blur,
illumination, and occlusion, where conventional visual quality metrics fail to
predict whether inputs are truly recognizable to the deployed encoder. Existing
FIQA methods typically rely on visual heuristics, curated annotations, or
computationally intensive generative pipelines, leaving their predictions
detached from the encoder's decision geometry. We introduce TransFIRA (Transfer
Learning for Face Image Recognizability Assessment), a lightweight and
annotation-free framework that grounds recognizability directly in embedding
space. TransFIRA delivers three advances: (i) a definition of recognizability
via class-center similarity (CCS) and class-center angular separation (CCAS),
yielding the first natural, decision-boundary--aligned criterion for filtering
and weighting; (ii) a recognizability-informed aggregation strategy that
achieves state-of-the-art verification accuracy on BRIAR and IJB-C while nearly
doubling correlation with true recognizability, all without external labels,
heuristics, or backbone-specific training; and (iii) new extensions beyond
faces, including encoder-grounded explainability that reveals how degradations
and subject-specific factors affect recognizability, and the first
recognizability-aware body recognition assessment. Experiments confirm
state-of-the-art results on faces, strong performance on body recognition, and
robustness under cross-dataset shifts. Together, these contributions establish
TransFIRA as a unified, geometry-driven framework for recognizability
assessment -- encoder-specific, accurate, interpretable, and extensible across
modalities -- significantly advancing FIQA in accuracy, explainability, and
scope.

</details>


### [18] [Road Surface Condition Detection with Machine Learning using New York State Department of Transportation Camera Images and Weather Forecast Data](https://arxiv.org/abs/2510.06440)
*Carly Sutter,Kara J. Sulia,Nick P. Bassill,Christopher D. Wirz,Christopher D. Thorncroft,Jay C. Rothenberger,Vanessa Przybylo,Mariana G. Cains,Jacob Radford,David Aaron Evans*

Main category: cs.CV

TL;DR: Trained CNNs and random forests on ~22k labeled camera images plus weather data to predict six road conditions; achieved 81.5% accuracy on unseen cameras.


<details>
  <summary>Details</summary>
Motivation: Reduce labor-intensive manual driving and live camera monitoring by NYSDOT during winter events by automating road condition classification.

Method: Collected ~22,000 hand-labeled images into six classes; trained convolutional neural networks and random forests using both images and weather features; prioritized generalizability by testing on completely unseen cameras.

Result: Models can classify road surface conditions from NYSDOT camera images to aid winter-weather decisions.

Conclusion: Weather-informed image models (CNN + RF) can generalize to new cameras and reach useful accuracy (81.5%) for operational support.

Abstract: The New York State Department of Transportation (NYSDOT) has a network of
roadside traffic cameras that are used by both the NYSDOT and the public to
observe road conditions. The NYSDOT evaluates road conditions by driving on
roads and observing live cameras, tasks which are labor-intensive but necessary
for making critical operational decisions during winter weather events.
However, machine learning models can provide additional support for the NYSDOT
by automatically classifying current road conditions across the state. In this
study, convolutional neural networks and random forests are trained on camera
images and weather data to predict road surface conditions. Models are trained
on a hand-labeled dataset of ~22,000 camera images, each classified by human
labelers into one of six road surface conditions: severe snow, snow, wet, dry,
poor visibility, or obstructed. Model generalizability is prioritized to meet
the operational needs of the NYSDOT decision makers, and the weather-related
road surface condition model in this study achieves an accuracy of 81.5% on
completely unseen cameras.

</details>


### [19] [TDiff: Thermal Plug-And-Play Prior with Patch-Based Diffusion](https://arxiv.org/abs/2510.06460)
*Piyush Dashpute,Niki Nezakati,Wolfgang Heidrich,Vishwanath Saragadam*

Main category: cs.CV

TL;DR: 提出了一个基于图像块的扩散模型TDiff，用于热成像的多任务恢复，通过在小热图像块上训练并重建重叠块来处理低成本热像仪常见的局部退化问题。


<details>
  <summary>Details</summary>
Motivation: 低成本热相机图像存在分辨率低、固定模式噪声及局部退化，且热像数据集规模有限，需一种能利用局部退化特点并在小数据下泛化的恢复方法。

Method: 在小尺寸热图像块上训练扩散模型，恢复时对完整图像进行重叠块的去噪并用平滑空间窗融合，建立了局部先验以应对固定图案噪声和局部退化。

Result: 在合成与真实热成像数据集上的去噪、超分辨率和去模糊实验中，TDiff展示了强有力的恢复效果，证明其作为统一热图像恢复方案的有效性。

Conclusion: TDiff作为首个基于图像块的热成像扩散先验模型，在去噪、超分和去模糊任务上对模拟和真实热数据均表现出色，提供了统一的恢复管线。

Abstract: Thermal images from low-cost cameras often suffer from low resolution, fixed
pattern noise, and other localized degradations. Available datasets for thermal
imaging are also limited in both size and diversity. To address these
challenges, we propose a patch-based diffusion framework (TDiff) that leverages
the local nature of these distortions by training on small thermal patches. In
this approach, full-resolution images are restored by denoising overlapping
patches and blending them using smooth spatial windowing. To our knowledge,
this is the first patch-based diffusion framework that models a learned prior
for thermal image restoration across multiple tasks. Experiments on denoising,
super-resolution, and deblurring demonstrate strong results on both simulated
and real thermal data, establishing our method as a unified restoration
pipeline.

</details>


### [20] [SIGMA-GEN: Structure and Identity Guided Multi-subject Assembly for Image Generation](https://arxiv.org/abs/2510.06469)
*Oindrila Saha,Vojtech Krs,Radomir Mech,Subhransu Maji,Kevin Blackburn-Matzen,Matheus Gadelha*

Main category: cs.CV

TL;DR: 提出SIGMA-GEN和合成数据集SIGMA-SET27K，实现单次多主体身份保留生成，支持从粗到精的多级用户引导。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法在单次生成中同时对多个主体进行身份保留并支持多级别空间/结构约束的问题，提供更灵活的用户引导和更高效的生成流程。

Method: 基于单模型，结合结构（如2D/3D框、分割、深度）与空间约束进行条件图像生成，训练数据来自SIGMA-SET27K，包含10万+独立身份和27k图像。

Result: SIGMA-GEN提出了一种统一框架，用于多身份保留的图像生成，支持结构和空间约束引导的单次多主体身份保留生成。

Conclusion: SIGMA-GEN在身份保留、生成质量和速度上达到或优于现有方法，并通过新的合成数据集支持多种精度的用户引导。

Abstract: We present SIGMA-GEN, a unified framework for multi-identity preserving image
generation. Unlike prior approaches, SIGMA-GEN is the first to enable
single-pass multi-subject identity-preserved generation guided by both
structural and spatial constraints. A key strength of our method is its ability
to support user guidance at various levels of precision -- from coarse 2D or 3D
boxes to pixel-level segmentations and depth -- with a single model. To enable
this, we introduce SIGMA-SET27K, a novel synthetic dataset that provides
identity, structure, and spatial information for over 100k unique subjects
across 27k images. Through extensive evaluation we demonstrate that SIGMA-GEN
achieves state-of-the-art performance in identity preservation, image
generation quality, and speed. Code and visualizations at
https://oindrilasaha.github.io/SIGMA-Gen/

</details>


### [21] [Superpixel Integrated Grids for Fast Image Segmentation](https://arxiv.org/abs/2510.06487)
*Jack Roberts,Jeova Farias Sales Rocha Neto*

Main category: cs.CV

TL;DR: SIGRID compresses images into shape-aware superpixel grids, enabling faster training and equal or better segmentation accuracy vs. pixel inputs.


<details>
  <summary>Details</summary>
Motivation: Irregular superpixel distributions hinder usage in deep learning despite efficiency potential; SIGRID seeks to retain superpixel benefits while enabling standard CNNs by regularizing spatial layout.

Method: Compute superpixels, extract classical shape descriptors (color, shape stats) for each superpixel, arrange descriptors into a regular grid (SIGRID), feed into standard convolutional segmentation architectures.

Result: SIGRID introduces a superpixel-integrated grid representation for image segmentation that encodes color and shape descriptors, reducing input dimensionality and accelerating training while maintaining or improving accuracy.

Conclusion: SIGRID offers an effective trade-off between computational efficiency and segmentation accuracy, making it a practical alternative to full-resolution inputs in CNN segmentation.

Abstract: Superpixels have long been used in image simplification to enable more
efficient data processing and storage. However, despite their computational
potential, their irregular spatial distribution has often forced deep learning
approaches to rely on specialized training algorithms and architectures,
undermining the original motivation for superpixelations. In this work, we
introduce a new superpixel-based data structure, SIGRID (Superpixel-Integrated
Grid), as an alternative to full-resolution images in segmentation tasks. By
leveraging classical shape descriptors, SIGRID encodes both color and shape
information of superpixels while substantially reducing input dimensionality.
We evaluate SIGRIDs on four benchmark datasets using two popular convolutional
segmentation architectures. Our results show that, despite compressing the
original data, SIGRIDs not only match but in some cases surpass the performance
of pixel-level representations, all while significantly accelerating model
training. This demonstrates that SIGRIDs achieve a favorable balance between
accuracy and computational efficiency.

</details>


### [22] [Text2Interact: High-Fidelity and Diverse Text-to-Two-Person Interaction Generation](https://arxiv.org/abs/2510.06504)
*Qingxuan Wu,Zhiyang Dou,Chuan Guo,Yiming Huang,Qiao Feng,Bing Zhou,Jian Wang,Lingjie Liu*

Main category: cs.CV

TL;DR: 过短总结


<details>
  <summary>Details</summary>
Motivation: 摘要动机

Method: 分析方法

Result: 主要结果

Conclusion: 结论

Abstract: Modeling human-human interactions from text remains challenging because it
requires not only realistic individual dynamics but also precise,
text-consistent spatiotemporal coupling between agents. Currently, progress is
hindered by 1) limited two-person training data, inadequate to capture the
diverse intricacies of two-person interactions; and 2) insufficiently
fine-grained text-to-interaction modeling, where language conditioning
collapses rich, structured prompts into a single sentence embedding. To address
these limitations, we propose our Text2Interact framework, designed to generate
realistic, text-aligned human-human interactions through a scalable
high-fidelity interaction data synthesizer and an effective spatiotemporal
coordination pipeline. First, we present InterCompose, a scalable
synthesis-by-composition pipeline that aligns LLM-generated interaction
descriptions with strong single-person motion priors. Given a prompt and a
motion for an agent, InterCompose retrieves candidate single-person motions,
trains a conditional reaction generator for another agent, and uses a neural
motion evaluator to filter weak or misaligned samples-expanding interaction
coverage without extra capture. Second, we propose InterActor, a
text-to-interaction model with word-level conditioning that preserves
token-level cues (initiation, response, contact ordering) and an adaptive
interaction loss that emphasizes contextually relevant inter-person joint
pairs, improving coupling and physical plausibility for fine-grained
interaction modeling. Extensive experiments show consistent gains in motion
diversity, fidelity, and generalization, including out-of-distribution
scenarios and user studies. We will release code and models to facilitate
reproducibility.

</details>


### [23] [From Captions to Keyframes: Efficient Video Summarization via Caption- and Context-Aware Frame Scoring](https://arxiv.org/abs/2510.06509)
*Shih-Yao Lin,Sibendu Paul,Caren Chen*

Main category: cs.CV

TL;DR: 提出基于字幕与视觉上下文的多模态帧评分（KeyScore）及时空自适应聚类候选生成（STACFP），实现极高帧压缩且在检索、字幕生成与视-文推理上优于基线。


<details>
  <summary>Details</summary>
Motivation: 长视频含大量冗余帧，直接编码所有帧在计算与存储上代价高昂，且可能稀释与文本信号的对齐；需要一种能保留文本相关信息并高效选择关键帧的方法。

Method: KeyScore结合语义相似度、时间多样性与上下文去除影响（contextual drop impact）对帧进行多模态评分；STACFP采用时空自适应聚类生成紧凑且多样的帧候选集合。两者联用在下游任务中替代全帧推理。

Result: 在MSRVTT、MSVD和DiDeMo数据集上，方法在保持或提升下游任务性能的同时实现高达99%的帧数减少，优于常用的8帧编码器。

Conclusion: KeyScore与STACFP在长视频的帧选择与候选生成上提供了一种高效多模态方案，能在保留语义与上下文信息的同时大幅减少推理帧数，从而实现可扩展的视频-语言理解。

Abstract: Efficient video-language understanding requires selecting a small set of
frames that retain semantic and contextual information from long videos. We
propose KeyScore, a multimodal frame scoring framework that jointly leverages
captions and visual context to estimate frame-level importance. By combining
semantic similarity, temporal diversity, and contextual drop impact, KeyScore
identifies the most informative frames for downstream tasks such as retrieval,
captioning, and video-language reasoning. To complement KeyScore, we introduce
STACFP (Spatio-Temporal Adaptive Clustering for Frame Proposals), which
generates compact and diverse frame candidates for long-form videos. Together,
these modules achieve up to 99\% frame reduction compared to full-frame
inference and substantially outperform standard 8-frame encoders on MSRVTT,
MSVD, and DiDeMo. Our results demonstrate that emphasizing multimodal alignment
between visual and textual signals enables scalable, efficient, and
caption-grounded video understanding -- without explicit video summarization.

</details>


### [24] [LogSTOP: Temporal Scores over Prediction Sequences for Matching and Retrieval](https://arxiv.org/abs/2510.06512)
*Avishree Khare,Hideki Okamoto,Bardh Hoxha,Georgios Fainekos,Rajeev Alur*

Main category: cs.CV

TL;DR: 本文提出LogSTOP方法，将局部检测分数提升为线性时序逻辑属性的序列级评分，显著优于现有基线，在查询匹配和视频检索任务上带来16%~19%以上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 将局部检测器（如YOLO、HuBERT）在帧/片段级别的置信分数提升到序列级的时序属性评分，以便支持查询匹配和排序检索等下游任务。

Method: 定义Scores for TempOral Properties (STOPs)问题，使用线性时序逻辑（LTL）表达时序查询，并设计LogSTOP评分函数，在对数概率空间进行动态规划/组合运算以高效计算LTL公式在序列上的满足概率或评分；在视觉和音频数据集上与多种基线比较。

Result: 提出LogSTOP scoring函数，能高效计算用线性时序逻辑表示的时序属性得分。在多个任务上（对象/情感的查询匹配与视频检索），使用YOLO/HuBERT或Grounding DINO/SlowR50时，LogSTOP相比大型视/听语言模型和其他时序逻辑基线在匹配任务上至少提升16%，在排序检索上mAP和召回分别提升至少19%和16%。

Conclusion: 通过将局部置信分数以概率/对数空间整合到线性时序逻辑模型，LogSTOP能更鲁棒地处理检测噪声并高效计算复杂时序查询的匹配分数，适用于多模态（视觉/音频）序列检索与匹配应用。

Abstract: Neural models such as YOLO and HuBERT can be used to detect local properties
such as objects ("car") and emotions ("angry") in individual frames of videos
and audio clips respectively. The likelihood of these detections is indicated
by scores in [0, 1]. Lifting these scores to temporal properties over sequences
can be useful for several downstream applications such as query matching (e.g.,
"does the speaker eventually sound happy in this audio clip?"), and ranked
retrieval (e.g., "retrieve top 5 videos with a 10 second scene where a car is
detected until a pedestrian is detected"). In this work, we formalize this
problem of assigning Scores for TempOral Properties (STOPs) over sequences,
given potentially noisy score predictors for local properties. We then propose
a scoring function called LogSTOP that can efficiently compute these scores for
temporal properties represented in Linear Temporal Logic. Empirically, LogSTOP,
with YOLO and HuBERT, outperforms Large Vision / Audio Language Models and
other Temporal Logic-based baselines by at least 16% on query matching with
temporal properties over objects-in-videos and emotions-in-speech respectively.
Similarly, on ranked retrieval with temporal properties over objects and
actions in videos, LogSTOP with Grounding DINO and SlowR50 reports at least a
19% and 16% increase in mean average precision and recall over zero-shot
text-to-video retrieval baselines respectively.

</details>


### [25] [Limited-Angle Tomography Reconstruction via Projector Guided 3D Diffusion](https://arxiv.org/abs/2510.06516)
*Zhantao Deng,Mériem Er-Rafik,Anna Sushko,Cécile Hébert,Pascal Fua*

Main category: cs.CV

TL;DR: 用FIB-SEM模拟训练的3D扩散重建方法TEMDiff，无需TEM真值即可在有限角度电子断层中显著减少缺失楔伪影并良好迁移到真实数据。


<details>
  <summary>Details</summary>
Motivation: 有限角度电子断层存在缺失楔导致严重伪影；而深度学习方法需大量带真实3D真值的TEM训练样本，但这类数据难以获得，故需要一种能利用现成体积数据并推广到真实TEM的无真值训练策略。

Method: 提出一种基于3D扩散模型的迭代重建框架：用FIB-SEM体积经模拟器生成TEM倾角序列作为训练数据，在体积域上进行扩散模型的去噪逆向过程，同时在每次迭代中使用数据一致性（由模拟器和投影到观测tilt数据）约束以恢复结构。

Result: 在模拟有限角度数据上，TEMDiff在重建质量上优于现有最先进方法；在真实TEM倾角数据上无需微调即可从仅8度范围（2度步长）的数据中准确恢复结构。

Conclusion: TEMDiff通过在3D体积上采用扩散模型并利用FIB-SEM数据模拟TEM倾斜投影训练，能在有限角度电子断层重建中显著减少缺失楔形伪影并提高重建质量，且可在极窄倾角范围下推广到真实TEM数据而无需微调。

Abstract: Limited-angle electron tomography aims to reconstruct 3D shapes from 2D
projections of Transmission Electron Microscopy (TEM) within a restricted range
and number of tilting angles, but it suffers from the missing-wedge problem
that causes severe reconstruction artifacts. Deep learning approaches have
shown promising results in alleviating these artifacts, yet they typically
require large high-quality training datasets with known 3D ground truth which
are difficult to obtain in electron microscopy. To address these challenges, we
propose TEMDiff, a novel 3D diffusion-based iterative reconstruction framework.
Our method is trained on readily available volumetric FIB-SEM data using a
simulator that maps them to TEM tilt series, enabling the model to learn
realistic structural priors without requiring clean TEM ground truth. By
operating directly on 3D volumes, TEMDiff implicitly enforces consistency
across slices without the need for additional regularization. On simulated
electron tomography datasets with limited angular coverage, TEMDiff outperforms
state-of-the-art methods in reconstruction quality. We further demonstrate that
a trained TEMDiff model generalizes well to real-world TEM tilts obtained under
different conditions and can recover accurate structures from tilt ranges as
narrow as 8 degrees, with 2-degree increments, without any retraining or
fine-tuning.

</details>


### [26] [VUGEN: Visual Understanding priors for GENeration](https://arxiv.org/abs/2510.06529)
*Xiangyi Chen,Théophane Vallaeys,Maha Elbayad,John Nguyen,Jakob Verbeek*

Main category: cs.CV

TL;DR: VUGEN提出利用预训练视觉-语言模型（VLM）的视觉理解表征来进行高质量图像生成，通过将VLM高维latent投射到低维可处理分布、在该空间上训练生成器并用像素扩散解码器重建图像，从而实现生成-理解表征的一致性和高效性。实验在COCO上提升了DPG Bench和FID，并保持VLM的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖重构导向的自动编码器，要么采用复杂的桥接机制，导致理解与生成表示不对齐或架构复杂；因此提出直接利用VLM的理解先验进行生成，以实现对齐且简洁高效的生成流程。

Method: （1）将VLM原生视觉编码器的高维latent映射到低维、尽可能保留视觉信息的可处理分布；（2）在该低维潜在空间中训练VLM生成器进行采样，以确保与视觉理解能力对齐；（3）使用专用的像素级扩散解码器（不依赖VAE）将生成的潜在映射回图像空间。

Result: 在COCO上，VUGEN将DPG Bench从71.17提升至74.32，FID从11.86降至9.06；并报告在保持VLM原有理解能力方面无明显退化。

Conclusion: VUGEN在保持VLM原有理解能力的同时，通过在降维的VLM原生视觉潜在空间上进行采样并使用VAE-free像素扩散解码器，显著提升图像生成质量，优于依赖VAE的复杂潜在扩散方案。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled unified
understanding across text and images, yet equipping these models with robust
image generation capabilities remains challenging. Existing approaches often
rely on reconstruction-oriented autoencoders or complex bridging mechanisms,
leading to misalignment between understanding and generation representations,
or architectural complexity. In this work, we propose VUGEN, a novel framework
that explicitly leverages VLM's pretrained visual understanding priors for
efficient and high-quality image generation. Our approach first transforms the
high-dimensional latent space of the VLM's native vision encoder into a
lower-dimensional, tractable distribution that maximally preserves visual
information. The VLM is then trained to sample within this reduced latent
space, ensuring alignment with its visual understanding capabilities. Finally,
a dedicated pixel decoder maps these generated latents back to the image space.
We find that a VAE-free pixel diffusion decoder to be on par or better than
commonly used complex latent diffusion decoders that internally rely on VAE
latents. Extensive experiments demonstrate that VUGEN achieves superior image
generation performance, improving DPG Bench from 71.17 to 74.32 and FID from
11.86 to 9.06 on COCO, while fully preserving the VLM's original understanding
capabilities.

</details>


### [27] [Cluster Paths: Navigating Interpretability in Neural Networks](https://arxiv.org/abs/2510.06541)
*Nicholas M. Kroeger,Vincent Bindschaedler*

Main category: cs.CV

TL;DR: 用中间层激活聚类并将样本表示为聚类ID序列，生成简洁可读的解释；四个量化指标评估可解释性、保真度与稳定性；可扩展到大模型并用于OOD检测。


<details>
  <summary>Details</summary>
Motivation: 现代深度模型虽性能强但不透明，存在错误信任、未察觉偏见与意外失败的风险，需一种可扩展、可量化且人类可读的解释方法以揭示模型内部使用的视觉概念和快捷方式。

Method: 对选定层的激活进行聚类（得到聚类中心/ID），将每个输入映射为这些层的聚类ID序列（路径）；用四个指标评估路径：路径复杂度、加权路径纯度、决策对齐保真度与路径一致性；在实验中还结合LLM对最小路径差异进行提示以生成概念描述。

Result: Cluster paths是一种通过对神经网络中间层激活进行聚类、将输入表示为聚类ID序列的后验可解释性方法；评估指标包括路径复杂度、加权路径纯度、决策对齐保真度和路径一致性；在合成偏差任务和CelebA任务中表现良好，并能扩展到大型ViT与基于LLM的概念解释，还能用于OOD检测。

Conclusion: Cluster paths能在多深度发现视觉概念、提供高保真与稳定的解释，并可作为有效的OOD检测器，且在大模型上保持可扩展性与可读性。

Abstract: While modern deep neural networks achieve impressive performance in vision
tasks, they remain opaque in their decision processes, risking unwarranted
trust, undetected biases and unexpected failures. We propose cluster paths, a
post-hoc interpretability method that clusters activations at selected layers
and represents each input as its sequence of cluster IDs. To assess these
cluster paths, we introduce four metrics: path complexity (cognitive load),
weighted-path purity (class alignment), decision-alignment faithfulness
(predictive fidelity), and path agreement (stability under perturbations). In a
spurious-cue CIFAR-10 experiment, cluster paths identify color-based shortcuts
and collapse when the cue is removed. On a five-class CelebA hair-color task,
they achieve 90% faithfulness and maintain 96% agreement under Gaussian noise
without sacrificing accuracy. Scaling to a Vision Transformer pretrained on
ImageNet, we extend cluster paths to concept paths derived from prompting a
large language model on minimal path divergences. Finally, we show that cluster
paths can serve as an effective out-of-distribution (OOD) detector, reliably
flagging anomalous samples before the model generates over-confident
predictions. Cluster paths uncover visual concepts, such as color palettes,
textures, or object contexts, at multiple network depths, demonstrating that
cluster paths scale to large vision models while generating concise and
human-readable explanations.

</details>


### [28] [HSNet: Heterogeneous Subgraph Network for Single Image Super-resolution](https://arxiv.org/abs/2510.06564)
*Qiongyang Hu,Wenyang Liu,Wenbin Zou,Yuejiao Su,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 提出HSNet，一种将全局图分解为可管理子图的超分辨率框架；包含CSSB（生成互补子图集）、SAB（子图特征自适应融合）和NSS（节点采样以减算量）；在重建质量和计算效率上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是克服现有CNN/注意力方法结构僵化与图方法计算复杂度高的矛盾，兼顾图建模灵活性与效率。

Method: 方法包括：1) Constructive Subgraph Set Block (CSSB)生成多样互补的子图以捕捉不同层次和关系；2) Subgraph Aggregation Block (SAB)对各子图特征进行自适应加权融合；3) Node Sampling Strategy (NSS)选择性保留重要节点以降低计算开销。

Result: 大量实验表明HSNet在重建质量与计算效率上达到或优于现有最先进方法；代码将在公开。

Conclusion: HSNet通过子图分解、跨子图自适应聚合和节点采样，在保持计算可行性的同时提升了图建模的表达能力，实现了图像超分辨率任务的SOTA性能。

Abstract: Existing deep learning approaches for image super-resolution, particularly
those based on CNNs and attention mechanisms, often suffer from structural
inflexibility. Although graph-based methods offer greater representational
adaptability, they are frequently impeded by excessive computational
complexity. To overcome these limitations, this paper proposes the
Heterogeneous Subgraph Network (HSNet), a novel framework that efficiently
leverages graph modeling while maintaining computational feasibility. The core
idea of HSNet is to decompose the global graph into manageable sub-components.
First, we introduce the Constructive Subgraph Set Block (CSSB), which generates
a diverse set of complementary subgraphs. Rather than relying on a single
monolithic graph, CSSB captures heterogeneous characteristics of the image by
modeling different relational patterns and feature interactions, producing a
rich ensemble of both local and global graph structures. Subsequently, the
Subgraph Aggregation Block (SAB) integrates the representations embedded across
these subgraphs. Through adaptive weighting and fusion of multi-graph features,
SAB constructs a comprehensive and discriminative representation that captures
intricate interdependencies. Furthermore, a Node Sampling Strategy (NSS) is
designed to selectively retain the most salient features, thereby enhancing
accuracy while reducing computational overhead. Extensive experiments
demonstrate that HSNet achieves state-of-the-art performance, effectively
balancing reconstruction quality with computational efficiency. The code will
be made publicly available.

</details>


### [29] [Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation](https://arxiv.org/abs/2510.06582)
*Fei Zhang,Rob Chancia,Josie Clapp,Amirhossein Hassanzadeh,Dimah Dera,Richard MacKenzie,Jan van Aardt*

Main category: cs.CV

TL;DR: 提出了一个面向TLS点云的半自动、不确定性感知标注流水线：球面投影、多源特征增强、模型集成生成伪标签与不确定性图，并引导针对性标注；反投影到3D生成密集标注点云，并配套三层可视化工具。基于此构建Mangrove3D数据集，研究显示约12个已标注扫描后性能趋于饱和，几何特征最关键，9通道紧凑特征栈几乎保留全部判别力，mIoU约0.76，并在其他数据集上验证了泛化性。


<details>
  <summary>Details</summary>
Motivation: TLS点云语义分割受限于昂贵的人工标注，需一种能在减少标注成本同时维持高精度的可扩展流水线，特别用于生态监测如红树林等场景。

Method: 将3D点云投影到2D球面网格，像素化并用来自多源的特征增强（包括几何、强度等），训练多个分割网络构成集成以生成伪标签与不确定性图；基于不确定性引导人工在高不确定区进行目标化标注；2D结果反投影为3D点标签并用三种可视化界面（2D特征图、3D彩色点云、虚拟紧凑球）辅助快速审核。

Result: 建立了Mangrove3D数据集；发现约12个已标注扫描能使mIoU收敛到约0.76；几何特征贡献最大，紧凑九通道特征几乎不损失性能；方法在ForestSemantic和Semantic3D上展现出良好的泛化性。

Conclusion: 该流水线能显著减少人工标注工作量，同时保持高分割精度；几何特征与紧凑特征组合对分割最有价值；构建的Mangrove3D及工具可推动生态监测中的TLS分割研究。

Abstract: Accurate semantic segmentation of terrestrial laser scanning (TLS) point
clouds is limited by costly manual annotation. We propose a semi-automated,
uncertainty-aware pipeline that integrates spherical projection, feature
enrichment, ensemble learning, and targeted annotation to reduce labeling
effort, while sustaining high accuracy. Our approach projects 3D points to a 2D
spherical grid, enriches pixels with multi-source features, and trains an
ensemble of segmentation networks to produce pseudo-labels and uncertainty
maps, the latter guiding annotation of ambiguous regions. The 2D outputs are
back-projected to 3D, yielding densely annotated point clouds supported by a
three-tier visualization suite (2D feature maps, 3D colorized point clouds, and
compact virtual spheres) for rapid triage and reviewer guidance. Using this
pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove
forests. We further evaluate data efficiency and feature importance to address
two key questions: (1) how much annotated data are needed and (2) which
features matter most. Results show that performance saturates after ~12
annotated scans, geometric features contribute the most, and compact
nine-channel stacks capture nearly all discriminative power, with the mean
Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm
the generalization of our feature-enrichment strategy through cross-dataset
tests on ForestSemantic and Semantic3D.
  Our contributions include: (i) a robust, uncertainty-aware TLS annotation
pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii)
empirical guidance on data efficiency and feature importance, thus enabling
scalable, high-quality segmentation of TLS point clouds for ecological
monitoring and beyond. The dataset and processing scripts are publicly
available at https://fz-rit.github.io/through-the-lidars-eye/.

</details>


### [30] [Improving Artifact Robustness for CT Deep Learning Models Without Labeled Artifact Images via Domain Adaptation](https://arxiv.org/abs/2510.06584)
*Justin Cheung,Samuel Savine,Calvin Nguyen,Lin Lu,Alhassan S. Yasin*

Main category: cs.CV

TL;DR: 使用无标签带状伪影图像的域对抗训练（DANN）可在不需额外标注下，显著提高CT图像分类器对新伪影的稳健性，效果接近有标签训练并对均匀噪声也有迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当CT扫描仪出现训练数据中未包含的新伪影时，深度学习模型性能会显著下降；获得该新分布的标注代价高，域适应提供了无需标注即可恢复鲁棒性的可行替代方案。

Method: 在OrganAMNIST腹部CT数据集上，通过在sinogram空间模拟探测器增益误差生成环状伪影，比较基线（仅干净图像训练）、传统数据增强（其他畸变）与DANN（使用无标签伪影域数据）在带伪影测试集上的分类性能。

Result: Domain-adapted model (DANN) using unlabeled artifact images preserves classification accuracy on CT images with simulated ring artifacts, matching performance of models trained with labeled artifacts and generalizing to uniform noise; baseline and augmentation fail.

Conclusion: 域适应（DANN）能有效应对医学影像中的分布偏移，避免昂贵的专家标注；对抗性训练在存在新伪影时保持分类性能，并可部分泛化到其他噪声类型。

Abstract: Deep learning models which perform well on images from their training
distribution can degrade substantially when applied to new distributions. If a
CT scanner introduces a new artifact not present in the training labels, the
model may misclassify the images. Although modern CT scanners include design
features which mitigate these artifacts, unanticipated or difficult-to-mitigate
artifacts can still appear in practice. The direct solution of labeling images
from this new distribution can be costly. As a more accessible alternative,
this study evaluates domain adaptation as an approach for training models that
maintain classification performance despite new artifacts, even without
corresponding labels. We simulate ring artifacts from detector gain error in
sinogram space and evaluate domain adversarial neural networks (DANN) against
baseline and augmentation-based approaches on the OrganAMNIST abdominal CT
dataset. Our results demonstrate that baseline models trained only on clean
images fail to generalize to images with ring artifacts, and traditional
augmentation with other distortion types provides no improvement on unseen
artifact domains. In contrast, the DANN approach successfully maintains high
classification accuracy on ring artifact images using only unlabeled artifact
data during training, demonstrating the viability of domain adaptation for
artifact robustness. The domain-adapted model achieved classification
performance on ring artifact test data comparable to models explicitly trained
with labeled artifact images, while also showing unexpected generalization to
uniform noise. These findings provide empirical evidence that domain adaptation
can effectively address distribution shift in medical imaging without requiring
expensive expert labeling of new artifact distributions, suggesting promise for
deployment in clinical settings where novel artifacts may emerge.

</details>


### [31] [Ming-UniVision: Joint Image Understanding and Generation with a Unified Continuous Tokenizer](https://arxiv.org/abs/2510.06590)
*Ziyuan Huang,DanDan Zheng,Cheng Zou,Rui Liu,Xiaolong Wang,Kaixiang Ji,Weilong Chai,Jianxin Sun,Libin Wang,Yongjie Lv,Taozhi Huang,Jiajia Liu,Qingpei Guo,Ming Yang,Jingdong Chen,Jun Zhou*

Main category: cs.CV

TL;DR: 提出MingTok，一种连续潜变量的视觉分词器（不离散量化），通过三阶段（低级编码、语义扩展、视觉重建）在生成与理解任务间折中；基于此构建Ming-UniVision，用统一连续空间下的自回归下一 token 预测实现多轮理解、生成与编辑，声称在理解与生成任务上达到SOTA并释放代码与权重。


<details>
  <summary>Details</summary>
Motivation: 当前离散潜码量化带来信息损失，限制语义表达与视觉-语言对齐；需要一种连续表征来兼顾生成与理解的不同需求，从而实现真正统一的视觉自回归模型。

Method: 设计三阶段流水线：1) 低级编码提取紧凑视觉表征；2) 语义扩展将低级码映射到高维语义空间以增强判别能力；3) 视觉重建从扩展的语义码恢复图像。整个流程在连续潜空间内进行并以下一 token 预测为训练目标，构建Ming-UniVision以统一多任务。

Result: 作者声称在理解与生成两类任务上都达到了SOTA级别表现，并发布了推理代码与模型权重以供社区使用。

Conclusion: 连续视觉表示可以同时满足理解任务对高维判别特征与生成任务对低维紧凑编码的需求，MingTok及其上层模型实现了统一自回归框架下的高性能理解与生成。

Abstract: Visual tokenization remains a core challenge in unifying visual understanding
and generation within the autoregressive paradigm. Existing methods typically
employ tokenizers in discrete latent spaces to align with the tokens from large
language models, where the quantization errors can limit semantic
expressiveness and degrade the capability of vision-language understanding. To
address this, we introduce MingTok, a new family of visual tokenizers with a
continuous latent space, for unified autoregressive generation and
understanding. While understanding tasks favor discriminative high-dimensional
features, generation tasks prefer compact low-level codes. Thus, to reconcile
these competing demands, MingTok adopts a three-stage sequential architecture
involving low-level encoding, semantic expansion, and visual reconstruction.
Built on top of it, Ming-UniVision eliminates the need for task-specific visual
representations, and unifies diverse vision-language tasks under a single
autoregrsssive prediction paradigm. By formulating both understanding and
generation as next-token prediction in a shared continuous space, it seamlessly
supports multi-round, in-context tasks such as iterative understanding,
generation and editing. Empirically, we find that using a unified continuous
visual representation reconciles the competing requirements on the tokenizers
by the understanding and generation tasks, thereby leading to state-of-the-art
level performance across both domains. We hope our findings will facilitate
unified visual tokenization in the continuous domain. Inference code and model
weights are released to benefit community.

</details>


### [32] [Adaptive Stain Normalization for Cross-Domain Medical Histology](https://arxiv.org/abs/2510.06592)
*Tianyue Xu,Yanlin Wu,Abhai K. Tripathi,Matthew M. Ippolito,Benjamin D. Haeffele*

Main category: cs.CV

TL;DR: 提出了一种基于Beer-Lambert定律并通过非负矩阵分解算法展开的可训练染色归一化模块（BeerLaNet），可与任意下游网络集成以缓解病理图像的染色域漂移，性能优于多种现有方法。


<details>
  <summary>Details</summary>
Motivation: Mitigate domain shift caused by staining and imaging variability in pathology images without artifacts or template selection required by prior color normalization approaches

Method: Algorithmic unrolling of NMF for stain normalization

Result: A trainable color normalization module (BeerLaNet) derived from Beer-Lambert law and NMF unrolling that extracts stain-invariant structure and integrates with backbones for detection/classification, outperforming state-of-the-art methods on public and malaria datasets

Conclusion: BeerLaNet能有效提取染色不变的结构信息，提升跨域检测与分类性能，代码公开。

Abstract: Deep learning advances have revolutionized automated digital pathology
analysis. However, differences in staining protocols and imaging conditions can
introduce significant color variability. In deep learning, such color
inconsistency often reduces performance when deploying models on data acquired
under different conditions from the training data, a challenge known as domain
shift. Many existing methods attempt to address this problem via color
normalization but suffer from several notable drawbacks such as introducing
artifacts or requiring careful choice of a template image for stain mapping. To
address these limitations, we propose a trainable color normalization model
that can be integrated with any backbone network for downstream tasks such as
object detection and classification. Based on the physics of the imaging
process per the Beer-Lambert law, our model architecture is derived via
algorithmic unrolling of a nonnegative matrix factorization (NMF) model to
extract stain-invariant structural information from the original pathology
images, which serves as input for further processing. Experimentally, we
evaluate the method on publicly available pathology datasets and an internally
curated collection of malaria blood smears for cross-domain object detection
and classification, where our method outperforms many state-of-the-art stain
normalization methods. Our code is available at
https://github.com/xutianyue/BeerLaNet.

</details>


### [33] [SDQM: Synthetic Data Quality Metric for Object Detection Dataset Evaluation](https://arxiv.org/abs/2510.06596)
*Ayush Zenith,Arnold Zumbrun,Neel Raut,Jing Lin*

Main category: cs.CV

TL;DR: SDQM是一种高效可扩展的合成数据质量评估指标，可在训练未收敛时预测数据对YOLOv11性能的影响，与mAP强相关并优于此前指标。


<details>
  <summary>Details</summary>
Motivation: 大型高质量标注数据稀缺，合成数据成为弥补数据不足的可行方案，但缺乏有效的质量评估指标导致合成数据生成与选择过程昂贵且低效，因此需要一个无需等待训练收敛即可评估合成数据质量的指标。

Method: 作者设计了一个在训练早期或中期提取用于评估的数据质量特征并组合成SDQM分数的流程；通过与多种已有指标的对比，展示SDQM在预测最终检测性能方面的优势，并在YOLOv11上进行验证。

Result: SDQM提出了一种在不需训练收敛的情况下评估用于目标检测任务的合成数据质量的指标。作者声称SDQM与YOLOv11的mAP有强相关性，优于现有度量，并能提供可操作的改进建议，从而降低昂贵的迭代训练成本，代码开源。

Conclusion: SDQM能够在资源受限的情况下，通过无需完成训练收敛就评估合成数据质量，从而加速数据生成与选择流程，并提供改进方向，实验证明其与YOLOv11的mAP高度相关。

Abstract: The performance of machine learning models depends heavily on training data.
The scarcity of large-scale, well-annotated datasets poses significant
challenges in creating robust models. To address this, synthetic data generated
through simulations and generative models has emerged as a promising solution,
enhancing dataset diversity and improving the performance, reliability, and
resilience of models. However, evaluating the quality of this generated data
requires an effective metric. This paper introduces the Synthetic Dataset
Quality Metric (SDQM) to assess data quality for object detection tasks without
requiring model training to converge. This metric enables more efficient
generation and selection of synthetic datasets, addressing a key challenge in
resource-constrained object detection tasks. In our experiments, SDQM
demonstrated a strong correlation with the mean Average Precision (mAP) scores
of YOLOv11, a leading object detection model, while previous metrics only
exhibited moderate or weak correlations. Additionally, it provides actionable
insights for improving dataset quality, minimizing the need for costly
iterative training. This scalable and efficient metric sets a new standard for
evaluating synthetic data. The code for SDQM is available at
https://github.com/ayushzenith/SDQM

</details>


### [34] [AIM 2025 Challenge on Real-World RAW Image Denoising](https://arxiv.org/abs/2510.06601)
*Feiran Li,Jiacheng Li,Marcos V. Conde,Beril Besbinar,Vlad Hosu,Daisuke Iso,Radu Timofte*

Main category: cs.CV

TL;DR: 举办旨在推动基于合成数据的相机无关低光RAW图像去噪比赛，使用五台DSLR真实低光噪声图像作为评测集，参赛者需设计噪声合成、网络结构与训练方法，评测基于PSNR/SSIM/LPIPS及无参考指标，目标推动实用稳健模型发展。


<details>
  <summary>Details</summary>
Motivation: 弥补真实低光RAW去噪数据稀缺与相机特性差异问题，通过合成数据与相机无关方法提升去噪模型的鲁棒性与实用性。

Method: 构建含五台DSLR真实低光噪声图像的评测基准，参赛者设计噪声合成流程、网络架构及训练策略，在合成数据上训练并在真实测试集上评估；采用全参考（PSNR/SSIM/LPIPS）与无参考（ARNIQA/TOPIQ）指标综合排名。

Result: Outlined challenge and dataset details; evaluation metrics and goals.

Conclusion: 该挑战通过标准化的真实低光RAW评测与综合指标评估，促进相机无关且训练于合成数据的去噪方法进步，有望推动图像恢复及夜间自动驾驶等应用的发展。

Abstract: We introduce the AIM 2025 Real-World RAW Image Denoising Challenge, aiming to
advance efficient and effective denoising techniques grounded in data
synthesis. The competition is built upon a newly established evaluation
benchmark featuring challenging low-light noisy images captured in the wild
using five different DSLR cameras. Participants are tasked with developing
novel noise synthesis pipelines, network architectures, and training
methodologies to achieve high performance across different camera models.
Winners are determined based on a combination of performance metrics, including
full-reference measures (PSNR, SSIM, LPIPS), and non-reference ones (ARNIQA,
TOPIQ). By pushing the boundaries of camera-agnostic low-light RAW image
denoising trained on synthetic data, the competition promotes the development
of robust and practical models aligned with the rapid progress in digital
photography. We expect the competition outcomes to influence multiple domains,
from image restoration to night-time autonomous driving.

</details>


### [35] [Self-supervised Physics-guided Model with Implicit Representation Regularization for Fast MRI Reconstruction](https://arxiv.org/abs/2510.06611)
*Jingran Xu,Yuanyuan Liu,Yanjie Zhu*

Main category: cs.CV

TL;DR: UnrollINR is a scan-specific, zero-shot self-supervised MRI reconstruction framework combining physics-guided unrolling with implicit neural representations, outperforming supervised baselines at high acceleration rates


<details>
  <summary>Details</summary>
Motivation: Reduce MRI scan times and avoid reliance on fully sampled external training data by enabling scan-specific reconstruction from undersampled k-space via self-supervision and implicit priors

Method: UnrollINR: zero-shot self-supervised scan-specific MRI reconstruction using physics-guided unrolled architecture and implicit neural representation prior

Result: Proposed UnrollINR integrates INR as regularizer within an unrolled iterative reconstruction, achieving superior reconstruction quality even at 10x acceleration compared to supervised methods

Conclusion: Integrating INR priors into physics-driven unrolled networks enables effective zero-shot MRI reconstruction without external data, improving interpretability and performance.

Abstract: Magnetic Resonance Imaging (MRI) is a vital clinical diagnostic tool, yet its
widespread application is limited by prolonged scan times. Fast MRI
reconstruction techniques effectively reduce acquisition duration by
reconstructing high-fidelity MR images from undersampled k-space data. In
recent years, deep learning-based methods have demonstrated remarkable progress
in this field, with self-supervised and unsupervised learning approaches
proving particularly valuable in scenarios where fully sampled data are
difficult to obtain. This paper proposes a novel zero-shot self-supervised
reconstruction framework named UnrollINR, which enables scan-specific MRI
reconstruction without relying on external training data. The method adopts a
physics-guided unrolled iterative reconstruction architecture and introduces
Implicit Neural Representation (INR) as a regularization prior to effectively
constrain the solution space. By combining a deep unrolled structure with the
powerful implicit representation capability of INR, the model's
interpretability and reconstruction performance are enhanced. Experimental
results demonstrate that even at a high acceleration rate of 10, UnrollINR
achieves superior reconstruction performance compared to the supervised
learning method, validating the superiority of the proposed method.

</details>


### [36] [A Bridge from Audio to Video: Phoneme-Viseme Alignment Allows Every Face to Speak Multiple Languages](https://arxiv.org/abs/2510.06612)
*Zibo Su,Kun Wei,Jiahua Li,Xu Yang,Cheng Deng*

Main category: cs.CV

TL;DR: 用音素和唇形作为跨模态中介，结合PG-MoE与PV-Align，并构建MTFB多语数据集，实现多语和零样本口型合成的显著提升。


<details>
  <summary>Details</summary>
Motivation: Existing TFS models are English-centric, causing poor cross-language performance due to dataset bias and lack of universal cross-modal features

Method: Phoneme-guided mixture-of-experts with phoneme-viseme alignment

Result: Proposed MuEx framework with PG-MoE and PV-Align, plus MTFB dataset; achieves superior multilingual and zero-shot performance

Conclusion: MuEx有效缓解语言偏差，通过音素-唇形对齐和专家网络提升多语音驱动口型合成质量与泛化能力。

Abstract: Speech-driven talking face synthesis (TFS) focuses on generating lifelike
facial animations from audio input. Current TFS models perform well in English
but unsatisfactorily in non-English languages, producing wrong mouth shapes and
rigid facial expressions. The terrible performance is caused by the
English-dominated training datasets and the lack of cross-language
generalization abilities. Thus, we propose Multilingual Experts (MuEx), a novel
framework featuring a Phoneme-Guided Mixture-of-Experts (PG-MoE) architecture
that employs phonemes and visemes as universal intermediaries to bridge audio
and video modalities, achieving lifelike multilingual TFS. To alleviate the
influence of linguistic differences and dataset bias, we extract audio and
video features as phonemes and visemes respectively, which are the basic units
of speech sounds and mouth movements. To address audiovisual synchronization
issues, we introduce the Phoneme-Viseme Alignment Mechanism (PV-Align), which
establishes robust cross-modal correspondences between phonemes and visemes. In
addition, we build a Multilingual Talking Face Benchmark (MTFB) comprising 12
diverse languages with 95.04 hours of high-quality videos for training and
evaluating multilingual TFS performance. Extensive experiments demonstrate that
MuEx achieves superior performance across all languages in MTFB and exhibits
effective zero-shot generalization to unseen languages without additional
training.

</details>


### [37] [MSITrack: A Challenging Benchmark for Multispectral Single Object Tracking](https://arxiv.org/abs/2510.06619)
*Tao Feng,Tingfa Xu,Haolin Qin,Tianhao Li,Shuaihao Han,Xuyang Zou,Zhan Lv,Jianan Li*

Main category: cs.CV

TL;DR: MSITrack: largest multispectral single-object tracking dataset (300 videos, 129k+ frames, 55 categories) with challenging attributes and rich scenes; improves tracking performance over RGB-only methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of RGB-only tracking by leveraging multispectral imagery to improve target discriminability and represent real-world challenges

Method: Dataset creation and evaluation

Result: MSITrack dataset with 300 videos, >129k frames, 55 categories, 300 scenes; careful annotation and verification; experiments show multispectral improves over RGB baselines

Conclusion: MSITrack fills a dataset gap for multispectral tracking and can drive future research; dataset publicly available.

Abstract: Visual object tracking in real-world scenarios presents numerous challenges
including occlusion, interference from similar objects and complex
backgrounds-all of which limit the effectiveness of RGB-based trackers.
Multispectral imagery, which captures pixel-level spectral reflectance,
enhances target discriminability. However, the availability of multispectral
tracking datasets remains limited. To bridge this gap, we introduce MSITrack,
the largest and most diverse multispectral single object tracking dataset to
date. MSITrack offers the following key features: (i) More Challenging
Attributes-including interference from similar objects and similarity in color
and texture between targets and backgrounds in natural scenarios, along with a
wide range of real-world tracking challenges; (ii) Richer and More Natural
Scenes-spanning 55 object categories and 300 distinct natural scenes, MSITrack
far exceeds the scope of existing benchmarks. Many of these scenes and
categories are introduced to the multispectral tracking domain for the first
time; (iii) Larger Scale-300 videos comprising over 129k frames of
multispectral imagery. To ensure annotation precision, each frame has undergone
meticulous processing, manual labeling and multi-stage verification. Extensive
evaluations using representative trackers demonstrate that the multispectral
data in MSITrack significantly improves performance over RGB-only baselines,
highlighting its potential to drive future advancements in the field. The
MSITrack dataset is publicly available at:
https://github.com/Fengtao191/MSITrack.

</details>


### [38] [StaR-KVQA: Structured Reasoning Traces for Implicit-Knowledge Visual Question Answering](https://arxiv.org/abs/2510.06638)
*Zhihao Wen,Wenkang Wei,Yuan Fang,Xingtong Yu,Hui Zhang,Weicheng Zhu,Xin Zhang*

Main category: cs.CV

TL;DR: 提出StaR-KVQA：基于离线构建的路径化推理轨迹对开源MLLM进行结构化自蒸馏，无需外部检索或知识库，实现更准且可验证的推理。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM作为唯一知识源时缺乏显式推理监督，产生不一致或不可验证的解释，且SFT后泛化能力差，需要透明可验证的推理监督。

Method: 离线构建双向符号关系路径和路径映射的自然语言解释，选择高质量轨迹构成带轨迹的数据集，对开源MLLM做结构化自蒸馏，使生成对齐到监督轨迹，推理时只需一次自回归生成。

Result: StaR-KVQA通过监督结构化推理轨迹（符号关系路径+路径驱动的自然语言解释），提高隐式知识型KVQA上MLLM的可解释性和准确性。

Conclusion: 在多个基准上，StaR-KVQA在准确率和可解释性上均优于强基线，在OK-VQA上最高提升11.3%，且具备良好跨域泛化能力。

Abstract: Knowledge-based Visual Question Answering (KVQA) requires models to ground
entities in images and reason over factual knowledge. We study its
implicit-knowledge variant, IK-KVQA, where a multimodal large language model
(MLLM) is the sole knowledge source, without external retrieval. Yet, MLLMs
lack explicit reasoning supervision and produce inconsistent justifications,
and generalize poorly after standard supervised fine-tuning (SFT). We present
StaR-KVQA (Structured Reasoning Traces for IK-KVQA), which supervises
structured traces - dual symbolic relation paths plus path-grounded
natural-language explanations - so that reasoning becomes transparent and
verifiable. With one open-source MLLM, StaR-KVQA constructs and selects
path-grounded reasoning traces to form a trace-enriched dataset, then
fine-tunes via structured self-distillation to align generation with
supervision; no external retrievers, verifiers, or curated knowledge bases
(KBs) are used, traces are built offline, and inference is a single
autoregressive pass. Across benchmarks, StaR-KVQA improves both accuracy and
interpretability, achieving up to +11.3% higher answer accuracy on OK-VQA over
the strongest baseline while exhibiting robust cross-domain generalization.

</details>


### [39] [Automated Neural Architecture Design for Industrial Defect Detection](https://arxiv.org/abs/2510.06669)
*Yuxi Liu,Yunfeng Ma,Yi Tang,Min Liu,Shuai Jiang,Yaonan Wang*

Main category: cs.CV

TL;DR: AutoNAD通过混合算子搜索、跨权重共享与可搜索的多尺度聚合模块，自动设计兼顾准确率与运行效率的缺陷检测网络，提升了对类内差异与类间相似性的处理能力。


<details>
  <summary>Details</summary>
Motivation: 工业表面缺陷形状和尺寸多样，导致类内差异大且类间相似度高；手工设计模型耗时且难以同时兼顾细粒度局部与长距离语义信息，因此需要自动化且能混合不同算子的设计方法来提升检测性能与部署效率。

Method: 方法包括：1) 在搜索空间中联合搜索卷积、Transformer、MLP三类算子以捕捉局部细节与全局语义；2) 采用跨权重共享策略加速超网收敛并提升子网性能；3) 设计可搜索的多层次特征聚合模块（MFAM）以增强多尺度学习；4) 引入延迟感知先验以优化运行时效率与部署速度。

Result: AutoNAD提出了一个用于工业表面缺陷检测的自动神经网络结构设计框架，联合搜索卷积、Transformer和MLP三种算子，并引入跨权重共享策略与可搜索的多层次特征聚合模块（MFAM），同时考虑延迟先验以兼顾运行效率。实验在三个工业缺陷数据集上验证了方法有效性，并已在缺陷成像检测平台中应用，代码将开源。

Conclusion: AutoNAD能在保持运行效率的前提下，通过自动化搜索混合算子与多层次特征聚合，提升工业表面缺陷检测性能，并降低手工设计成本。

Abstract: Industrial surface defect detection (SDD) is critical for ensuring product
quality and manufacturing reliability. Due to the diverse shapes and sizes of
surface defects, SDD faces two main challenges: intraclass difference and
interclass similarity. Existing methods primarily utilize manually designed
models, which require extensive trial and error and often struggle to address
both challenges effectively. To overcome this, we propose AutoNAD, an automated
neural architecture design framework for SDD that jointly searches over
convolutions, transformers, and multi-layer perceptrons. This hybrid design
enables the model to capture both fine-grained local variations and long-range
semantic context, addressing the two key challenges while reducing the cost of
manual network design. To support efficient training of such a diverse search
space, AutoNAD introduces a cross weight sharing strategy, which accelerates
supernet convergence and improves subnet performance. Additionally, a
searchable multi-level feature aggregation module (MFAM) is integrated to
enhance multi-scale feature learning. Beyond detection accuracy, runtime
efficiency is essential for industrial deployment. To this end, AutoNAD
incorporates a latency-aware prior to guide the selection of efficient
architectures. The effectiveness of AutoNAD is validated on three industrial
defect datasets and further applied within a defect imaging and detection
platform. Code will be available at https://github.com/Yuxi104/AutoNAD.

</details>


### [40] [Heptapod: Language Modeling on Visual Signals](https://arxiv.org/abs/2510.06673)
*Yongxin Zhu,Jiawei Chen,Yuanzhe Chen,Zhuo Chen,Dongya Jia,Jian Cong,Xiaobin Zhuang,Yuping Wang,Yuxuan Wang*

Main category: cs.CV

TL;DR: Heptapod applies causal attention and a reconstruction-focused visual tokenizer to predict 2D spatial distributions autoregressively, merging benefits of autoregression and masked reconstruction; yields FID 2.70 on ImageNet.


<details>
  <summary>Details</summary>
Motivation: Bring language-modeling principles (causal attention, no CFG, no semantic tokenizers) to image generation, unifying autoregressive sequential modeling and masked autoencoding to capture full image semantics.

Method: Next 2D distribution prediction with causal attention and reconstruction-focused tokenizer

Result: Heptapod, an autoregressive model predicting distributions over 2D image grids per timestep, achieves state-of-the-art ImageNet generation FID 2.70, outperforming prior causal autoregressive methods.

Conclusion: Reframes visual language modeling by using causal Transformers and next-2D-distribution objective without CFG or semantic tokenizers, suggesting a principled path for visual generative modeling.

Abstract: We introduce Heptapod, an image autoregressive model that adheres to the
foundational principles of language modeling. Heptapod employs \textbf{causal
attention}, \textbf{eliminates reliance on CFG}, and \textbf{eschews the trend
of semantic tokenizers}. Our key innovation is \textit{next 2D distribution
prediction}: a causal Transformer with reconstruction-focused visual tokenizer,
learns to predict the distribution over the entire 2D spatial grid of images at
each timestep. This learning objective unifies the sequential modeling of
autoregressive framework with the holistic self-supervised learning of masked
autoencoding, enabling the model to capture comprehensive image semantics via
generative training. On the ImageNet generation benchmark, Heptapod achieves an
FID of $2.70$, significantly outperforming previous causal autoregressive
approaches. We hope our work inspires a principled rethinking of language
modeling on visual signals and beyond.

</details>


### [41] [DreamOmni2: Multimodal Instruction-based Editing and Generation](https://arxiv.org/abs/2510.06679)
*Bin Xia,Bohao Peng,Yuechen Zhang,Junjia Huang,Jiyang Liu,Jingyao Li,Haoru Tan,Sitong Wu,Chengyao Wang,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 本论文提出了两项新任务：多模态指令驱动的图像编辑与生成（支持文本+图像指令，涵盖具体与抽象概念），并构建了DreamOmni2系统，通过数据合成流水线与模型框架改进解决数据与多图输入问题，同时给出基准评测。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑仅依赖文本，难以精确描述编辑细节需参考图像；而基于主体的生成受限于具体实体，缺乏抽象概念建模。为提升实用性，提出支持文本+图像指令且覆盖具体与抽象概念的多模态任务。

Method: 方法包括三步数据合成：1) 特征混合生成用于抽象与具体概念的提取数据；2) 用编辑与提取模型生成多模态指令编辑训练数据；3) 再用提取模型扩展训练数据。模型方面引入索引编码与位置编码位移以区分多张输入图像并避免像素混淆，同时与视觉语言模型联合训练以更好理解复杂指令。

Result: 实验显示DreamOmni2在所设立的多模态指令编辑与生成基准上取得显著性能提升，表明该方法在处理复杂多图输入与抽象概念方面有效。

Conclusion: DreamOmni2通过特征混合的数据合成、索引编码与位置编码偏移的多图输入方案，以及与VLM的联合训练，显著提升了多模态指令编辑与生成的能力，并在新建基准上表现优异，推动此类任务向更实用方向发展。

Abstract: Recent advancements in instruction-based image editing and subject-driven
generation have garnered significant attention, yet both tasks still face
limitations in meeting practical user needs. Instruction-based editing relies
solely on language instructions, which often fail to capture specific editing
details, making reference images necessary. Meanwhile, subject-driven
generation is limited to combining concrete objects or people, overlooking
broader, abstract concepts. To address these challenges, we propose two novel
tasks: multimodal instruction-based editing and generation. These tasks support
both text and image instructions and extend the scope to include both concrete
and abstract concepts, greatly enhancing their practical applications. We
introduce DreamOmni2, tackling two primary challenges: data creation and model
framework design. Our data synthesis pipeline consists of three steps: (1)
using a feature mixing method to create extraction data for both abstract and
concrete concepts, (2) generating multimodal instruction-based editing training
data using the editing and extraction models, and (3) further applying the
extraction model to create training data for multimodal instruction-based
editing. For the framework, to handle multi-image input, we propose an index
encoding and position encoding shift scheme, which helps the model distinguish
images and avoid pixel confusion. Additionally, we introduce joint training
with the VLM and our generation/editing model to better process complex
instructions. In addition, we have proposed comprehensive benchmarks for these
two new tasks to drive their development. Experiments show that DreamOmni2 has
achieved impressive results. Models and codes will be released.

</details>


### [42] [Semantic Segmentation Algorithm Based on Light Field and LiDAR Fusion](https://arxiv.org/abs/2510.06687)
*Jie Luo,Yuxuan Jiang,Xin Jin,Mingyu Liu,Yihui Fan*

Main category: cs.CV

TL;DR: 构建首个光场+点云分割数据集，提出Mlpfseg网络通过特征补全与深度感知增强模态融合，在语义分割上优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶场景下，光场提供视差信息、LiDAR提供稠密深度点，两者互补，但因视角有限和模态差异难以融合，且遮挡导致分割困难，故提出多模态数据集与融合网络改善鲁棒性。

Method: 构建光场+点云数据集；设计Mlpfseg网络，包含特征补全模块（对点云特征图进行差别重建以弥补像素密度差异）和深度感知模块（基于深度信息调整注意力以提升遮挡感知）；同时对图像与点云进行联合分割训练与推理。

Result: 提出了首个将光场与点云融合用于语义分割的数据集，并设计了多模态融合网络Mlpfseg。该网络包含特征补全模块（解决点云与像素密度不匹配）和深度感知模块（提高遮挡物体分割表现）。在该数据集上，方法相比仅用图像提升1.71 mIoU，相比仅用点云提升2.38 mIoU。

Conclusion: 将光场与点云联合用于语义分割，并通过差异重建的特征补全与关注度增强的深度感知模块，有效改善了遮挡与密度不匹配问题，带来明显mIoU提升。

Abstract: Semantic segmentation serves as a cornerstone of scene understanding in
autonomous driving but continues to face significant challenges under complex
conditions such as occlusion. Light field and LiDAR modalities provide
complementary visual and spatial cues that are beneficial for robust
perception; however, their effective integration is hindered by limited
viewpoint diversity and inherent modality discrepancies. To address these
challenges, the first multimodal semantic segmentation dataset integrating
light field data and point cloud data is proposed. Based on this dataset, we
proposed a multi-modal light field point-cloud fusion segmentation
network(Mlpfseg), incorporating feature completion and depth perception to
segment both camera images and LiDAR point clouds simultaneously. The feature
completion module addresses the density mismatch between point clouds and image
pixels by performing differential reconstruction of point-cloud feature maps,
enhancing the fusion of these modalities. The depth perception module improves
the segmentation of occluded objects by reinforcing attention scores for better
occlusion awareness. Our method outperforms image-only segmentation by 1.71
Mean Intersection over Union(mIoU) and point cloud-only segmentation by 2.38
mIoU, demonstrating its effectiveness.

</details>


### [43] [SCas4D: Structural Cascaded Optimization for Boosting Persistent 4D Novel View Synthesis](https://arxiv.org/abs/2510.06694)
*Jipeng Lyu,Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: SCas4D通过层次化的高斯簇变形优化，实现快速收敛和高效动态场景建模，显著减少训练迭代并兼顾跟踪与新视角合成。


<details>
  <summary>Details</summary>
Motivation: 动态场景建模需同时获得精准变形与计算效率。观察到真实物体变形往往呈层次性和群组一致性，可通过利用该结构性降低优化难度与训练时间，同时支持跟踪和新视角合成。

Method: 构建基于3D Gaussian Splatting的级联优化流程：先对部件级（group/part）进行粗变形求解，再逐步细化到点级（individual Gaussians）；在每一阶段用相应的正则化和一致性约束保持结构连贯；通过共享变换假设减少参数维度，从而实现快速收敛。

Result: SCas4D提出了一种级联优化框架，利用3D Gaussian Splatting中的结构模式，通过从粗到细逐步优化变形（从部件级到点级），在每帧约100次迭代内收敛，训练迭代次数显著少于现有方法（约1/20），并在自监督关节化物体分割、视角合成和稠密点跟踪上表现良好。

Conclusion: 利用现实中变形的层次性，共享变换的高斯群组能加速优化并保持质量，SCas4D在效率和多任务应用上优于或可比现有方法。

Abstract: Persistent dynamic scene modeling for tracking and novel-view synthesis
remains challenging due to the difficulty of capturing accurate deformations
while maintaining computational efficiency. We propose SCas4D, a cascaded
optimization framework that leverages structural patterns in 3D Gaussian
Splatting for dynamic scenes. The key idea is that real-world deformations
often exhibit hierarchical patterns, where groups of Gaussians share similar
transformations. By progressively refining deformations from coarse part-level
to fine point-level, SCas4D achieves convergence within 100 iterations per time
frame and produces results comparable to existing methods with only
one-twentieth of the training iterations. The approach also demonstrates
effectiveness in self-supervised articulated object segmentation, novel view
synthesis, and dense point tracking tasks.

</details>


### [44] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 针对历史OCR的评估框架：引入HCPR与AIR、污染控制和稳定性测试；评估12个多模态LLM，发现模型优于传统OCR但存在过度古化，后处理有害。


<details>
  <summary>Details</summary>
Motivation: 数字人文研究者在用大模型进行历史文献数字化时，缺乏能捕捉年代偏差和时期特有错误的评估框架，传统OCR指标不能反映这些关键问题，从而影响历史语料库构建质量。

Method: 提出污染控制与稳定性测试协议，设计并计算新的指标（HCPR、AIR），在18世纪俄文Civil字体文本上对12个多模态大模型进行评估，比较模型与传统OCR的表现，并分析后处理校正的影响。

Result: Gemini与Qwen系列多模态模型在整体字符识别上优于传统OCR，但表现出将不同时期的古体字符错误插入（过度古化），且后处理校正未能提升反而降低总体质量。提出的方法为模型选择与质量评估提供了实践指南。

Conclusion: 本文提出了一套针对基于大模型的历史文献OCR的评估方法，揭示了传统指标对年代偏差和时期特定错误的不足，并引入了新的衡量指标以量化历史字符保真性和古体插入问题。实验显示部分多模态大模型优于传统OCR，但存在“过度古化”问题，且后续校正反而降低性能。

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


### [45] [DeRainMamba: A Frequency-Aware State Space Model with Detail Enhancement for Image Deraining](https://arxiv.org/abs/2510.06746)
*Zhiliang Zhu,Tao Zeng,Tao Yang,Guoliang Luo,Jiyong Zeng*

Main category: cs.CV

TL;DR: 提出DeRainMamba：在状态空间框架中加入频域感知模块与多向卷积以兼顾雨去除与细节保留，性能更好且更高效。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在序列建模上高效但难以捕获细粒度细节且缺乏频域意识，限制了单幅图像去雨性能。

Method: 提出将FASSM（利用傅里叶变换在频域分离雨条纹与高频细节）嵌入Mamba型状态空间框架，并设计MDPConv以多分支卷积捕获各向异性梯度特征并高效融合。

Result: 在四个公开基准上，DeRainMamba在PSNR和SSIM上稳定超越最先进方法，同时参数更少、计算开销更低。

Conclusion: DeRainMamba有效地结合频域感知的状态空间模块与多向感知卷积，在保持细节的同时更精确地去除雨条纹，实验结果显示在PSNR和SSIM上优于现有方法且参数和计算量更低。

Abstract: Image deraining is crucial for improving visual quality and supporting
reliable downstream vision tasks. Although Mamba-based models provide efficient
sequence modeling, their limited ability to capture fine-grained details and
lack of frequency-domain awareness restrict further improvements. To address
these issues, we propose DeRainMamba, which integrates a Frequency-Aware
State-Space Module (FASSM) and Multi-Directional Perception Convolution
(MDPConv). FASSM leverages Fourier transform to distinguish rain streaks from
high-frequency image details, balancing rain removal and detail preservation.
MDPConv further restores local structures by capturing anisotropic gradient
features and efficiently fusing multiple convolution branches. Extensive
experiments on four public benchmarks demonstrate that DeRainMamba consistently
outperforms state-of-the-art methods in PSNR and SSIM, while requiring fewer
parameters and lower computational costs. These results validate the
effectiveness of combining frequency-domain modeling and spatial detail
enhancement within a state-space framework for single image deraining.

</details>


### [46] [OBS-Diff: Accurate Pruning For Diffusion Models in One-Shot](https://arxiv.org/abs/2510.06751)
*Junhan Zhu,Hesong Wang,Mingluo Su,Zefang Wang,Huan Wang*

Main category: cs.CV

TL;DR: OBS-Diff扩展了Optimal Brain Surgeon到扩散模型，使用时间步加权的Hessian和分组顺序剪枝，实现训练免费的一次性高效剪枝


<details>
  <summary>Details</summary>
Motivation: 降低大规模文本到图像扩散模型的计算开销，使得无需训练即可进行高效压缩

Method: 将OBS拓展到复杂扩散模型架构，支持非结构化、N:M和结构化剪枝；提出基于误差累积视角的时间步长加权Hessian（对早期步重视）；并设计了高效的分组顺序剪枝以降低标定成本。

Result: 提出OBS-Diff，一种适配扩散模型的one-shot剪枝框架，支持多种稀疏形式并引入时间步长感知的Hessian构建与分组顺序剪枝，达到在微小画质损失下显著加速推理

Conclusion: OBS-Diff在无需额外训练的条件下，为大规模文本到图像扩散模型提供了兼顾压缩率与视觉质量的高效剪枝方案，适用多种稀疏结构并在实验中表现出色

Abstract: Large-scale text-to-image diffusion models, while powerful, suffer from
prohibitive computational cost. Existing one-shot network pruning methods can
hardly be directly applied to them due to the iterative denoising nature of
diffusion models. To bridge the gap, this paper presents OBS-Diff, a novel
one-shot pruning framework that enables accurate and training-free compression
of large-scale text-to-image diffusion models. Specifically, (i) OBS-Diff
revitalizes the classic Optimal Brain Surgeon (OBS), adapting it to the complex
architectures of modern diffusion models and supporting diverse pruning
granularity, including unstructured, N:M semi-structured, and structured (MHA
heads and FFN neurons) sparsity; (ii) To align the pruning criteria with the
iterative dynamics of the diffusion process, by examining the problem from an
error-accumulation perspective, we propose a novel timestep-aware Hessian
construction that incorporates a logarithmic-decrease weighting scheme,
assigning greater importance to earlier timesteps to mitigate potential error
accumulation; (iii) Furthermore, a computationally efficient group-wise
sequential pruning strategy is proposed to amortize the expensive calibration
process. Extensive experiments show that OBS-Diff achieves state-of-the-art
one-shot pruning for diffusion models, delivering inference acceleration with
minimal degradation in visual quality.

</details>


### [47] [Transforming Noise Distributions with Histogram Matching: Towards a Single Denoiser for All](https://arxiv.org/abs/2510.06757)
*Sheng Fu,Junchao Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: Use histogram-based transformations to map diverse noises to a known Gaussian, and iterate denoise-and-transform in a reinforcing cycle to improve generalization of a single Gaussian denoiser to many OOD noises.


<details>
  <summary>Details</summary>
Motivation: Supervised Gaussian denoisers fail on out-of-distribution noises; transforming arbitrary noise into a Gaussian distribution of known intensity allows reuse of a single trained Gaussian denoiser for many noise types.

Method: Apply local histogram matching for signal-dependent noise, intrapatch permutation for channel-related noise, frequency-domain histogram matching with pixel-shuffle down-sampling to break spatial correlation; then iteratively apply noise transformation and Gaussian denoising in a mutually reinforcing cycle.

Result: The paper proposes converting arbitrary noise into a Gaussian distribution via histogram matching, then using a supervised Gaussian denoiser in a cycle with iterative refinement to handle OOD noises. They address signal-dependent, channel-correlated, and spatially-correlated noises with local histogram matching, intrapatch permutation, and frequency-domain matching plus pixel-shuffle down-sampling. Experiments show strong generalization to synthetic and real-world noises.

Conclusion: Histogram matching and iterative denoise-transform cycle enable a single supervised Gaussian denoiser to generalize to diverse out-of-distribution noises, validated on synthetic and real datasets.

Abstract: Supervised Gaussian denoisers exhibit limited generalization when confronted
with out-of-distribution noise, due to the diverse distributional
characteristics of different noise types. To bridge this gap, we propose a
histogram matching approach that transforms arbitrary noise towards a target
Gaussian distribution with known intensity. Moreover, a mutually reinforcing
cycle is established between noise transformation and subsequent denoising.
This cycle progressively refines the noise to be converted, making it
approximate the real noise, thereby enhancing the noise transformation effect
and further improving the denoising performance. We tackle specific noise
complexities: local histogram matching handles signal-dependent noise,
intrapatch permutation processes channel-related noise, and frequency-domain
histogram matching coupled with pixel-shuffle down-sampling breaks spatial
correlation. By applying these transformations, a single Gaussian denoiser
gains remarkable capability to handle various out-of-distribution noises,
including synthetic noises such as Poisson, salt-and-pepper and repeating
pattern noises, as well as complex real-world noises. Extensive experiments
demonstrate the superior generalization and effectiveness of our method.

</details>


### [48] [A deep multiple instance learning approach based on coarse labels for high-resolution land-cover mapping](https://arxiv.org/abs/2510.06769)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出一种基于DMIL和PUL的弱监督高分辨率土地覆盖分类方法，通过可学习池化层将高分辨率像素语义与低分辨率标注关联，实现像素级分类器在无直接像素标签下的训练，实验证明优于常规模型。


<details>
  <summary>Details</summary>
Motivation: Obtain large quantities of weak labels from low-resolution or obsolete products to train high-resolution land-cover classifiers without costly pixel-wise annotations

Method: Deep Multiple Instance Learning with Positive-Unlabeled strategy for weakly supervised land-cover mapping

Result: A DMIL-based framework that trains pixel-level multi-class classifiers using flexible pooling to connect high-res pixels to low-res labels; reframes MIL in multi-class and multi-label settings and uses PUL in multi-label case; shows improved performance on 2020 IEEE GRSS DFC dataset

Conclusion: 该方法能有效利用低分辨率弱标签训练高分辨率像素级分类器，在多类和多标签MIL设置下均表现良好，特别是结合PUL策略解决低分辨率单标签信息的不足。

Abstract: The quantity and the quality of the training labels are central problems in
high-resolution land-cover mapping with machine-learning-based solutions. In
this context, weak labels can be gathered in large quantities by leveraging on
existing low-resolution or obsolete products. In this paper, we address the
problem of training land-cover classifiers using high-resolution imagery (e.g.,
Sentinel-2) and weak low-resolution reference data (e.g., MODIS -derived
land-cover maps). Inspired by recent works in Deep Multiple Instance Learning
(DMIL), we propose a method that trains pixel-level multi-class classifiers and
predicts low-resolution labels (i.e., patch-level classification), where the
actual high-resolution labels are learned implicitly without direct
supervision. This is achieved with flexible pooling layers that are able to
link the semantics of the pixels in the high-resolution imagery to the
low-resolution reference labels. Then, the Multiple Instance Learning (MIL)
problem is re-framed in a multi-class and in a multi-label setting. In the
former, the low-resolution annotation represents the majority of the pixels in
the patch. In the latter, the annotation only provides us information on the
presence of one of the land-cover classes in the patch and thus multiple labels
can be considered valid for a patch at a time, whereas the low-resolution
labels provide us only one label. Therefore, the classifier is trained with a
Positive-Unlabeled Learning (PUL) strategy. Experimental results on the 2020
IEEE GRSS Data Fusion Contest dataset show the effectiveness of the proposed
framework compared to standard training strategies.

</details>


### [49] [TTRV: Test-Time Reinforcement Learning for Vision Language Models](https://arxiv.org/abs/2510.06783)
*Akshit Singh,Shyam Marjit,Wei Lin,Paul Gavrikov,Serena Yeung-Levy,Hilde Kuehne,Rogerio Feris,Sivan Doveh,James Glass,M. Jehanzeb Mirza*

Main category: cs.CV

TL;DR: 提出TTRV：一种在测试时无需标签的强化学习自适应方法，通过基于输出频率和低熵奖励在GRPO上进行多次推理，显著提升VLM的识别与VQA性能，且在多项基准上达到或超过专有模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有奖励提取方法依赖标注和专门训练划分，违背人类从环境直接学习的方式；提出无标签的测试时自适应方法以更贴近这种学习方式并提升VLM泛化与稳健性。

Method: 在GRPO框架上进行扩展：对每个测试样本重复推理多次，根据基线模型输出的频率设计正向奖励，并同时用输出经验分布的低熵作为奖励以控制输出多样性，实现无监督的测试时强化学习适配。

Result: 在16个数据集上平均提升对象识别24.6%和VQA 10.0%，单项最大提升分别达52.4%与29.8%；在图像识别上对InternVL 8B的应用平均超越GPT-4o 2.3%，并在极端数据受限（单个无标签样本）场景下仍有最高5.5%的提升。

Conclusion: TTRV在无标签测试时通过基于输出频率和熵的奖励对视觉语言模型（VLM）进行实时自适应，从而显著提升了对象识别和VQA性能，且在部分基准上超越了最强的专有模型。

Abstract: Existing methods for extracting reward signals in Reinforcement Learning
typically rely on labeled data and dedicated training splits, a setup that
contrasts with how humans learn directly from their environment. In this work,
we propose TTRV to enhance vision language understanding by adapting the model
on the fly at inference time, without the need for any labeled data.
Concretely, we enhance the Group Relative Policy Optimization (GRPO) framework
by designing rewards based on the frequency of the base model's output, while
inferring on each test sample multiple times. Further, we also propose to
control the diversity of the model's output by simultaneously rewarding the
model for obtaining low entropy of the output empirical distribution. Our
approach delivers consistent gains across both object recognition and visual
question answering (VQA), with improvements of up to 52.4% and 29.8%,
respectively, and average boosts of 24.6% and 10.0% across 16
datasets.Remarkably, on image recognition, TTRV applied to InternVL 8B
surpasses GPT-4o by an average of 2.3% over 8 benchmarks, while remaining
highly competitive on VQA, demonstrating that test-time reinforcement learning
can match or exceed the strongest proprietary models. Finally, we find many
interesting properties of test-time RL for VLMs: for example, even in extremely
data-constrained scenarios, where adaptation is performed on a single randomly
chosen unlabeled test example, TTRV still yields non-trivial improvements of up
to 5.5% in recognition tasks.

</details>


### [50] [Extreme Amodal Face Detection](https://arxiv.org/abs/2510.06791)
*Changlin Song,Yunzhong Hou,Michael Randall Barnes,Rahul Shome,Dylan Campbell*

Main category: cs.CV

TL;DR: 提出一种基于热图的单图极端非完整检测方法，利用上下文和选择性粗到细解码器高效预测画外人脸位置，在新任务上表现优异，优于部分生成式方法。


<details>
  <summary>Details</summary>
Motivation: Introduce extreme amodal detection for predicting object locations outside image (expanded FOV) with focus on faces due to safety/privacy, addressing limitations of sequence-based or generative methods by proposing single-image method.

Method: Design a heatmap-based detector that predicts object centers and extents beyond image bounds using contextual cues; employs selective coarse-to-fine decoder to focus computation and refine predictions, avoiding sampling/generative approaches.

Result: Propose a heatmap-based extreme amodal face detector using contextual cues and selective coarse-to-fine decoder, achieving strong results outperforming some generative approaches.

Conclusion: Single-image heatmap-based selective coarse-to-fine decoder can effectively infer out-of-frame face locations using image context, offering an efficient alternative to sequence or generative methods with strong empirical performance.

Abstract: Extreme amodal detection is the task of inferring the 2D location of objects
that are not fully visible in the input image but are visible within an
expanded field-of-view. This differs from amodal detection, where the object is
partially visible within the input image, but is occluded. In this paper, we
consider the sub-problem of face detection, since this class provides
motivating applications involving safety and privacy, but do not tailor our
method specifically to this class. Existing approaches rely on image sequences
so that missing detections may be interpolated from surrounding frames or make
use of generative models to sample possible completions. In contrast, we
consider the single-image task and propose a more efficient, sample-free
approach that makes use of the contextual cues from the image to infer the
presence of unseen faces. We design a heatmap-based extreme amodal object
detector that addresses the problem of efficiently predicting a lot (the
out-of-frame region) from a little (the image) with a selective coarse-to-fine
decoder. Our method establishes strong results for this new task, even
outperforming less efficient generative approaches.

</details>


### [51] [VA-Adapter: Adapting Ultrasound Foundation Model to Echocardiography Probe Guidance](https://arxiv.org/abs/2510.06809)
*Teng Wang,Haojun Jiang,Yuxuan Wang,Zhenguo Sun,Shiji Song,Gao Huang*

Main category: cs.CV

TL;DR: 提出一种参数高效的VA-Adapter，使超声基础模型能通过少量参数微调学习探头调整策略，从而为初级超声员提供实时高质量影像采集指导，实验中超越强基线。


<details>
  <summary>Details</summary>
Motivation: 当前心脏超声操作难度高、专家稀缺，虽有超声基础模型但缺乏将其知识应用于实时探头引导的研究，需帮助初级操作人员获得高质量图像。

Method: 设计了一个轻量级的Vision-Action Adapter，用于对图像编码器进行视觉-动作序列编码，内置顺序推理能力，微调时仅更新少量参数。

Result: 在大量实验中，VA-Adapter在探头引导任务上优于强基线模型，证明了方法的有效性。

Conclusion: 本文提出了VA-Adapter，通过参数高效微调，将超声基础模型学到的医学知识转移到探头引导任务，实现对初级超声员的实时操作建议。

Abstract: Echocardiography is a critical tool for detecting heart diseases. Recently,
ultrasound foundation models have demonstrated remarkable capabilities in
cardiac ultrasound image analysis. However, obtaining high-quality ultrasound
images is a prerequisite for accurate diagnosis. Due to the exceptionally high
operational difficulty of cardiac ultrasound, there is a shortage of highly
skilled personnel, which hinders patients from receiving timely examination
services. In this paper, we aim to adapt the medical knowledge learned by
foundation models from vast datasets to the probe guidance task, which is
designed to provide real-time operational recommendations for junior
sonographers to acquire high-quality ultrasound images. Moreover, inspired by
the practice where experts optimize action decisions based on past
explorations, we meticulously design a parameter-efficient Vision-Action
Adapter (VA-Adapter) to enable foundation model's image encoder to encode
vision-action sequences, thereby enhancing guidance performance. With built-in
sequential reasoning capabilities in a compact design, the VA-Adapter enables a
pre-trained ultrasound foundation model to learn precise probe adjustment
strategies by fine-tuning only a small subset of parameters. Extensive
experiments demonstrate that the VA-Adapter can surpass strong probe guidance
models. Our code will be released after acceptance.

</details>


### [52] [Efficient Discriminative Joint Encoders for Large Scale Vision-Language Reranking](https://arxiv.org/abs/2510.06820)
*Mitchell Keren Taraday,Shahaf Wagner,Chaim Baskin*

Main category: cs.CV

TL;DR: 通过离线预计算并压缩视觉tokens，EDJE大幅降低联合编码器在线计算与存储需求，同时保持检索性能，支持高吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言联合编码器在推理时被昂贵的视觉特征提取阶段所限制，无法在大规模检索中实用；需要在保持检索性能的同时显著降低在线计算和存储开销。

Method: EDJE: Efficient Discriminative Joint Encoder

Result: EDJE通过离线预计算视觉tokens，并用轻量注意力适配器压缩，使在线仅对小量视觉tokens和文本运行紧凑联合编码器；实现每秒处理5万图文对，图像在磁盘上占用49KB，与Flickr零样本和COCO微调结果相当。

Conclusion: EDJE为视觉-语言重排器在大规模检索中的实用化提供了可行路径，结合预计算压缩机制可以在不显著损失性能的情况下实现高吞吐与低存储。

Abstract: Multimodal retrieval still leans on embedding-based models like CLIP for fast
vector search over pre-computed image embeddings. Yet, unlike text retrieval,
where joint-encoder rerankers are standard, comparable vision--language
rerankers are largely absent. We find that seminal joint encoders such as BLIP
are severely bottlenecked by an expensive visual feature-extraction stage,
preventing practical deployment at scale. Motivated by this bottleneck, we
introduce EDJE, an Efficient Discriminative Joint Encoder that precomputes
vision tokens offline and compresses them via a lightweight attention-based
adapter, so online inference runs only a compact joint encoder over a small set
of visual tokens plus the text. EDJE preserves strong retrieval performance
while drastically reducing storage and online compute, enabling high-throughput
inference. Specifically, EDJE processes 50k image--text pairs/second while
requiring 49kB of disk storage per image, matching prior art on Flickr
(zero-shot) and COCO (fine-tuned) retrieval. The implementation and checkpoints
will be made publicly available shortly.

</details>


### [53] [StyleKeeper: Prevent Content Leakage using Negative Visual Query Guidance](https://arxiv.org/abs/2510.06827)
*Jaeseok Jeong,Junho Kim,Gayoung Lee,Yunjey Choi,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出NVQG：通过在自注意力中构造负查询并扩展CFG来抑制视觉风格提示的内容泄露，从而在风格保留与文本一致性间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉提示方法在传递风格时常会同时带入参考图像中的不希望出现的具体内容（内容泄露），这降低了生成图像与文本描述的一致性与多样性，因而需要一种能保留风格但抑制内容转移的机制。

Method: 1) 将CFG扩展为利用交换自注意力（swapping self-attention）；2) 提出NVQG，通过构造负样本情景（将自注意力层中的query与style prompt交换，而非key和value），在采样时使用负得分抑制不期望的内容转移；并提供实图作为视觉风格提示的实用方案。

Result: 在多种风格与文本提示下，方法在减少内容泄露、反映参考风格并保持与文本一致性方面优于现有方法；实验展示了显著改进，且代码已开源。

Conclusion: 该论文提出了一种基于扩散模型的视觉提示（visual prompt）风格控制方法，重点解决风格提示中不期望的内容泄露问题，通过扩展classifier-free guidance并引入负视觉查询引导（NVQG）实现更好的风格保留与内容一致性。

Abstract: In the domain of text-to-image generation, diffusion models have emerged as
powerful tools. Recently, studies on visual prompting, where images are used as
prompts, have enabled more precise control over style and content. However,
existing methods often suffer from content leakage, where undesired elements of
the visual style prompt are transferred along with the intended style. To
address this issue, we 1) extend classifier-free guidance (CFG) to utilize
swapping self-attention and propose 2) negative visual query guidance (NVQG) to
reduce the transfer of unwanted contents. NVQG employs negative score by
intentionally simulating content leakage scenarios that swap queries instead of
key and values of self-attention layers from visual style prompts. This simple
yet effective method significantly reduces content leakage. Furthermore, we
provide careful solutions for using a real image as visual style prompts.
Through extensive evaluation across various styles and text prompts, our method
demonstrates superiority over existing approaches, reflecting the style of the
references, and ensuring that resulting images match the text prompts. Our code
is available \href{https://github.com/naver-ai/StyleKeeper}{here}.

</details>


### [54] [Lattice-allocated Real-time Line Segment Feature Detection and Tracking Using Only an Event-based Camera](https://arxiv.org/abs/2510.06829)
*Mikihiro Ikura,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

TL;DR: 提出单纯使用高分辨率事件相机的实时直线段检测与跟踪流水线，包含速度不变表示、拟合分数检测和端点扰动跟踪，在速度和精度上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外帧相机或在高事件率下性能下降，亟需一种仅凭高分辨率事件相机即可实时且鲁棒地完成直线段检测与跟踪的方案。

Method: 1) 将事件按照速度不变的表示进行聚合（lattice-allocated）；2) 基于像素或格点上的拟合分数进行直线段检测；3) 对检测到的直线段通过端点微扰和评分来进行跟踪和更新。

Result: 该论文提出了一种仅使用高分辨率事件相机进行实时直线段检测与跟踪的方法。提出的lattice-allocated流水线包括：速度不变的事件表示、基于拟合分数的直线段检测、以及通过端点扰动进行直线段跟踪。实验在自录数据集和公开数据集上验证，结果显示在实时性和精度上优于现有基于事件的和事件-帧混合方法，实现了事件相机在真实场景中的独立运行。

Conclusion: 该方法可在高事件率、高分辨率事件相机上实现实时、准确的直线段提取与跟踪，支持事件相机独立应用于真实环境。

Abstract: Line segment extraction is effective for capturing geometric features of
human-made environments. Event-based cameras, which asynchronously respond to
contrast changes along edges, enable efficient extraction by reducing redundant
data. However, recent methods often rely on additional frame cameras or
struggle with high event rates. This research addresses real-time line segment
detection and tracking using only a modern, high-resolution (i.e., high event
rate) event-based camera. Our lattice-allocated pipeline consists of (i)
velocity-invariant event representation, (ii) line segment detection based on a
fitting score, (iii) and line segment tracking by perturbating endpoints.
Evaluation using ad-hoc recorded dataset and public datasets demonstrates
real-time performance and higher accuracy compared to state-of-the-art
event-only and event-frame hybrid baselines, enabling fully stand-alone event
camera operation in real-world settings.

</details>


### [55] [Continual Action Quality Assessment via Adaptive Manifold-Aligned Graph Regularization](https://arxiv.org/abs/2510.06842)
*Kanglei Zhou,Qingyi Pan,Xingxing Zhang,Hubert P. H. Shum,Frederick W. B. Li,Xiaohui Liang,Liyuan Wang*

Main category: cs.CV

TL;DR: 提出CAQA任务与MAGR++方法，通过稳定浅层微调、深层适配、流形投影与图正则化，有效缓解FPFT引发的遗忘与特征漂移，在多个基准上显著提升AQA持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中动作质量分布非平稳，传统AQA方法泛化能力受限，需引入持续学习能力以应对分布演化并防止灾难性遗忘，同时探索微调策略与特征对齐方法以平衡表征学习与稳定性。

Method: 提出Adaptive Manifold-Aligned Graph Regularization（MAGR++）：1) 骨干网络微调策略——稳定浅层、适配深层的FPFT，以实现有效表征学习；2) 两步特征整流：流形投影器将历史特征映射到当前表示空间；图正则器对齐局部与全局分布，缓解特征漂移与遗忘；并构建四个CAQA基准与评估协议进行系统评估。

Result: 在四个CAQA基准上，MAGR++取得最先进的性能：与最强基线相比，线下平均相关性提升3.6%，在线上提升12.2%，表明方法在稳健性与有效性上的优势；并提供代码实现。

Conclusion: 本文提出针对动作质量评估（AQA）面对非平稳质量分布的问题，引入持续学习（Continual Learning）框架并提出CAQA任务；通过实证与理论分析指出：参数高效微调不足以应对CAQA，需全参数微调（FPFT）以学习有效表征，但FPFT会导致过拟合与特征流形漂移，从而加剧遗忘；为此提出MAGR++方法，结合稳健的骨干网络微调策略与两步特征校正（流形投影器与图正则器），在四个构建的基准数据集上验证并显著优于基线。

Abstract: Action Quality Assessment (AQA) quantifies human actions in videos,
supporting applications in sports scoring, rehabilitation, and skill
evaluation. A major challenge lies in the non-stationary nature of quality
distributions in real-world scenarios, which limits the generalization ability
of conventional methods. We introduce Continual AQA (CAQA), which equips AQA
with Continual Learning (CL) capabilities to handle evolving distributions
while mitigating catastrophic forgetting. Although parameter-efficient
fine-tuning of pretrained models has shown promise in CL for image
classification, we find it insufficient for CAQA. Our empirical and theoretical
analyses reveal two insights: (i) Full-Parameter Fine-Tuning (FPFT) is
necessary for effective representation learning; yet (ii) uncontrolled FPFT
induces overfitting and feature manifold shift, thereby aggravating forgetting.
To address this, we propose Adaptive Manifold-Aligned Graph Regularization
(MAGR++), which couples backbone fine-tuning that stabilizes shallow layers
while adapting deeper ones with a two-step feature rectification pipeline: a
manifold projector to translate deviated historical features into the current
representation space, and a graph regularizer to align local and global
distributions. We construct four CAQA benchmarks from three datasets with
tailored evaluation protocols and strong baselines, enabling systematic
cross-dataset comparison. Extensive experiments show that MAGR++ achieves
state-of-the-art performance, with average correlation gains of 3.6% offline
and 12.2% online over the strongest baseline, confirming its robustness and
effectiveness. Our code is available at https://github.com/ZhouKanglei/MAGRPP.

</details>


### [56] [Online Generic Event Boundary Detection](https://arxiv.org/abs/2510.06855)
*Hyungrok Jung,Daneul Kim,Seunggyun Lim,Jeany Son,Jonghyun Choi*

Main category: cs.CV

TL;DR: Short TL;DR summary


<details>
  <summary>Details</summary>
Motivation: Explain why the online GEBD problem is important and challenging

Method: Briefly describe the methodological contributions of the paper

Result: Summarize key experimental findings

Conclusion: Give concise conclusion and potential future work

Abstract: Generic Event Boundary Detection (GEBD) aims to interpret long-form videos
through the lens of human perception. However, current GEBD methods require
processing complete video frames to make predictions, unlike humans processing
data online and in real-time. To bridge this gap, we introduce a new task,
Online Generic Event Boundary Detection (On-GEBD), aiming to detect boundaries
of generic events immediately in streaming videos. This task faces unique
challenges of identifying subtle, taxonomy-free event changes in real-time,
without the access to future frames. To tackle these challenges, we propose a
novel On-GEBD framework, Estimator, inspired by Event Segmentation Theory (EST)
which explains how humans segment ongoing activity into events by leveraging
the discrepancies between predicted and actual information. Our framework
consists of two key components: the Consistent Event Anticipator (CEA), and the
Online Boundary Discriminator (OBD). Specifically, the CEA generates a
prediction of the future frame reflecting current event dynamics based solely
on prior frames. Then, the OBD measures the prediction error and adaptively
adjusts the threshold using statistical tests on past errors to capture
diverse, subtle event transitions. Experimental results demonstrate that
Estimator outperforms all baselines adapted from recent online video
understanding models and achieves performance comparable to prior offline-GEBD
methods on the Kinetics-GEBD and TAPOS datasets.

</details>


### [57] [Explaining raw data complexity to improve satellite onboard processing](https://arxiv.org/abs/2510.06858)
*Adrien Dorise,Marjorie Bellizzi,Adrien Girard,Benjamin Francesconi,Stéphane May*

Main category: cs.CV

TL;DR: 训练在原始（raw）数据上的模型在低中置信度下与L1预处理数据表现相近，但在高置信度时边界识别能力下降，需改进轮廓建模以提升卫星端实时检测。


<details>
  <summary>Details</summary>
Motivation: 推动在卫星端部署AI，直接处理未预处理的传感器原始数据，以减少对地面处理依赖并应对带宽/实时性限制。

Method: 构建从高分辨率L1影像生成raw-like产品的仿真流程；分别用raw和L1数据训练YOLOv11s与YOLOX-S；用标准检测指标（AP、precision/recall等）与可解释性工具比较模型行为。

Result: Raw-data-trained models perform comparably to L1-trained models at low/medium confidence but underperform in precise object boundary detection at high confidence; contouring-focused architectural adaptations recommended.

Conclusion: 利用模拟生成的raw-like数据进行训练可行，但需针对原始传感器数据调整网络架构（如更强的轮廓感知模块、边界损失或多尺度特征融合）以恢复高置信度边界精度。

Abstract: With increasing processing power, deploying AI models for remote sensing
directly onboard satellites is becoming feasible. However, new constraints
arise, mainly when using raw, unprocessed sensor data instead of preprocessed
ground-based products. While current solutions primarily rely on preprocessed
sensor images, few approaches directly leverage raw data. This study
investigates the effects of utilising raw data on deep learning models for
object detection and classification tasks. We introduce a simulation workflow
to generate raw-like products from high-resolution L1 imagery, enabling
systemic evaluation. Two object detection models (YOLOv11s and YOLOX-S) are
trained on both raw and L1 datasets, and their performance is compared using
standard detection metrics and explainability tools. Results indicate that
while both models perform similarly at low to medium confidence thresholds, the
model trained on raw data struggles with object boundary identification at high
confidence levels. It suggests that adapting AI architectures with improved
contouring methods can enhance object detection on raw images, improving
onboard AI for remote sensing.

</details>


### [58] [HARP-NeXt: High-Speed and Accurate Range-Point Fusion Network for 3D LiDAR Semantic Segmentation](https://arxiv.org/abs/2510.06876)
*Samir Abou Haidar,Alexandre Chariot,Mehdi Darouich,Cyril Joly,Jean-Emmanuel Deschaud*

Main category: cs.CV

TL;DR: HARP-NeXt 通过高效预处理、Conv-SE-NeXt 特征块与多尺度范围-点融合，实现了面向嵌入式系统的高效且精确的 LiDAR 语义分割，在常用基准上表现出色且显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有 LiDAR 语义分割方法存在精度与速度权衡：点云方法准确但慢，投影方法快速但丢失几何信息，且许多方法依赖 TTA 和昂贵的预处理，不利于嵌入式实时部署。

Method: 提出三大技术：1) 改进的预处理流程以显著降低计算开销；2) Conv-SE-NeXt 特征提取块，通过高效特征学习避免每阶段深层堆叠；3) 多尺度范围-点融合骨干网，在多个抽象层次融合距离图与点云信息以保留几何细节。

Result: 在 nuScenes 与 SemanticKITTI 数据集上，HARP-NeXt 达到优越的速度-精度折中；在不使用 TTA 或集成模型下，性能可比拟排名靠前的 PTv3，但推理速度提高约 24 倍。

Conclusion: HARP-NeXt 在准确性与实时性之间实现了优良平衡，尤其适合资源受限的嵌入式平台；通过高效预处理、Conv-SE-NeXt 特征块及多尺度范围-点融合骨干网，减少计算开销并保留几何细节，可在不依赖 TTA 或集成模型的情况下提供与最先进方法相当的性能。

Abstract: LiDAR semantic segmentation is crucial for autonomous vehicles and mobile
robots, requiring high accuracy and real-time processing, especially on
resource-constrained embedded systems. Previous state-of-the-art methods often
face a trade-off between accuracy and speed. Point-based and sparse
convolution-based methods are accurate but slow due to the complexity of
neighbor searching and 3D convolutions. Projection-based methods are faster but
lose critical geometric information during the 2D projection. Additionally,
many recent methods rely on test-time augmentation (TTA) to improve
performance, which further slows the inference. Moreover, the pre-processing
phase across all methods increases execution time and is demanding on embedded
platforms. Therefore, we introduce HARP-NeXt, a high-speed and accurate LiDAR
semantic segmentation network. We first propose a novel pre-processing
methodology that significantly reduces computational overhead. Then, we design
the Conv-SE-NeXt feature extraction block to efficiently capture
representations without deep layer stacking per network stage. We also employ a
multi-scale range-point fusion backbone that leverages information at multiple
abstraction levels to preserve essential geometric details, thereby enhancing
accuracy. Experiments on the nuScenes and SemanticKITTI benchmarks show that
HARP-NeXt achieves a superior speed-accuracy trade-off compared to all
state-of-the-art methods, and, without relying on ensemble models or TTA, is
comparable to the top-ranked PTv3, while running 24$\times$ faster. The code is
available at https://github.com/SamirAbouHaidar/HARP-NeXt

</details>


### [59] [Lung Infection Severity Prediction Using Transformers with Conditional TransMix Augmentation and Cross-Attention](https://arxiv.org/abs/2510.06887)
*Bouthaina Slika,Fadi Dornaika,Fares Bougourzi,Karim Hammoudi*

Main category: cs.CV

TL;DR: 提出基于并行Transformer与混合标签在线增强的模型，有效提升CT/X光肺感染严重度预测，公开了代码。


<details>
  <summary>Details</summary>
Motivation: 肺部感染（如肺炎）严重度评估对临床决策至关重要，尤其在疫情期间需要自动化、准确且稳健的影像评估工具；现有方法在多尺度特征提取及不平衡数据处理上存在不足。

Method: 提出QCross-Att-PVT：基于PVT的并行编码器架构，加入交叉门控注意力（cross-gated attention）和特征聚合模块以捕捉多尺度特征；并设计Conditional Online TransMix数据增强，在训练时生成带混合标签的图像块以缓解数据不平衡。

Result: 在RALO CXR与Per-COVID-19 CT两个基准数据集上，方法优于多种SOTA深度学习模型，显示数据增强和门控注意力能显著提升鲁棒性与预测精度。

Conclusion: 论文提出了一种适用于CT与X光的肺部感染严重度评估方法，通过改进的Transformer结构与定制数据增强提升预测性能。

Abstract: Lung infections, particularly pneumonia, pose serious health risks that can
escalate rapidly, especially during pandemics. Accurate AI-based severity
prediction from medical imaging is essential to support timely clinical
decisions and optimize patient outcomes. In this work, we present a novel
method applicable to both CT scans and chest X-rays for assessing lung
infection severity. Our contributions are twofold: (i) QCross-Att-PVT, a
Transformer-based architecture that integrates parallel encoders, a cross-gated
attention mechanism, and a feature aggregator to capture rich multi-scale
features; and (ii) Conditional Online TransMix, a custom data augmentation
strategy designed to address dataset imbalance by generating mixed-label image
patches during training. Evaluated on two benchmark datasets, RALO CXR and
Per-COVID-19 CT, our method consistently outperforms several state-of-the-art
deep learning models. The results emphasize the critical role of data
augmentation and gated attention in improving both robustness and predictive
accuracy. This approach offers a reliable, adaptable tool to support clinical
diagnosis, disease monitoring, and personalized treatment planning. The source
code of this work is available at https://github.com/bouthainas/QCross-Att-PVT.

</details>


### [60] [Label-frugal satellite image change detection with generative virtual exemplar learning](https://arxiv.org/abs/2510.06926)
*Hichem Sahbi*

Main category: cs.CV

TL;DR: 通过可逆图卷积网络生成对抗性虚拟示例的主动学习方法，选择最具代表性、多样性和模糊性的样本用于标注，提升遥感变化检测的标签效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感变化检测中标注数据稀缺和主观性问题，通过主动学习减少标注量并提高模型性能。

Method: 设计一个评价未标记样本重要性的模型；用可逆图卷积网络生成虚拟示例作为对抗性最优解，损失函数结合代表性、多样性和模糊性；在主动学习迭代中选取这些示例供人工标注并更新检测器。

Result: 提出基于主动学习的新模型，使用可逆图卷积网络生成对抗性虚拟示例，衡量未标记样本的重要性并提供代表性、多样性和模糊性高的样本供标注，实验显示提高了标注效率和检测性能。

Conclusion: 该方法能有效减少人工标注量，同时通过挑战当前检测准则改进模型，实验证明在多项比较方法中表现优越。

Abstract: Change detection is a major task in remote sensing which consists in finding
all the occurrences of changes in multi-temporal satellite or aerial images.
The success of existing methods, and particularly deep learning ones, is
tributary to the availability of hand-labeled training data that capture the
acquisition conditions and the subjectivity of the user (oracle). In this
paper, we devise a novel change detection algorithm, based on active learning.
The main contribution of our work resides in a new model that measures how
important is each unlabeled sample, and provides an oracle with only the most
critical samples (also referred to as virtual exemplars) for further labeling.
These exemplars are generated, using an invertible graph convnet, as the
optimum of an adversarial loss that (i) measures representativity, diversity
and ambiguity of the data, and thereby (ii) challenges (the most) the current
change detection criteria, leading to a better re-estimate of these criteria in
the subsequent iterations of active learning. Extensive experiments show the
positive impact of our label-efficient learning model against comparative
methods.

</details>


### [61] [IAR2: Improving Autoregressive Visual Generation with Semantic-Detail Associated Token Prediction](https://arxiv.org/abs/2510.06928)
*Ran Yi,Teng Hu,Zihan Su,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出IAR2，通过语义/细节双码本和分层自回归预测实现粗到细生成并动态条件指导，显著提升图像生成质量（ImageNet FID 1.50）和效率。


<details>
  <summary>Details</summary>
Motivation: Autoregressive models ignore structural properties; prior IAR reorganized codebook by embedding similarity but limited by fixed codebooks and hard clustering.

Method: Semantic-Detail Associated Dual Codebook（语义码本+细节码本）；Semantic-Detail Autoregressive Prediction（先预测语义再细节）；Local-Context Enhanced Autoregressive Head；Progressive Attention-Guided Adaptive CFG用于条件生成。

Result: IAR2 introduces dual codebook (semantic + detail), hierarchical semantic-detail synthesis, polynomial-scale quantization, semantic-detail autoregressive prediction with local-context head, Progressive Attention-Guided Adaptive CFG; achieves SOTA FID 1.50 on ImageNet and better efficiency.

Conclusion: IAR2有效解耦语义与细节、扩大量化容量并采用层次化预测与自适应条件引导，在生成质量与计算效率上超越先前自回归方法。

Abstract: Autoregressive models have emerged as a powerful paradigm for visual content
creation, but often overlook the intrinsic structural properties of visual
data. Our prior work, IAR, initiated a direction to address this by
reorganizing the visual codebook based on embedding similarity, thereby
improving generation robustness. However, it is constrained by the rigidity of
pre-trained codebooks and the inaccuracies of hard, uniform clustering. To
overcome these limitations, we propose IAR2, an advanced autoregressive
framework that enables a hierarchical semantic-detail synthesis process. At the
core of IAR2 is a novel Semantic-Detail Associated Dual Codebook, which
decouples image representations into a semantic codebook for global semantic
information and a detail codebook for fine-grained refinements. It expands the
quantization capacity from a linear to a polynomial scale, significantly
enhancing expressiveness. To accommodate this dual representation, we propose a
Semantic-Detail Autoregressive Prediction scheme coupled with a Local-Context
Enhanced Autoregressive Head, which performs hierarchical prediction-first the
semantic token, then the detail token-while leveraging a local context window
to enhance spatial coherence. Furthermore, for conditional generation, we
introduce a Progressive Attention-Guided Adaptive CFG mechanism that
dynamically modulates the guidance scale for each token based on its relevance
to the condition and its temporal position in the generation sequence,
improving conditional alignment without sacrificing realism. Extensive
experiments demonstrate that IAR2 sets a new state-of-the-art for
autoregressive image generation, achieving a FID of 1.50 on ImageNet. Our model
not only surpasses previous methods in performance but also demonstrates
superior computational efficiency, highlighting the effectiveness of our
structured, coarse-to-fine generation strategy.

</details>


### [62] [OBJVanish: Physically Realizable Text-to-3D Adv. Generation of LiDAR-Invisible Objects](https://arxiv.org/abs/2510.06952)
*Bing Li,Wuqi Wang,Yanan Zhang,Jingzheng Li,Haigen Min,Wei Feng,Xingyu Zhao,Jie Zhang,Qing Guo*

Main category: cs.CV

TL;DR: 本文提出了文本到3D对抗生成方法（Phy3DAdvGen），通过在CARLA中操控行人模型的拓扑、连接性和反射强度，并基于物理可实现的13种真实物体组合生成能使LiDAR检测器“看不见”的行人，从而在仿真和现实中成功对抗多款SOTA 3D检测器。


<details>
  <summary>Details</summary>
Motivation: 现有通过对点云加扰动的对抗攻击难以在物理世界实现且很少能导致目标完全消失，因此需要一种能生成物理可实现且能使目标在LiDAR检测器中“隐身”的攻击方法，以暴露检测器在真实部署前的脆弱性。

Method: 首先在CARLA中系统性研究影响检测的因素（拓扑、连通性、反射强度、物体组合），然后提出基于文本提示的迭代优化流程，细化动词、对象与姿态，受限于一个包含13个真实3D模型的物体池以保证物理可实现性，最终生成LiDAR不可见的行人3D模型并在仿真与现实中验证。

Result: 在CARLA和物理环境中，生成的对抗3D行人成功使六种SOTA LiDAR 3D检测器漏检，验证了方法有效性与物理可实现性。

Conclusion: Phy3DAdvGen能生成在仿真与现实环境中逃避六种SOTA LiDAR 3D检测器的3D行人模型，揭示了现有检测系统的脆弱性并展示了物理可实现攻击的可行性。

Abstract: LiDAR-based 3D object detectors are fundamental to autonomous driving, where
failing to detect objects poses severe safety risks. Developing effective 3D
adversarial attacks is essential for thoroughly testing these detection systems
and exposing their vulnerabilities before real-world deployment. However,
existing adversarial attacks that add optimized perturbations to 3D points have
two critical limitations: they rarely cause complete object disappearance and
prove difficult to implement in physical environments. We introduce the
text-to-3D adversarial generation method, a novel approach enabling physically
realizable attacks that can generate 3D models of objects truly invisible to
LiDAR detectors and be easily realized in the real world. Specifically, we
present the first empirical study that systematically investigates the factors
influencing detection vulnerability by manipulating the topology, connectivity,
and intensity of individual pedestrian 3D models and combining pedestrians with
multiple objects within the CARLA simulation environment. Building on the
insights, we propose the physically-informed text-to-3D adversarial generation
(Phy3DAdvGen) that systematically optimizes text prompts by iteratively
refining verbs, objects, and poses to produce LiDAR-invisible pedestrians. To
ensure physical realizability, we construct a comprehensive object pool
containing 13 3D models of real objects and constrain Phy3DAdvGen to generate
3D objects based on combinations of objects in this set. Extensive experiments
demonstrate that our approach can generate 3D pedestrians that evade six
state-of-the-art (SOTA) LiDAR 3D detectors in both CARLA simulation and
physical environments, thereby highlighting vulnerabilities in safety-critical
applications.

</details>


### [63] [Generating Surface for Text-to-3D using 2D Gaussian Splatting](https://arxiv.org/abs/2510.06967)
*Huanning Dong,Fan Li,Ping Kuang,Jianwen Min*

Main category: cs.CV

TL;DR: 提出DirectGaussian：基于surfel的Text-to-3D方法，采用2D Gaussian splatting与多视角法线/纹理先验，并在优化中加入曲率约束以提升几何一致性，能生成高质量3D表面。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-3D方法在恢复复杂自然物体几何形状时面临挑战；已有方法依赖2D扩散先验或特定3D表示训练，本工作通过直接生成surfel表面并用高斯点渲染与曲率约束改善几何一致性。

Method: 使用条件文本生成模型指导3D表面生成，采用2D Gaussian splatting渲染surfel表面，结合多视角法线与纹理先验；在优化阶段引入曲率约束以处理多视角几何一致性问题。

Result: 实验表明DirectGaussian能生成多样且高保真的3D内容，几何一致性和细节质量优于部分现有方法（根据摘要的描述）。

Conclusion: DirectGaussian提出了一种基于surfel（表面点）表示的Text-to-3D方法，通过2D高斯点渲染结合多视角法线和纹理先验，并在优化中加入曲率约束以增强几何一致性，能够生成多样且高保真的3D表面。

Abstract: Recent advancements in Text-to-3D modeling have shown significant potential
for the creation of 3D content. However, due to the complex geometric shapes of
objects in the natural world, generating 3D content remains a challenging task.
Current methods either leverage 2D diffusion priors to recover 3D geometry, or
train the model directly based on specific 3D representations. In this paper,
we propose a novel method named DirectGaussian, which focuses on generating the
surfaces of 3D objects represented by surfels. In DirectGaussian, we utilize
conditional text generation models and the surface of a 3D object is rendered
by 2D Gaussian splatting with multi-view normal and texture priors. For
multi-view geometric consistency problems, DirectGaussian incorporates
curvature constraints on the generated surface during optimization process.
Through extensive experiments, we demonstrate that our framework is capable of
achieving diverse and high-fidelity 3D content creation.

</details>


### [64] [Learning Global Representation from Queries for Vectorized HD Map Construction](https://arxiv.org/abs/2510.06969)
*Shoumeng Qiu,Xinrun Li,Yang Long,Xiangyang Xue,Varun Ojha,Jian Pu*

Main category: cs.CV

TL;DR: MapGR adds global representation learning and guidance to query-based HD map construction, improving mAP vs baselines


<details>
  <summary>Details</summary>
Motivation: Current DETR-style methods use independent learnable queries leading to local perspectives; need to leverage global map structure to improve detection and consistency

Method: DETR-based global representation for HD maps

Result: Propose MapGR with GRL (holistic segmentation aligning queries distribution to global map) and GRG (inject global context into queries); show substantial mAP gains on nuScenes and Argoverse2

Conclusion: Global representations and query-level guidance improve HD map vectorization; MapGR achieves strong performance on benchmarks

Abstract: The online construction of vectorized high-definition (HD) maps is a
cornerstone of modern autonomous driving systems. State-of-the-art approaches,
particularly those based on the DETR framework, formulate this as an instance
detection problem. However, their reliance on independent, learnable object
queries results in a predominantly local query perspective, neglecting the
inherent global representation within HD maps. In this work, we propose
\textbf{MapGR} (\textbf{G}lobal \textbf{R}epresentation learning for HD
\textbf{Map} construction), an architecture designed to learn and utilize a
global representations from queries. Our method introduces two synergistic
modules: a Global Representation Learning (GRL) module, which encourages the
distribution of all queries to better align with the global map through a
carefully designed holistic segmentation task, and a Global Representation
Guidance (GRG) module, which endows each individual query with explicit,
global-level contextual information to facilitate its optimization. Evaluations
on the nuScenes and Argoverse2 datasets validate the efficacy of our approach,
demonstrating substantial improvements in mean Average Precision (mAP) compared
to leading baselines.

</details>


### [65] [Addressing the ID-Matching Challenge in Long Video Captioning](https://arxiv.org/abs/2510.06973)
*Zhantao Yang,Huangji Wang,Ruili Feng,Han Zhang,Yuting Hu,Shangwen Zhu,Junyan Li,Yu Liu,Fan Cheng*

Main category: cs.CV

TL;DR: 提出基于LVLM先验的RICE方法与新基准，旨在提升长视频字幕中的身份匹配能力。实验证明在GPT-4o上显著提高精确率与召回率，能更稳定地在长视频中识别并跟踪人物。


<details>
  <summary>Details</summary>
Motivation: 长视频字幕生成需在多帧间识别相同人物（ID-Matching），此前工作关注较少或受限于点对点匹配，泛化能力差。作者希望利用LVLM内在能力提升该任务效果。

Method: 基于调查，发现提升ID匹配可通过加强图像信息利用和增加人物描述信息量实现；据此设计RICE方法（细节包含更好地调用图像内容与丰富个人描述信息以在长视频生成中持续追踪人物）。构建新基准来评估ID匹配性能，并在GPT-4o上进行大规模实验。

Result: 在GPT-4o上，RICE使ID匹配精确率从50%提升到90%，召回率从15%提升到80%；表明RICE能在长视频字幕中持续跟踪不同个体，且在字幕质量与ID匹配上均优于基线。

Conclusion: 本文提出RICE方法，通过利用LVLM（含GPT-4o）中的先验能力提升长视频描述中的身份匹配能力（ID-Matching），并通过新基准评估验证其有效性，显著提高了ID匹配的精确率与召回率。

Abstract: Generating captions for long and complex videos is both critical and
challenging, with significant implications for the growing fields of
text-to-video generation and multi-modal understanding. One key challenge in
long video captioning is accurately recognizing the same individuals who appear
in different frames, which we refer to as the ID-Matching problem. Few prior
works have focused on this important issue. Those that have, usually suffer
from limited generalization and depend on point-wise matching, which limits
their overall effectiveness. In this paper, unlike previous approaches, we
build upon LVLMs to leverage their powerful priors. We aim to unlock the
inherent ID-Matching capabilities within LVLMs themselves to enhance the
ID-Matching performance of captions. Specifically, we first introduce a new
benchmark for assessing the ID-Matching capabilities of video captions. Using
this benchmark, we investigate LVLMs containing GPT-4o, revealing key insights
that the performance of ID-Matching can be improved through two methods: 1)
enhancing the usage of image information and 2) increasing the quantity of
information of individual descriptions. Based on these insights, we propose a
novel video captioning method called Recognizing Identities for Captioning
Effectively (RICE). Extensive experiments including assessments of caption
quality and ID-Matching performance, demonstrate the superiority of our
approach. Notably, when implemented on GPT-4o, our RICE improves the precision
of ID-Matching from 50% to 90% and improves the recall of ID-Matching from 15%
to 80% compared to baseline. RICE makes it possible to continuously track
different individuals in the captions of long videos.

</details>


### [66] [No MoCap Needed: Post-Training Motion Diffusion Models with Reinforcement Learning using Only Textual Prompts](https://arxiv.org/abs/2510.06988)
*Girolamo Macaluso,Lorenzo Mandelli,Mirko Bicchierai,Stefano Berretti,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: 提出一种基于RL的后训练框架，利用文本-动作检索奖励对预训练扩散模型进行微调，无需动作标注，提升生成动作质量与多样性并保持原性能


<details>
  <summary>Details</summary>
Motivation: Reduce cost and scalability issues of adapting diffusion models to new actions/styles without motion ground truth; enable adaptation using only text prompts

Method: Reinforcement Learning-based post-training of motion diffusion models

Result: Improved quality and diversity in generated motions and preserved original distribution performance across datasets and architectures; successful cross-dataset and leave-one-out adaptation

Conclusion: 方法为数据高效、灵活且隐私友好的动作适配方案，能在不同扩散架构与数据集上实现有效的文本驱动域迁移

Abstract: Diffusion models have recently advanced human motion generation, producing
realistic and diverse animations from textual prompts. However, adapting these
models to unseen actions or styles typically requires additional motion capture
data and full retraining, which is costly and difficult to scale. We propose a
post-training framework based on Reinforcement Learning that fine-tunes
pretrained motion diffusion models using only textual prompts, without
requiring any motion ground truth. Our approach employs a pretrained
text-motion retrieval network as a reward signal and optimizes the diffusion
policy with Denoising Diffusion Policy Optimization, effectively shifting the
model's generative distribution toward the target domain without relying on
paired motion data. We evaluate our method on cross-dataset adaptation and
leave-one-out motion experiments using the HumanML3D and KIT-ML datasets across
both latent- and joint-space diffusion architectures. Results from quantitative
metrics and user studies show that our approach consistently improves the
quality and diversity of generated motions, while preserving performance on the
original distribution. Our approach is a flexible, data-efficient, and
privacy-preserving solution for motion adaptation.

</details>


### [67] [Bayesian Modelling of Multi-Year Crop Type Classification Using Deep Neural Networks and Hidden Markov Models](https://arxiv.org/abs/2510.07008)
*Gianmarco Perantoni,Giulio Weikmann,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 将Transformer编码器的深度特征与HMM序列建模结合，用于提高年度卫星影像时间序列（SITS）多年份作物分类的时序一致性与F1表现。


<details>
  <summary>Details</summary>
Motivation: 关注生成具有年度一致性的土地覆盖/作物类型序列，以便更好地建模多年演变规律，减少单年分类噪声导致的不合理类别跳变，提高应用在土地变化监测和农业分析中的可靠性。

Method: 使用Transformer Encoder提取每年的时序特征并经深度神经网络分类，然后在其之上构建隐藏马尔可夫模型（HMM）层来对多年度作物类别序列进行级联分类与平滑，利用HMM捕捉类别转移概率与观测噪声，从而修正年度独立预测。

Result: 在跨6年、47类作物的验证集上，加入HMM层的模型在F1分数和时序一致性指标上均有明显提升，表明HMM能有效利用多年份标签间的依赖性来纠正孤立误判。

Conclusion: 在47类作物、6年Sentinel-2数据上，TE+DNN+HMM的组合相比仅用TE显著提升了预测标签的时序一致性和整体F1分数，证明了对多年份标签序列建模的重要性。

Abstract: The temporal consistency of yearly land-cover maps is of great importance to
model the evolution and change of the land cover over the years. In this paper,
we focus the attention on a novel approach to classification of yearly
satellite image time series (SITS) that combines deep learning with Bayesian
modelling, using Hidden Markov Models (HMMs) integrated with Transformer
Encoder (TE) based DNNs. The proposed approach aims to capture both i)
intricate temporal correlations in yearly SITS and ii) specific patterns in
multiyear crop type sequences. It leverages the cascade classification of an
HMM layer built on top of the TE, discerning consistent yearly crop-type
sequences. Validation on a multiyear crop type classification dataset spanning
47 crop types and six years of Sentinel-2 acquisitions demonstrates the
importance of modelling temporal consistency in the predicted labels. HMMs
enhance the overall performance and F1 scores, emphasising the effectiveness of
the proposed approach.

</details>


### [68] [U-Bench: A Comprehensive Understanding of U-Net through 100-Variant Benchmarking](https://arxiv.org/abs/2510.07041)
*Fenghe Tang,Chengqi Dong,Wenxin Ma,Zikang Xu,Heqin Zhu,Zihang Jiang,Rongsheng Wang,Yuhao Wang,Chenxu Wu,Shaohua Kevin Zhou*

Main category: cs.CV

TL;DR: U-Bench是首个大规模严格的U-Net变体基准，覆盖100个模型和多模态数据，提出U-Score并给出模型选择建议，全部资源开源。


<details>
  <summary>Details</summary>
Motivation: 提出一个全面、公正的基准来评估大量U-Net变体在医学图像分割中的性能与效率，弥补以往研究在统计检验、泛化能力和计算效率方面的不足。

Method: 在28个医学影像数据集上对100个U-Net变体进行统计显著性检验、零-shot泛化测试和计算效率评估，设计U-Score综合度量，并开发模型顾问来推荐适配模型。

Result: 构建了U-Bench：评估100个U-Net变体、28个数据集、10种成像模态，提出U-Score用于性能与效率权衡，并提供模型顾问与全部代码与权重。

Conclusion: U-Bench揭示了以往评估的不足，提供了可重复、实用的评测框架和工具，促进未来U-Net分割模型的公平比较与部署导向评估。

Abstract: Over the past decade, U-Net has been the dominant architecture in medical
image segmentation, leading to the development of thousands of U-shaped
variants. Despite its widespread adoption, there is still no comprehensive
benchmark to systematically evaluate their performance and utility, largely
because of insufficient statistical validation and limited consideration of
efficiency and generalization across diverse datasets. To bridge this gap, we
present U-Bench, the first large-scale, statistically rigorous benchmark that
evaluates 100 U-Net variants across 28 datasets and 10 imaging modalities. Our
contributions are threefold: (1) Comprehensive Evaluation: U-Bench evaluates
models along three key dimensions: statistical robustness, zero-shot
generalization, and computational efficiency. We introduce a novel metric,
U-Score, which jointly captures the performance-efficiency trade-off, offering
a deployment-oriented perspective on model progress. (2) Systematic Analysis
and Model Selection Guidance: We summarize key findings from the large-scale
evaluation and systematically analyze the impact of dataset characteristics and
architectural paradigms on model performance. Based on these insights, we
propose a model advisor agent to guide researchers in selecting the most
suitable models for specific datasets and tasks. (3) Public Availability: We
provide all code, models, protocols, and weights, enabling the community to
reproduce our results and extend the benchmark with future methods. In summary,
U-Bench not only exposes gaps in previous evaluations but also establishes a
foundation for fair, reproducible, and practically relevant benchmarking in the
next decade of U-Net-based segmentation models. The project can be accessed at:
https://fenghetan9.github.io/ubench. Code is available at:
https://github.com/FengheTan9/U-Bench.

</details>


### [69] [Concept Retrieval -- What and How?](https://arxiv.org/abs/2510.07058)
*Ori nizan,Oren Shrout,Ayellet Tal*

Main category: cs.CV

TL;DR: Proposes concept-based image retrieval by modeling neighbor embeddings as bimodal Gaussians to find shared concepts; validated qualitatively, quantitatively, and via human eval.


<details>
  <summary>Details</summary>
Motivation: Define and retrieve images that share the central concept(s) of a query image, beyond simple visual/semantic similarity.

Method: Retrieve nearest neighbors in embedding space, model neighborhood as a bimodal Gaussian distribution to separate concept clusters, identify concepts and retrieve images sharing the same central concept.

Result: A method modeling neighbor embeddings with a bimodal Gaussian to identify shared concepts; evaluation shows effectiveness; package available on PyPI.

Conclusion: Bimodal Gaussian modeling of neighbor embeddings reveals meaningful structure enabling concept identification for retrieval, outperforming conventional similarity-based methods.

Abstract: A concept may reflect either a concrete or abstract idea. Given an input
image, this paper seeks to retrieve other images that share its central
concepts, capturing aspects of the underlying narrative. This goes beyond
conventional retrieval or clustering methods, which emphasize visual or
semantic similarity. We formally define the problem, outline key requirements,
and introduce appropriate evaluation metrics. We propose a novel approach
grounded in two key observations: (1) While each neighbor in the embedding
space typically shares at least one concept with the query, not all neighbors
necessarily share the same concept with one another. (2) Modeling this
neighborhood with a bimodal Gaussian distribution uncovers meaningful structure
that facilitates concept identification. Qualitative, quantitative, and human
evaluations confirm the effectiveness of our approach. See the package on PyPI:
https://pypi.org/project/coret/

</details>


### [70] [DADO: A Depth-Attention framework for Object Discovery](https://arxiv.org/abs/2510.07089)
*Federico Gonzalez,Estefania Talavera,Petia Radeva*

Main category: cs.CV

TL;DR: DADO通过注意力+深度的自监督融合及动态加权，自适应应对噪声注意力和复杂深度平面，在标准基准上无微调即超越现有无监督对象发现方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种结合注意力机制和深度信息的无监督对象发现方法，以在无标签图像中识别和定位物体，提升准确性和鲁棒性。

Method: 使用自监督注意力模型与深度估计器生成特征图，设计动态加权机制根据图像全局统计在注意力图与深度图间分配权重，最后基于融合映射进行物体候选区域提取与定位评估。

Result: 提出DADO模型：融合注意力与深度估计，并通过动态加权根据图像全局特性自适应强调注意力或深度特征，实现优于SOTA的对象发现性能且无需微调。

Conclusion: 将注意力与深度信息结合并用动态权重自适应显著提升了无监督对象发现的准确性与鲁棒性，证明了深度信息在无标签对象定位中的价值。

Abstract: Unsupervised object discovery, the task of identifying and localizing objects
in images without human-annotated labels, remains a significant challenge and a
growing focus in computer vision. In this work, we introduce a novel model,
DADO (Depth-Attention self-supervised technique for Discovering unseen
Objects), which combines an attention mechanism and a depth model to identify
potential objects in images. To address challenges such as noisy attention maps
or complex scenes with varying depth planes, DADO employs dynamic weighting to
adaptively emphasize attention or depth features based on the global
characteristics of each image. We evaluated DADO on standard benchmarks, where
it outperforms state-of-the-art methods in object discovery accuracy and
robustness without the need for fine-tuning.

</details>


### [71] [Enhancing Concept Localization in CLIP-based Concept Bottleneck Models](https://arxiv.org/abs/2510.07115)
*Rémi Kazmierczak,Steve Azzolin,Eloïse Berthier,Goran Frehse,Gianni Franchi*

Main category: cs.CV

TL;DR: Introduce CHILI to reduce CLIP concept hallucination in zero-shot CBMs by localizing concept pixels and creating more faithful saliency explanations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve explainable AI by addressing concept hallucination in Concept Bottleneck Models that rely on CLIP without explicit concept annotations.

Method: Use localized interpretability to disentangle image embeddings and map concepts to pixel regions, enabling inhibition of hallucinated concepts and generation of saliency explanations.

Result: They propose CHILI, which disentangles image embeddings and localizes pixels for target concepts, reducing hallucination and producing better saliency-based explanations.

Conclusion: CHILI mitigates CLIP's concept hallucination, improves faithfulness of concept predictions in zero-shot CBMs, and yields more interpretable saliency maps.

Abstract: This paper addresses explainable AI (XAI) through the lens of Concept
Bottleneck Models (CBMs) that do not require explicit concept annotations,
relying instead on concepts extracted using CLIP in a zero-shot manner. We show
that CLIP, which is central in these techniques, is prone to concept
hallucination, incorrectly predicting the presence or absence of concepts
within an image in scenarios used in numerous CBMs, hence undermining the
faithfulness of explanations. To mitigate this issue, we introduce Concept
Hallucination Inhibition via Localized Interpretability (CHILI), a technique
that disentangles image embeddings and localizes pixels corresponding to target
concepts. Furthermore, our approach supports the generation of saliency-based
explanations that are more interpretable.

</details>


### [72] [MoRe: Monocular Geometry Refinement via Graph Optimization for Cross-View Consistency](https://arxiv.org/abs/2510.07119)
*Dongki Jung,Jaehoon Choi,Yonghan Lee,Sungmin Eum,Heesung Kwon,Dinesh Manocha*

Main category: cs.CV

TL;DR: MoRe：利用单目基础模型的深度与法线、帧间特征匹配和图优化的局部平面约束，免训练地实现尺度对齐与视图一致性提升，改善3D重建与稀疏视图渲染。


<details>
  <summary>Details</summary>
Motivation: 单目基础模型虽能提供丰富的几何先验，但存在尺度不确定性与跨视图不一致，无法直接用于需要尺度对齐或多视图一致性的应用；因此设计一个免训练的后处理几何优化模块以提升重建与新视图合成质量。

Method: 先在帧间做特征匹配得到2D对应点，利用单目基础模型估计的深度反投影为3D点并获取表面法线；构建基于这些点与法线的图结构，采用局部平面近似作为约束并在图上优化顶点尺度/位置以实现跨视图尺度对齐与几何一致性；该方法无训练，侧重几何正则化而非直接学习。

Result: MoRe在稀疏视图场景下显著提升了3D重建精度和新视图合成效果，解决了单目几何先验的尺度歧义并加强了跨视图一致性（根据摘要所述）。

Conclusion: MoRe是一种免训练的单目几何优化方法，通过帧间特征匹配建立对应关系，结合单目基础模型提供的深度与法线，在图优化框架中进行局部平面拟合，从而解决尺度歧义并提升跨视图一致性与尺度对齐。

Abstract: Monocular 3D foundation models offer an extensible solution for perception
tasks, making them attractive for broader 3D vision applications. In this
paper, we propose MoRe, a training-free Monocular Geometry Refinement method
designed to improve cross-view consistency and achieve scale alignment. To
induce inter-frame relationships, our method employs feature matching between
frames to establish correspondences. Rather than applying simple least squares
optimization on these matched points, we formulate a graph-based optimization
framework that performs local planar approximation using the estimated 3D
points and surface normals estimated by monocular foundation models. This
formulation addresses the scale ambiguity inherent in monocular geometric
priors while preserving the underlying 3D structure. We further demonstrate
that MoRe not only enhances 3D reconstruction but also improves novel view
synthesis, particularly in sparse view rendering scenarios.

</details>


### [73] [Validation of Various Normalization Methods for Brain Tumor Segmentation: Can Federated Learning Overcome This Heterogeneity?](https://arxiv.org/abs/2510.07126)
*Jan Fiszer,Dominika Ciupek,Maciej Malawski*

Main category: cs.CV

TL;DR: 本文通过对MRI强度归一化差异的模拟来研究非IID环境下联邦学习的鲁棒性，结果表明FL在异构归一化数据上仍能达到与集中式训练接近的性能（3D Dice≈92%），支持在医疗场景中应用FL以保护隐私。


<details>
  <summary>Details</summary>
Motivation: DL在医疗成像中需大量数据，但数据隐私、存储与传输限制了集中式训练。联邦学习能在本地保留数据、共享模型更新，理论上解决隐私问题，但需验证其在现实中常见的非IID（如不同归一化）条件下的有效性。

Method: 模拟非IID场景：通过对数据子集应用不同的MRI强度归一化方法来制造异质性；在这些子集上训练分割模型，比较联邦学习方法与集中式训练的性能；评估训练与推理阶段受各归一化方法影响的程度，使用3D Dice等指标量化。

Result: 在不同归一化造成的非IID情形下，联邦学习方法取得了约92% 3D Dice，与集中式模型相当，表明FL可在不违反隐私的情况下有效训练高性能分割模型；并分析了不同归一化方法对训练与推理的影响。

Conclusion: FL在处理由不同MRI强度归一化引起的非IID数据时表现出强鲁棒性，能在保证数据隐私的前提下训练出与集中式模型相当的高性能脑肿瘤分割模型。

Abstract: Deep learning (DL) has been increasingly applied in medical imaging, however,
it requires large amounts of data, which raises many challenges related to data
privacy, storage, and transfer. Federated learning (FL) is a training paradigm
that overcomes these issues, though its effectiveness may be reduced when
dealing with non-independent and identically distributed (non-IID) data. This
study simulates non-IID conditions by applying different MRI intensity
normalization techniques to separate data subsets, reflecting a common cause of
heterogeneity. These subsets are then used for training and testing models for
brain tumor segmentation. The findings provide insights into the influence of
the MRI intensity normalization methods on segmentation models, both training
and inference. Notably, the FL methods demonstrated resilience to
inconsistently normalized data across clients, achieving the 3D Dice score of
92%, which is comparable to a centralized model (trained using all data). These
results indicate that FL is a solution to effectively train high-performing
models without violating data privacy, a crucial concern in medical
applications. The code is available at:
https://github.com/SanoScience/fl-varying-normalization.

</details>


### [74] [Graph Conditioned Diffusion for Controllable Histopathology Image Generation](https://arxiv.org/abs/2510.07129)
*Sarah Cechnicka,Matthew Baugh,Weitong Zhang,Mischa Dombrowski,Zhe Li,Johannes C. Paetzold,Candice Roufosse,Bernhard Kainz*

Main category: cs.CV

TL;DR: 本文提出一种基于图的对象级表示与图条件扩散（Graph-Conditioned-Diffusion, GCD）方法，通过将图节点表示图像中各主要结构并融入扩散模型的文本条件机制，实现对医学图像（病理切片）生成的细粒度控制。实验显示生成数据可用于替代真实标注数据进行分割任务。


<details>
  <summary>Details</summary>
Motivation: 医学影像生成需要保留严格的结构化先验（空间布局、形状、纹理），现有DPM在噪声潜空间缺乏语义结构和强先验，难以实现有意义的控制。

Method: 构建每个感兴趣结构的图节点，包含形状、位置、纹理等特征；使用Transformer处理图节点关系；将输出作为文本条件嵌入融入扩散模型以控制生成过程。

Result: 在真实世界病理学数据集上验证，生成样本用于训练分割模型可达到与真实标注数据相近的性能，证明方法在数据替代与隐私保护方面的潜力。

Conclusion: 所提方法通过对象级图表示和Transformer模块将结构化先验融入扩散过程，在病理图像生成与下游分割任务中表现良好，生成数据可有效替代带标注的真实患者数据。

Abstract: Recent advances in Diffusion Probabilistic Models (DPMs) have set new
standards in high-quality image synthesis. Yet, controlled generation remains
challenging, particularly in sensitive areas such as medical imaging. Medical
images feature inherent structure such as consistent spatial arrangement, shape
or texture, all of which are critical for diagnosis. However, existing DPMs
operate in noisy latent spaces that lack semantic structure and strong priors,
making it difficult to ensure meaningful control over generated content. To
address this, we propose graph-based object-level representations for
Graph-Conditioned-Diffusion. Our approach generates graph nodes corresponding
to each major structure in the image, encapsulating their individual features
and relationships. These graph representations are processed by a transformer
module and integrated into a diffusion model via the text-conditioning
mechanism, enabling fine-grained control over generation. We evaluate this
approach using a real-world histopathology use case, demonstrating that our
generated data can reliably substitute for annotated patient data in downstream
segmentation tasks. The code is available here.

</details>


### [75] [Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models](https://arxiv.org/abs/2510.07135)
*Karim El Khoury,Maxime Zanella,Christophe De Vleeschouwer,Benoit Macq*

Main category: cs.CV

TL;DR: 构建并开源了第一个RSVLM少样本适配基准，在10个数据集上比较5种适配方法与3种模型，发现少样本适配表现高度不稳定，需发展专门的少样本方法。


<details>
  <summary>Details</summary>
Motivation: 尽管RSVLMs在大规模预训练下零样本性能优秀，但其在低数据（少样本）情境下的泛化能力尚未充分研究，需系统评估少样本自适配方法的效果。

Method: 在10个遥感场景分类数据集上，对3种具有不同主干网络的先进RSVLMs，应用5种常用的少样本自适应策略进行全面实验比较；评估指标包括少样本下的分类性能并分析不同模型与方法的适应性差异。

Result: 实验显示：具有类似零样本表现的模型在少样本适配时可能表现迥异；某些RSVLM天然更易于少样本适配；不同适配方法在不同模型与数据集上无统一优胜者，性能波动显著。

Conclusion: 本文构建了第一个用于评估遥感视觉-语言模型（RSVLMs）少样本自适应的结构化基准，发现现有RSVLM在少样本情形下表现差异大且无统一最优方法，强调需要为遥感量身定制更稳健的少样本适配方法，并开源了可复现框架。

Abstract: Remote Sensing Vision-Language Models (RSVLMs) have shown remarkable
potential thanks to large-scale pretraining, achieving strong zero-shot
performance on various tasks. However, their ability to generalize in low-data
regimes, such as few-shot learning, remains insufficiently explored. In this
work, we present the first structured benchmark for evaluating few-shot
adaptation methods on RSVLMs. We conduct comprehensive experiments across ten
remote sensing scene classification datasets, applying five widely used
few-shot adaptation strategies to three state-of-the-art RSVLMs with varying
backbones. Our findings reveal that models with similar zero-shot performance
can exhibit markedly different behavior under few-shot adaptation, with some
RSVLMs being inherently more amenable to such adaptation than others. The
variability of performance and the absence of a clear winner among existing
methods highlight the need for the development of more robust methods for
few-shot adaptation tailored to RS. To facilitate future research, we provide a
reproducible benchmarking framework and open-source code to systematically
evaluate RSVLMs under few-shot conditions. The source code is publicly
available on Github: https://github.com/elkhouryk/fewshot_RSVLMs

</details>


### [76] [Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods](https://arxiv.org/abs/2510.07143)
*Chenfei Liao,Wensong Wang,Zichen Wen,Xu Zheng,Yiyu Wang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Xin Zou,Yuqian Fu,Bin Ren,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出VTC-Bench：通过对现有多模态视觉token压缩基准进行基于下采样的数据过滤，去噪并重新评测压缩方法，发现简单下采样常胜于复杂压缩方法，说明原基准对压缩评估有任务不匹配和噪声问题。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM推理加速工作多集中于视觉token压缩，但评估直接使用原有感知/推理基准会导致任务不匹配，影响压缩方法的真实效果评估。发现下采样表现优异，提示基准存在噪声和不适合压缩评估的问题，需要更合适的评测框架。

Method: 分析并比较多种视觉token压缩方法与简单下采样在常用基准上的表现；通过观察下采样在评估中的性能表现，设计基于下采样的数据过滤机制来识别并剔除噪声样本，构建VTC-Bench评估框架。

Result: 通过大量实验发现：1) 现有基准对视觉token压缩任务噪声较大；2) 下采样在多数基准上胜过复杂压缩方法；3) 基于下采样的数据过滤可以有效去噪，从而使得VTC-Bench能够更公平地评估压缩方法。

Conclusion: 现有基准对视觉token压缩评估存在噪声和任务不匹配问题。通过下采样作为数据筛选，VTC-Bench能去除噪声样本，从而提供更公平、准确的压缩方法评估基准。

Abstract: Recent endeavors to accelerate inference in Multimodal Large Language Models
(MLLMs) have primarily focused on visual token compression. The effectiveness
of these methods is typically assessed by measuring the accuracy drop on
established benchmarks, comparing model performance before and after
compression. However, these benchmarks are originally designed to assess the
perception and reasoning capabilities of MLLMs, rather than to evaluate
compression techniques. As a result, directly applying them to visual token
compression introduces a task mismatch. Strikingly, our investigation reveals
that simple image downsampling consistently outperforms many advanced
compression methods across multiple widely used benchmarks. Through extensive
experiments, we make the following observations: (i) Current benchmarks are
noisy for the visual token compression task. (ii) Down-sampling is able to
serve as a data filter to evaluate the difficulty of samples in the visual
token compression task. Motivated by these findings, we introduce VTC-Bench, an
evaluation framework that incorporates a data filtering mechanism to denoise
existing benchmarks, thereby enabling fairer and more accurate assessment of
visual token compression methods. All data and code are available at
https://github.com/Chenfei-Liao/VTC-Bench.

</details>


### [77] [MV-Performer: Taming Video Diffusion Model for Faithful and Synchronized Multi-view Performer Synthesis](https://arxiv.org/abs/2510.07190)
*Yihao Zhi,Chenghong Li,Hongjie Liao,Xihe Yang,Zhengwentai Sun,Jiahao Chang,Xiaodong Cun,Wensen Feng,Xiaoguang Han*

Main category: cs.CV

TL;DR: 提出MV-Performer：利用相机依赖法线图与多视角视频扩散模型，从单目全身捕获生成同步的360度新视角视频，实验显示效果与鲁棒性领先。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在摄像机轨迹重定向方面受限，难以实现360度视点变化，尤其在人类全身场景下更具挑战性。为此提出专门针对人类子域的解决方案，通过额外的条件信息与多视图融合保持同步与真实感。

Method: 利用MVHumanNet大规模数据集，采用相机依赖的法线图（由有向部分点云渲染）作为条件信号，设计多视角人类视频扩散模型融合参考视频、部分渲染与目标视角信息，并提出用于野外(in-the-wild)视频的稳健推理流程以降低单目深度估计误差带来的伪影。

Result: 在三个数据集上进行大量实验，结果显示MV-Performer在生成质量与鲁棒性上优于现有方法，能够生成同步一致的360度新视角人类视频，并能在真实世界单目视频中显著减少伪影。

Conclusion: MV-Performer成功提出了一种面向人类的多视角视频生成框架，能从单目全身捕获生成同步的360度新视角视频。该方法通过结合相机相关的法线图、部分点云渲染与参考视频信息、并使用多视角人类视频扩散模型，有效缓解了视角不一致和深度估计不精确带来的伪影，实验表明在多个数据集上具有更强的效果与鲁棒性。

Abstract: Recent breakthroughs in video generation, powered by large-scale datasets and
diffusion techniques, have shown that video diffusion models can function as
implicit 4D novel view synthesizers. Nevertheless, current methods primarily
concentrate on redirecting camera trajectory within the front view while
struggling to generate 360-degree viewpoint changes. In this paper, we focus on
human-centric subdomain and present MV-Performer, an innovative framework for
creating synchronized novel view videos from monocular full-body captures. To
achieve a 360-degree synthesis, we extensively leverage the MVHumanNet dataset
and incorporate an informative condition signal. Specifically, we use the
camera-dependent normal maps rendered from oriented partial point clouds, which
effectively alleviate the ambiguity between seen and unseen observations. To
maintain synchronization in the generated videos, we propose a multi-view
human-centric video diffusion model that fuses information from the reference
video, partial rendering, and different viewpoints. Additionally, we provide a
robust inference procedure for in-the-wild video cases, which greatly mitigates
the artifacts induced by imperfect monocular depth estimation. Extensive
experiments on three datasets demonstrate our MV-Performer's state-of-the-art
effectiveness and robustness, setting a strong model for human-centric 4D novel
view synthesis.

</details>


### [78] [Resolution scaling governs DINOv3 transfer performance in chest radiograph classification](https://arxiv.org/abs/2510.07191)
*Soroosh Tayebi Arasteh,Mina Shaigan,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 在胸片任务中，使用DINOv3初始化并在512x512输入下微调的中等规模ConvNeXt-B可获得最佳性价比与性能，尤其提升小且边界敏感病灶的检测；更大分辨率或冻结超大模型回报有限。


<details>
  <summary>Details</summary>
Motivation: 评估现代自监督（尤其DINOv3）设计在胸片迁移学习中的实际价值，明确分辨率、骨干结构与是否微调等因素对临床任务性能的影响。

Method: 在七个胸片数据集（总样本>814k）上，对比DINOv3、DINOv2与ImageNet初始化；使用ViT-B/16与ConvNeXt-B两种骨干；评估224/512/1024分辨率；主要指标为各标签平均AUROC，同时测试冻结7B特征。

Result: DINOv3在胸片上相比DINOv2和ImageNet初始化具有分辨率敏感的优势，尤其在512x512时效果最佳；ConvNeXt-B优于ViT-B/16；大型冻结7B特征劣于微调中等规模模型；1024x1024收益有限。

Conclusion: 选择DINOv3初始化的ConvNeXt-B并在512x512分辨率下微调，是胸片自动化解读的实用最佳方案；需微调以适应领域，且超高分辨率和冻结超大模型并不划算。

Abstract: Self-supervised learning (SSL) has advanced visual representation learning,
but its value in chest radiography, a high-volume imaging modality with
fine-grained findings, remains unclear. Meta's DINOv3 extends earlier SSL
models through Gram-anchored self-distillation. Whether these design choices
improve transfer learning for chest radiography has not been systematically
tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across
seven datasets (n>814,000). Two representative backbones were evaluated:
ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and
1024x1024 pixels. We additionally assessed frozen features from a 7B model. The
primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2
achieved comparable performance on adult datasets. Increasing resolution to
512x512 yielded consistent improvements for DINOv3 over both DINOv2 and
ImageNet. In contrast, results in pediatric cohort showed no differences across
initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models
using frozen DINOv3-7B features underperformed relative to fully finetuned
86-89M-parameter backbones, highlighting the importance of domain adaptation.
Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains
were most evident for boundary-dependent and small focal abnormalities. In
chest radiography, higher input resolution is critical for leveraging the
benefits of modern self-supervised models. 512x512 pixels represent a practical
upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest
performance, while larger inputs offer minimal return on cost. Clinically,
these findings support use of finetuned, mid-sized backbones at 512x512 for
chest radiograph interpretation, with the greatest gains expected in detecting
subtle or boundary-centered lesions relevant to emergency and critical care
settings.

</details>


### [79] [EigenScore: OOD Detection using Covariance in Diffusion Models](https://arxiv.org/abs/2510.07206)
*Shirin Shoushtari,Yi Wang,Xiao Shi,M. Salman Asif,Ulugbek S. Kamilov*

Main category: cs.CV

TL;DR: EigenScore用扩散模型后验协方差的前导特征值作为异常分数，通过雅可比无关的子空间迭代只用前向评估估算特征值，显著提升OOD检测性能，尤其在近OOD场景。


<details>
  <summary>Details</summary>
Motivation: 提高OOD检测在关键应用中的可靠性，利用扩散模型的后验协方差作为分布错配信号。

Method: 基于扩散模型计算后验协方差的谱信息；分析证明谱与分布错配的联系；采用雅可比无关的子空间迭代只用去噪器前向评估来估算主特征值和迹，作为OOD分数。

Result: 提出EigenScore方法，通过扩散模型诱导的后验协方差的特征值谱进行OOD检测，在多项基准上实现SOTA，AUROC提升最多5%，在近OOD场景更稳健。

Conclusion: 后验协方差的谱特征（如迹和最大特征值）是可靠的分布偏移信号；EigenScore既有理论支持又在实验上优于现有方法，且计算上可行。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of
machine learning systems in safety-sensitive domains. Diffusion models have
recently emerged as powerful generative models, capable of capturing complex
data distributions through iterative denoising. Building on this progress,
recent work has explored their potential for OOD detection. We propose
EigenScore, a new OOD detection method that leverages the eigenvalue spectrum
of the posterior covariance induced by a diffusion model. We argue that
posterior covariance provides a consistent signal of distribution shift,
leading to larger trace and leading eigenvalues on OOD inputs, yielding a clear
spectral signature. We further provide analysis explicitly linking posterior
covariance to distribution mismatch, establishing it as a reliable signal for
OOD detection. To ensure tractability, we adopt a Jacobian-free subspace
iteration method to estimate the leading eigenvalues using only forward
evaluations of the denoiser. Empirically, EigenScore achieves SOTA performance,
with up to 5% AUROC improvement over the best baseline. Notably, it remains
robust in near-OOD settings such as CIFAR-10 vs CIFAR-100, where existing
diffusion-based methods often fail.

</details>


### [80] [GenPilot: A Multi-Agent System for Test-Time Prompt Optimization in Image Generation](https://arxiv.org/abs/2510.07217)
*Wen Ye,Zhaocheng Liu,Yuwei Gui,Tingyu Yuan,Yunyue Su,Bowen Fang,Chaoyang Zhao,Qiang Liu,Liang Wang*

Main category: cs.CV

TL;DR: GenPilot是一个面向复杂长提示词的模型无关测试时提示词优化框架，结合错误分析、聚类探索、细粒度验证与记忆迭代，显著提升文本到图像生成的语义一致性与结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖模型微调，要么只在噪声/采样上扩展，且先前自动提示优化缺乏系统性的错误分析和精细的验证策略，导致对复杂长提示词的处理不可靠。因而提出一种可解释、模型无关的测试时提示优化策略以提高生成一致性。

Method: 系统在测试时对提示词进行迭代改写：首先用错误分析模块识别语义缺失与冲突，聚类模块对潜在改写方向进行自适应探索生成候选提示，细粒度验证模块按子目标对生成图像进行评估，并利用记忆模块保留有效改写用于下一轮优化。该方法不依赖特定模型，可插拔多智能体协作实现。

Result: 在DPG-bench和Geneval数据集上，GenPilot在文本-图像一致性和结构连贯性指标上分别提升最多16.9%和5.7%，展示了显著效果，并总结了常见错误模式与改写策略。

Conclusion: GenPilot提出了一种在推理时直接优化输入文本的可插拔多智能体系统，通过错误分析、聚类自适应探索、细粒度验证与记忆模块进行迭代优化，能有效提升长复杂提示词在文本到图像生成中的一致性与结构连贯性。

Abstract: Text-to-image synthesis has made remarkable progress, yet accurately
interpreting complex and lengthy prompts remains challenging, often resulting
in semantic inconsistencies and missing details. Existing solutions, such as
fine-tuning, are model-specific and require training, while prior automatic
prompt optimization (APO) approaches typically lack systematic error analysis
and refinement strategies, resulting in limited reliability and effectiveness.
Meanwhile, test-time scaling methods operate on fixed prompts and on noise or
sample numbers, limiting their interpretability and adaptability. To solve
these, we introduce a flexible and efficient test-time prompt optimization
strategy that operates directly on the input text. We propose a plug-and-play
multi-agent system called GenPilot, integrating error analysis,
clustering-based adaptive exploration, fine-grained verification, and a memory
module for iterative optimization. Our approach is model-agnostic,
interpretable, and well-suited for handling long and complex prompts.
Simultaneously, we summarize the common patterns of errors and the refinement
strategy, offering more experience and encouraging further exploration.
Experiments on DPG-bench and Geneval with improvements of up to 16.9% and 5.7%
demonstrate the strong capability of our methods in enhancing the text and
image consistency and structural coherence of generated images, revealing the
effectiveness of our test-time prompt optimization strategy. The code is
available at https://github.com/27yw/GenPilot.

</details>


### [81] [TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video Generation](https://arxiv.org/abs/2510.07249)
*Jiaben Chen,Zixin Wang,Ailing Zeng,Yang Fu,Xueyang Yu,Siyuan Cen,Julian Tanke,Yihang Chen,Koichi Saito,Yuki Mitsufuji,Chuang Gan*

Main category: cs.CV

TL;DR: 构建了大规模多镜头演讲视频数据集TalkCuts并提出Orator基线，展示在多镜头、姿态与音频条件下能生成更连贯、更具电影感的长演讲视频。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多为单镜头静态视角，无法支持多镜头、长时序、人机协同的演讲视频生成，因而需要一个大规模、多样化、具备详细运动与文本注释的数据集以推动可控多镜头演讲视频生成研究。

Method: 收集并注释164k剪辑（500+小时），包含文本描述、2D关键点、3D SMPL-X动作及多镜头类型；提出Orator框架，使用大语言模型作为“导演”生成摄像机切换、手势与声音调度，然后由多模态生成模块合成多镜头长视频。分别在姿态引导与音频驱动下进行大量实验。

Result: TalkCuts包含164k剪辑、500+小时、10k+身份及多模态注释；在多项实验中，使用TalkCuts训练的模型在摄像机连贯性、视觉吸引力及多镜头生成质量上显著优于使用现有数据集训练的模型。

Conclusion: 该论文构建了一个大规模、多镜头的人类演讲视频数据集（TalkCuts），并提出了一个基于大模型的多模态生成基线（Orator），证明在该数据集上训练能提升多镜头演讲视频生成的电影感与连贯性。

Abstract: In this work, we present TalkCuts, a large-scale dataset designed to
facilitate the study of multi-shot human speech video generation. Unlike
existing datasets that focus on single-shot, static viewpoints, TalkCuts offers
164k clips totaling over 500 hours of high-quality human speech videos with
diverse camera shots, including close-up, half-body, and full-body views. The
dataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X
motion annotations, covering over 10k identities, enabling multimodal learning
and evaluation. As a first attempt to showcase the value of the dataset, we
present Orator, an LLM-guided multi-modal generation framework as a simple
baseline, where the language model functions as a multi-faceted director,
orchestrating detailed specifications for camera transitions, speaker
gesticulations, and vocal modulation. This architecture enables the synthesis
of coherent long-form videos through our integrated multi-modal video
generation module. Extensive experiments in both pose-guided and audio-driven
settings show that training on TalkCuts significantly enhances the
cinematographic coherence and visual appeal of generated multi-shot speech
videos. We believe TalkCuts provides a strong foundation for future work in
controllable, multi-shot speech video generation and broader multimodal
learning.

</details>


### [82] [Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema Detection](https://arxiv.org/abs/2510.07277)
*Franco Javier Arellano,José Ignacio Orlando*

Main category: cs.CV

TL;DR: 尽管规模更大，基础模型未必适合精细的眼科任务，轻量级微调CNN在资源受限场景仍具有竞争力，提示工程可提升某些FM的零样本能力。


<details>
  <summary>Details</summary>
Motivation: 动机是评估大规模预训练基础模型在眼底图像中用于DME检测的有效性，特别是在标注数据稀缺的情况下，探究FM能否带来优于传统迁移学习的性能。

Method: 作者比较了两种主流眼底基础模型（RETFound与FLAIR）与EfficientNet-B0在不同训练策略（包括微调与零样本）和多个数据集（IDRiD、MESSIDOR-2、OEFI）上的表现，使用AUC-ROC与AUC-PR等指标进行评估，并分析提示工程对FLAIR零样本性能的影响。

Result: 结果显示：1）FM并未在所有设置下优于微调的CNN；2）EfficientNet-B0在大部分评估中在AUC-ROC与AUC-PR上名列前茅；3）RETFound仅在OEFI数据集上表现较好；4）FLAIR在合适提示下展现出具有竞争力的零样本AUC-PR。

Conclusion: 本文结论为：在糖尿病性黄斑水肿（DME）检测任务上，大型基础模型（FM）并不始终优于经过微调的轻量级卷积神经网络（CNN），在数据稀缺的眼科精细任务中，EfficientNet-B0等轻量级CNN依然是强有力的基线。

Abstract: Diabetic Macular Edema (DME) is a leading cause of vision loss among patients
with Diabetic Retinopathy (DR). While deep learning has shown promising results
for automatically detecting this condition from fundus images, its application
remains challenging due the limited availability of annotated data. Foundation
Models (FM) have emerged as an alternative solution. However, it is unclear if
they can cope with DME detection in particular. In this paper, we
systematically compare different FM and standard transfer learning approaches
for this task. Specifically, we compare the two most popular FM for retinal
images--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different
training regimes and evaluation settings in IDRiD, MESSIDOR-2 and
OCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do
not consistently outperform fine-tuned CNNs in this task. In particular, an
EfficientNet-B0 ranked first or second in terms of area under the ROC and
precision/recall curves in most evaluation settings, with RETFound only showing
promising results in OEFI. FLAIR, on the other hand, demonstrated competitive
zero-shot performance, achieving notable AUC-PR scores when prompted
appropriately. These findings reveal that FM might not be a good tool for
fine-grained ophthalmic tasks such as DME detection even after fine-tuning,
suggesting that lightweight CNNs remain strong baselines in data-scarce
environments.

</details>


### [83] [SpecGuard: Spectral Projection-based Advanced Invisible Watermarking](https://arxiv.org/abs/2510.07302)
*Inzamamul Alam,Md Tanvir Islam,Khan Muhammad,Simon S. Woo*

Main category: cs.CV

TL;DR: SpecGuard embeds watermarks into hidden convolutional layer frequency bands via wavelet-decomposed spectral projection and FFT approximation, with strength factor and Parseval-theorem-based decoder, yielding superior robustness and invisibility


<details>
  <summary>Details</summary>
Motivation: Improve watermark robustness against distortions, regeneration, and adversarial attacks by embedding in frequency components of hidden conv layers using wavelet decomposition and spectral projection

Method: Spectral wavelet embedding with FFT approximation and Parseval-based decoding

Result: SpecGuard achieves higher invisibility, capacity, and robustness than prior methods; code released

Conclusion: Embedding in higher-frequency spectral components of hidden layers and using Parseval-guided decoding with adjustable strength yields a robust, invisible watermarking method outperforming SOTA

Abstract: Watermarking embeds imperceptible patterns into images for authenticity
verification. However, existing methods often lack robustness against various
transformations primarily including distortions, image regeneration, and
adversarial perturbation, creating real-world challenges. In this work, we
introduce SpecGuard, a novel watermarking approach for robust and invisible
image watermarking. Unlike prior approaches, we embed the message inside hidden
convolution layers by converting from the spatial domain to the frequency
domain using spectral projection of a higher frequency band that is decomposed
by wavelet projection. Spectral projection employs Fast Fourier Transform
approximation to transform spatial data into the frequency domain efficiently.
In the encoding phase, a strength factor enhances resilience against diverse
attacks, including adversarial, geometric, and regeneration-based distortions,
ensuring the preservation of copyrighted information. Meanwhile, the decoder
leverages Parseval's theorem to effectively learn and extract the watermark
pattern, enabling accurate retrieval under challenging transformations. We
evaluate the proposed SpecGuard based on the embedded watermark's invisibility,
capacity, and robustness. Comprehensive experiments demonstrate the proposed
SpecGuard outperforms the state-of-the-art models. To ensure reproducibility,
the full code is released on
\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}.

</details>


### [84] [MATRIX: Mask Track Alignment for Interaction-aware Video Generation](https://arxiv.org/abs/2510.07310)
*Siyoon Jin,Seongchan Kim,Dahyun Chung,Jaeho Lee,Hyunwook Choi,Jisu Nam,Jiyoung Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: paper studies video DiT attention, proposes dataset MATRIX-11K and regularization MATRIX to align attention with instance masks, improving interaction modeling


<details>
  <summary>Details</summary>
Motivation: understand interaction representation in video DiTs and improve it

Method: analysis of methods

Result: MATRIX regularization improves grounding and propagation, better interaction fidelity and reduced hallucination

Conclusion: MATRIX leverages interaction-dominant layers to regularize attention, enhancing multi-instance interaction modeling in video generation

Abstract: Video DiTs have advanced video generation, yet they still struggle to model
multi-instance or subject-object interactions. This raises a key question: How
do these models internally represent interactions? To answer this, we curate
MATRIX-11K, a video dataset with interaction-aware captions and multi-instance
mask tracks. Using this dataset, we conduct a systematic analysis that
formalizes two perspectives of video DiTs: semantic grounding, via
video-to-text attention, which evaluates whether noun and verb tokens capture
instances and their relations; and semantic propagation, via video-to-video
attention, which assesses whether instance bindings persist across frames. We
find both effects concentrate in a small subset of interaction-dominant layers.
Motivated by this, we introduce MATRIX, a simple and effective regularization
that aligns attention in specific layers of video DiTs with multi-instance mask
tracks from the MATRIX-11K dataset, enhancing both grounding and propagation.
We further propose InterGenEval, an evaluation protocol for interaction-aware
video generation. In experiments, MATRIX improves both interaction fidelity and
semantic alignment while reducing drift and hallucination. Extensive ablations
validate our design choices. Codes and weights will be released.

</details>


### [85] [WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation](https://arxiv.org/abs/2510.07313)
*Zezhong Qian,Xiaowei Chi,Yuming Li,Shizun Wang,Zhiyuan Qin,Xiaozhu Ju,Sirui Han,Shanghang Zhang*

Main category: cs.CV

TL;DR: WristWorld是首个能仅用anchor视角生成腕部视角视频的4D世界模型，通过SPC损失的几何一致性重建和后续的视频生成，改善空间一致性并提升VLA任务表现。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中缺乏腕部视角记录，但腕部视角对捕捉细粒度手-物交互至关重要，现有世界模型需要腕部首帧输入，无法仅用anchor视角生成腕部视频，因此提出通过几何与跨视角先验来跨越视角差距。

Method: 两阶段方法：1) 重建：扩展VGGT并引入SPC损失，估计腕部位姿和4D点云，确保几何一致性；2) 生成：基于重建的视角使用视频生成模型合成时序一致的视频。

Result: WristWorld提出了一种从anchor视角生成腕部视角（wrist-view）视频的4D世界模型，通过重建和生成两阶段实现。重建阶段基于VGGT并引入空间投影一致性（SPC）损失来估计几何一致的腕部位姿与4D点云；生成阶段则从该重建视角合成时序连贯的视频。实验在Droid、Calvin、Franka Panda数据集上显示在视频生成空间一致性方面取得了SOTA，并提升了VLA性能，在Calvin上平均任务完成长度提升3.81%，缩小了锚点视角与腕部视角之间42.4%的差距。

Conclusion: WristWorld有效弥合了anchor视角与腕部视角之间的数据缺口，提供几何一致且时序连贯的腕部视频生成，进而提升基于视觉的强化学习（VLA）性能。

Abstract: Wrist-view observations are crucial for VLA models as they capture
fine-grained hand-object interactions that directly enhance manipulation
performance. Yet large-scale datasets rarely include such recordings, resulting
in a substantial gap between abundant anchor views and scarce wrist views.
Existing world models cannot bridge this gap, as they require a wrist-view
first frame and thus fail to generate wrist-view videos from anchor views
alone. Amid this gap, recent visual geometry models such as VGGT emerge with
geometric and cross-view priors that make it possible to address extreme
viewpoint shifts. Inspired by these insights, we propose WristWorld, the first
4D world model that generates wrist-view videos solely from anchor views.
WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and
incorporates our Spatial Projection Consistency (SPC) Loss to estimate
geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,
which employs our video generation model to synthesize temporally coherent
wrist-view videos from the reconstructed perspective. Experiments on Droid,
Calvin, and Franka Panda demonstrate state-of-the-art video generation with
superior spatial consistency, while also improving VLA performance, raising the
average task completion length on Calvin by 3.81% and closing 42.4% of the
anchor-wrist view gap.

</details>


### [86] [Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers](https://arxiv.org/abs/2510.07316)
*Gangwei Xu,Haotong Lin,Hongcheng Luo,Xianqi Wang,Jingfeng Yao,Lianghui Zhu,Yuechuan Pu,Cheng Chi,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Sida Peng,Xin Yang*

Main category: cs.CV

TL;DR: 在像素空间进行扩散生成，结合语义提示的DiT和级联令牌扩展，消除VAE引入的飞像素，获得最优生成深度与边缘感知点云表现。


<details>
  <summary>Details</summary>
Motivation: 现有生成式深度估计模型依赖VAE将深度图压缩到潜在空间，导致边缘与细节出现飞像素，故提出在像素空间直接生成以消除此类伪影并提升点云质量。

Method: 1) 在像素空间执行扩散生成而非潜在空间，避免VAE压缩失真。2) Semantics-Prompted Diffusion Transformers (SP-DiT)：将视觉基础模型的语义表征融入DiT中作为提示，以维护全局语义一致性并改善细节。3) 级联DiT设计：递增令牌数量，分阶段生成以提高效率与精度。

Result: Pixel-Perfect Depth提出了一种基于像素空间扩散生成的单目深度估计模型，避免了VAE压缩带来的飞像素问题，通过SP-DiT和级联DiT设计在保持全局语义一致性的同时提高细节质量和效率。

Conclusion: 该方法通过跳过VAE并在像素空间直接扩散生成，有效去除了飞像素，提升了细节与边缘保真度，在五个基准上领先其他生成模型，尤其在边缘感知点云评估上优势显著。

Abstract: This paper presents Pixel-Perfect Depth, a monocular depth estimation model
based on pixel-space diffusion generation that produces high-quality,
flying-pixel-free point clouds from estimated depth maps. Current generative
depth estimation models fine-tune Stable Diffusion and achieve impressive
performance. However, they require a VAE to compress depth maps into latent
space, which inevitably introduces \textit{flying pixels} at edges and details.
Our model addresses this challenge by directly performing diffusion generation
in the pixel space, avoiding VAE-induced artifacts. To overcome the high
complexity associated with pixel-space generation, we introduce two novel
designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which
incorporate semantic representations from vision foundation models into DiT to
prompt the diffusion process, thereby preserving global semantic consistency
while enhancing fine-grained visual details; and 2) Cascade DiT Design that
progressively increases the number of tokens to further enhance efficiency and
accuracy. Our model achieves the best performance among all published
generative models across five benchmarks, and significantly outperforms all
other models in edge-aware point cloud evaluation.

</details>


### [87] [Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms](https://arxiv.org/abs/2510.07317)
*Natacha Kuete Meli,Shuteng Wang,Marcel Seelbach Benkner,Michele Sasdelli,Tat-Jun Chin,Tolga Birdal,Michael Moeller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: 本文為針對計算機視覺社群的量子增強計算機視覺綜述，介紹基本原理、兩大量子範式的應用方法、工具與教學資源，並討論挑戰與影響。


<details>
  <summary>Details</summary>
Motivation: 介紹並推動量子增強計算機視覺（QeCV）領域，為計算機視覺研究者提供兼容量子硬體的方法與資源，彌補經典方法在時間複雜度與近似解上的不足。

Method: 闡述閘式量子電路與量子退火兩種方法，介紹參數化量子電路作為類神經網路的替代方案，並討論如何將計算機視覺問題形式化以適配量子硬體。

Result: 提供全面的QeCV綜述，包括兩大量子計算範式（閘式量子計算與量子退火）的理論基礎、可用工具、編程與模擬方法，並討論發表審稿建議、開放挑戰及社會影響。

Conclusion: QeCV為計算機視覺帶來潛在的計算優勢與新演算法路徑，但需要專門設計的量子相容演算法、更多工具、教學資源與跨領域合作才能實現可行性與普及。

Abstract: Quantum-enhanced Computer Vision (QeCV) is a new research field at the
intersection of computer vision, optimisation theory, machine learning and
quantum computing. It has high potential to transform how visual signals are
processed and interpreted with the help of quantum computing that leverages
quantum-mechanical effects in computations inaccessible to classical (i.e.
non-quantum) computers. In scenarios where existing non-quantum methods cannot
find a solution in a reasonable time or compute only approximate solutions,
quantum computers can provide, among others, advantages in terms of better time
scalability for multiple problem classes. Parametrised quantum circuits can
also become, in the long term, a considerable alternative to classical neural
networks in computer vision. However, specialised and fundamentally new
algorithms must be developed to enable compatibility with quantum hardware and
unveil the potential of quantum computational paradigms in computer vision.
This survey contributes to the existing literature on QeCV with a holistic
review of this research field. It is designed as a quantum computing reference
for the computer vision community, targeting computer vision students,
scientists and readers with related backgrounds who want to familiarise
themselves with QeCV. We provide a comprehensive introduction to QeCV, its
specifics, and methodologies for formulations compatible with quantum hardware
and QeCV methods, leveraging two main quantum computational paradigms, i.e.
gate-based quantum computing and quantum annealing. We elaborate on the
operational principles of quantum computers and the available tools to access,
program and simulate them in the context of QeCV. Finally, we review existing
quantum computing tools and learning materials and discuss aspects related to
publishing and reviewing QeCV papers, open challenges and potential social
implications.

</details>


### [88] [Temporal Prompting Matters: Rethinking Referring Video Object Segmentation](https://arxiv.org/abs/2510.07319)
*Ci-Siang Lin,Min-Hung Chen,I-Jieh Liu,Chien-Yi Wang,Sifei Liu,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: Tenet利用检测器/跟踪器生成时序提示并用Prompt Preference Learning筛选，将图像分割模型扩展到RVOS，减少端到端训练需求且在基准上效果良好。


<details>
  <summary>Details</summary>
Motivation: 许多现有RVOS方法依赖端到端训练和密集掩码标注，计算开销大且难以扩展。作者希望利用图像基础分割模型的能力，通过分解问题并用高效的提示生成与选择来避免昂贵的训练。

Method: 使用现成目标检测器和跟踪器生成与查询句子关联的时序prompts；提出Prompt Preference Learning来评估并选择高质量prompt；将选定prompt作为指令输入图像基础分割模型以生成每帧的掩码，实现将视频与指称信息对齐而无需对分割模型进行密集标注的端到端训练。

Result: 在多个RVOS基准数据集上，Tenet框架显著提升了利用图像基础分割模型处理指代视频分割的性能，表明通过生成与筛选时序prompts可以高效适配现有分割模型。

Conclusion: 本文提出了Tenet框架，通过将RVOS任务拆解为referring、video和segmentation三部分，利用检测器/跟踪器生成时序提示并用Prompt Preference Learning选择高质量提示，从而借助图像基础分割模型高效完成视屏中被指代目标的分割。实验表明该方法在RVOS基准上有效。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment the object
referred to by the query sentence in the video. Most existing methods require
end-to-end training with dense mask annotations, which could be
computation-consuming and less scalable. In this work, we rethink the RVOS
problem and aim to investigate the key to this task. Based on existing
foundation segmentation models, we decompose the RVOS task into referring,
video, and segmentation factors, and propose a Temporal Prompt Generation and
Selection (Tenet) framework to address the referring and video factors while
leaving the segmentation problem to foundation models. To efficiently adapt
image-based foundation segmentation models to referring video object
segmentation, we leverage off-the-shelf object detectors and trackers to
produce temporal prompts associated with the referring sentence. While
high-quality temporal prompts could be produced, they can not be easily
identified from confidence scores. To tackle this issue, we propose Prompt
Preference Learning to evaluate the quality of the produced temporal prompts.
By taking such prompts to instruct image-based foundation segmentation models,
we would be able to produce high-quality masks for the referred object,
enabling efficient model adaptation to referring video object segmentation.
Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet
framework.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [89] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi,Adrian Rebmann,Timotheus Kampik,Matthias Weidlich,Mathias Weske*

Main category: cs.DB

TL;DR: 提出把传统的命令式流程模型转换为可在关系数据库上运行的SQL查询，以便在数据驱动的流程管理中复用现有模型，桥接模型化与数据驱动分析的差距。


<details>
  <summary>Details</summary>
Motivation: 关系型数据库中存有大量结构化的流程执行数据，但传统的命令式流程模型难以直接应用于这些数据，造成模型与数据驱动分析之间的脱节。该工作旨在弥合这一差距，复用现有流程模型以支持基于数据的一致性检验。

Method: 通过定义从Petri网等命令式模型到“松散流程数据查询”的形式化映射，生成可在关系数据库上执行的SQL查询，用于对事件数据进行一致性检查。

Result: 这篇论文实现了将命令式流程模型（如Petri网）自动转换为在关系数据库上可执行的松散流程数据查询（SQL），用于一致性检验（conformance checking）。

Conclusion: 命令式流程模型仍然对数据驱动的流程管理有价值；将其转换为行为足迹或其他声明式查询能实现与关系数据库中执行数据的对齐，从而促进模型与数据的整合。

Abstract: Business process management is increasingly practiced using data-driven
approaches. Still, classical imperative process models, which are typically
formalized using Petri nets, are not straightforwardly applicable to the
relational databases that contain much of the available structured process
execution data. This creates a gap between the traditional world of process
modeling and recent developments around data-driven process analysis,
ultimately leading to the under-utilization of often readily available process
models. In this paper, we close this gap by providing an approach for
translating imperative models into relaxed process data queries, specifically
SQL queries executable on relational databases, for conformance checking. Our
results show the continued relevance of imperative process models to
data-driven process management, as well as the importance of behavioral
footprints and other declarative approaches for integrating model-based and
data-driven process management.

</details>


### [90] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang,Runyuan He,Suyang Zhong,Xiaoxuan Liu,Huanchen Zhang,Alvin Cheung*

Main category: cs.DB

TL;DR: Argus用“受约束抽象查询”结合LLM与形式化求解器自动发现并安全实例化等价查询，高效发现大量DBMS逻辑bug，降低LLM幻觉带来的误报和调用成本。


<details>
  <summary>Details</summary>
Motivation: 自动化DBMS测试的关键——测试判定器（test oracle）依赖手工设计等价查询对，限制了全面自动化。希望利用大模型自动生成并实例化等价查询来自动发现DBMS逻辑错误，同时避免大模型幻觉和高成本带来的误报与效率问题。

Method: 定义Constrained Abstract Query抽象语法与占位符约束；用LLM生成等价抽象查询对；用SQL等价性求解器证明等价性以保证无幻觉误报；再用LLM生成可重用SQL片段实例化占位符以构建复杂测试用例；实现并在多DBMS上评测。

Result: 提出Argus框架：引入受约束抽象查询（带占位符及其约束），用LLM生成等价抽象查询对，借助SQL等价性求解器形式化验证等价性，随后用LLM合成可重用SQL片段实例化占位符以生成测试用例。实现并在五个DBMS上发现40个新bug（35个逻辑bug），36个已确认，26个被修复。

Conclusion: 通过将LLM的创造性与形式化验证结合，Argus实现了自动、安全且高效的DBMS判定器发现与实例化，显著提升了自动化测试能力并找到真实的逻辑缺陷。

Abstract: Since 2020, automated testing for Database Management Systems (DBMSs) has
flourished, uncovering hundreds of bugs in widely-used systems. A cornerstone
of these techniques is test oracle, which typically implements a mechanism to
generate equivalent query pairs, thereby identifying bugs by checking the
consistency between their results. However, while applying these oracles can be
automated, their design remains a fundamentally manual endeavor. This paper
explores the use of large language models (LLMs) to automate the discovery and
instantiation of test oracles, addressing a long-standing bottleneck towards
fully automated DBMS testing. Although LLMs demonstrate impressive creativity,
they are prone to hallucinations that can produce numerous false positive bug
reports. Furthermore, their significant monetary cost and latency mean that LLM
invocations should be limited to ensure that bug detection is efficient and
economical.
  To this end, we introduce Argus, a novel framework built upon the core
concept of the Constrained Abstract Query - a SQL skeleton containing
placeholders and their associated instantiation conditions (e.g., requiring a
placeholder to be filled by a boolean column). Argus uses LLMs to generate
pairs of these skeletons that are asserted to be semantically equivalent. This
equivalence is then formally proven using a SQL equivalence solver to ensure
soundness. Finally, the placeholders within the verified skeletons are
instantiated with concrete, reusable SQL snippets that are also synthesized by
LLMs to efficiently produce complex test cases. We implemented Argus and
evaluated it on five extensively tested DBMSs, discovering 40 previously
unknown bugs, 35 of which are logic bugs, with 36 confirmed and 26 already
fixed by the developers.

</details>


### [91] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao,Jingxi Zhang,Lijian Chen,Tong Chen,Lizhen Cui,Hongzhi Yin*

Main category: cs.DB

TL;DR: 提出RDD，将大规模关系型数据库蒸馏为紧凑的异构图，保留列信息与主外键关系，使用核岭回归与伪标签目标避免双层蒸馏，实验证明在分类/回归任务上压缩数据同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Relational databases are large and complex; recent graph representation learning methods capture inter-table relations but suffer storage and training costs due to scale and message passing. The paper proposes to compress RDBs into compact heterogeneous graphs retaining predictive utility.

Method: 将表中记录作为节点、多模态列编码为节点特征、主外键编码为异构边，构建紧凑图。为避免昂贵的双层蒸馏，提出基于核岭回归的目标与伪标签生成策略来优化蒸馏后的节点特征，使其适用于下游图模型训练。

Result: They propose Relational Database Distillation (RDD) that creates compact heterogeneous graphs preserving multi-modal column info as node features and primary-foreign keys as edges, and design a kernel ridge regression-guided objective with pseudo-labels to produce quality features without bi-level distillation. Experiments show large size reduction with competitive performance on classification and regression.

Conclusion: RDD有效压缩RDB为小型异构图，保留关键结构与特征，通过核岭回归引导的目标产生有用伪标签，实验证明可在多个真实数据集上保持预测性能并显著降低存储与训练开销。

Abstract: Relational databases (RDBs) underpin the majority of global data management
systems, where information is structured into multiple interdependent tables.
To effectively use the knowledge within RDBs for predictive tasks, recent
advances leverage graph representation learning to capture complex inter-table
relations as multi-hop dependencies. Despite achieving state-of-the-art
performance, these methods remain hindered by the prohibitive storage overhead
and excessive training time, due to the massive scale of the database and the
computational burden of intensive message passing across interconnected tables.
To alleviate these concerns, we propose and study the problem of Relational
Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs
into compact heterogeneous graphs while retaining the predictive power (i.e.,
utility) required for training graph-based models. Multi-modal column
information is preserved through node features, and primary-foreign key
relations are encoded via heterogeneous edges, thereby maintaining both data
fidelity and relational structure. To ensure adaptability across diverse
downstream tasks without engaging the traditional, inefficient bi-level
distillation framework, we further design a kernel ridge regression-guided
objective with pseudo-labels, which produces quality features for the distilled
graph. Extensive experiments on multiple real-world RDBs demonstrate that our
solution substantially reduces the data size while maintaining competitive
performance on classification and regression tasks, creating an effective
pathway for scalable learning with RDBs.

</details>


### [92] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield,Liat Peterfreund*

Main category: cs.DB

TL;DR: 论文形式化并划分SQL/PGQ的三类片段，发现图创建与视图标识符元数决定表达能力：只读<读写<扩展（=NL），并在有序结构上发生塌缩。


<details>
  <summary>Details</summary>
Motivation: 随着SQL/PGQ成为查询属性图的ISO新标准，理解其不同特性（尤其图的创建与视图定义）如何影响语言的表达能力与复杂度是理论与实践都关心的问题。研究旨在为语言设计、优化及实现提供理论基础。

Method: 使用形式化语义和复杂度理论方法，对三类SQL/PGQ片段进行表达能力分类和证明。具体手段包括构造性证明展示从读写片段到NL的上界与下界、使用视图定义中标识符元数的变化构造严格分离的片段，以及在有序结构中基于传递闭包性质的等价性论证。

Result: 证明只读核心弱于读写扩展，读写扩展表达能力低于NL；允许任意元数标识符的扩展片段精确捕获NL；由此形成严格层次，其并集为NL。在有序结构上，允许二元标识符即能达到全部功效，较高元数无增益。

Conclusion: 本文将SQL/PGQ分为三类片段：只读核心、可读写扩展和具有更丰富视图定义的扩展变体，并证明图创建操作决定了表达能力。只读片段严格弱于可读写片段，后者的表达能力仍低于复杂度类NL。通过允许任意元组数标识符的视图定义可弥补此差距，使扩展片段恰好捕获NL，从而得到严格的片段层次，其并集覆盖所有NL查询。在有序结构上，该层次塌缩：一旦允许二元标识符，更高元数不再增加表达能力，类似于经典的传递闭包塌缩，突出视图构造在属性图查询中的核心作用。

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as
views over relational data. We formalize its expressive power across three
fragments: the read-only core, the read-write extension, and an extended
variant with richer view definitions. Our results show that graph creation
plays a central role in determining the expressiveness. The read-only fragment
is strictly weaker than the read-write fragment, and the latter is still below
the complexity class NL. Extending view definitions with arbitrary arity
identifiers closes this gap: the extended fragment captures exactly NL. This
yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL
queries. On ordered structures the hierarchy collapses: once arity-2
identifiers are allowed, higher arities add no power, mirroring the classical
transitive-closure collapse and underscoring the central role of view
construction in property graph querying.

</details>
