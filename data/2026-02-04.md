<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 109]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [WorldVQA: Measuring Atomic World Knowledge in Multimodal Large Language Models](https://arxiv.org/abs/2602.02537)
*Runjie Zhou,Youbo Shao,Haoyu Lu,Bowei Xing,Tongtong Bai,Yujie Chen,Jie Zhao,Lin Sui,Haotian Yao,Zijia Zhao,Hao Yang,Haoning Wu,Zaida Zhou,Jinguo Zhu,Zhiqi Huang,Yiping Bao,Yangyang Liu,Y. Charles,Xinyu Zhou*

Main category: cs.CV

TL;DR: WorldVQA是为测量MLLM原子视觉记忆（命名/识别视觉实体）而设计的分层基准，强调解耦推理以评估视觉事实性和幻觉率。


<details>
  <summary>Details</summary>
Motivation: 现有评测往往将视觉知识检索与推理混为一谈，难以单独评估模型记忆的视觉事实性；因此需要一个只测“模型记住了什么”的基准。

Method: 构建分层分类的视觉实体词汇表（从常见头部类到长尾罕见项），设计仅需标注与命名的测试题以避免推理成分，并要求模型直接检索或复述其视觉知识。

Result: 提出WorldVQA基准和任务定义，提供分层语料覆盖长尾实体，有助于测量模型在不同频次实体上的识记率与幻觉倾向，预计成为评估视觉百科广度的标准。

Conclusion: WorldVQA通过解耦视觉记忆与推理，专注于原子级视觉知识评估，为衡量MLLM的视觉事实性和幻觉率提供了严格基准。

Abstract: We introduce WorldVQA, a benchmark designed to evaluate the atomic visual world knowledge of Multimodal Large Language Models (MLLMs). Unlike current evaluations, which often conflate visual knowledge retrieval with reasoning, WorldVQA decouples these capabilities to strictly measure "what the model memorizes." The benchmark assesses the atomic capability of grounding and naming visual entities across a stratified taxonomy, spanning from common head-class objects to long-tail rarities. We expect WorldVQA to serve as a rigorous test for visual factuality, thereby establishing a standard for assessing the encyclopedic breadth and hallucination rates of current and next-generation frontier models.

</details>


### [2] [AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process](https://arxiv.org/abs/2602.02676)
*Xintong Zhang,Xiaowen Zhang,Jongrong Wu,Zhi Gao,Shilin Yan,Zhenxin Diao,Kunpeng Gao,Xuanyan Chen,Yuwei Wu,Yunde Jia,Qing Li*

Main category: cs.CV

TL;DR: 提出AdaptMMBench：基于模型能力边界动态划分难度，使用MCC评估自适应模式选择，并进行多维过程评估，揭示模式选择与最终性能的解耦及工具效用不稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前自适应多模态推理评测依赖静态难度标签和粗糙指标，无法反映难度随模型能力变化的动态性，也掩盖了模式选择与模型总体能力的区别，缺乏细粒度过程分析。

Method: 构建跨五类任务（现实场景、OCR、GUI、知识、数学）的数据集，基于模型能力边界动态判定题目难度；用Matthews相关系数(MCC)评估模式选择合理性；并设计多维过程评估指标：关键步骤覆盖、工具有效性和计算效率。

Result: 实验表明：1) 自适应模式选择随模型容量提升，但与最终准确率解耦；2) 关键步骤覆盖与性能高度相关；3) 工具效用在不同模型间差异显著；并通过MCC成功量化模式选择合理性。

Conclusion: 本文提出AdaptMMBench，为自适应多模态推理提供了全面基准，揭示模型在模式选择上的能力随规模提升但与最终准确率解耦；关键步骤覆盖率与性能相关，而工具效用在不同架构间不稳定。

Abstract: Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models' capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.

</details>


### [3] [End-to-end reconstruction of OCT optical properties and speckle-reduced structural intensity via physics-based learning](https://arxiv.org/abs/2602.02721)
*Jinglun Yu,Yaning Wang,Wenhan Guo,Yuan Gao,Yu Sun,Jin U. Kang*

Main category: cs.CV

TL;DR: 将物理驱动的正向模型融入端到端深度网络，用蒙特卡洛模拟训练，实现对OCT的多参数定量重建与降噪结构恢复，提升稳健性和保真度。


<details>
  <summary>Details</summary>
Motivation: OCT逆散射问题因信号衰减、斑点噪声及参数间强耦合导致参数恢复困难，传统方法难以同时给出高保真结构和准确的多参数定量图。结合物理先验与深度学习可提升稳健性和精度。

Method: 基于蒙特卡洛模拟生成的真实标签训练网络，网络在估计参数后嵌入基于物理的OCT正向模型，将估计参数生成的预测信号与观测数据进行比对，作为物理一致性的监督，网络同时输出光学参数图与去斑结构图，采用正则化策略抑制耦合伪影与噪声。

Result: 在合成角膜OCT数据集上实验表明：在噪声存在下，方法能稳健恢复光学参数映射，分辨率与结构保真度均有提升，同时有效抑制伪影和斑点噪声。

Conclusion: 本文提出一种结合物理模型的端到端深度学习框架，用于光学相干断层成像（OCT）的逆散射问题，能够联合重建光学参数映射（折射率、散射系数、各向异性）与降噪结构强度图，提升参数恢复的稳健性与结构保真度。

Abstract: Inverse scattering in optical coherence tomography (OCT) seeks to recover both structural images and intrinsic tissue optical properties, including refractive index, scattering coefficient, and anisotropy. This inverse problem is challenging due to attenuation, speckle noise, and strong coupling among parameters. We propose a regularized end-to-end deep learning framework that jointly reconstructs optical parameter maps and speckle-reduced OCT structural intensity for layer visualization. Trained with Monte Carlo-simulated ground truth, our network incorporates a physics-based OCT forward model that generates predicted signals from the estimated parameters, providing physics-consistent supervision for parameter recovery and artifact suppression. Experiments on the synthetic corneal OCT dataset demonstrate robust optical map recovery under noise, improved resolution, and enhanced structural fidelity. This approach enables quantitative multi-parameter tissue characterization and highlights the benefit of combining physics-informed modeling with deep learning for computational OCT.

</details>


### [4] [SVD-ViT: Does SVD Make Vision Transformers Attend More to the Foreground?](https://arxiv.org/abs/2602.02765)
*Haruhiko Murata,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: 作者提出SVD-ViT，通过对Transformer特征做奇异值分解并选择聚合与前景相关的奇异向量，抑制背景噪声，从而提升分类性能。


<details>
  <summary>Details</summary>
Motivation: ViT缺乏显式的前景/背景区分机制，全局自注意力可能学习到与任务无关的背景特征和伪影，影响分类性能，因此希望通过低秩分解聚焦前景信息。

Method: 设计了三部分：SPC模块负责生成局部协方差或特征子空间，SSVA对奇异向量进行选择与加权聚合，ID-RSVD用于高效且可区分地计算或近似奇异值分解以保持计算代价可控。整体在Transformer的自注意力特征上应用SVD处理，重建或强调前景表示。

Result: 实验证明SVD-ViT在分类精度上有提升，并且学得的表示更聚焦于前景，减少了背景影响（作者报告的定量和可视化证据）。

Conclusion: SVD-ViT通过SVD提取和聚合与前景相关的奇异向量，有效抑制背景噪声和伪影，从而提升ViT的分类性能。

Abstract: Vision Transformers (ViT) have been established as large-scale foundation models. However, because self-attention operates globally, they lack an explicit mechanism to distinguish foreground from background. As a result, ViT may learn unnecessary background features and artifacts, leading to degraded classification performance. To address this issue, we propose SVD-ViT, which leverages singular value decomposition (SVD) to prioritize the learning of foreground features. SVD-ViT consists of three components-\textbf{SPC module}, \textbf{SSVA}, and \textbf{ID-RSVD}-and suppresses task-irrelevant factors such as background noise and artifacts by extracting and aggregating singular vectors that capture object foreground information. Experimental results demonstrate that our method improves classification accuracy and effectively learns informative foreground representations while reducing the impact of background noise.

</details>


### [5] [LmPT: Conditional Point Transformer for Anatomical Landmark Detection on 3D Point Clouds](https://arxiv.org/abs/2602.02808)
*Matteo Bastico,Pierre Onghena,David Ryckelynck,Beatriz Marcotegui,Santiago Velasco-Forero,Laurent Corté,Caroline Robine--Decourcelle,Etienne Decencière*

Main category: cs.CV

TL;DR: 提出基于点云的LmPT模型，通过条件机制实现跨物种股骨标志点自动检测，在人类与犬类数据上验证了泛化性与有效性，数据与代码将开源。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注耗时且存在观察者间差异，基于规则的方法往往对几何或标志点集合有强依赖。点云作为轻量表示，便于处理和跨物种迁移学习，因此提出自动化且可泛化的标志点检测方法。

Method: LmPT 在点云上采用Transformer 风格的架构，并引入条件机制（conditioning）来使模型能接受不同类型的输入，从而实现跨物种（如人类与犬类股骨）泛化。训练和评估使用人类与新标注的犬股骨点云数据。

Result: 在股骨标志点检测任务上，LmPT 在人类与犬类数据上均表现出良好的一般化能力与有效性。作者将公开代码与犬股骨数据集以便复现和后续研究。

Conclusion: 本文提出了Landmark Point Transformer (LmPT)，一种基于点云的自动解剖标志点检测方法，能够跨物种进行学习并适配不同输入类型。

Abstract: Accurate identification of anatomical landmarks is crucial for various medical applications. Traditional manual landmarking is time-consuming and prone to inter-observer variability, while rule-based methods are often tailored to specific geometries or limited sets of landmarks. In recent years, anatomical surfaces have been effectively represented as point clouds, which are lightweight structures composed of spatial coordinates. Following this strategy and to overcome the limitations of existing landmarking techniques, we propose Landmark Point Transformer (LmPT), a method for automatic anatomical landmark detection on point clouds that can leverage homologous bones from different species for translational research. The LmPT model incorporates a conditioning mechanism that enables adaptability to different input types to conduct cross-species learning. We focus the evaluation of our approach on femoral landmarking using both human and newly annotated dog femurs, demonstrating its generalization and effectiveness across species. The code and dog femur dataset will be publicly available at: https://github.com/Pierreoo/LandmarkPointTransformer.

</details>


### [6] [Self-Supervised Uncalibrated Multi-View Video Anonymization in the Operating Room](https://arxiv.org/abs/2602.02850)
*Keqi Chen,Vinkle Srivastav,Armine Vardazaryan,Cindy Rolland,Didier Mutter,Nicolas Padoy*

Main category: cs.CV

TL;DR: 提出一种无需标注和相机标定的自监督多视角视频匿名化方法，通过时序与未标定多视角关联检索单视图漏检并生成伪标签进行迭代微调，实现在手术室场景中>97%召回率的高效匿名化。


<details>
  <summary>Details</summary>
Motivation: 动机是解决OR视频匿名化中对高召回率的严格需求，同时克服现有方法在不同临床站点上需要人工标注和多摄像头重定位时需重新标定的可扩展性瓶颈。

Method: 方法包括：1) 在每个视角以较低置信度阈值运行现成全身检测器，收集候选框；2) 通过跟踪与未标定的多视角关联，利用与高置信度检测一致性的低置信候选项作为漏检恢复；3) 将恢复的伪标签用于迭代微调检测器（自监督域自适应）；4) 对检测到的人体进行全身姿态估计，并用高置信度预测作为伪标签继续微调姿态模型。整个流程无需人工标注或相机标定。

Result: 在4D-OR模拟手术数据集与真实手术数据集上，方法实现了超过97%的召回率；并用伪标签训练出实时全身检测器，性能可比且具有实际应用潜力。开源代码已提供。

Conclusion: 该论文提出了一种无需标注与相机标定的自监督多视角视频匿名化框架，通过时序与多视角上下文“检索”单视图漏检并进行领域自适应，从而显著提升手术室（OR）场景中整人人体检测与姿态估计的召回率。

Abstract: Privacy preservation is a prerequisite for using video data in Operating Room (OR) research. Effective anonymization relies on the exhaustive localization of every individual; even a single missed detection necessitates extensive manual correction. However, existing approaches face two critical scalability bottlenecks: (1) they usually require manual annotations of each new clinical site for high accuracy; (2) while multi-camera setups have been widely adopted to address single-view ambiguity, camera calibration is typically required whenever cameras are repositioned. To address these problems, we propose a novel self-supervised multi-view video anonymization framework consisting of whole-body person detection and whole-body pose estimation, without annotation or camera calibration. Our core strategy is to enhance the single-view detector by "retrieving" false negatives using temporal and multi-view context, and conducting self-supervised domain adaptation. We first run an off-the-shelf whole-body person detector in each view with a low-score threshold to gather candidate detections. Then, we retrieve the low-score false negatives that exhibit consistency with the high-score detections via tracking and self-supervised uncalibrated multi-view association. These recovered detections serve as pseudo labels to iteratively fine-tune the whole-body detector. Finally, we apply whole-body pose estimation on each detected person, and fine-tune the pose model using its own high-score predictions. Experiments on the 4D-OR dataset of simulated surgeries and our dataset of real surgeries show the effectiveness of our approach achieving over 97% recall. Moreover, we train a real-time whole-body detector using our pseudo labels, achieving comparable performance and highlighting our method's practical applicability. Code is available at https://github.com/CAMMA-public/OR_anonymization.

</details>


### [7] [ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying](https://arxiv.org/abs/2602.02873)
*Weihang You,Qingchan Zhu,David Liu,Yi Pan,Geng Yuan,Hanqi Jiang*

Main category: cs.CV

TL;DR: ViThinker使视-文模型学会主动生成最小充分视觉查询并内部合成专家级视觉特征，通过两阶段蒸馏与稀疏查询训练，提升感知定位与CoT推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT在视觉-语言模型中因过早将连续视觉信息转为文本而损失几何与空间等连续信息；且现有方法被动处理预先计算的输入，缺乏主动寻求与任务相关视觉细节的能力。

Method: ViThinker通过两阶段训练：1) 将冻结的视觉专家蒸馏到模型参数中，使模型学会生成与专家对齐的视觉特征；2) 在推理阶段通过学习稀疏查询（带稀疏惩罚）来发现每一步推理所需的最小充分感知信息，从而实现任务驱动的主动查询与生成。模型在推理时不调用外部工具，而是内部进行生成性‘心理模拟’。

Result: 在多个以视觉为中心的基准上，ViThinker在感知定位和推理准确率上均优于被动枚举或注意选择方法，验证了主动查询生成在感知定位和推理精度上的有效性。

Conclusion: 提出主动生成查询令牌以触发按需合成视觉特征，可提升视-文推理的感知与推理性能。

Abstract: Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.

</details>


### [8] [DoubleTake: Contrastive Reasoning for Faithful Decision-Making in Medical Imaging](https://arxiv.org/abs/2602.02894)
*Daivik Patel,Shrenik Patel*

Main category: cs.CV

TL;DR: 通过文档感知的对比性参考选择与反事实对比推理，构建紧凑区分性证据集，在MediConfusion上显著提高医学影像判别准确率并减少混淆。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖最近邻检索，返回冗余近似证据并强化单一假设，在需要区分易混淆病症时效果欠佳；ROCO虽提供大规模图文对，但未定义用于对比推理的参考选择策略，且天真的检索常产生来自同一文档的近重复图像。

Method: 构建以ROCO嵌入和元数据为基础的参考选择协议与参考库，设计多目标检索策略平衡视觉相关性、嵌入多样性和文档级别来源，随后提出Counterfactual-Contrastive Inference（反事实对比推理）：进行结构化的成对视觉比较，并用基于边际的决策规则聚合证据，引入可信的放弃机制。

Result: 在MediConfusion基准上，本文方法达到了最先进性能：相较先前方法，集合级准确率提升近15%，同时减少混淆并提升个体病例的准确率。

Conclusion: 本文提出了用于医学影像判别推理的对比性、文档感知参考选择框架与基于反事实对比的推理方法，通过构建紧凑且兼顾视觉相关性、嵌入多样性与来源溯源的证据集合，改善了重复证据问题并提高判别性能。

Abstract: Accurate decision making in medical imaging requires reasoning over subtle visual differences between confusable conditions, yet most existing approaches rely on nearest neighbor retrieval that returns redundant evidence and reinforces a single hypothesis. We introduce a contrastive, document-aware reference selection framework that constructs compact evidence sets optimized for discrimination rather than similarity by explicitly balancing visual relevance, embedding diversity, and source-level provenance using ROCO embeddings and metadata. While ROCO provides large-scale image-caption pairs, it does not specify how references should be selected for contrastive reasoning, and naive retrieval frequently yields near-duplicate figures from the same document. To address this gap, we release a reproducible reference selection protocol and curated reference bank that enable a systematic study of contrastive retrieval in medical image reasoning. Building on these contrastive evidence sets, we propose Counterfactual-Contrastive Inference, a confidence-aware reasoning framework that performs structured pairwise visual comparisons and aggregates evidence using margin-based decision rules with faithful abstention. On the MediConfusion benchmark, our approach achieves state-of-the-art performance, improving set-level accuracy by nearly 15% relative to prior methods while reducing confusion and improving individual accuracy.

</details>


### [9] [FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction](https://arxiv.org/abs/2602.02914)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: 像素重建指标（PSNR/SSIM）不能代表PPFR的真实隐私；FaceLinkGen能直接从受保护模板进行高成功率的身份链接与人脸再生，暴露了当前评估与防护的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有PPFR评估主要用像素级重建质量（PSNR、SSIM）来衡量隐私，但这忽视了从受保护模板中直接提取身份信息或进行链接匹配的风险。作者旨在揭示这种评估范式的不足并展示真实攻击威胁。

Method: 提出了FaceLinkGen攻击：不依赖像素重建，而是直接从受保护模板中提取用于链接/匹配的特征，并利用这些特征生成可识别的人脸图像。实验在三种近期PPFR系统上评估，包括常见的变换/嵌入基方法，并在不同知识量（包括近零知识）设置下测试攻击效果。

Result: FaceLinkGen在三种PPFR系统上实现了超过98.5%的匹配准确率和超过96%的再生成功率；在近零知识条件下仍超过92%匹配和94%再生。实验表明，即使像素失真很高，身份信息仍可被外部入侵者与不受信任的服务提供者广泛提取。

Conclusion: 本文结论是：以像素重建为评估标准的隐私保护人脸识别（PPFR）方法存在重大漏洞，视觉混淆并不能充分保护身份信息，攻击者可以直接从受保护模板进行身份链接与人脸再生，导致高精度匹配与再生成功率。

Abstract: Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\% matching accuracy and above 96\% regeneration success, and still exceeds 92\% matching and 94\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers.

</details>


### [10] [A Multi-scale Linear-time Encoder for Whole-Slide Image Analysis](https://arxiv.org/abs/2602.02918)
*Jagan Mohan Reddy Dwarampudi,Joshua Wong,Hien Van Nguyen,Tania Banerjee*

Main category: cs.CV

TL;DR: MARBLE是首个纯Mamba-based多尺度MIL框架，通过并行多尺度处理和线性时间序列建模在WSI任务上以更少参数实现更好性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法多为单尺度处理，Transformer等基于注意力的方法在处理WSI时受二次注意力开销限制，难以高效建模多尺度信息与跨尺度依赖。

Method: 在并行处理多放大倍数的同时，使用线性时间的状态空间（Mamba-based）模型将不同尺度的特征在序列上融合，实现跨尺度依赖建模；框架模块化，参数量小于基于注意力的模型，时间复杂度为线性。

Result: 在五个公开数据集上，MARBLE在AUC上最高提升6.9%，准确率最高提升20.3%，C-index提升2.3%，展示了更高的效率和泛化能力。

Conclusion: MARBLE提出了一种基于Mamba的纯多态多实例学习框架，能在多尺度平行处理中以线性时间状态空间模型实现粗到细推理，解决了WSI多尺度与超高分辨率问题。

Abstract: We introduce Multi-scale Adaptive Recurrent Biomedical Linear-time Encoder (MARBLE), the first \textit{purely Mamba-based} multi-state multiple instance learning (MIL) framework for whole-slide image (WSI) analysis. MARBLE processes multiple magnification levels in parallel and integrates coarse-to-fine reasoning within a linear-time state-space model, efficiently capturing cross-scale dependencies with minimal parameter overhead. WSI analysis remains challenging due to gigapixel resolutions and hierarchical magnifications, while existing MIL methods typically operate at a single scale and transformer-based approaches suffer from quadratic attention costs. By coupling parallel multi-scale processing with linear-time sequence modeling, MARBLE provides a scalable and modular alternative to attention-based architectures. Experiments on five public datasets show improvements of up to \textbf{6.9\%} in AUC, \textbf{20.3\%} in accuracy, and \textbf{2.3\%} in C-index, establishing MARBLE as an efficient and generalizable framework for multi-scale WSI analysis.

</details>


### [11] [SRA-Seg: Synthetic to Real Alignment for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2602.02944)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: 提出SRA-Seg：用DINOv2嵌入做相似性对齐、软边缘融合与EMA伪标签的组合，使合成数据在少标注情形下显著提升医学分割性能。


<details>
  <summary>Details</summary>
Motivation: 合成医学图像虽然视觉逼真，但在语义特征空间与真实图像存在域差，导致半监督方法无法有效利用合成数据。需要显式对齐两者特征分布以缩小域差。

Method: 框架包括：1) 使用冻结的DINOv2嵌入计算相似性对齐(SA)损失，将合成样本表征拉向最近真实样本表征；2) 采用软边缘融合生成平滑的解剖学过渡及连续标签，避免传统copy-paste引入的硬边界；3) 使用EMA教师模型为合成图像生成伪标签，并在混合区域采用考虑不确定性的软分割损失。

Result: 在仅10%真实标注+90%合成未标注的数据设置下，SRA-Seg在ACDC数据集达89.34% Dice，在FIVES达84.42% Dice，显著优于现有半监督方法，并可与使用真实未标注数据的方法性能相当。

Conclusion: SRA-Seg通过在语义特征空间对齐合成与真实数据的表征，有效利用合成数据提升医学图像分割性能，实验证明在少量真实标注下可显著改进Dice得分并可与使用真实未标注数据的方法相媲美。

Abstract: Synthetic data, an appealing alternative to extensive expert-annotated data for medical image segmentation, consistently fails to improve segmentation performance despite its visual realism. The reason being that synthetic and real medical images exist in different semantic feature spaces, creating a domain gap that current semi-supervised learning methods cannot bridge. We propose SRA-Seg, a framework explicitly designed to align synthetic and real feature distributions for medical image segmentation. SRA-Seg introduces a similarity-alignment (SA) loss using frozen DINOv2 embeddings to pull synthetic representations toward their nearest real counterparts in semantic space. We employ soft edge blending to create smooth anatomical transitions and continuous labels, eliminating the hard boundaries from traditional copy-paste augmentation. The framework generates pseudo-labels for synthetic images via an EMA teacher model and applies soft-segmentation losses that respect uncertainty in mixed regions. Our experiments demonstrate strong results: using only 10% labeled real data and 90% synthetic unlabeled data, SRA-Seg achieves 89.34% Dice on ACDC and 84.42% on FIVES, significantly outperforming existing semi-supervised methods and matching the performance of methods using real unlabeled data.

</details>


### [12] [Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning](https://arxiv.org/abs/2602.02951)
*Yihong Huang,Fei Ma,Yihua Shao,Jingcai Guo,Zitong Yu,Laizhong Cui,Qi Tian*

Main category: cs.CV

TL;DR: 提出Nüwa——一种两阶段（视觉端的分离/对齐/聚合 + LLM端的文本引导）Token剪枝方法，通过保留空间锚点与任务相关Token，实现加速的同时显著改善视觉定位性能，并在VQA上保持SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有基于全局语义相似度或注意力得分的剪枝策略丧失了全局空间参考框架，导致在视觉定位类任务上性能大幅下降，因此需要一种在加速同时保持空间信息的剪枝方法。

Method: Nüwa为两阶段Token剪枝框架：第一阶段在视觉编码器后进行分离（separation）、对齐（alignment）、聚合（aggregation）三步操作，采用类群智能启发策略保留全局空间锚点；第二阶段在LLM内进行文本引导的剪枝以保留任务相关的视觉Token。

Result: 在多项VQA基准上取得SOTA（94%→95%），并在视觉定位任务上从7%提升到47%，显示出对VG任务的大幅改进以及在VQA上的稳健性。

Conclusion: 本文提出的Nüwa在VQA任务上保持了优秀性能，并显著提升了视觉定位（VG）任务表现，证明了其在保留空间完整性的同时实现高效Token剪枝的可行性。

Abstract: Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM's processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens' positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).

</details>


### [13] [TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation](https://arxiv.org/abs/2602.02963)
*OFM Riaz Rahman Aranya,Kevin Desai*

Main category: cs.CV

TL;DR: 提出首个能在先后胸片上同时做时间比较、变化分类与边界框定位的模型TRACE；在超过90%的定位准确率下，发现联合学习定位与时序比较是实现变化检测的关键。


<details>
  <summary>Details</summary>
Motivation: 临床需在影像对比中检测病变进展、治疗反应与新发病灶，但现有V-L模型仅限单张影像报告或可视化定位，缺乏时序变化检测与定位的联合方法。

Method: 基于视觉-语言模型，输入先前与当前胸片，联合训练时间比较、变化分类与边界框定位；通过联合损失促使模型同时学习时序特征与空间注意。

Result: TRACE在空间定位上表现优异，定位准确率超过90%；消融实验显示，只有在同时学习时序比较与空间定位时，模型才具有显著的变化检测能力，表明空间定位为时序推理提供关键的空间注意机制。

Conclusion: TRACE有效整合时序比较与空间定位，实现了胸片变化的自然语言描述与定位。

Abstract: Temporal comparison of chest X-rays is fundamental to clinical radiology, enabling detection of disease progression, treatment response, and new findings. While vision-language models have advanced single-image report generation and visual grounding, no existing method combines these capabilities for temporal change detection. We introduce Temporal Radiology with Anatomical Change Explanation (TRACE), the first model that jointly performs temporal comparison, change classification, and spatial localization. Given a prior and current chest X-ray, TRACE generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates. TRACE demonstrates effective spatial localization with over 90% grounding accuracy, establishing a foundation for this challenging new task. Our ablation study uncovers an emergent capability: change detection arises only when temporal comparison and spatial grounding are jointly learned, as neither alone enables meaningful change detection. This finding suggests that grounding provides a spatial attention mechanism essential for temporal reasoning.

</details>


### [14] [Dynamic High-frequency Convolution for Infrared Small Target Detection](https://arxiv.org/abs/2602.02969)
*Ruojing Li,Chao Xiao,Qian Yin,Wei An,Nuo Chen,Xinyi Ying,Miao Li,Yingqian Wang*

Main category: cs.CV

TL;DR: 提出DHiF——一种针对高频成分的动态卷积，通过生成对称零中心参数的滤波器库来增强红外小目标的判别，能替换标准卷积并在多数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 红外小目标属于图像高频成分，现有深度学习方法未显式建模各类高频干扰（如亮角、断云等），导致检测困难，需增强对高频特征的判别能力。

Method: 基于傅里叶变换性质，设计可生成对称零中心范围内参数的动态滤波器，DHiF与标准卷积结合可自适应处理不同高频区域并捕捉灰度变化特征，可无缝替换标准卷积并保持计算效率。

Result: 在多个真实场景数据集和不同单帧红外小目标检测网络上进行广泛实验，DHiF在检测性能上优于其它最先进卷积操作并带来显著改进，同时保持计算效率。

Conclusion: 该论文针对红外小目标检测提出了动态高频卷积（DHiF），通过生成动态局部滤波器库来显式建模高频成分，从而改善目标与高频背景杂波的区分能力。

Abstract: Infrared small targets are typically tiny and locally salient, which belong to high-frequency components (HFCs) in images. Single-frame infrared small target (SIRST) detection is challenging, since there are many HFCs along with targets, such as bright corners, broken clouds, and other clutters. Current learning-based methods rely on the powerful capabilities of deep networks, but neglect explicit modeling and discriminative representation learning of various HFCs, which is important to distinguish targets from other HFCs. To address the aforementioned issues, we propose a dynamic high-frequency convolution (DHiF) to translate the discriminative modeling process into the generation of a dynamic local filter bank. Especially, DHiF is sensitive to HFCs, owing to the dynamic parameters of its generated filters being symmetrically adjusted within a zero-centered range according to Fourier transformation properties. Combining with standard convolution operations, DHiF can adaptively and dynamically process different HFC regions and capture their distinctive grayscale variation characteristics for discriminative representation learning. DHiF functions as a drop-in replacement for standard convolution and can be used in arbitrary SIRST detection networks without significant decrease in computational efficiency. To validate the effectiveness of our DHiF, we conducted extensive experiments across different SIRST detection networks on real-scene datasets. Compared to other state-of-the-art convolution operations, DHiF exhibits superior detection performance with promising improvement. Codes are available at https://github.com/TinaLRJ/DHiF.

</details>


### [15] [Fisheye Stereo Vision: Depth and Range Error](https://arxiv.org/abs/2602.02973)
*Leaf Jiang,Matthew Holzel,Bernhard Kaplan,Hsiou-Yuan Liu,Sabyasachi Paul,Karen Rankin,Piotr Swierczynski*

Main category: cs.CV

TL;DR: 给出鱼眼立体视觉在大视角下深度和测程误差的闭式解析表达式，基于投影模型和误差传播，揭示误差随距离和视角的非线性增长并验证了公式。


<details>
  <summary>Details</summary>
Motivation: 传统针孔立体视觉的误差模型在小视角下有效，但对鱼眼镜头在大视角（近周边）下的几何畸变和非线性不足以适用；需要针对鱼眼立体系统给出闭式误差表达式，帮助在广角场景中评估可达距离和深度精度，指导硬件配置与算法设计。

Method: 基于鱼眼镜头的投影模型（例如等距或双球模型），建立立体几何关系，线性化远近轴偏差，利用误差传播理论（偏导数求取雅可比矩阵）从像素视差的不确定性推导深度和水平距离的误差表达式；对大视角情形保留高阶项以捕捉非线性效应，并通过仿真或标定数据验证解析结果。

Result: 得到关于物体距离Z、基线B、相机焦距/等效参数、视角θ及像素视差σ_d的显式误差公式，表明深度误差σ_Z与Z^2/ (B * f_effective)级别相关，并且随视角增大呈非线性放大；同时给出在极大视角下误差饱和或急剧上升的条件。仿真与实测结果与解析式吻合良好。

Conclusion: 本文推导出适用于鱼眼立体视觉系统的深度和距离（测程）误差的解析表达式，考虑了大视角下的测量精度；结论指出误差与物距及视角有关，并在大视角时显著增大，提供了用于系统设计和误差评估的闭式公式。

Abstract: This study derives analytical expressions for the depth and range error of fisheye stereo vision systems as a function of object distance, specifically accounting for accuracy at large angles.

</details>


### [16] [SceneLinker: Compositional 3D Scene Generation via Semantic Scene Graph from RGB Sequences](https://arxiv.org/abs/2602.02974)
*Seok-Young Kim,Dooyoung Kim,Woojin Cho,Hail Song,Suji Kang,Woontack Woo*

Main category: cs.CV

TL;DR: SceneLinker 通过对场景图预测的交叉注意力图网络和联合形状-布局的 graph-VAE，从 RGB 序列生成与真实布局一致的可组合 3D 场景，提升了 MR 场景生成的语义一致性与排列一致性。


<details>
  <summary>Details</summary>
Motivation: 为了在 MR 中根据每个用户的物理空间自适应呈现内容，需要紧凑捕捉周围语义线索以反映真实世界布局；现有方法难以完整捕捉对象之间的上下文关系或主要专注于形状多样性，导致无法生成与对象排列一致的 3D 场景。

Method: 设计了带交叉核验特征注意力（cross-check feature attention）的图网络用于场景图预测，并构建图形变分自编码器（graph-VAE），包含联合形状与布局模块以生成 3D 场景。

Result: 在 3RScan/3DSSG 和 SG-FRONT 数据集上的实验表明，SceneLinker 在定量与定性评估中均优于现有最先进方法，在复杂室内环境和严格的场景图约束下仍能表现出色。

Conclusion: SceneLinker 提出了一种从 RGB 序列通过语义场景图生成可组合 3D 场景的新框架，能够在混合现实中根据用户空间生成反映真实布局的 3D 场景。

Abstract: We introduce SceneLinker, a novel framework that generates compositional 3D scenes via semantic scene graph from RGB sequences. To adaptively experience Mixed Reality (MR) content based on each user's space, it is essential to generate a 3D scene that reflects the real-world layout by compactly capturing the semantic cues of the surroundings. Prior works struggled to fully capture the contextual relationship between objects or mainly focused on synthesizing diverse shapes, making it challenging to generate 3D scenes aligned with object arrangements. We address these challenges by designing a graph network with cross-check feature attention for scene graph prediction and constructing a graph-variational autoencoder (graph-VAE), which consists of a joint shape and layout block for 3D scene generation. Experiments on the 3RScan/3DSSG and SG-FRONT datasets demonstrate that our approach outperforms state-of-the-art methods in both quantitative and qualitative evaluations, even in complex indoor environments and under challenging scene graph constraints. Our work enables users to generate consistent 3D spaces from their physical environments via scene graphs, allowing them to create spatial MR content. Project page is https://scenelinker2026.github.io.

</details>


### [17] [Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding](https://arxiv.org/abs/2602.02977)
*Byeongju Woo,Zilin Wang,Byeonghyun Pak,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: CAFT通过层级跨域对齐，在不依赖区域标注的情况下提升长文本下的细粒度图文对齐和检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉-语言模型对长描述对齐不足，因整体对齐忽视局部细节，而单域层级结构在跨域对齐上存在语义不匹配或过度碎片化的问题。

Method: 提出CAFT框架：结合自下而上的视觉编码器和层级文本Transformer，使用层级对齐损失同时匹配整图-整句并偏置区域-句子对应，训练于30M图文对。

Result: 在六个长文本检索基准上实现SOTA，展现良好扩展性，并在无需区域级监督下学出有视觉依据的细粒度表示。

Conclusion: CAFT通过层级对齐图像和长文本，在无像素级监督下实现细粒度的视觉-文本表示学习，改进了长文本检索和跨域语义一致性。

Abstract: Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.

</details>


### [18] [SharpTimeGS: Sharp and Stable Dynamic Gaussian Splatting via Lifespan Modulation](https://arxiv.org/abs/2602.02989)
*Zhanfeng Liao,Jiajun Zhang,Hanzhang Tu,Zhixi Wang,Yunqi Gao,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: 引入可学习寿命参数和寿命-速度感知致密化，SharpTimeGS在统一的4D高斯表示中实现对静态/动态区域的时间自适应建模，提升长期稳定性与动态保真，同时保持实时高分辨率渲染性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的实时渲染方法难以在表示与优化上同时兼顾长期静态区域与短期动态区域，导致长时段点漂移或动态表现受限。

Method: 提出可学习寿命参数，将高斯的时间可见性从高斯衰减改为平顶（flat-top）配置；寿命同时调制每个基元的运动（motion decoupling）；以及设计寿命-速度感知的致密化（densification）策略，按运动显著性分配表示容量。

Result: 在多个基准上达到最先进性能，并在单块RTX 4090上支持4K分辨率、100 FPS的实时渲染。

Conclusion: SharpTimeGS通过引入可学习的寿命参数，在统一的4D高斯表示下对静态与动态区域实现时间自适应建模，兼顾长期稳定性与动态保真度，从而提升长短期运动的表示与优化平衡。

Abstract: Novel view synthesis of dynamic scenes is fundamental to achieving photorealistic 4D reconstruction and immersive visual experiences. Recent progress in Gaussian-based representations has significantly improved real-time rendering quality, yet existing methods still struggle to maintain a balance between long-term static and short-term dynamic regions in both representation and optimization. To address this, we present SharpTimeGS, a lifespan-aware 4D Gaussian framework that achieves temporally adaptive modeling of both static and dynamic regions under a unified representation. Specifically, we introduce a learnable lifespan parameter that reformulates temporal visibility from a Gaussian-shaped decay into a flat-top profile, allowing primitives to remain consistently active over their intended duration and avoiding redundant densification. In addition, the learned lifespan modulates each primitives' motion, reducing drift in long-lived static points while retaining unrestricted motion for short-lived dynamic ones. This effectively decouples motion magnitude from temporal duration, improving long-term stability without compromising dynamic fidelity. Moreover, we design a lifespan-velocity-aware densification strategy that mitigates optimization imbalance between static and dynamic regions by allocating more capacity to regions with pronounced motion while keeping static areas compact and stable. Extensive experiments on multiple benchmarks demonstrate that our method achieves state-of-the-art performance while supporting real-time rendering up to 4K resolution at 100 FPS on one RTX 4090.

</details>


### [19] [Video-OPD: Efficient Post-Training of Multimodal Large Language Models for Temporal Video Grounding via On-Policy Distillation](https://arxiv.org/abs/2602.02994)
*Jiaze Li,Hao Yin,Haoran Xu,Boshen Xu,Wenhui Tan,Zewen He,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 提出一种基于on-policy蒸馏的高效TVG后训练框架Video-OPD及其样本选择策略TVDF，解决稀疏奖励和高开销问题，优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的后训练强化学习方法在TVG中受限于稀疏奖励信号和高计算成本；利用on-policy蒸馏将稀疏的episode级反馈转换为细粒度的step-wise监督以提高效率。

Method: 在当前策略采样的轨迹上使用一个frontier teacher，通过反向KL散度提供逐步的token级稠密监督；并提出TVDF课程，基于教师可靠性和学生信息量优先选择轨迹训练。

Result: Video-OPD在多项TVG基准上性能优于GRPO，收敛更快，计算成本更低，证明了on-policy蒸馏是TVG后训练的有效替代方法。

Conclusion: Video-OPD通过在on-policy轨迹上进行蒸馏并引入教师验证的样本选择方法，有效缓解了稀疏奖励和计算开销问题，成为TVG任务中比GRPO更高效的后训练范式。

Abstract: Reinforcement learning has emerged as a principled post-training paradigm for Temporal Video Grounding (TVG) due to its on-policy optimization, yet existing GRPO-based methods remain fundamentally constrained by sparse reward signals and substantial computational overhead. We propose Video-OPD, an efficient post-training framework for TVG inspired by recent advances in on-policy distillation. Video-OPD optimizes trajectories sampled directly from the current policy, thereby preserving alignment between training and inference distributions, while a frontier teacher supplies dense, token-level supervision via a reverse KL divergence objective. This formulation preserves the on-policy property critical for mitigating distributional shift, while converting sparse, episode-level feedback into fine-grained, step-wise learning signals. Building on Video-OPD, we introduce Teacher-Validated Disagreement Focusing (TVDF), a lightweight training curriculum that iteratively prioritizes trajectories that are both teacher-reliable and maximally informative for the student, thereby improving training efficiency. Empirical results demonstrate that Video-OPD consistently outperforms GRPO while achieving substantially faster convergence and lower computational cost, establishing on-policy distillation as an effective alternative to conventional reinforcement learning for TVG.

</details>


### [20] [VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering](https://arxiv.org/abs/2602.03007)
*Rahul Atul Bhope,K. R. Jayaram,Vinod Muthusamy,Ritesh Kumar,Vatche Isahagian,Nalini Venkatasubramanian*

Main category: cs.CV

TL;DR: VOILA在检索前基于问题预测并校准不同图像保真度的正确率，按成本-效用选择最低成本保真度，可大幅降低成本且保持高准确率。


<details>
  <summary>Details</summary>
Motivation: 多模态视觉-语言系统通常以固定高保真度检索图像，导致高昂的检索与处理成本。提出在检索前自适应选择保真度以节约资源同时保持性能。

Method: 两个阶段：1）梯度提升回归器用问题特征估计每种保真度的正确率；2）各保真度的预测概率经各向同性（isotonic）校准以得到可靠概率。基于预测准确率与检索成本最大化期望效用以选择最低成本保真度。

Result: 在五个数据集（VQA-v2, GQA, TextVQA, LoCoMo, FloodNet）和六个VLM（7B-235B）上评估，VOILA实现了50-60%的成本降低，同时保留90-95%的全分辨率准确率，适用于多种查询类型与模型架构。

Conclusion: VOILA通过在检索前基于问题特征预测不同图像保真度下模型正确率，并结合成本进行期望效用最大化选择最低成本保真度，能在减少检索/处理成本的同时维持接近全分辨率的准确率。

Abstract: Despite significant costs from retrieving and processing high-fidelity visual inputs, most multimodal vision-language systems operate at fixed fidelity levels. We introduce VOILA, a framework for Value-Of-Information-driven adaptive fidelity selection in Visual Question Answering (VQA) that optimizes what information to retrieve before model execution. Given a query, VOILA uses a two-stage pipeline: a gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, then an isotonic calibrator refines these probabilities for reliable decision-making. The system selects the minimum-cost fidelity maximizing expected utility given predicted accuracy and retrieval costs. We evaluate VOILA across three deployment scenarios using five datasets (VQA-v2, GQA, TextVQA, LoCoMo, FloodNet) and six Vision-Language Models (VLMs) with 7B-235B parameters. VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures, demonstrating that pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints.

</details>


### [21] [Thinking inside the Convolution for Image Inpainting: Reconstructing Texture via Structure under Global and Local Side](https://arxiv.org/abs/2602.03013)
*Haipeng Liu,Yang Wang,Biao Qian,Yong Rui,Meng Wang*

Main category: cs.CV

TL;DR: 作者通过在下采样阶段用结构/纹理特征的统计归一化与反归一化进行重建引导，缓解特征信息丢失，从而提升图像修复质量，在256/512分辨率上优于SOTA并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的图像修复在卷积下采样过程中会丢失结构与纹理信息，导致上采样输出不理想。作者旨在探究结构与纹理特征图如何互相帮助以减轻这种信息丢失。

Method: 在编码器下采样阶段分别提取结构（高频）与纹理（低频）特征图，使用统计归一化（normalization）与反归一化（denormalization）策略在特征层面进行相互引导，整合到现有编码器中以替代原有编码器。

Result: 在多组实验（分辨率256×256与512×512）上优于现有方法，替换编码器后效果更明显。并在代码仓库公布实现。

Conclusion: 本文提出在卷积下采样过程中利用结构和纹理特征图的统计归一化与反归一化来引导重建，从而缓解信息丢失，提高上采样恢复质量。

Abstract: Image inpainting has earned substantial progress, owing to the encoder-and-decoder pipeline, which is benefited from the Convolutional Neural Networks (CNNs) with convolutional downsampling to inpaint the masked regions semantically from the known regions within the encoder, coupled with an upsampling process from the decoder for final inpainting output. Recent studies intuitively identify the high-frequency structure and low-frequency texture to be extracted by CNNs from the encoder, and subsequently for a desirable upsampling recovery. However, the existing arts inevitably overlook the information loss for both structure and texture feature maps during the convolutional downsampling process, hence suffer from a non-ideal upsampling output. In this paper, we systematically answer whether and how the structure and texture feature map can mutually help to alleviate the information loss during the convolutional downsampling. Given the structure and texture feature maps, we adopt the statistical normalization and denormalization strategy for the reconstruction guidance during the convolutional downsampling process. The extensive experimental results validate its advantages to the state-of-the-arts over the images from low-to-high resolutions including 256*256 and 512*512, especially holds by substituting all the encoders by ours. Our code is available at https://github.com/htyjers/ConvInpaint-TSGL

</details>


### [22] [A Vision-Based Analysis of Congestion Pricing in New York City](https://arxiv.org/abs/2602.03015)
*Mehmet Kerem Turkcan,Jhonatan Tavori,Javad Ghaderi,Gil Zussman,Zoran Kostic,Andrew Smyth*

Main category: cs.CV

TL;DR: 通过900+摄像头的自动化视觉分析，本文量化了纽约拥堵定价自2025年实施以来（覆盖至2026年1月）车辆密度的系统性变化，但摘要缺乏具体数值与显著性信息。


<details>
  <summary>Details</summary>
Motivation: 评估纽约市拥堵定价政策（2025年1月实施）对道路交通影响，利用大规模、客观的摄像头数据弥补传统流量监测（如感应线圈、手工调查）的不足。

Method: 构建自动化计算机视觉管线，对900+摄像头的视频帧进行检测与计数、时空聚合并与基线期（2024年11月至2025年1月）比较，目的是量化车辆密度随时间和地点的变化。

Result: 作者声称通过对比实施前后的视频数据，发现监测区域内存在系统性的车辆密度变化（节律性或空间分布变化），但摘要未给出具体方向性、统计显著性或空间细节。

Conclusion: 未提供全文，仅基于摘要可得初步结论：研究利用计算机视觉从900余摄像头自动提取交通密度数据，覆盖2024年11月至2026年1月，建立基线并识别出车辆密度的系统性变化，表明拥堵定价实施后交通格局发生了可测量变化。

Abstract: We examine the impact of New York City's congestion pricing program through automated analysis of traffic camera data. Our computer vision pipeline processes footage from over 900 cameras distributed throughout Manhattan and New York, comparing traffic patterns from November 2024 through the program's implementation in January 2025 until January 2026. We establish baseline traffic patterns and identify systematic changes in vehicle density across the monitored region.

</details>


### [23] [MUSE: A Multi-agent Framework for Unconstrained Story Envisioning via Closed-Loop Cognitive Orchestration](https://arxiv.org/abs/2602.03028)
*Wenzhang Sun,Zhenyu Wang,Zhangchi Hu,Chunfeng Wang,Hao Li,Wei Chen*

Main category: cs.CV

TL;DR: MUSE通过多智能体闭环控制，将高层叙事意图转为可执行约束并在生成中迭代修正，从而显著改善长篇视听故事的连贯性与一致性。


<details>
  <summary>Details</summary>
Motivation: 当前从短提示生成长篇视听故事存在意图-执行差距，传统前馈或仅提示精炼方法在长序列下易导致语义漂移和身份不一致。

Method: 提出多智能体框架MUSE，将叙事意图转换为可执行的控制（身份、空间构图、时间连续性），并在生成过程中应用有针对性的多模态反馈以纠正违例。框架包含计划、执行、验证、修正四个环节循环迭代。

Result: 在新引入的MUSEBench参考-免费评价协议上，实验显示MUSE在长时域叙事连贯性、跨模态身份一致性和电影感上明显优于基线方法，且该评价协议经人工评判验证。

Conclusion: MUSE通过闭环多代理计划-执行-验证-修正流程，有效减少长序列生成中的语义漂移和身份不一致，提高了叙事一致性和影视质量。

Abstract: Generating long-form audio-visual stories from a short user prompt remains challenging due to an intent-execution gap, where high-level narrative intent must be preserved across coherent, shot-level multimodal generation over long horizons. Existing approaches typically rely on feed-forward pipelines or prompt-only refinement, which often leads to semantic drift and identity inconsistency as sequences grow longer. We address this challenge by formulating storytelling as a closed-loop constraint enforcement problem and propose MUSE, a multi-agent framework that coordinates generation through an iterative plan-execute-verify-revise loop. MUSE translates narrative intent into explicit, machine-executable controls over identity, spatial composition, and temporal continuity, and applies targeted multimodal feedback to correct violations during generation. To evaluate open-ended storytelling without ground-truth references, we introduce MUSEBench, a reference-free evaluation protocol validated by human judgments. Experiments demonstrate that MUSE substantially improves long-horizon narrative coherence, cross-modal identity consistency, and cinematic quality compared with representative baselines.

</details>


### [24] [Bongards at the Boundary of Perception and Reasoning: Programs or Language?](https://arxiv.org/abs/2602.03038)
*Cassidy Langenfeld,Claas Beger,Gloria Geng,Wasu Top Piriyakulkij,Keya Hu,Yewen Pu,Kevin Ellis*

Main category: cs.CV

TL;DR: 本文提出用LLM生成参数化程序并用贝叶斯优化拟合参数的神经符号方法，解决Bongard视觉推理问题，在已知规则和从头求解上均表现出可行性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在常规图像理解上表现良好，但缺乏在全新、抽象视觉推理任务（如Bongard问题）上的泛化能力；论文旨在赋予VLM在抽象规则发现与执行上的能力，模仿人类在新情境中快速构造规则并验证的能力。

Method: 先由人类假设解规则或由模型生成候选规则；将规则交给大语言模型生成参数化的程序化表示；用贝叶斯优化对程序中的参数进行拟合，使其在训练图像上匹配正负样例；最后用拟合后的程序对新图像分类或验证规则。

Result: 在已知规则的分类任务上，方法能准确将Bongard图像分为符合/不符合规则；在从头求解时，方法通过生成与拟合候选程序能在多个Bongard问题上找到合理解，展示了神经符号结合策略的有效性，但具体精度与基线比较需查看原文实验表格。

Conclusion: 该论文提出了一种结合神经网络与符号方法的框架，利用大语言模型生成参数化程序表示并用贝叶斯优化进行参数拟合，以解决Bongard问题，能在已知规则和从头求解两种设置下取得有效分类与求解结果。

Abstract: Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.

</details>


### [25] [HP-GAN: Harnessing pretrained networks for GAN improvement with FakeTwins and discriminator consistency](https://arxiv.org/abs/2602.03039)
*Geonhui Son,Jeong Ryong Lee,Dosik Hwang*

Main category: cs.CV

TL;DR: HP-GAN利用FakeTwins自监督损失与CNN-ViT判别器一致性两项技术，深化对预训练网络先验的利用，在多数据集上显著降低FID并提升生成图像的质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 现有GAN经常利用预训练网络计算感知损失或特征空间；作者希望扩展预训练网络的作用，通过自监督和多判别器一致性更深层次地利用先验，从而提升生成图像的多样性与质量并增强训练鲁棒性。

Method: 提出两大策略：1) FakeTwins：将预训练网络作为编码器计算自监督损失，并将该损失通过生成图像回传到生成器以增强多样性和质量；2) 判别器一致性：对来自CNN与ViT特征网络的特征图在多个判别器之间强制一致性，促使判别器协同评估并提高训练稳定性。

Result: 在17个数据集（大/小/受限数据及多域）上进行广泛评估，HP-GAN在FID上持续优于现有最先进方法，显著提升图像质量与多样性。代码已公开。

Conclusion: HP-GAN通过结合自监督损失（FakeTwins）与判别器一致性机制，利用预训练网络先验提升GAN生成效果，结论是能在多种数据规模和域上显著改善FID、图像质量与多样性。

Abstract: Generative Adversarial Networks (GANs) have made significant progress in enhancing the quality of image synthesis. Recent methods frequently leverage pretrained networks to calculate perceptual losses or utilize pretrained feature spaces. In this paper, we extend the capabilities of pretrained networks by incorporating innovative self-supervised learning techniques and enforcing consistency between discriminators during GAN training. Our proposed method, named HP-GAN, effectively exploits neural network priors through two primary strategies: FakeTwins and discriminator consistency. FakeTwins leverages pretrained networks as encoders to compute a self-supervised loss and applies this through the generated images to train the generator, thereby enabling the generation of more diverse and high quality images. Additionally, we introduce a consistency mechanism between discriminators that evaluate feature maps extracted from Convolutional Neural Network (CNN) and Vision Transformer (ViT) feature networks. Discriminator consistency promotes coherent learning among discriminators and enhances training robustness by aligning their assessments of image quality. Our extensive evaluation across seventeen datasets-including scenarios with large, small, and limited data, and covering a variety of image domains-demonstrates that HP-GAN consistently outperforms current state-of-the-art methods in terms of Fréchet Inception Distance (FID), achieving significant improvements in image diversity and quality. Code is available at: https://github.com/higun2/HP-GAN.

</details>


### [26] [IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning](https://arxiv.org/abs/2602.03060)
*Zhichao Sun,Yidong Ma,Gang Liu,Yibo Chen,Xu Tang,Yao Hu,Yongchao Xu*

Main category: cs.CV

TL;DR: IVC-Prune识别RoPE中的隐式视觉坐标并结合语义前景令牌进行剪枝，训练免费、提示感知，在多模型多基准上能大幅降低令牌数且基本不损失性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉令牌剪枝侧重语义相关性，可能丢弃对空间推理至关重要的令牌。需要一种同时保留空间信息和语义信息的剪枝策略。

Method: 通过解析RoPE的数学性质，识别出近似单位矩阵或90°旋转矩阵的位置作为IVC令牌；同时采用无训练、提示感知的两阶段前景令牌识别：语义种子发现 + 基于value向量相似度的上下文精炼。最终在四个LVLM和20个基准上评估。

Result: IVC-Prune在减少约50%视觉令牌的同时，仍能保持>=99%的原始性能，并在若干基准上实现提升。

Conclusion: 论文提出IVC-Prune，通过识别并保留隐式视觉坐标（IVC）令牌和语义前景令牌，实现高分辨率视觉输入下的高效推理而不损失性能。

Abstract: Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.

</details>


### [27] [JRDB-Pose3D: A Multi-person 3D Human Pose and Shape Estimation Dataset for Robotics](https://arxiv.org/abs/2602.03064)
*Sandika Biswas,Kian Izadpanah,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: JRDB-Pose3D是在移动机器人上采集的多人体3D姿态数据集，提供一致的SMPL参数、个体跟踪与丰富的继承注释，专为现实世界复杂场景的感知与理解任务设计。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态数据集多为单人或实验室环境，难以反映现实世界中拥挤、遮挡和截断等复杂情况，影响自动驾驶、机器人感知与人机交互等应用的实用性；因此需要一个在移动平台上采集、多人体并具备丰富注释的真实场景数据集。

Method: 在移动机器人平台上采集高密度多人体视频，并基于SMPL模型对每帧中的多人体进行3D姿态拟合，保证一致的身体形状参数和跨帧的跟踪ID；将生成的3D注释与JRDB已有的2D姿态、全景语义掩码、社交与个体属性注释等进行对齐和继承。

Result: JRDB-Pose3D平均每帧包含5-10个人体姿态，某些场景同时包含多达35人。数据集呈现频繁遮挡、身体截断与画外部位等挑战，并继承JRDB的多种注释，支持多任务下游研究。

Conclusion: JRDB-Pose3D通过在移动机器人平台上采集多人体室内外场景并提供一致的SMPL姿态与人体形状参数、时间连续的跟踪ID以及丰富的继承注释（如社交分组、活动、交互、语义掩码等），弥合了受控实验室数据与真实复杂环境之间的差距，成为面向现实世界感知任务的综合数据集。

Abstract: Real-world scenes are inherently crowded. Hence, estimating 3D poses of all nearby humans, tracking their movements over time, and understanding their activities within social and environmental contexts are essential for many applications, such as autonomous driving, robot perception, robot navigation, and human-robot interaction. However, most existing 3D human pose estimation datasets primarily focus on single-person scenes or are collected in controlled laboratory environments, which restricts their relevance to real-world applications. To bridge this gap, we introduce JRDB-Pose3D, which captures multi-human indoor and outdoor environments from a mobile robotic platform. JRDB-Pose3D provides rich 3D human pose annotations for such complex and dynamic scenes, including SMPL-based pose annotations with consistent body-shape parameters and track IDs for each individual over time. JRDB-Pose3D contains, on average, 5-10 human poses per frame, with some scenes featuring up to 35 individuals simultaneously. The proposed dataset presents unique challenges, including frequent occlusions, truncated bodies, and out-of-frame body parts, which closely reflect real-world environments. Moreover, JRDB-Pose3D inherits all available annotations from the JRDB dataset, such as 2D pose, information about social grouping, activities, and interactions, full-scene semantic masks with consistent human- and object-level tracking, and detailed annotations for each individual, such as age, gender, and race, making it a holistic dataset for a wide range of downstream perception and human-centric understanding tasks.

</details>


### [28] [Finding Optimal Video Moment without Training: Gaussian Boundary Optimization for Weakly Supervised Video Grounding](https://arxiv.org/abs/2602.03071)
*Sunoh Kim,Kimin Yun,Daeho Um*

Main category: cs.CV

TL;DR: 提出无训练的Gaussian Boundary Optimization，通过求解平衡覆盖与紧凑性的闭式最优化问题，替代启发式高斯到边界映射，显著提升弱监督时序视频定位性能并兼容多种提议架构。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯的时序提议方法在推理阶段依赖经验性的参数到边界的映射，导致定位精度受限。需要一种有理论依据且通用的推理方法以提升定位性能与泛化性。

Method: GBO将边界预测表述为一个平衡覆盖率与紧凑性的凸优化问题，导出其闭式最优解并分析惩罚参数对解的影响；在实际应用中将该推理方法直接替换原有的启发式映射，适配单高斯与混合高斯提议体系，且不依赖额外训练。

Result: 在多个标准数据集上，GBO显著提升了定位精度并达到了SOTA；同时展示了在不同提议方案上的高效性与泛化能力。代码开源以便复现。

Conclusion: 本文提出的Gaussian Boundary Optimization (GBO)通过将高斯参数到时段边界的映射替换为一个有理论支撑的最优化问题，有效提升了弱监督时序视频定位的推理性能。GBO在不同惩罚范式下的最优性得到解析，并推出闭式解，兼容单高斯与混合高斯方案，且无需训练即可直接应用，实验证明达到或超越当前基线与SOTA。

Abstract: Weakly supervised temporal video grounding aims to localize query-relevant segments in untrimmed videos using only video-sentence pairs, without requiring ground-truth segment annotations that specify exact temporal boundaries. Recent approaches tackle this task by utilizing Gaussian-based temporal proposals to represent query-relevant segments. However, their inference strategies rely on heuristic mappings from Gaussian parameters to segment boundaries, resulting in suboptimal localization performance. To address this issue, we propose Gaussian Boundary Optimization (GBO), a novel inference framework that predicts segment boundaries by solving a principled optimization problem that balances proposal coverage and segment compactness. We derive a closed-form solution for this problem and rigorously analyze the optimality conditions under varying penalty regimes. Beyond its theoretical foundations, GBO offers several practical advantages: it is training-free and compatible with both single-Gaussian and mixture-based proposal architectures. Our experiments show that GBO significantly improves localization, achieving state-of-the-art results across standard benchmarks. Extensive experiments demonstrate the efficiency and generalizability of GBO across various proposal schemes. The code is available at \href{https://github.com/sunoh-kim/gbo}{https://github.com/sunoh-kim/gbo}.

</details>


### [29] [A generalizable large-scale foundation model for musculoskeletal radiographs](https://arxiv.org/abs/2602.03076)
*Shinn Kim,Soobin Lee,Kyoungseob Shin,Han-Soo Kim,Yongsung Kim,Minsu Kim,Juhong Nam,Somang Ko,Daeheon Kwon,Wook Huh,Ilkyu Han,Sunghoon Kwon*

Main category: cs.CV

TL;DR: SKELEX：基于1.2M肌骨X线自监督训练的基础模型，标签高效、泛化性强，支持多任务诊断与零样本定位，并实现可解释的骨肿瘤预测与公开部署。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型多为特定任务、依赖标注且泛化性差；公开数据集规模与多样性不足，无法支持跨病种与解剖部位的通用基础模型训练，因此需要大规模、自监督训练的通用肌骨X线基础模型。

Method: 作者收集了1.2百万张多样化、富含病况的肌骨X线片，采用自监督学习训练了基础模型，并在12项下游诊断任务上进行评估；此外利用模型的零样本异常定位能力开发了可解释的区域引导骨肿瘤预测模型，并部署为网页应用。

Result: SKELEX在骨折检测、骨关节炎分级、骨肿瘤分类等任务上普遍优于基线模型，展现了零样本异常定位能力；基于该能力的区域引导模型在独立外部数据集上保持稳健性能，并公开部署。

Conclusion: SKELEX构建了一个可扩展、标签高效且具广泛泛化能力的肌肉骨骼X线影像基础模型，为临床应用与高效研究奠定基础。

Abstract: Artificial intelligence (AI) has shown promise in detecting and characterizing musculoskeletal diseases from radiographs. However, most existing models remain task-specific, annotation-dependent, and limited in generalizability across diseases and anatomical regions. Although a generalizable foundation model trained on large-scale musculoskeletal radiographs is clinically needed, publicly available datasets remain limited in size and lack sufficient diversity to enable training across a wide range of musculoskeletal conditions and anatomical sites. Here, we present SKELEX, a large-scale foundation model for musculoskeletal radiographs, trained using self-supervised learning on 1.2 million diverse, condition-rich images. The model was evaluated on 12 downstream diagnostic tasks and generally outperformed baselines in fracture detection, osteoarthritis grading, and bone tumor classification. Furthermore, SKELEX demonstrated zero-shot abnormality localization, producing error maps that identified pathologic regions without task-specific training. Building on this capability, we developed an interpretable, region-guided model for predicting bone tumors, which maintained robust performance on independent external datasets and was deployed as a publicly accessible web application. Overall, SKELEX provides a scalable, label-efficient, and generalizable AI framework for musculoskeletal imaging, establishing a foundation for both clinical translation and data-efficient research in musculoskeletal radiology.

</details>


### [30] [Gromov Wasserstein Optimal Transport for Semantic Correspondences](https://arxiv.org/abs/2602.03105)
*Francis Snelgar,Stephen Gould,Ming Xu,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: 用带Gromov-Wasserstein先验的最优传输替代最近邻匹配，提升DINOv2语义对应性能并显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法将DINOv2与Stable Diffusion特征联合以兼顾精确性与空间一致性，但计算开销大；因此希望用更高效的匹配算法替代SD特征以降低资源消耗。

Method: 基线为DINOv2特征，替换常规最近邻匹配为含GW空间一致性先验的最优传输算法，以获得更平滑、空间一致的匹配。

Result: 在多个语义对应评测上，方法显著提升DINOv2基线性能，能与甚至超过部分使用SD特征的SOTA方法，同时在计算上快5--10倍。代码已开源。

Conclusion: 本文提出用带Gromov-Wasserstein空间平滑先验的最优传输（GW-OT）替代Stable Diffusion特征匹配，从而在保持或超越SOTA性能的同时显著提高效率。

Abstract: Establishing correspondences between image pairs is a long studied problem in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, current state-of-the-art methods for semantic correspondence involve combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps from large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient. We make code available at https://github.com/fsnelgar/semantic_matching_gwot .

</details>


### [31] [Beyond Cropping and Rotation: Automated Evolution of Powerful Task-Specific Augmentations with Generative Models](https://arxiv.org/abs/2602.03123)
*Judah Goldfeder,Shreyes Kaliyur,Vaibhav Sourirajan,Patrick Minwan Puma,Philippe Martin Wyder,Yuhang Hu,Jiong Lin,Hod Lipson*

Main category: cs.CV

TL;DR: 提出EvoAug：结合生成模型与进化搜索，学习层级随机增强树以自动化发现任务特定的生成式数据增强，在细粒度和少样本任务上显著提升性能，并能发现与领域知识一致的增强。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强（裁剪、旋转等）简单且稳定，但生成模型提供了更高多样性与逼真度的合成数据，有潜力进一步提升鲁棒性；但生成式增强若与任务不匹配可能降性能，因此需要自动化方法学习最优增强策略。

Method: 利用条件生成模型（如条件扩散模型、few-shot NeRFs）生成高多样性样本，并用高效的进化算法搜索随机化的层级增强树（stochastic augmentation trees），在树结构中分层组合增强算子以获得结构化、可适应的变换；在搜索时评估对下游任务表现的影响并优化。

Result: 在细粒度分类和少样本学习基准上获得明显改进；发现的增强策略符合领域知识，即使在低数据设置下也能学习有意义的变换；总体展示了学习型生成增强在训练鲁棒模型方面的潜力。

Conclusion: EvoAug通过结合生成模型与进化算法，实现了自动学习任务特定的数据增强策略，从而在细粒度分类和少样本学习任务上提升鲁棒性和性能。

Abstract: Data augmentation has long been a cornerstone for reducing overfitting in vision models, with methods like AutoAugment automating the design of task-specific augmentations. Recent advances in generative models, such as conditional diffusion and few-shot NeRFs, offer a new paradigm for data augmentation by synthesizing data with significantly greater diversity and realism. However, unlike traditional augmentations like cropping or rotation, these methods introduce substantial changes that enhance robustness but also risk degrading performance if the augmentations are poorly matched to the task. In this work, we present EvoAug, an automated augmentation learning pipeline, which leverages these generative models alongside an efficient evolutionary algorithm to learn optimal task-specific augmentations. Our pipeline introduces a novel approach to image augmentation that learns stochastic augmentation trees that hierarchically compose augmentations, enabling more structured and adaptive transformations. We demonstrate strong performance across fine-grained classification and few-shot learning tasks. Notably, our pipeline discovers augmentations that align with domain knowledge, even in low-data settings. These results highlight the potential of learned generative augmentations, unlocking new possibilities for robust model training.

</details>


### [32] [Feature, Alignment, and Supervision in Category Learning: A Comparative Approach with Children and Neural Networks](https://arxiv.org/abs/2602.03124)
*Fanxiao Wani Qiu,Oscar Leong*

Main category: cs.CV

TL;DR: 在人类与CNN的少样本半监督类别学习比较中，儿童快速从少量标签泛化但受特征与对齐影响强，CNN对监督量更敏感，强调应关注监督、特征结构与对齐性三者的交互。


<details>
  <summary>Details</summary>
Motivation: 理解在人类认知与机器学习中，如何在样本稀少时学习类别，尤其比较儿童与深度学习模型在相似环境下的行为差异，以便更准确地评估模型是否捕捉人类学习机制。

Method: 种属公平设计：儿童与CNN在相同条件下进行少-shot半监督学习任务。给定混合的有标签与无标签示例，操控监督量（1、3、6个标签）、目标特征（尺寸、形状、图案）和感知对齐（高/低）。测量两类学习者的泛化表现并分析监督、特征与对齐性的交互影响。

Result: 实验结果：儿童在极少标签下即可迅速泛化，但表现高度依赖特征类型和感知对齐；CNN对标签数更敏感，增加监督通常提高性能，但其效果受对齐和特征结构的调节。总体显示两者在交互模式上有显著差异。

Conclusion: 作者结论是：在人类（儿童）和CNN的少样本半监督类别学习中，二者表现不同。儿童能从极少标签快速泛化，但受特征类型（尺寸、形状、图案）和感知对齐度影响较大；CNN随标签增加表现改进，但对齐性和特征结构会调节额外监督的效果。因此比较人类与模型需在合适条件下进行，应关注监督量、特征结构与对齐性之间的交互，而非单一准确率指标。

Abstract: Understanding how humans and machines learn from sparse data is central to cognitive science and machine learning. Using a species-fair design, we compare children and convolutional neural networks (CNNs) in a few-shot semi-supervised category learning task. Both learners are exposed to novel object categories under identical conditions. Learners receive mixtures of labeled and unlabeled exemplars while we vary supervision (1/3/6 labels), target feature (size, shape, pattern), and perceptual alignment (high/low). We find that children generalize rapidly from minimal labels but show strong feature-specific biases and sensitivity to alignment. CNNs show a different interaction profile: added supervision improves performance, but both alignment and feature structure moderate the impact additional supervision has on learning. These results show that human-model comparisons must be drawn under the right conditions, emphasizing interactions among supervision, feature structure, and alignment rather than overall accuracy.

</details>


### [33] [Flexible Geometric Guidance for Probabilistic Human Pose Estimation with Diffusion Models](https://arxiv.org/abs/2602.03126)
*Francis Snelgar,Ming Xu,Stephen Gould,Liang Zheng,Akshay Asthana*

Main category: cs.CV

TL;DR: 使用扩散模型与2D热图梯度引导，论文实现了从单张图像采样多样化且与2D观测一致的3D人体姿态，减少配对数据需求并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 2D到3D姿态估计存在深度模糊和遮挡导致的多解性，而多数工作仅返回单一确定性解且依赖大量成对2D-3D数据，泛化性差；因此需要一种能表示多模态解且减少对配对数据依赖的方法。

Method: 论文训练了一个仅基于3D姿态数据的无条件扩散模型，推理时在条件生成的指导框架下利用2D关键点检测器热图的梯度对扩散采样过程进行引导，从而生成与输入2D图像兼容的多样化3D姿态样本。

Result: 在Human3.6M数据集上采用best-of-m多假设评估时，该方法在无需配对2D-3D训练数据的同类方法中取得了最先进的性能；在MPI-INF-3DHP和3DPW上也展示了具有竞争力的泛化性能，并演示了扩散框架在姿态生成与姿态补全等任务上的应用灵活性。

Conclusion: 该论文提出用扩散模型解决2D图像到3D人体姿态估计的深度二义性与遮挡问题，能够生成多个与2D检测热图一致的合理3D姿态样本，从而摆脱了确定性映射的限制，并减少对配对2D-3D训练数据的依赖。

Abstract: 3D human pose estimation from 2D images is a challenging problem due to depth ambiguity and occlusion. Because of these challenges the task is underdetermined, where there exists multiple -- possibly infinite -- poses that are plausible given the image. Despite this, many prior works assume the existence of a deterministic mapping and estimate a single pose given an image. Furthermore, methods based on machine learning require a large amount of paired 2D-3D data to train and suffer from generalization issues to unseen scenarios. To address both of these issues, we propose a framework for pose estimation using diffusion models, which enables sampling from a probability distribution over plausible poses which are consistent with a 2D image. Our approach falls under the guidance framework for conditional generation, and guides samples from an unconditional diffusion model, trained only on 3D data, using the gradients of the heatmaps from a 2D keypoint detector. We evaluate our method on the Human 3.6M dataset under best-of-$m$ multiple hypothesis evaluation, showing state-of-the-art performance among methods which do not require paired 2D-3D data for training. We additionally evaluate the generalization ability using the MPI-INF-3DHP and 3DPW datasets and demonstrate competitive performance. Finally, we demonstrate the flexibility of our framework by using it for novel tasks including pose generation and pose completion, without the need to train bespoke conditional models. We make code available at https://github.com/fsnelgar/diffusion_pose .

</details>


### [34] [FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation](https://arxiv.org/abs/2602.03130)
*Chenxi Zhang,Ziliang Gan,Liyun Zhu,Youwei Pang,Qing Zhang,Rongjunchen Zhang*

Main category: cs.CV

TL;DR: FinMTM：11k+双语金融多模态多轮基准，丰富任务类型与评价指标，揭露VLM在金融场景下的关键短板。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准以单轮、单一题型为主，不能反映真实应用中多轮交互、复杂推理和代理操作的需求，因此需要一个多轮多模态的更全面基准。

Method: 构建11,133条中英双语的金融视觉问答对，覆盖烛线图、统计图和报告图；设计多任务评测（单/多选题、多轮开放式对话、代理任务）并提出针对性评分规则（集合重叠得分、多轮加权评分、代理复合指标）。对22个VLM进行系统评估。

Result: 发布了FinMTM数据集与评测协议，实验证明多数VLM在精细视觉感知、长上下文推理和复杂代理工作流上表现不足，为未来模型改进指明方向。

Conclusion: FinMTM揭示了现有VLMs在金融多模态场景中的显著不足，并通过多维任务与双语数据提高评估覆盖面。

Abstract: The financial domain poses substantial challenges for vision-language models (VLMs) due to specialized chart formats and knowledge-intensive reasoning requirements. However, existing financial benchmarks are largely single-turn and rely on a narrow set of question formats, limiting comprehensive evaluation in realistic application scenarios. To address this gap, we propose FinMTM, a multi-turn multimodal benchmark that expands diversity along both data and task dimensions. On the data side, we curate and annotate 11{,}133 bilingual (Chinese and English) financial QA pairs grounded in financial visuals, including candlestick charts, statistical plots, and report figures. On the task side, FinMTM covers single- and multiple-choice questions, multi-turn open-ended dialogues, and agent-based tasks. We further design task-specific evaluation protocols, including a set-overlap scoring rule for multiple-choice questions, a weighted combination of turn-level and session-level scores for multi-turn dialogues, and a composite metric that integrates planning quality with final outcomes for agent tasks. Extensive experimental evaluation of 22 VLMs reveal their limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows.

</details>


### [35] [SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass](https://arxiv.org/abs/2602.03134)
*Chen Qian,Xinran Yu,Danyang Li,Guoxuan Chi,Zheng Yang,Qiang Ma,Xin Miao*

Main category: cs.CV

TL;DR: 提出bypass范式与无训练SwiftVLM，通过保留并在后续层重新评估未选token，避免早期剪枝造成的关键信息丢失，在多模型多任务上实现更好准确率-效率平衡。


<details>
  <summary>Details</summary>
Motivation: 现有依赖早期剪枝的VLM视觉token剪枝方法在粗粒度任务上有效，但在需要细粒度视觉信息的任务上性能大幅下降，原因是浅层被判定为不重要的token在后层可能变得重要，从而导致不可逆的信息丢失。

Method: 通过层级分析发现视觉token在各层的重要性存在显著差异；提出bypass范式，将未被选中的视觉tokens保留并传递到后续剪枝阶段以供重新评估；基于此范式设计SwiftVLM，一种无需训练、在模型特定层进行剪枝并允许层间独立决策的简单方法。

Result: 在多个VLM模型和基准上，SwiftVLM在准确性-效率权衡上优于现有剪枝策略，并表现出更可信的视觉token选择行为。

Conclusion: 本文提出的bypass范式与SwiftVLM方法有效避免了早期剪枝导致的关键信息不可逆丢失，从而在保持效率的同时显著改善细粒度视觉-文本任务的性能。

Abstract: Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.

</details>


### [36] [FSOD-VFM: Few-Shot Object Detection with Vision Foundation Models and Graph Diffusion](https://arxiv.org/abs/2602.03137)
*Chen-Bin Feng,Youyang Sha,Longfei Liu,Yongjun Yu,Chi Man Vong,Xuanlong Yu,Xi Shen*

Main category: cs.CV

TL;DR: 提出FSOD-VFM：结合UPN、SAM2与DINOv2并用图扩散重加权置信度，解决候选框碎片化问题，在多数据集上无训练显著提升少样本检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉基础模型泛化能力强，但UPN生成的候选框易过度碎片化，导致大量小的误报，影响少样本检测性能；需要一种无需额外训练即可提升候选框质量的方法。

Method: 构建包含UPN（类别无关候选框生成）、SAM2（掩码提取）、DINOv2特征适配的新框架；针对UPN产生的过碎候选框，提出基于有向图的置信度重加权，通过图扩散传播置信度以提升完整目标框的分数并抑制局部碎片。

Result: 在Pascal-5^i、COCO-20^i、CD-FSOD等数据集上大量实验表明方法优于现有无训练方法；在跨域CD-FSOD的10-shot设置上达到31.6 AP，显著优于先前的21.4 AP。

Conclusion: FSOD-VFM利用视觉基础模型和图扩散的置信度重加权显著提升了少样本目标检测的准确性与检测粒度，尤其在跨域数据集上表现优异。

Abstract: In this paper, we present FSOD-VFM: Few-Shot Object Detectors with Vision Foundation Models, a framework that leverages vision foundation models to tackle the challenge of few-shot object detection. FSOD-VFM integrates three key components: a universal proposal network (UPN) for category-agnostic bounding box generation, SAM2 for accurate mask extraction, and DINOv2 features for efficient adaptation to new object categories. Despite the strong generalization capabilities of foundation models, the bounding boxes generated by UPN often suffer from overfragmentation, covering only partial object regions and leading to numerous small, false-positive proposals rather than accurate, complete object detections. To address this issue, we introduce a novel graph-based confidence reweighting method. In our approach, predicted bounding boxes are modeled as nodes in a directed graph, with graph diffusion operations applied to propagate confidence scores across the network. This reweighting process refines the scores of proposals, assigning higher confidence to whole objects and lower confidence to local, fragmented parts. This strategy improves detection granularity and effectively reduces the occurrence of false-positive bounding box proposals. Through extensive experiments on Pascal-5$^i$, COCO-20$^i$, and CD-FSOD datasets, we demonstrate that our method substantially outperforms existing approaches, achieving superior performance without requiring additional training. Notably, on the challenging CD-FSOD dataset, which spans multiple datasets and domains, our FSOD-VFM achieves 31.6 AP in the 10-shot setting, substantially outperforming previous training-free methods that reach only 21.4 AP. Code is available at: https://intellindust-ai-lab.github.io/projects/FSOD-VFM.

</details>


### [37] [Diversity-Preserved Distribution Matching Distillation for Fast Visual Synthesis](https://arxiv.org/abs/2602.03139)
*Tianhe Wu,Ruibin Li,Lei Zhang,Kede Ma*

Main category: cs.CV

TL;DR: 通过将蒸馏步骤角色分离：首步保多样性（目标预测），后步保质量（DMD），并在首步阻断DMD梯度，DP-DMD简单高效地解决了DMD的模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 现有的分布匹配蒸馏（DMD）因逆KL倾向于模式寻求导致模式崩溃，现有解决方法依赖感知或对抗正则化，带来额外计算开销和训练不稳定性，需一种简单有效的方法保持多样性同时保证质量。

Method: 将蒸馏步骤分成两类：第1步采用目标预测（如v-prediction）损失以保留样本多样性，同时在第1步阻断来自DMD（逆KL）目标的梯度；后续步骤仍使用标准的分布匹配蒸馏损失以精炼质量。

Result: 在大规模文本到图像实验中，DP-DMD在不使用感知骨干、判别器或额外真值图像的条件下，既保留了样本多样性，又达到了与最先进方法相当的视觉质量。

Conclusion: 提出了DP-DMD框架，通过角色分离防止模式崩溃，在首步保留多样性、后续步骤进行质量优化，并在首步阻断DMD梯度，从而无需感知或对抗正则化仍保持高质量与多样性。

Abstract: Distribution matching distillation (DMD) aligns a multi-step generator with its few-step counterpart to enable high-quality generation under low inference cost. However, DMD tends to suffer from mode collapse, as its reverse-KL formulation inherently encourages mode-seeking behavior, for which existing remedies typically rely on perceptual or adversarial regularization, thereby incurring substantial computational overhead and training instability. In this work, we propose a role-separated distillation framework that explicitly disentangles the roles of distilled steps: the first step is dedicated to preserving sample diversity via a target-prediction (e.g., v-prediction) objective, while subsequent steps focus on quality refinement under the standard DMD loss, with gradients from the DMD objective blocked at the first step. We term this approach Diversity-Preserved DMD (DP-DMD), which, despite its simplicity -- no perceptual backbone, no discriminator, no auxiliary networks, and no additional ground-truth images -- preserves sample diversity while maintaining visual quality on par with state-of-the-art methods in extensive text-to-image experiments.

</details>


### [38] [Fully Kolmogorov-Arnold Deep Model in Medical Image Segmentation](https://arxiv.org/abs/2602.03156)
*Xingyu Qiu,Xinghua Ma,Dong Liang,Gongning Luo,Wei Wang,Kuanquan Wang,Shuo Li*

Main category: cs.CV

TL;DR: 提出SaKAN与Grad-Free Spline，构建首个全KA深度网络ALL U-KAN，显著降低参数与内存并提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 深度堆叠的KAN在训练困难和内存需求方面存在实际障碍，导致现有研究仅能使用少量层，限制对KA网络（KANs）的系统探索。

Method: 提出Share-activation KAN（SaKAN）简化Sprecher变体的参数化以易于优化；引入Grad-Free Spline以避免样条梯度带来的巨大内存和计算开销；在此基础上构建ALL U-KAN，将KA/KAonv层替换FC/Conv层，形成深度全KA架构。

Result: 在三个医学图像分割任务上，ALL U-KAN优于部分KA或传统架构，分割精度更高；与直接深度堆叠的KAN相比，参数量减少10倍，内存消耗降低超过20倍。

Conclusion: 本文提出了首个完全基于Kolmogorov–Arnold（KA）表示的深度模型ALL U-KAN，证明KA层可完全替代传统FC/Conv层并具备更强学习能力。

Abstract: Deeply stacked KANs are practically impossible due to high training difficulties and substantial memory requirements. Consequently, existing studies can only incorporate few KAN layers, hindering the comprehensive exploration of KANs. This study overcomes these limitations and introduces the first fully KA-based deep model, demonstrating that KA-based layers can entirely replace traditional architectures in deep learning and achieve superior learning capacity. Specifically, (1) the proposed Share-activation KAN (SaKAN) reformulates Sprecher's variant of Kolmogorov-Arnold representation theorem, which achieves better optimization due to its simplified parameterization and denser training samples, to ease training difficulty, (2) this paper indicates that spline gradients contribute negligibly to training while consuming huge GPU memory, thus proposes the Grad-Free Spline to significantly reduce memory usage and computational overhead. (3) Building on these two innovations, our ALL U-KAN is the first representative implementation of fully KA-based deep model, where the proposed KA and KAonv layers completely replace FC and Conv layers. Extensive evaluations on three medical image segmentation tasks confirm the superiority of the full KA-based architecture compared to partial KA-based and traditional architectures, achieving all higher segmentation accuracy. Compared to directly deeply stacked KAN, ALL U-KAN achieves 10 times reduction in parameter count and reduces memory consumption by more than 20 times, unlocking the new explorations into deep KAN architectures.

</details>


### [39] [Human-in-the-loop Adaptation in Group Activity Feature Learning for Team Sports Video Retrieval](https://arxiv.org/abs/2602.03157)
*Chihiro Nakatani,Hiroaki Kawashima,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出无标签下的人机交互微调GAFL用于视频检索：自监督预训练GAF + 用户标注的高效选样 + 对比微调，使检索效果明显提升。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义的组活动类别与监督标签，限制泛化与微调能力；本工作希望在无标签条件下，通过人类反馈高效地适配特征空间以改善基于查询的视频检索效果。

Method: 方法包括两阶段：首先采用自监督学习预训练组活动特征空间（GAF），基于视频间组活动相似性而非预定义类别监督；然后通过人机交互的高效选样与对比学习微调GAF。交互过程中系统从数据库中选择若干视频供用户标注为正/负样本，利用这些标签通过对比损失调整特征空间，使正样本靠近查询视频、负样本远离。

Result: 在两个团队运动数据集上的实验表明，所提方法显著提升检索性能，消融实验表明各组件（自监督预训练、高效选样、对比微调）均对性能提升有贡献。

Conclusion: 本文提出了一种在无组活动标签情况下通过人机交互微调的组活动特征学习（GAFL）方法，并将其应用于组活动视频检索，显著提升检索性能。

Abstract: This paper proposes human-in-the-loop adaptation for Group Activity Feature Learning (GAFL) without group activity annotations. This human-in-the-loop adaptation is employed in a group-activity video retrieval framework to improve its retrieval performance. Our method initially pre-trains the GAF space based on the similarity of group activities in a self-supervised manner, unlike prior work that classifies videos into pre-defined group activity classes in a supervised learning manner. Our interactive fine-tuning process updates the GAF space to allow a user to better retrieve videos similar to query videos given by the user. In this fine-tuning, our proposed data-efficient video selection process provides several videos, which are selected from a video database, to the user in order to manually label these videos as positive or negative. These labeled videos are used to update (i.e., fine-tune) the GAF space, so that the positive and negative videos move closer to and farther away from the query videos through contrastive learning. Our comprehensive experimental results on two team sports datasets validate that our method significantly improves the retrieval performance. Ablation studies also demonstrate that several components in our human-in-the-loop adaptation contribute to the improvement of the retrieval performance. Code: https://github.com/chihina/GAFL-FINE-CVIU.

</details>


### [40] [BinaryDemoire: Moiré-Aware Binarization for Image Demoiréing](https://arxiv.org/abs/2602.03176)
*Zheng Chen,Zhi Yang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 为了解决在二值网络下去摩尔纹性能低下的问题，本文提出了频率感知的二值门控（MABG）与分组打乱残差适配器（SGRA），在四个基准上显著优于现有二值化方法。


<details>
  <summary>Details</summary>
Motivation: 摩尔纹是一种高度频率相关且跨尺度/方向变化的结构化退化，而现有全精度去摩尔纹模型对部署成本高；二值化能极大压缩但直接应用会导致性能严重下降，因此需要专门设计以适应摩尔纹的频率特性。

Method: 提出了两个关键模块：1) Moiré-aware Binary Gate (MABG)，结合轻量频率描述子和激活统计，预测通道级门控系数以条件化二值卷积响应的聚合；2) Shuffle-Grouped Residual Adapter (SGRA)，实现结构化稀疏的短接对齐并通过交错混合促进不同通道分区间的信息交换。整体在二值化网络中保留频率敏感性。

Result: 在四个基准数据集上，BinaryDemoire优于现有二值化方法，显示其在保持极低位宽的同时实现了更好的去摩尔纹效果。代码已开源。

Conclusion: 该论文提出了BinaryDemoire，一种针对去摩尔纹任务的二值化网络框架，通过频率感知门控和分组残差适配器在极低位宽下取得了优异性能。

Abstract: Image demoiréing aims to remove structured moiré artifacts in recaptured imagery, where degradations are highly frequency-dependent and vary across scales and directions. While recent deep networks achieve high-quality restoration, their full-precision designs remain costly for deployment. Binarization offers an extreme compression regime by quantizing both activations and weights to 1-bit. Yet, it has been rarely studied for demoiréing and performs poorly when naively applied. In this work, we propose BinaryDemoire, a binarized demoiréing framework that explicitly accommodates the frequency structure of moiré degradations. First, we introduce a moiré-aware binary gate (MABG) that extracts lightweight frequency descriptors together with activation statistics. It predicts channel-wise gating coefficients to condition the aggregation of binary convolution responses. Second, we design a shuffle-grouped residual adapter (SGRA) that performs structured sparse shortcut alignment. It further integrates interleaved mixing to promote information exchange across different channel partitions. Extensive experiments on four benchmarks demonstrate that the proposed BinaryDemoire surpasses current binarization methods. Code: https://github.com/zhengchen1999/BinaryDemoire.

</details>


### [41] [LSGQuant: Layer-Sensitivity Guided Quantization for One-Step Diffusion Real-World Video Super-Resolution](https://arxiv.org/abs/2602.03182)
*Tianxing Wu,Zheng Chen,Cirou Xu,Bowen Chai,Yong Guo,Yutong Liu,Linghe Kong,Yulun Zhang*

Main category: cs.CV

TL;DR: 为解决扩散Transformer在视频超分任务上难以量化的问题，LSGQuant提出了DRAQ、VOLTS与QAO三项技术，基于层敏感度自适应量化并联合优化，实验证明能在低比特下保持接近全精度性能并超越现有方法。


<details>
  <summary>Details</summary>
Motivation: DiTs模型体积大、计算开销高，而输入潜在表示动态范围大、层行为差异显著，现有低比特量化方法在此类模型上效果有限。论文动机是设计专门针对扩散Transformer和视频token特性的一套量化与微调策略，以在低精度下尽量保留性能。

Method: 方法包含三部分：1) 动态范围自适应量化器（DRAQ），用于适配视频token激活的高动态范围；2) 基于校准阶段层级统计分析的方差导向层训练策略（VOLTS），通过估计层敏感度对不同层采用不同训练/量化策略；3) 量化感知优化（QAO），联合微调量化分支与保留的高精度分支以恢复性能。

Result: 大量实验表明，LSGQuant在接近原始全精度模型性能的同时，显著优于现有量化方法，且在实践中可显著降低模型存储与推理成本。论文并发布了代码。

Conclusion: 该论文提出了LSGQuant，一种针对一阶扩散模型（One-Step Diffusion）视频超分的分层敏感度引导量化方法，旨在在保持原始模型性能的同时大幅压缩模型并降低推理计算成本。

Abstract: One-Step Diffusion Models have demonstrated promising capability and fast inference in video super-resolution (VSR) for real-world. Nevertheless, the substantial model size and high computational cost of Diffusion Transformers (DiTs) limit downstream applications. While low-bit quantization is a common approach for model compression, the effectiveness of quantized models is challenged by the high dynamic range of input latent and diverse layer behaviors. To deal with these challenges, we introduce LSGQuant, a layer-sensitivity guided quantizing approach for one-step diffusion-based real-world VSR. Our method incorporates a Dynamic Range Adaptive Quantizer (DRAQ) to fit video token activations. Furthermore, we estimate layer sensitivity and implement a Variance-Oriented Layer Training Strategy (VOLTS) by analyzing layer-wise statistics in calibration. We also introduce Quantization-Aware Optimization (QAO) to jointly refine the quantized branch and a retained high-precision branch. Extensive experiments demonstrate that our method has nearly performance to origin model with full-precision and significantly exceeds existing quantization techniques. Code is available at: https://github.com/zhengchen1999/LSGQuant.

</details>


### [42] [From Single Scan to Sequential Consistency: A New Paradigm for LIDAR Relocalization](https://arxiv.org/abs/2602.03198)
*Minghang Zhu,Zhijing Wang,Yuxin Guo,Wen Li,Sheng Ao,Cheng Wang*

Main category: cs.CV

TL;DR: TempLoc通过预测点级全局坐标与不确定度，并结合注意力估计的帧间对应与不确定度引导融合，实现更鲁棒的时序一致性LiDAR重定位，显著优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 回归型方法在动态或歧义场景下易失败，主要因为只依赖单帧或忽视扫描间时空一致性，故引入序列级别的对应建模以增强稳定性。

Method: 提出三个模块：1) 全局坐标估计模块，逐点预测全局坐标及不确定度；2) 先验坐标生成模块，利用注意力估计帧间点对应；3) 不确定度引导的坐标融合模块，将预测的对应与不确定性端到端融合以估计6-DoF位姿。

Result: 在NCLT与Oxford Robot-Car数据集上，TempLoc相较于最新方法有明显提升，展示了时序感知对应建模的有效性。

Conclusion: 本文提出TempLoc，通过建模序列一致性提升LiDAR重定位鲁棒性，实验证明在NCLT和Oxford Robot-Car上优于现有方法。

Abstract: LiDAR relocalization aims to estimate the global 6-DoF pose of a sensor in the environment. However, existing regression-based approaches are prone to dynamic or ambiguous scenarios, as they either solely rely on single-frame inference or neglect the spatio-temporal consistency across scans. In this paper, we propose TempLoc, a new LiDAR relocalization framework that enhances the robustness of localization by effectively modeling sequential consistency. Specifically, a Global Coordinate Estimation module is first introduced to predict point-wise global coordinates and associated uncertainties for each LiDAR scan. A Prior Coordinate Generation module is then presented to estimate inter-frame point correspondences by the attention mechanism. Lastly, an Uncertainty-Guided Coordinate Fusion module is deployed to integrate both predictions of point correspondence in an end-to-end fashion, yielding a more temporally consistent and accurate global 6-DoF pose. Experimental results on the NCLT and Oxford Robot-Car benchmarks show that our TempLoc outperforms stateof-the-art methods by a large margin, demonstrating the effectiveness of temporal-aware correspondence modeling in LiDAR relocalization. Our code will be released soon.

</details>


### [43] [Hand3R: Online 4D Hand-Scene Reconstruction in the Wild](https://arxiv.org/abs/2602.03200)
*Wendi Hu,Haonan Zhou,Wenhao Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: Hand3R是首个在线从单目视频联合重建4D手与场景的框架，结合手部先验与4D场景模型，通过场景感知提示实现单次前向传递同时产出高精度手网格与度量尺度场景几何，性能优于需离线优化的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只在局部坐标系中重建独立的手部，忽略了周围的3D环境，难以理解手与场景的物理交互，因此需要一种能同时恢复手与场景的在线方法。

Method: 通过场景感知视觉提示机制，将高保真手部先验注入到持久化的场景记忆中，使系统能在单次前向传递中输出局部精确的手部网格和全局一致的场景几何。

Result: 实验证明Hand3R避免了对离线优化的依赖，在局部手部重建和全局定位上均达到有竞争力的性能。

Conclusion: Hand3R提出了一个在线单目视频4D手-场景联合重建框架，通过将预训练手部专家与4D场景基础模型结合，实现了同时恢复高精度手网格与全局度量尺度场景几何。

Abstract: For Embodied AI, jointly reconstructing dynamic hands and the dense scene context is crucial for understanding physical interaction. However, most existing methods recover isolated hands in local coordinates, overlooking the surrounding 3D environment. To address this, we present Hand3R, the first online framework for joint 4D hand-scene reconstruction from monocular video. Hand3R synergizes a pre-trained hand expert with a 4D scene foundation model via a scene-aware visual prompting mechanism. By injecting high-fidelity hand priors into a persistent scene memory, our approach enables simultaneous reconstruction of accurate hand meshes and dense metric-scale scene geometry in a single forward pass. Experiments demonstrate that Hand3R bypasses the reliance on offline optimization and delivers competitive performance in both local hand reconstruction and global positioning.

</details>


### [44] [VIRAL: Visual In-Context Reasoning via Analogy in Diffusion Transformers](https://arxiv.org/abs/2602.03210)
*Zhiwen Li,Zhongjie Duan,Jinyan Ye,Cen Chen,Daoyuan Chen,Yaliang Li,Yingda Chen*

Main category: cs.CV

TL;DR: VIRAL通过视觉类比的条件生成、角色感知多图像条件化与MoE-LoRA在冻结Diffusion Transformer上实现统一的视觉ICL，配合大规模新数据集，在各种视觉任务上表现领先。


<details>
  <summary>Details</summary>
Motivation: 动机是解决视觉领域中ICL复现困难的问题，主要挑战为任务异质性导致的模型适配及上下文表示问题，因此将ICL转化为视觉类比形式以统一多样任务。

Method: 方法上，VIRAL在冻结的Diffusion Transformer(DiT)基础上引入了角色感知的多图像条件化机制，并采用Mixture-of-Experts LoRA来减少多任务训练时的梯度干扰；同时构建了一个涵盖感知、恢复与编辑三大类任务的大规模视觉上下文数据集。

Result: 实验表明VIRAL在多个视觉任务上优于现有方法，验证了统一的V-ICL范式能覆盖大多数视觉任务并支持开放域编辑。作者同时公开了代码与新构建的数据集。

Conclusion: 本文提出了VIRAL框架，通过将视觉上下文学习(ICL)表述为视觉类比的条件生成问题，从预训练图像编辑模型中引出视觉推理能力。

Abstract: Replicating In-Context Learning (ICL) in computer vision remains challenging due to task heterogeneity. We propose \textbf{VIRAL}, a framework that elicits visual reasoning from a pre-trained image editing model by formulating ICL as conditional generation via visual analogy ($x_s : x_t :: x_q : y_q$). We adapt a frozen Diffusion Transformer (DiT) using role-aware multi-image conditioning and introduce a Mixture-of-Experts LoRA to mitigate gradient interference across diverse tasks. Additionally, to bridge the gaps in current visual context datasets, we curate a large-scale dataset spanning perception, restoration, and editing. Experiments demonstrate that VIRAL outperforms existing methods, validating that a unified V-ICL paradigm can handle the majority of visual tasks, including open-domain editing. Our code is available at https://anonymous.4open.science/r/VIRAL-744A

</details>


### [45] [ConsisDrive: Identity-Preserving Driving World Models for Video Generation by Instance Mask](https://arxiv.org/abs/2602.03213)
*Zhuoran Yang,Yanyong Zhang*

Main category: cs.CV

TL;DR: ConsisDrive 通过实例掩码的注意力和损失设计，强化实例级时序一致性，缓解身份漂移，提升驾驶视频生成与下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 世界模型在生成驾驶视频时成本低但易出现身份漂移（同一对象在不同时刻外观或类别变化），主要因缺少实例级时间约束。为提高生成视频的身份一致性和下游任务性能，需要引入实例级的时序约束。

Method: 引入两项关键机制：(1) Instance-Masked Attention：在注意力模块中引入实例身份掩码和轨迹掩码，使视觉 token 仅与对应实例的时空特征交互；(2) Instance-Masked Loss：通过概率性实例掩码自适应增强前景区域损失，抑制背景噪声同时保持场景整体一致性。

Result: 在 nuScenes 数据集上，ConsisDrive 在视频生成质量上达到或超过现有方法，并在下游自动驾驶任务（如感知或预测）上表现出显著提升。

Conclusion: ConsisDrive 提出了一种通过实例级时间一致性约束来减少世界模型中身份漂移的新方法，达到了更高的驾驶视频生成质量并在 nuScenes 数据集上的下游任务中取得显著提升。

Abstract: Autonomous driving relies on robust models trained on large-scale, high-quality multi-view driving videos. Although world models provide a cost-effective solution for generating realistic driving data, they often suffer from identity drift, where the same object changes its appearance or category across frames due to the absence of instance-level temporal constraints. We introduce ConsisDrive, an identity-preserving driving world model designed to enforce temporal consistency at the instance level. Our framework incorporates two key components: (1) Instance-Masked Attention, which applies instance identity masks and trajectory masks within attention blocks to ensure that visual tokens interact only with their corresponding instance features across spatial and temporal dimensions, thereby preserving object identity consistency; and (2) Instance-Masked Loss, which adaptively emphasizes foreground regions with probabilistic instance masking, reducing background noise while maintaining overall scene fidelity. By integrating these mechanisms, ConsisDrive achieves state-of-the-art driving video generation quality and demonstrates significant improvements in downstream autonomous driving tasks on the nuScenes dataset. Our project page is https://shanpoyang654.github.io/ConsisDrive/page.html.

</details>


### [46] [FARTrack: Fast Autoregressive Visual Tracking with High Performance](https://arxiv.org/abs/2602.03214)
*Guijie Wang,Tong Lin,Yifan Bai,Anjia Cao,Shiyi Liang,Wangbo Zhao,Xing Wei*

Main category: cs.CV

TL;DR: FARTrack: an autoregressive tracker using layer-wise task-specific self-distillation and inter-frame sparsification to compress and speed up models, reaching 70.6% AO on GOT-10k and up to 343 FPS (GPU) / 121 FPS (CPU).


<details>
  <summary>Details</summary>
Motivation: High-performance visual trackers are often too slow for resource-constrained devices; autoregression preserves temporal trajectory modeling while enabling efficient execution, motivating techniques to compress and sparsify without harming accuracy.

Method: Proposes an autoregressive tracking framework (FARTrack) that uses Task-Specific Self-Distillation to compress models by progressive, layer-wise distillation of task-specific tokens, and Inter-frame Autoregressive Sparsification to sequentially condense multiple templates for runtime-efficient temporal sparsification.

Result: FARTrack attains competitive accuracy (AO 70.6% on GOT-10k) in real-time and achieves very high speeds (up to 343 FPS on GPU and 121 FPS on CPU), demonstrating good trade-offs between speed and tracking performance.

Conclusion: FARTrack achieves a practical balance between high tracking accuracy and very fast inference by combining autoregressive modeling with two efficiency techniques: Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification.

Abstract: Inference speed and tracking performance are two critical evaluation metrics in the field of visual tracking. However, high-performance trackers often suffer from slow processing speeds, making them impractical for deployment on resource-constrained devices. To alleviate this issue, we propose FARTrack, a Fast Auto-Regressive Tracking framework. Since autoregression emphasizes the temporal nature of the trajectory sequence, it can maintain high performance while achieving efficient execution across various devices. FARTrack introduces Task-Specific Self-Distillation and Inter-frame Autoregressive Sparsification, designed from the perspectives of shallow-yet-accurate distillation and redundant-to-essential token optimization, respectively. Task-Specific Self-Distillation achieves model compression by distilling task-specific tokens layer by layer, enhancing the model's inference speed while avoiding suboptimal manual teacher-student layer pairs assignments. Meanwhile, Inter-frame Autoregressive Sparsification sequentially condenses multiple templates, avoiding additional runtime overhead while learning a temporally-global optimal sparsification strategy. FARTrack demonstrates outstanding speed and competitive performance. It delivers an AO of 70.6% on GOT-10k in real-time. Beyond, our fastest model achieves a speed of 343 FPS on the GPU and 121 FPS on the CPU.

</details>


### [47] [PokeFusion Attention: Enhancing Reference-Free Style-Conditioned Generation](https://arxiv.org/abs/2602.03220)
*Jingbang Tang*

Main category: cs.CV

TL;DR: 在扩散解码器中引入解耦的风格-文本融合交叉注意力（PokeFusion Attention），实现无参考、参数高效且可插拔的风格化角色生成，提升风格保真与形状一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖文本提示（易导致风格漂移和几何不一致）或在推理时依赖参考图像（增加复杂性和部署难度）；目标是在不使用参考图像的情况下实现高保真且几何一致的风格化角色生成，同时保持模型轻量与可移植。

Method: 在扩散模型解码器中引入轻量级的交叉注意力模块（PokeFusion Attention），将文本条件和风格嵌入在注意力级别解耦并融合；仅训练解码器的交叉注意力层和一个紧凑的风格投影模块，保持主干冻结，参数高效且可插拔。

Result: 在宝可梦风格化角色生成基准上，PokeFusion Attention较代表性基于适配器的方法，在风格保真度、语义对齐和角色形状一致性上均有稳定提升，同时额外参数和推理复杂度较低。

Conclusion: 提出的PokeFusion Attention在解码器层通过融合文本语义与学习到的风格嵌入，实现了无需参考图像的风格条件角色生成，且在保持预训练扩散主干冻结的同时，只训练少量参数，达到了风格一致性与结构稳定性的折中。

Abstract: This paper studies reference-free style-conditioned character generation in text-to-image diffusion models, where high-quality synthesis requires both stable character structure and consistent, fine-grained style expression across diverse prompts. Existing approaches primarily rely on text-only prompting, which is often under-specified for visual style and tends to produce noticeable style drift and geometric inconsistency, or introduce reference-based adapters that depend on external images at inference time, increasing architectural complexity and limiting deployment flexibility.We propose PokeFusion Attention, a lightweight decoder-level cross-attention mechanism that fuses textual semantics with learned style embeddings directly inside the diffusion decoder. By decoupling text and style conditioning at the attention level, our method enables effective reference-free stylized generation while keeping the pretrained diffusion backbone fully frozen.PokeFusion Attention trains only decoder cross-attention layers together with a compact style projection module, resulting in a parameter-efficient and plug-and-play control component that can be easily integrated into existing diffusion pipelines and transferred across different backbones.Experiments on a stylized character generation benchmark (Pokemon-style) demonstrate that our method consistently improves style fidelity, semantic alignment, and character shape consistency compared with representative adapter-based baselines, while maintaining low parameter overhead and inference-time simplicity.

</details>


### [48] [Spiral RoPE: Rotate Your Rotary Positional Embeddings in the 2D Plane](https://arxiv.org/abs/2602.03227)
*Haoyu Liu,Sucheng Ren,Tingyu Zhu,Peng Wang,Cihang Xie,Alan Yuille,Zeyu Zheng,Feng Wang*

Main category: cs.CV

TL;DR: 针对视觉Transformer中轴向2D RoPE只能沿水平/垂直编码的位置限制，提出Spiral RoPE：将通道分组并按多方向旋转位置编码，能捕捉斜向关系，带来多任务性能提升和更合理的注意力分布。


<details>
  <summary>Details</summary>
Motivation: 标准轴向2D RoPE仅将二维位置分解为水平和垂直分量，固有地限制了只能沿轴向编码位置，无法充分表示图像中存在的倾斜或多方向空间关系，导致建模能力受限。

Method: 将Embedding通道划分为多个组，每组对应一个均匀分布的方向；对每组根据patch位置在该方向上的投影进行旋转（Spiral RoPE），从而在超过水平和垂直轴的多个方向上编码位置关系。

Result: 在多个视觉任务（分类、分割、生成）上，Spiral RoPE 带来一致的性能提升；注意力可视化显示其注意力更集中于语义相关目标且更好遵循对象局部边界。

Conclusion: Spiral RoPE 解决了标准轴向2D RoPE 在视觉任务中对方向的限制，通过将通道分组并为每组分配均匀方向来实现多方向旋转编码，从而更好地捕捉倾斜/斜向空间关系。实验显示在分类、分割和生成任务上均有稳定提升，并在注意力图上表现出更集中和边界敏感的激活。

Abstract: Rotary Position Embedding (RoPE) is the de facto positional encoding in large language models due to its ability to encode relative positions and support length extrapolation. When adapted to vision transformers, the standard axial formulation decomposes two-dimensional spatial positions into horizontal and vertical components, implicitly restricting positional encoding to axis-aligned directions. We identify this directional constraint as a fundamental limitation of the standard axial 2D RoPE, which hinders the modeling of oblique spatial relationships that naturally exist in natural images. To overcome this limitation, we propose Spiral RoPE, a simple yet effective extension that enables multi-directional positional encoding by partitioning embedding channels into multiple groups associated with uniformly distributed directions. Each group is rotated according to the projection of the patch position onto its corresponding direction, allowing spatial relationships to be encoded beyond the horizontal and vertical axes. Across a wide range of vision tasks including classification, segmentation, and generation, Spiral RoPE consistently improves performance. Qualitative analysis of attention maps further show that Spiral RoPE exhibits more concentrated activations on semantically relevant objects and better respects local object boundaries, highlighting the importance of multi-directional positional encoding in vision transformers.

</details>


### [49] [EventFlash: Towards Efficient MLLMs for Event-Based Vision](https://arxiv.org/abs/2602.03230)
*Shaoyu Liu,Jianing Li,Guanghui Zhao,Yunjian Zhang,Wen Jiang,Ming Li,Xiangyang Ji*

Main category: cs.CV

TL;DR: EventFlash通过时空令牌稀疏化（自适应时间窗口聚合+稀疏密度引导注意力）和大规模事件指令集EventMind，实现了在长时序事件流上的高效推理，吞吐量显著提升且保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的MLLM多采用类图像密集处理，忽视事件流的时空稀疏性，计算开销大，难以处理长时序事件流。

Method: 构建大规模指令集数据集EventMind进行课程训练；设计自适应时间窗口聚合模块以压缩时间令牌；设计稀疏密度引导注意力模块以选择重要空间区域并抑制稀疏区域。

Result: 在与EventFlash-Zero的对比中，吞吐量提升12.4倍，同时性能保持可比；支持最多1000个时间bin的长序列处理，远超EventGPT的5个bin。

Conclusion: EventFlash提出了面向事件流的高效多模态大模型，通过时空稀疏化策略在保持性能的同时显著提升吞吐量，适合长时序事件流处理。

Abstract: Event-based multimodal large language models (MLLMs) enable robust perception in high-speed and low-light scenarios, addressing key limitations of frame-based MLLMs. However, current event-based MLLMs often rely on dense image-like processing paradigms, overlooking the spatiotemporal sparsity of event streams and resulting in high computational cost. In this paper, we propose EventFlash, a novel and efficient MLLM to explore spatiotemporal token sparsification for reducing data redundancy and accelerating inference. Technically, we build EventMind, a large-scale and scene-diverse dataset with over 500k instruction sets, providing both short and long event stream sequences to support our curriculum training strategy. We then present an adaptive temporal window aggregation module for efficient temporal sampling, which adaptively compresses temporal tokens while retaining key temporal cues. Finally, a sparse density-guided attention module is designed to improve spatial token efficiency by selecting informative regions and suppressing empty or sparse areas. Experimental results show that EventFlash achieves a $12.4\times$ throughput improvement over the baseline (EventFlash-Zero) while maintaining comparable performance. It supports long-range event stream processing with up to 1,000 bins, significantly outperforming the 5-bin limit of EventGPT. We believe EventFlash serves as an efficient foundation model for event-based vision.

</details>


### [50] [InstaDrive: Instance-Aware Driving World Models for Realistic and Consistent Video Generation](https://arxiv.org/abs/2602.03242)
*Zhuoran Yang,Xi Guo,Chenjing Ding,Chiyu Wang,Wei Wu,Yanyong Zhang*

Main category: cs.CV

TL;DR: InstaDrive通过实例流引导和空间几何对齐两项机制，解决了世界模型在驾驶视频生成中的实例一致性与几何精度问题，从而提升生成质量并改善自动驾驶任务与安全评估。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在生成驾驶视频时缺乏实例级时间一致性和空间几何准确性，限制了生成数据在自动驾驶训练与评估中的实用性；需要一种既经济又能保持实例细节的生成方法。

Method: 提出两大模块：1) Instance Flow Guider：提取并跨帧传播实例特征以保持实例身份一致性；2) Spatial Geometric Aligner：增强空间推理，精确定位实例并显式建模遮挡层级。并结合CARLA仿真生成少见但关键的安全场景用于评估。

Result: 在nuScenes数据集上，InstaDrive在视频生成质量上达到或优于现有方法，并提升了下游自动驾驶任务性能；同时通过CARLA生成的罕见场景增强了安全性评估能力。

Conclusion: InstaDrive通过引入实例感知的时间和空间对齐机制，有效提升了驾驶视频世界模型的实例级别时间一致性和空间几何保真性，进而改善生成视频质量并推动下游自动驾驶任务表现。

Abstract: Autonomous driving relies on robust models trained on high-quality, large-scale multi-view driving videos. While world models offer a cost-effective solution for generating realistic driving videos, they struggle to maintain instance-level temporal consistency and spatial geometric fidelity. To address these challenges, we propose InstaDrive, a novel framework that enhances driving video realism through two key advancements: (1) Instance Flow Guider, which extracts and propagates instance features across frames to enforce temporal consistency, preserving instance identity over time. (2) Spatial Geometric Aligner, which improves spatial reasoning, ensures precise instance positioning, and explicitly models occlusion hierarchies. By incorporating these instance-aware mechanisms, InstaDrive achieves state-of-the-art video generation quality and enhances downstream autonomous driving tasks on the nuScenes dataset. Additionally, we utilize CARLA's autopilot to procedurally and stochastically simulate rare but safety-critical driving scenarios across diverse maps and regions, enabling rigorous safety evaluation for autonomous systems. Our project page is https://shanpoyang654.github.io/InstaDrive/page.html.

</details>


### [51] [LaVPR: Benchmarking Language and Vision for Place Recognition](https://arxiv.org/abs/2602.03253)
*Ofer Idan,Dan Badur,Yosi Keller,Yoli Shavit*

Main category: cs.CV

TL;DR: LaVPR：含65万+自然语言描述的VPR基准。多模态融合和跨模态检索两条路线均证明语言可显著提升视觉定位，特别惠及小型模型；提出LoRA+Multi-Similarity跨模态基线并开源数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有VPR在极端环境变化和感知别名（perceptual aliasing）下表现差；且无法仅凭口头描述进行定位，限制了在救援等场景的应用。引入语言信息可以弥补视觉退化并支持从语言进行定位。

Method: 构建了包含65万+自然语言描述的大规模基准数据集LaVPR；研究两种范式：多模态融合（图像+文本输入）提升鲁棒性，以及跨模态检索（语言→视觉）实现盲定位。跨模态检索采用LoRA微调与Multi-Similarity损失，作为基线方法。

Result: 在视觉退化条件下，加入语言描述能稳定提升性能，尤其对小型backbone影响最大，使其能接近大型vision-only模型；跨模态基线（LoRA+Multi-Similarity）明显优于标准对比学习方法。LaVPR促成更鲁棒且适合资源受限部署的定位系统。

Conclusion: LaVPR通过将自然语言描述与视觉数据结合，显著提升在极端环境变化和感知混淆情况下的定位鲁棒性，并使小型模型在资源受限场景中达到接近大型视觉架构的性能。

Abstract: Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform "blind" localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.

</details>


### [52] [HypCBC: Domain-Invariant Hyperbolic Cross-Branch Consistency for Generalizable Medical Image Analysis](https://arxiv.org/abs/2602.03264)
*Francesco Di Salvo,Sebastian Doerrich,Jonas Alle,Christian Ledig*

Main category: cs.CV

TL;DR: 将特征嵌入双曲空间并加上跨分支一致性，可学习更具层次结构和域不变性的表示，在多模态医学影像域泛化任务上带来稳健且显著的AUC提升。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺且存在设备、成像协议和人群差异，欧氏流形难以刻画数据的复杂层次结构，需探索适合层次结构的流形进行表示学习以提升泛化。

Method: 提出将ViT模型的特征映射到双曲流形并引入无监督的跨分支一致性约束（hyperbolic cross-branch consistency），在训练中同时优化分类损失与一致性损失以学习域不变特征。

Result: 在11个同分布数据集和3个ViT模型上验证，且在Fitzpatrick17k、Camelyon17-WILDS和视网膜跨数据集三项域泛化基准上平均提高约+2.1% AUC，表明方法能显著提升跨域泛化。

Conclusion: 本文证明在医学影像分析中，超曲率（双曲）表示学习能提升模型的域泛化能力，优于欧氏方法。

Abstract: Robust generalization beyond training distributions remains a critical challenge for deep neural networks. This is especially pronounced in medical image analysis, where data is often scarce and covariate shifts arise from different hardware devices, imaging protocols, and heterogeneous patient populations. These factors collectively hinder reliable performance and slow down clinical adoption. Despite recent progress, existing learning paradigms primarily rely on the Euclidean manifold, whose flat geometry fails to capture the complex, hierarchical structures present in clinical data. In this work, we exploit the advantages of hyperbolic manifolds to model complex data characteristics. We present the first comprehensive validation of hyperbolic representation learning for medical image analysis and demonstrate statistically significant gains across eleven in-distribution datasets and three ViT models. We further propose an unsupervised, domain-invariant hyperbolic cross-branch consistency constraint. Extensive experiments confirm that our proposed method promotes domain-invariant features and outperforms state-of-the-art Euclidean methods by an average of $+2.1\%$ AUC on three domain generalization benchmarks: Fitzpatrick17k, Camelyon17-WILDS, and a cross-dataset setup for retinal imaging. These datasets span different imaging modalities, data sizes, and label granularities, confirming generalization capabilities across substantially different conditions. The code is available at https://github.com/francescodisalvo05/hyperbolic-cross-branch-consistency .

</details>


### [53] [Global Geometry Is Not Enough for Vision Representations](https://arxiv.org/abs/2602.03282)
*Jiwan Chung,Seon Joo Kim*

Main category: cs.CV

TL;DR: 全局几何不足以预测组合绑定，函数敏感性（雅可比）是关键互补指标。


<details>
  <summary>Details</summary>
Motivation: 目前代表学习偏重全局嵌入几何作为评估与训练目标，但组合结构（如何组成元素）可能未被捕捉，需检验几何度量能否预测组合绑定能力。

Method: 对21个视觉编码器进行实验比较，计算标准几何统计量与输入-输出雅可比，并测量对组合绑定能力的相关性；并提供解析性理论解释损失如何影响几何与函数映射。

Result: 几何统计量与组合绑定能力相关性接近零；而输入-输出雅可比能可靠追踪此能力；理论分析表明现有目标约束几何但不约束局部映射，导致差异。

Conclusion: 全球嵌入几何只能部分反映表征能力，需引入输入-输出雅可比描述函数敏感性。

Abstract: A common assumption in representation learning is that globally well-distributed embeddings support robust and generalizable representations. This focus has shaped both training objectives and evaluation protocols, implicitly treating global geometry as a proxy for representational competence. While global geometry effectively encodes which elements are present, it is often insensitive to how they are composed. We investigate this limitation by testing the ability of geometric metrics to predict compositional binding across 21 vision encoders. We find that standard geometry-based statistics exhibit near-zero correlation with compositional binding. In contrast, functional sensitivity, as measured by the input-output Jacobian, reliably tracks this capability. We further provide an analytic account showing that this disparity arises from objective design, as existing losses explicitly constrain embedding geometry but leave the local input-output mapping unconstrained. These results suggest that global embedding geometry captures only a partial view of representational competence and establish functional sensitivity as a critical complementary axis for modeling composite structure.

</details>


### [54] [A3-TTA: Adaptive Anchor Alignment Test-Time Adaptation for Image Segmentation](https://arxiv.org/abs/2602.03292)
*Jianghao Wu,Xiangde Luo,Yubo Zhou,Lianming Wu,Guotai Wang,Shaoting Zhang*

Main category: cs.CV

TL;DR: 提出A3-TTA：通过锚点样本指导伪标签并结合语义/边界正则与自适应EMA，显著提升分割任务的测试时自适应效果并抑制灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有伪标签型TTA依赖于无分布支撑的扰动集成启发式方法，导致训练信号不稳定，引发误差累积与灾难性遗忘，亟需构建更可靠的伪标签与稳定的更新机制。

Method: 基于类内紧凑密度度量选取‘锚点’样本（预测置信且分布接近源域），用这些锚点作为稳定参考生成伪标签；结合语义一致性、边界感知熵最小化等正则项；并引入自适应指数移动平均策略平滑标签噪声与模型更新。

Result: 在心脏与前列腺等多域医学图像及自然图像上，A3-TTA较源模型平均Dice提升10.40-17.68个百分点，优于多种SOTA TTA方法；在持续顺序域适应中表现出较强抗遗忘能力。

Conclusion: A3-TTA通过锚点引导伪标签生成与多重正则化策略，有效缓解伪标签噪声和灾难性遗忘，显著提升在多领域医学与自然图像分割的测试时自适应性能。

Abstract: Test-Time Adaptation (TTA) offers a practical solution for deploying image segmentation models under domain shift without accessing source data or retraining. Among existing TTA strategies, pseudo-label-based methods have shown promising performance. However, they often rely on perturbation-ensemble heuristics (e.g., dropout sampling, test-time augmentation, Gaussian noise), which lack distributional grounding and yield unstable training signals. This can trigger error accumulation and catastrophic forgetting during adaptation. To address this, we propose \textbf{A3-TTA}, a TTA framework that constructs reliable pseudo-labels through anchor-guided supervision. Specifically, we identify well-predicted target domain images using a class compact density metric, under the assumption that confident predictions imply distributional proximity to the source domain. These anchors serve as stable references to guide pseudo-label generation, which is further regularized via semantic consistency and boundary-aware entropy minimization. Additionally, we introduce a self-adaptive exponential moving average strategy to mitigate label noise and stabilize model update during adaptation. Evaluated on both multi-domain medical images (heart structure and prostate segmentation) and natural images, A3-TTA significantly improves average Dice scores by 10.40 to 17.68 percentage points compared to the source model, outperforming several state-of-the-art TTA methods under different segmentation model architectures. A3-TTA also excels in continual TTA, maintaining high performance across sequential target domains with strong anti-forgetting ability. The code will be made publicly available at https://github.com/HiLab-git/A3-TTA.

</details>


### [55] [LEVIO: Lightweight Embedded Visual Inertial Odometry for Resource-Constrained Devices](https://arxiv.org/abs/2602.03294)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: LEVIO是在超低功耗硬件上实现实时VIO的系统，使用ORB与BA并行化优化，在RISC-V SoC上20FPS、<100mW，并开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有VIO系统计算需求高，难以部署在微控制器和低功耗SoC上，迫切需要低功耗、低资源占用的六自由度跟踪方案。

Method: 在算法上采用ORB特征跟踪与光束法调整，进行并行化设计、内存优化与硬件-软件协同，针对RISC-V超低功耗SoC进行实现与优化。

Result: 在并行处理的超低功耗RISC-V SoC上实现20 FPS，功耗低于100 mW，并在公开数据集上进行了基准测试，展示了在效率与精度间的良好折衷。

Conclusion: 本文提出的LEVIO实现了在超低功耗平台上实时六自由度视觉惯性里程计，兼顾效率与精度，适合微型无人机和智能眼镜等资源受限设备。

Abstract: Accurate, infrastructure-less sensor systems for motion tracking are essential for mobile robotics and augmented reality (AR) applications. The most popular state-of-the-art visual-inertial odometry (VIO) systems, however, are too computationally demanding for resource-constrained hardware, such as micro-drones and smart glasses. This work presents LEVIO, a fully featured VIO pipeline optimized for ultra-low-power compute platforms, allowing six-degrees-of-freedom (DoF) real-time sensing. LEVIO incorporates established VIO components such as Oriented FAST and Rotated BRIEF (ORB) feature tracking and bundle adjustment, while emphasizing a computationally efficient architecture with parallelization and low memory usage to suit embedded microcontrollers and low-power systems-on-chip (SoCs). The paper proposes and details the algorithmic design choices and the hardware-software co-optimization approach, and presents real-time performance on resource-constrained hardware. LEVIO is validated on a parallel-processing ultra-low-power RISC-V SoC, achieving 20 FPS while consuming less than 100 mW, and benchmarked against public VIO datasets, offering a compelling balance between efficiency and accuracy. To facilitate reproducibility and adoption, the complete implementation is released as open-source.

</details>


### [56] [Full end-to-end diagnostic workflow automation of 3D OCT via foundation model-driven AI for retinal diseases](https://arxiv.org/abs/2602.03302)
*Jinze Zhang,Jian Zhong,Li Lin,Jiaxiong Li,Ke Ma,Naiyang Li,Meng Li,Yuan Pan,Zeyu Meng,Mengyun Zhou,Shang Huang,Shilong Yu,Zhengyu Duan,Sutong Li,Honghui Xia,Juping Liu,Dan Liang,Yantao Wei,Xiaoying Tang,Jin Yuan,Peng Xiao*

Main category: cs.CV

TL;DR: 提出基于视觉基础模型的FOCUS全流程OCT自动诊断系统，通过图像质量筛选、切片级检测与自适应聚合实现可靠的病人级诊断，在多中心数据上表现稳定并与专家可比，推动无人值守眼科筛查的可行性。


<details>
  <summary>Details</summary>
Motivation: 目前OCT在临床的自动化受限于多阶段流程与传统单切片单任务模型，难以实现端到端及病人级诊断。论文旨在构建一个可在真实世界中稳定运行的全流程自动化系统。

Method: 流水线式框架：1) 使用EfficientNetV2-S进行图像质量评估；2) 基于微调的视觉基础模型进行切片级异常检测与多病种分类；3) 采用统一自适应聚合方法将2D切片预测整合为3D病人级诊断。

Result: 在内部3300名病人（40,672切片）与外部1345名病人（18,498切片）验证中，质量评估F1=99.01%，异常检测F1=97.46%，病人级诊断F1=94.39%；跨中心稳定性F1范围90.22%-95.24%；与专家对比在异常检测与多病种诊断上表现相当或更优，同时效率更高。

Conclusion: FOCUS实现了从OCT图像到病人级诊断的端到端自动化，显著提升了诊断流程一致性与效率，可行性在多中心、多设备数据上得到验证。

Abstract: Optical coherence tomography (OCT) has revolutionized retinal disease diagnosis with its high-resolution and three-dimensional imaging nature, yet its full diagnostic automation in clinical practices remains constrained by multi-stage workflows and conventional single-slice single-task AI models. We present Full-process OCT-based Clinical Utility System (FOCUS), a foundation model-driven framework enabling end-to-end automation of 3D OCT retinal disease diagnosis. FOCUS sequentially performs image quality assessment with EfficientNetV2-S, followed by abnormality detection and multi-disease classification using a fine-tuned Vision Foundation Model. Crucially, FOCUS leverages a unified adaptive aggregation method to intelligently integrate 2D slices-level predictions into comprehensive 3D patient-level diagnosis. Trained and tested on 3,300 patients (40,672 slices), and externally validated on 1,345 patients (18,498 slices) across four different-tier centers and diverse OCT devices, FOCUS achieved high F1 scores for quality assessment (99.01%), abnormally detection (97.46%), and patient-level diagnosis (94.39%). Real-world validation across centers also showed stable performance (F1: 90.22%-95.24%). In human-machine comparisons, FOCUS matched expert performance in abnormality detection (F1: 95.47% vs 90.91%) and multi-disease diagnosis (F1: 93.49% vs 91.35%), while demonstrating better efficiency. FOCUS automates the image-to-diagnosis pipeline, representing a critical advance towards unmanned ophthalmology with a validated blueprint for autonomous screening to enhance population scale retinal care accessibility and efficiency.

</details>


### [57] [PQTNet: Pixel-wise Quantitative Thermography Neural Network for Estimating Defect Depth in Polylactic Acid Parts by Additive Manufacturing](https://arxiv.org/abs/2602.03314)
*Lei Deng,Wenhao Huang,Chao Yang,Haoyuan Zheng,Yinbin Tian,Yue Ma*

Main category: cs.CV

TL;DR: 提出一种将像素热演化重构为条带图像的增强策略，结合EfficientNetV2-S与残差回归头，PQT-Net实现了对PLA件像素级缺陷深度的高精度量化（MAE 0.0094 mm，R>99%）。


<details>
  <summary>Details</summary>
Motivation: 当前基于热成像的NDT方法难以准确量化增材制造件内部缺陷的深度，亟需一种能保留像素时间-温度信息并能进行精确回归的深度学习方法。

Method: 将热序列按像素时间演化重构为二维条带图像进行数据增强，使用预训练EfficientNetV2-S作为主干网络，配合自定义的残差回归头（RRH）进行像素级回归输出，训练以最小化像素深度误差。

Result: 与其他深度学习模型比较，PQT-Net达到了最低MAE 0.0094 mm，决定系数R超过99%，显示出极高的预测精度。

Conclusion: 本文提出的PQT-Net在PLA材料的增材制造件无损检测深度量化方面表现出很高的精度，表明其对像素级缺陷深度估计具有实用性与鲁棒性。

Abstract: Defect depth quantification in additively manufactured (AM) components remains a significant challenge for non-destructive testing (NDT). This study proposes a Pixel-wise Quantitative Thermography Neural Network (PQT-Net) to address this challenge for polylactic acid (PLA) parts. A key innovation is a novel data augmentation strategy that reconstructs thermal sequence data into two-dimensional stripe images, preserving the complete temporal evolution of heat diffusion for each pixel. The PQT-Net architecture incorporates a pre-trained EfficientNetV2-S backbone and a custom Residual Regression Head (RRH) with learnable parameters to refine outputs. Comparative experiments demonstrate the superiority of PQT-Net over other deep learning models, achieving a minimum Mean Absolute Error (MAE) of 0.0094 mm and a coefficient of determination (R) exceeding 99%. The high precision of PQT-Net underscores its potential for robust quantitative defect characterization in AM.

</details>


### [58] [Invisible Clean-Label Backdoor Attacks for Generative Data Augmentation](https://arxiv.org/abs/2602.03316)
*Ting Xiang,Jinhui Zhao,Changjian Chen,Zhuo Tang*

Main category: cs.CV

TL;DR: 提出了基于潜在扰动的不可见干净标签后门攻击InvLBA，在生成数据增强场景下显著提高攻击成功率并保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 像COMBAT等像素级干净标签后门方法直接作用于生成图像时，实验证明攻击成功率较低。为增强对生成式数据增强中后门攻击的效果，需要转向更能影响语义特征的潜在层。

Method: 作者提出在潜在特征层面进行扰动而非像素层触发器。具体做法是在生成模型的潜在编码上注入微小不可见扰动以植入后门，并提供理论保证以确保干净准确率和攻击成功率的泛化。

Result: 在多数据集实验中，InvLBA平均提升攻击成功率46.43%，几乎不降低干净精度，且对多种SOTA防御方法表现出较高的鲁棒性。

Conclusion: 本文提出的InvLBA通过在生成模型的潜在特征空间施加不可见扰动实现了对生成数据增强的干净标签后门攻击。实验与理论分析表明该方法在不降低干净精度的前提下显著提升了攻击成功率，并对现有防御方法具备较高鲁棒性。

Abstract: With the rapid advancement of image generative models, generative data augmentation has become an effective way to enrich training images, especially when only small-scale datasets are available. At the same time, in practical applications, generative data augmentation can be vulnerable to clean-label backdoor attacks, which aim to bypass human inspection. However, based on theoretical analysis and preliminary experiments, we observe that directly applying existing pixel-level clean-label backdoor attack methods (e.g., COMBAT) to generated images results in low attack success rates. This motivates us to move beyond pixel-level triggers and focus instead on the latent feature level. To this end, we propose InvLBA, an invisible clean-label backdoor attack method for generative data augmentation by latent perturbation. We theoretically prove that the generalization of the clean accuracy and attack success rates of InvLBA can be guaranteed. Experiments on multiple datasets show that our method improves the attack success rate by 46.43% on average, with almost no reduction in clean accuracy and high robustness against SOTA defense methods.

</details>


### [59] [MedSAM-Agent: Empowering Interactive Medical Image Segmentation with Multi-turn Agentic Reinforcement Learning](https://arxiv.org/abs/2602.03320)
*Shengyuan Liu,Liuxin Bao,Qi Yang,Wanting Geng,Boyun Zheng,Chenxin Li,Wenting Chen,Houwen Peng,Yixuan Yuan*

Main category: cs.CV

TL;DR: MedSAM-Agent通过专家轨迹提示和两阶段训练，把交互分割建成多步自治决策，提升了临床级分割性能与交互效率，在多模态数据集上达到了SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的自动分割方法多为单轮、刚性交互，缺乏过程级监督，导致无法充分利用交互工具的动态能力并产生冗余动作，需要将分割任务建模为多步自治决策并引入人类式策略与过程监督。

Method: 提出混合提示策略生成专家策划轨迹，并设计两阶段训练流程：多轮端到端结果验证结合临床保真度的过程级奖励，以优化交互次数和决策效率；在执行时将MLLM作为自治代理，调用SAM等工具进行多步迭代分割。

Result: 在6种医疗影像模态和21个数据集上的大量实验表明，MedSAM-Agent在性能上优于现有方法，具备更好的迭代优化能力和决策简洁性。

Conclusion: MedSAM-Agent有效将交互式分割重构为多步决策流程，通过专家轨迹提示和两阶段训练实现了更高的临床保真度和交互简洁性，达到了多模态、多数据集的最先进性能。

Abstract: Medical image segmentation is evolving from task-specific models toward generalizable frameworks. Recent research leverages Multi-modal Large Language Models (MLLMs) as autonomous agents, employing reinforcement learning with verifiable reward (RLVR) to orchestrate specialized tools like the Segment Anything Model (SAM). However, these approaches often rely on single-turn, rigid interaction strategies and lack process-level supervision during training, which hinders their ability to fully exploit the dynamic potential of interactive tools and leads to redundant actions. To bridge this gap, we propose MedSAM-Agent, a framework that reformulates interactive segmentation as a multi-step autonomous decision-making process. First, we introduce a hybrid prompting strategy for expert-curated trajectory generation, enabling the model to internalize human-like decision heuristics and adaptive refinement strategies. Furthermore, we develop a two-stage training pipeline that integrates multi-turn, end-to-end outcome verification with a clinical-fidelity process reward design to promote interaction parsimony and decision efficiency. Extensive experiments across 6 medical modalities and 21 datasets demonstrate that MedSAM-Agent achieves state-of-the-art performance, effectively unifying autonomous medical reasoning with robust, iterative optimization. Code is available \href{https://github.com/CUHK-AIM-Group/MedSAM-Agent}{here}.

</details>


### [60] [PWAVEP: Purifying Imperceptible Adversarial Perturbations in 3D Point Clouds via Spectral Graph Wavelets](https://arxiv.org/abs/2602.03333)
*Haoran Li,Renyang Liu,Hongjia Liu,Chen Wang,Long Yin,Jian Xu*

Main category: cs.CV

TL;DR: PWAVEP利用谱图小波显著性与局部稀疏性对点云进行分层净化：删去最显著的离群点并对其余点做图小波高频衰减，简单无侵入却能显著提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 近来对3D点云的对抗攻击在实现空间不可感知性和高攻击成功率方面进展迅速，现有防御方法要么需要修改模型、昂贵训练或额外数据，故提出非侵入、轻量的谱域防御。

Method: 方法包含计算谱图小波域显著性得分与局部稀疏性得分，对点按显著性分层处理：删除最高显著性点（难以恢复的对抗离群点），并对中等显著性点施加基于图小波变换的谱滤波以衰减高频系数，从而抑制对抗噪声。

Result: 大量评估表明PWAVEP在准确性和鲁棒性上优于现有净化方法，推进了3D点云净化的最先进水平。

Conclusion: 本文提出了PWAVEP，一种基于谱域的非侵入、即插即用的3D点云净化防御方法，理论与实验证明通过抑制高频谱分量和移除显著点可有效抵御空间不可感知的对抗扰动。

Abstract: Recent progress in adversarial attacks on 3D point clouds, particularly in achieving spatial imperceptibility and high attack performance, presents significant challenges for defenders. Current defensive approaches remain cumbersome, often requiring invasive model modifications, expensive training procedures or auxiliary data access. To address these threats, in this paper, we propose a plug-and-play and non-invasive defense mechanism in the spectral domain, grounded in a theoretical and empirical analysis of the relationship between imperceptible perturbations and high-frequency spectral components. Building upon these insights, we introduce a novel purification framework, termed PWAVEP, which begins by computing a spectral graph wavelet domain saliency score and local sparsity score for each point. Guided by these values, PWAVEP adopts a hierarchical strategy, it eliminates the most salient points, which are identified as hardly recoverable adversarial outliers. Simultaneously, it applies a spectral filtering process to a broader set of moderately salient points. This process leverages a graph wavelet transform to attenuate high-frequency coefficients associated with the targeted points, thereby effectively suppressing adversarial noise. Extensive evaluations demonstrate that the proposed PWAVEP achieves superior accuracy and robustness compared to existing approaches, advancing the state-of-the-art in 3D point cloud purification. Code and datasets are available at https://github.com/a772316182/pwavep

</details>


### [61] [Composable Visual Tokenizers with Generator-Free Diagnostics of Learnability](https://arxiv.org/abs/2602.03339)
*Bingchen Zhao,Qiushan Guo,Ye Wang,Yixuan Huang,Zhonghua Zhai,Yu Tian*

Main category: cs.CV

TL;DR: CompTok 用 token 条件扩散解码器、InfoGAN 风格识别与 token 交换训练并结合对抗流正则化，学得可组合性强的视觉 tokenizer，从而提升类别条件生成并支持语义级编辑，同时提出两项评估 token 空间的指标。


<details>
  <summary>Details</summary>
Motivation: 目标是学习可组合（compositional）的视觉 token，使得单个 token 能明确控制图像的局部或语义属性，从而支持通过 token 组合或交换进行可控图像生成与编辑，同时提高生成器在类别条件任务上的性能。

Method: 核心方法包括：1) 使用 token 条件的扩散解码器作为生成器；2) 引入 InfoGAN 式的识别器，迫使解码器不能忽略任一 token（互信息最大化）；3) 在训练中引入 token 子集交换以增强可组合性；4) 对无配对的交换生成采用对抗流正则化来约束生成落在自然图像流形上；5) 提出两种衡量 token 空间景观的新指标。

Result: 实验显示 CompTok 在图像类别条件生成上取得了 SOTA 性能；token 交换可实现高层语义编辑；在提出的两项 token 空间指标上也优于对比方法，表明其 token 空间更有利于生成器学习与组合。

Conclusion: CompTok 提出了一种基于 token 条件扩散解码器和 InfoGAN 风格目标的训练框架，旨在学习具有良好多组合性的视觉 tokenizer。通过识别模型重构 token 并进行 token 交换与流式对抗正则化，CompTok 在类别条件生成上达到了 SOTA 并支持语义级别的 token 交换编辑。

Abstract: We introduce CompTok, a training framework for learning visual tokenizers whose tokens are enhanced for compositionality. CompTok uses a token-conditioned diffusion decoder. By employing an InfoGAN-style objective, where we train a recognition model to predict the tokens used to condition the diffusion decoder using the decoded images, we enforce the decoder to not ignore any of the tokens. To promote compositional control, besides the original images, CompTok also trains on tokens formed by swapping token subsets between images, enabling more compositional control of the token over the decoder. As the swapped tokens between images do not have ground truth image targets, we apply a manifold constraint via an adversarial flow regularizer to keep unpaired swap generations on the natural-image distribution. The resulting tokenizer not only achieves state-of-the-art performance on image class-conditioned generation, but also demonstrates properties such as swapping tokens between images to achieve high level semantic editing of an image. Additionally, we propose two metrics that measures the landscape of the token space that can be useful to describe not only the compositionality of the tokens, but also how easy to learn the landscape is for a generator to be trained on this space. We show in experiments that CompTok can improve on both of the metrics as well as supporting state-of-the-art generators for class conditioned generation.

</details>


### [62] [Tiled Prompts: Overcoming Prompt Underspecification in Image and Video Super-Resolution](https://arxiv.org/abs/2602.03342)
*Bryan Sangwoo Kim,Jonghyun Park,Jong Chul Ye*

Main category: cs.CV

TL;DR: 为每个潜在切片生成局部提示并在局部文本条件下进行超分辨率，解决全局提示在latent tiling下的欠约束，提升质量并减少伪影与幻觉。


<details>
  <summary>Details</summary>
Motivation: 在高分辨率超分辨率流程中常用潜在拼接（latent tiling）以扩展尺度，但单一全局文本提示会导致提示欠约束：既会缺失局部细节（提示稀疏），又会产生与局部不相关的误导性指导（提示误导），尤其在使用高强度的分类器自由引导时被放大，因此需要局部化的提示机制。

Method: 对输入图像/视频的潜在表示进行切片，对每个切片生成对应的局部提示（tile-specific prompt），并在这些局部提示下对切片执行扩散模型的超分辨率重建，最终合成高分辨率输出。方法与分类器自由引导相结合，以避免全局提示带来的误导。

Result: 在高分辨率真实图像与视频上实验表明，Tiled Prompts在感知质量和文本对齐度上均有稳定提升，同时相较于全局提示基线减少了幻觉和切片级伪影。

Conclusion: 本文提出了Tiled Prompts方法，通过为每个潜在切片生成局部提示并在局部文本条件后验下进行超分辨率重建，从而解决全局提示在高分辨率潜在拼接中的欠约束问题。该方法能提高视觉质量和文本对齐，减少幻觉与拼接伪影。

Abstract: Text-conditioned diffusion models have advanced image and video super-resolution by using prompts as semantic priors, but modern super-resolution pipelines typically rely on latent tiling to scale to high resolutions, where a single global caption causes prompt underspecification. A coarse global prompt often misses localized details (prompt sparsity) and provides locally irrelevant guidance (prompt misguidance) that can be amplified by classifier-free guidance. We propose Tiled Prompts, a unified framework for image and video super-resolution that generates a tile-specific prompt for each latent tile and performs super-resolution under locally text-conditioned posteriors, providing high-information guidance that resolves prompt underspecification with minimal overhead. Experiments on high resolution real-world images and videos show consistent gains in perceptual quality and text alignment, while reducing hallucinations and tile-level artifacts relative to global-prompt baselines.

</details>


### [63] [Z3D: Zero-Shot 3D Visual Grounding from Images](https://arxiv.org/abs/2602.03361)
*Nikita Drozdov,Andrey Lemeshko,Nikita Gavrilov,Anton Konushin,Danila Rukhovich,Maksim Kolodiazhnyi*

Main category: cs.CV

TL;DR: Z3D：一个基于多视角图像的无几何监督零样本3D视觉定位方法，通过高质量3D候选框生成与基于提示的VLM推理，显著提升零样本定位性能，在ScanRefer和Nr3D上领先。


<details>
  <summary>Details</summary>
Motivation: 研究者希望在没有几何标注、没有预定义对象类别或其他3D先验的情况下，实现从多视角图像直接进行3D视觉定位，推动零样本3DVG的实用性与普适性。

Method: 提出了一个通用的多视角图像到3D定位的流水线Z3D，核心包括：1) 使用最先进的零样本3D实例分割技术从多视角图像中生成高质量的3D边界框候选；2) 采用基于提示的分割（prompt-based segmentation）利用现代视觉语言模型（VLM）的能力进行高级语义推理；3) 管线可选地使用相机位姿与深度图以提升性能。

Result: 在ScanRefer和Nr3D两个基准测试上进行大量实验，结果显示Z3D在零样本设置下优于现有方法，达到了新的最优表现。作者同时开源了代码。

Conclusion: 该论文提出了Z3D，一种基于多视角图像的零样本3D视觉定位方法，能够在无几何监督和无对象先验下对3D场景中的对象进行定位。通过改进实例分割生成高质量3D候选框及基于提示的分割推理，显著提升了零样本性能，在ScanRefer和Nr3D数据集上取得了当前零样本方法的最好结果。

Abstract: 3D visual grounding (3DVG) aims to localize objects in a 3D scene based on natural language queries. In this work, we explore zero-shot 3DVG from multi-view images alone, without requiring any geometric supervision or object priors. We introduce Z3D, a universal grounding pipeline that flexibly operates on multi-view images while optionally incorporating camera poses and depth maps. We identify key bottlenecks in prior zero-shot methods causing significant performance degradation and address them with (i) a state-of-the-art zero-shot 3D instance segmentation method to generate high-quality 3D bounding box proposals and (ii) advanced reasoning via prompt-based segmentation, which utilizes full capabilities of modern VLMs. Extensive experiments on the ScanRefer and Nr3D benchmarks demonstrate that our approach achieves state-of-the-art performance among zero-shot methods. Code is available at https://github.com/col14m/z3d .

</details>


### [64] [Symbol-Aware Reasoning with Masked Discrete Diffusion for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2602.03370)
*Takaya Kawakatsu,Ryo Ishiyama*

Main category: cs.CV

TL;DR: 将HMER重塑为离散扩散的迭代符号修正，结合符号感知分词与随机掩码互学习，显著提高结构一致性与识别性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在处理多符号和二维结构布局时存在因顺序生成导致的因果依赖、曝光偏差和结构不一致问题，需寻找更结构感知的识别范式。

Method: 设计了一个基于离散扩散的多步重掩码（remasking）框架，对符号和结构关系进行逐步修正；引入符号感知分词（tokenization）和随机掩码互学习（Random-Masking Mutual Learning）以提升语法对齐与对手写多样性的鲁棒性。

Result: 在MathWriting基准上取得CER 5.56%和EM 60.42%，优于强Transformer和商业基线；在CROHME 2014–2023上也有持续提升，表明离散扩散为结构感知视觉识别提供了新范式。

Conclusion: 该论文提出将手写数学表达式识别（HMER）问题从自回归生成重构为离散扩散的迭代符号修正，从而缓解曝光偏差和语法不一致问题。

Abstract: Handwritten Mathematical Expression Recognition (HMER) requires reasoning over diverse symbols and 2D structural layouts, yet autoregressive models struggle with exposure bias and syntactic inconsistency. We present a discrete diffusion framework that reformulates HMER as iterative symbolic refinement instead of sequential generation. Through multi-step remasking, the proposal progressively refines both symbols and structural relations, removing causal dependencies and improving structural consistency. A symbol-aware tokenization and Random-Masking Mutual Learning further enhance syntactic alignment and robustness to handwriting diversity. On the MathWriting benchmark, the proposal achieves 5.56\% CER and 60.42\% EM, outperforming strong Transformer and commercial baselines. Consistent gains on CROHME 2014--2023 demonstrate that discrete diffusion provides a new paradigm for structure-aware visual recognition beyond generative modeling.

</details>


### [65] [Multi-Resolution Alignment for Voxel Sparsity in Camera-Based 3D Semantic Scene Completion](https://arxiv.org/abs/2602.03371)
*Zhiwen Yang,Yuxin Peng*

Main category: cs.CV

TL;DR: 提出MRA方法：在多分辨率3D特征上做场景级的种子融合对齐与实例级的立方语义显著性引导下的关键体素分布一致性约束，以缓解体素稀疏并提升相机驱动SSC性能。


<details>
  <summary>Details</summary>
Motivation: 现有相机驱动的SSC方法仅依赖体素标签监督，而自动驾驶场景中大量体素为空，造成监督信号稀疏，影响优化效率与最终性能；因此需引入额外的、多尺度的对齐约束作为辅助监督以缓解体素稀疏问题。

Method: 方法包含三大模块：1）Multi-resolution View Transformer（多分辨率视图变换器）：将2D图像特征投影为多分辨率3D特征，并通过融合判别性种子特征实现场景级对齐；2）Cubic Semantic Anisotropy（立方语义各向异性）：在立方邻域内评估每个体素相对于邻居的语义重要性，用于识别实例级语义显著性；3）Critical Distribution Alignment（关键分布对齐）：借助语义各向异性选择关键体素作为实例级锚点，施加循环分布一致性损失以在不同分辨率间实现关键特征分布的一致性。

Result: 论文通过在多分辨率特征空间上进行场景与实例级对齐，并在关键体素上施加循环一致性损失，实现了比仅依赖体素标签的基线更好的优化与准确率（论文提供代码与实验以验证改进）。

Conclusion: 本文提出的多分辨率对齐（MRA）方法通过在场景级与实例级引入辅助监督，缓解了相机驱动的3D语义场景完成中体素稀疏导致的优化低效问题，从而提升了模型性能。

Abstract: Camera-based 3D semantic scene completion (SSC) offers a cost-effective solution for assessing the geometric occupancy and semantic labels of each voxel in the surrounding 3D scene with image inputs, providing a voxel-level scene perception foundation for the perception-prediction-planning autonomous driving systems. Although significant progress has been made in existing methods, their optimization rely solely on the supervision from voxel labels and face the challenge of voxel sparsity as a large portion of voxels in autonomous driving scenarios are empty, which limits both optimization efficiency and model performance. To address this issue, we propose a \textit{Multi-Resolution Alignment (MRA)} approach to mitigate voxel sparsity in camera-based 3D semantic scene completion, which exploits the scene and instance level alignment across multi-resolution 3D features as auxiliary supervision. Specifically, we first propose the Multi-resolution View Transformer module, which projects 2D image features into multi-resolution 3D features and aligns them at the scene level through fusing discriminative seed features. Furthermore, we design the Cubic Semantic Anisotropy module to identify the instance-level semantic significance of each voxel, accounting for the semantic differences of a specific voxel against its neighboring voxels within a cubic area. Finally, we devise a Critical Distribution Alignment module, which selects critical voxels as instance-level anchors with the guidance of cubic semantic anisotropy, and applies a circulated loss for auxiliary supervision on the critical feature distribution consistency across different resolutions. The code is available at https://github.com/PKU-ICST-MIPL/MRA_TIP.

</details>


### [66] [SLIM-Diff: Shared Latent Image-Mask Diffusion with Lp loss for Data-Scarce Epilepsy FLAIR MRI](https://arxiv.org/abs/2602.03372)
*Mario Pascual-González,Ariadna Jiménez-Partinen,R. M. Luque-Baena,Fátima Nagib-Raya,Ezequiel López-Rubio*

Main category: cs.CV

TL;DR: 为稀少且微小的FCD病灶提出紧凑的联合扩散生成器（共享瓶颈U-Net + 可调Lp损失），发现x0预测最优，L1.5提升图像质量，L2更保掩码形状。


<details>
  <summary>Details</summary>
Motivation: FCD病灶在FLAIR MRI上很微小且稀少，导致联合图像—掩码生成容易不稳定并出现记忆化，需要紧耦合的生成架构与损失设计来提高合成质量和多样性。

Method: 使用单个共享瓶颈的U-Net对图像与掩码进行联合建模（2通道输入），并在训练中比较不同的扩散目标参数化（x0预测 vs ε预测）以及不同Lp损失（包括L1.5与L2），在匹配设置下分析其对图像真实感与掩码形态保持的影响。

Result: 实验表明x0预测在联合合成中表现最好；分数次次二惩罚（L1.5）提高图像保真度，而L2更有利于保持病灶掩码的形态。代码与模型权重公开。

Conclusion: 提出了SLIM-Diff，一种紧凑的联合扩散生成模型，能够在2通道图像+掩码表示上通过共享瓶颈的U-Net强耦合解剖与病灶几何，并通过可调Lp损失优化几何特性。

Abstract: Focal cortical dysplasia (FCD) lesions in epilepsy FLAIR MRI are subtle and scarce, making joint image--mask generative modeling prone to instability and memorization. We propose SLIM-Diff, a compact joint diffusion model whose main contributions are (i) a single shared-bottleneck U-Net that enforces tight coupling between anatomy and lesion geometry from a 2-channel image+mask representation, and (ii) loss-geometry tuning via a tunable $L_p$ objective. As an internal baseline, we include the canonical DDPM-style objective ($ε$-prediction with $L_2$ loss) and isolate the effect of prediction parameterization and $L_p$ geometry under a matched setup. Experiments show that $x_0$-prediction is consistently the strongest choice for joint synthesis, and that fractional sub-quadratic penalties ($L_{1.5}$) improve image fidelity while $L_2$ better preserves lesion mask morphology. Our code and model weights are available in https://github.com/MarioPasc/slim-diff

</details>


### [67] [Unifying Watermarking via Dimension-Aware Mapping](https://arxiv.org/abs/2602.03373)
*Jiale Meng,Runyi Hu,Jie Zhang,Zheming Lu,Ivor Tsang,Tianwei Zhang*

Main category: cs.CV

TL;DR: DiM通过把水印看作不同维度payload并调整嵌入/提取维度，功能上统一并扩展水印能力，视频域实验验证。


<details>
  <summary>Details</summary>
Motivation: 现有深度水印方法架构相似但行为差异大，需从功能层面统一解释并利用维度属性扩展能力。

Method: 提出DiM框架，定义1D/2D/3D payloads，设计在视频上不同嵌入和提取维度的映射实现，保持相同架构仅改维度配置以生成不同功能。

Result: 实验表明仅改变嵌入/提取的维度配置可实现时空篡改定位、本地嵌入控制和在帧扰动下的时序恢复等能力，无需改网络结构。

Conclusion: DiM将水印视为多维payload，通过维度感知映射统一解释不同方法，结论是映射维度决定行为：同维映射保留结构，跨维映射实现定位；在视频域展示多种能力。

Abstract: Deep watermarking methods often share similar encoder-decoder architectures, yet differ substantially in their functional behaviors. We propose DiM, a new multi-dimensional watermarking framework that formulates watermarking as a dimension-aware mapping problem, thereby unifying existing watermarking methods at the functional level. Under DiM, watermark information is modeled as payloads of different dimensionalities, including one-dimensional binary messages, two-dimensional spatial masks, and three-dimensional spatiotemporal structures. We find that the dimensional configuration of embedding and extraction largely determines the resulting watermarking behavior. Same-dimensional mappings preserve payload structure and support fine-grained control, while cross-dimensional mappings enable spatial or spatiotemporal localization. We instantiate DiM in the video domain, where spatiotemporal representations enable a broader set of dimension mappings. Experiments demonstrate that varying only the embedding and extraction dimensions, without architectural changes, leads to different watermarking capabilities, including spatiotemporal tamper localization, local embedding control, and recovery of temporal order under frame disruptions.

</details>


### [68] [Seeing Through the Chain: Mitigate Hallucination in Multimodal Reasoning Models via CoT Compression and Contrastive Preference Optimization](https://arxiv.org/abs/2602.03380)
*Hao Fang,Jinyu Li,Jiawei Kong,Tianqu Zhuang,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang*

Main category: cs.CV

TL;DR: 提出C3PO：通过压缩链式思维和基于高质量AI反馈的对比偏好训练，抑制多模态推理中的幻觉，理论与实验证明其在多模型多基准上有效。


<details>
  <summary>Details</summary>
Motivation: 动机是解决多模态推理模型在引入推理机制后更易出现幻觉的问题，观察到CoT中冗余文本和弱视觉线索是主要原因，且低质量的推理痕迹会导致后续回复产生幻觉。

Method: 方法包括两部分：1) CoT压缩：选择性过滤冗余的思维标记，生成紧凑且保留任务相关视觉信息的CoT表示；2) 对比偏好优化：用高质量AI反馈构建训练对，设计诱发幻觉的多模态刺激器生成负样本，进行对比学习以纠正模型偏差。

Result: 结果显示C3PO在多个多模态模型和基准上稳定减少幻觉，理论上证明了方法有效性，并通过实验验证压缩CoT与对比优化带来的改进。

Conclusion: 论文结论：引入链式思维（CoT）虽增强推理能力，但会加重模型对语言先验的依赖、忽视视觉输入，导致幻觉。通过链式思维压缩和对比偏好优化（C3PO）可有效减少幻觉，在多种多模态模型和基准上表现一致提升。

Abstract: While multimodal reasoning models (MLRMs) have exhibited impressive capabilities, they remain prone to hallucinations, and effective solutions are still underexplored. In this paper, we experimentally analyze the hallucination cause and propose C3PO, a training-based mitigation framework comprising \textbf{C}hain-of-Thought \textbf{C}ompression and \textbf{C}ontrastive \textbf{P}reference \textbf{O}ptimization. Firstly, we identify that introducing reasoning mechanisms exacerbates models' reliance on language priors while overlooking visual inputs, which can produce CoTs with reduced visual cues but redundant text tokens. To this end, we propose to selectively filter redundant thinking tokens for a more compact and signal-efficient CoT representation that preserves task-relevant information while suppressing noise. In addition, we observe that the quality of the reasoning trace largely determines whether hallucination emerges in subsequent responses. To leverage this insight, we introduce a reasoning-enhanced preference tuning scheme that constructs training pairs using high-quality AI feedback. We further design a multimodal hallucination-inducing mechanism that elicits models' inherent hallucination patterns via carefully crafted inducers, yielding informative negative signals for contrastive correction. We provide theoretical justification for the effectiveness and demonstrate consistent hallucination reduction across diverse MLRMs and benchmarks.

</details>


### [69] [From Vicious to Virtuous Cycles: Synergistic Representation Learning for Unsupervised Video Object-Centric Learning](https://arxiv.org/abs/2602.03390)
*Hyun Seok Seong,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: SRL通过让encoder的锐利边界和decoder的空间一致性互相矫正，并用slot正则的warm-up稳定训练，解决了重建训练中encoder- decoder表征不匹配的问题，从而在视频对象中心任务上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 重建损失导致encoder产生锐利、高频但噪声的注意图，而decoder为最小化重建误差产生空间上连贯但模糊的输出，二者之间的不匹配形成恶性循环，阻碍模型学习高质量对象表示。SRL旨在弥合encoder与decoder的表征差异，建立互利循环。

Method: 提出Synergistic Representation Learning (SRL)，包含互相去模糊与去噪的模块：利用encoder的高频边界信息去锐化decoder输出的语义边界，利用decoder的空间一致性去去噪encoder特征；并以一个带slot正则的warm-up阶段稳定互相精炼过程，初期分配明确的实体到slot。

Result: 在视频对象中心学习基准上实现了最先进的结果（state-of-the-art），并公开了代码。

Conclusion: SRL通过让encoder与decoder互相精炼表征，打破了重建训练中导致的“encoder锐利/decoder模糊”恶性循环，从而改善了slot-based无监督对象中心学习的性能。

Abstract: Unsupervised object-centric learning models, particularly slot-based architectures, have shown great promise in decomposing complex scenes. However, their reliance on reconstruction-based training creates a fundamental conflict between the sharp, high-frequency attention maps of the encoder and the spatially consistent but blurry reconstruction maps of the decoder. We identify that this discrepancy gives rise to a vicious cycle: the noisy feature map from the encoder forces the decoder to average over possibilities and produce even blurrier outputs, while the gradient computed from blurry reconstruction maps lacks high-frequency details necessary to supervise encoder features. To break this cycle, we introduce Synergistic Representation Learning (SRL) that establishes a virtuous cycle where the encoder and decoder mutually refine one another. SRL leverages the encoder's sharpness to deblur the semantic boundary within the decoder output, while exploiting the decoder's spatial consistency to denoise the encoder's features. This mutual refinement process is stabilized by a warm-up phase with a slot regularization objective that initially allocates distinct entities per slot. By bridging the representational gap between the encoder and decoder, SRL achieves state-of-the-art results on video object-centric learning benchmarks. Codes are available at https://github.com/hynnsk/SRL.

</details>


### [70] [UnHype: CLIP-Guided Hypernetworks for Dynamic LoRA Unlearning](https://arxiv.org/abs/2602.03410)
*Piotr Wójcik,Maksym Petrenko,Wojciech Gromski,Przemysław Spurek,Maciej Zieba*

Main category: cs.CV

TL;DR: UnHype把超网络用于生成上下文敏感的LoRA权重，从而实现更可扩展且语义适应性更强的概念去学习，适用于单/多概念擦除并在多种任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 大型扩散模型易被滥用用于生成有害或敏感内容，现有LoRA去学习方法在语义适应性、多概念同时擦除与泛化保持之间存在权衡和扩展性问题，需提出能按语境动态调整且可扩展的去学习方案。

Method: 在Stable Diffusion及流式文本到图像模型上，UnHype用超网络根据CLIP嵌入动态生成LoRA权重；训练阶段将超网络与LoRA联合优化以获得上下文感知的低秩适配；支持单概念与多概念同时训练，提升标度性与训练稳定性。

Result: 在对象擦除、人名/名人擦除与露骨内容移除等任务上，UnHype展示了更稳定的训练、更好的概念控制能力与可扩展性；在不同模型架构上均能插拔使用，且推理时根据CLIP embedding动态生成LoRA权重实现上下文感知的效果。

Conclusion: UnHype通过将超网络(hypernetwork)融入LoRA训练，实现了更具语义适应性的单/多概念“机器遗忘”，在保留生成能力的同时更有效地消除特定概念。

Abstract: Recent advances in large-scale diffusion models have intensified concerns about their potential misuse, particularly in generating realistic yet harmful or socially disruptive content. This challenge has spurred growing interest in effective machine unlearning, the process of selectively removing specific knowledge or concepts from a model without compromising its overall generative capabilities. Among various approaches, Low-Rank Adaptation (LoRA) has emerged as an effective and efficient method for fine-tuning models toward targeted unlearning. However, LoRA-based methods often exhibit limited adaptability to concept semantics and struggle to balance removing closely related concepts with maintaining generalization across broader meanings. Moreover, these methods face scalability challenges when multiple concepts must be erased simultaneously. To address these limitations, we introduce UnHype, a framework that incorporates hypernetworks into single- and multi-concept LoRA training. The proposed architecture can be directly plugged into Stable Diffusion as well as modern flow-based text-to-image models, where it demonstrates stable training behavior and effective concept control. During inference, the hypernetwork dynamically generates adaptive LoRA weights based on the CLIP embedding, enabling more context-aware, scalable unlearning. We evaluate UnHype across several challenging tasks, including object erasure, celebrity erasure, and explicit content removal, demonstrating its effectiveness and versatility. Repository: https://github.com/gmum/UnHype.

</details>


### [71] [Socratic-Geo: Synthetic Data Generation and Geometric Reasoning via Multi-Agent Interaction](https://arxiv.org/abs/2602.03414)
*Zhengbo Jiao,Shaobo Wang,Zifan Zhang,Wei Wang,Bing Zhao,Hu Wei,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出一个教师-解算器-生成器三智能体闭环框架，通过反思与偏好学习实现高纯度图文对自动合成与生成，显著提升几何推理与图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM在几何推理上因高质量图文对极度稀缺而表现不足，人工标注成本高昂，自动化方法难以保证数据纯度与训练效率，且现有方法未将数据生成与学习目标紧密耦合。

Method: 提出Teacher、Solver、Generator三智能体：Teacher基于参数化Python脚本生成图文对并通过Reflect与RePI反馈保证可解性与视觉有效性；Solver通过偏好学习优化推理并利用失败路径驱动定向增强；Generator在积累的“图像-代码-指令”三元组上学习图像生成，将程序化绘图能力蒸馏为视觉生成能力。

Result: 在仅108个种子问题的情况下，Socratic-Solver在六项基准上取得49.11分（使用基线数据的1/4），优于强基线2.43分；Socratic-Generator在GenExam上达42.4%，优于Seedream-4.0（39.8%）并接近Gemini-2.5-Flash-Image（43.1%）。

Conclusion: Socratic-Geo通过多智能体交互将数据合成与模型学习耦合，显著缓解了几何推理数据匮乏问题，并在多项基准上超过强基线。

Abstract: Multimodal Large Language Models (MLLMs) have significantly advanced vision-language understanding. However, even state-of-the-art models struggle with geometric reasoning, revealing a critical bottleneck: the extreme scarcity of high-quality image-text pairs. Human annotation is prohibitively expensive, while automated methods fail to ensure fidelity and training effectiveness. Existing approaches either passively adapt to available images or employ inefficient random exploration with filtering, decoupling generation from learning needs. We propose Socratic-Geo, a fully autonomous framework that dynamically couples data synthesis with model learning through multi-agent interaction. The Teacher agent generates parameterized Python scripts with reflective feedback (Reflect for solvability, RePI for visual validity), ensuring image-text pair purity. The Solver agent optimizes reasoning through preference learning, with failure paths guiding Teacher's targeted augmentation. Independently, the Generator learns image generation capabilities on accumulated "image-code-instruction" triplets, distilling programmatic drawing intelligence into visual generation. Starting from only 108 seed problems, Socratic-Solver achieves 49.11 on six benchmarks using one-quarter of baseline data, surpassing strong baselines by 2.43 points. Socratic-Generator achieves 42.4% on GenExam, establishing new state-of-the-art for open-source models, surpassing Seedream-4.0 (39.8%) and approaching Gemini-2.5-Flash-Image (43.1%).

</details>


### [72] [ConsistentRFT: Reducing Visual Hallucinations in Flow-based Reinforcement Fine-Tuning](https://arxiv.org/abs/2602.03425)
*Xiaofeng Tan,Jun Liu,Yuanting Fan,Bin-Bin Gao,Xi Jiang,Xiaochen Chen,Jinlong Peng,Chengjie Wang,Hongsong Wang,Feng Zheng*

Main category: cs.CV

TL;DR: 提出 ConsistentRFT：通过动态噪声调度（DGR）和一致性保持的策略梯度（CPGO）来解决流模型强化微调导致的视觉幻觉，显著降低幻觉并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 解决流式生成模型在强化微调中出现的视觉幻觉（过度优化细节与语义错位），提高生成质量与对偏外域数据的鲁棒性。

Method: 从统一视角分析现有 RFT，识别问题为 SDE rollout 探索受限与策略梯度破坏模型向量场一致性；设计 Dynamic Granularity Rollout 动态调度噪声源以平衡全局/局部探索，和 Consistent Policy Gradient Optimization 将当前策略与更稳定先验对齐以保持一致性。

Result: 实验证明 ConsistentRFT 在低层次感知幻觉上平均减少49%，高层次减少38%；并在外域指标上优于其他 RFT 方法，在 FLUX1.dev 上比基线改善5.1%（基线为-0.4%）。

Conclusion: 本文归纳：RFT 在流模型上常导致视觉幻觉，源于探索不足与策略梯度引起的轨迹模仿导致的一致性破坏；提出 ConsistentRFT（含 DGR 与 CPGO）可减轻幻觉并提升泛化表现。

Abstract: Reinforcement Fine-Tuning (RFT) on flow-based models is crucial for preference alignment. However, they often introduce visual hallucinations like over-optimized details and semantic misalignment. This work preliminarily explores why visual hallucinations arise and how to reduce them. We first investigate RFT methods from a unified perspective, and reveal the core problems stemming from two aspects, exploration and exploitation: (1) limited exploration during stochastic differential equation (SDE) rollouts, leading to an over-emphasis on local details at the expense of global semantics, and (2) trajectory imitation process inherent in policy gradient methods, distorting the model's foundational vector field and its cross-step consistency. Building on this, we propose ConsistentRFT, a general framework to mitigate these hallucinations. Specifically, we design a Dynamic Granularity Rollout (DGR) mechanism to balance exploration between global semantics and local details by dynamically scheduling different noise sources. We then introduce a Consistent Policy Gradient Optimization (CPGO) that preserves the model's consistency by aligning the current policy with a more stable prior. Extensive experiments demonstrate that ConsistentRFT significantly mitigates visual hallucinations, achieving average reductions of 49\% for low-level and 38\% for high-level perceptual hallucinations. Furthermore, ConsistentRFT outperforms other RFT methods on out-of-domain metrics, showing an improvement of 5.1\% (v.s. the baseline's decrease of -0.4\%) over FLUX1.dev. This is \href{https://xiaofeng-tan.github.io/projects/ConsistentRFT}{Project Page}.

</details>


### [73] [Hierarchical Concept-to-Appearance Guidance for Multi-Subject Image Generation](https://arxiv.org/abs/2602.03448)
*Yijia Xu,Zihao Wang,Jinshi Cui*

Main category: cs.CV

TL;DR: 提出CAG：概念层用VAE dropout强化VLM语义，外观层用对应感知掩码注意力绑定文本与参考区域，从而提升多主体生成的一致性与提示遵从性。


<details>
  <summary>Details</summary>
Motivation: 现有多主体生成方法依赖扩散模型隐式地将文本与参考图像关联，导致身份不一致和组合控制能力差。需要显式且分层的监督来分别保证概念级一致性和外观级属性精确绑定。

Method: 在概念层，使用VAE dropout随机丢弃参考VAE特征，迫使模型依赖来自视觉语言模型（VLM）的语义信号；在外观层，将VLM得到的文本-图像对应关系嵌入到Diffusion Transformer的对应感知掩码注意力模块中，使每个文本token仅能与匹配的参考区域交互。两者结合提供从高层语义到细粒度外观的结构化监督。

Result: 大量实验显示，CAG在多主体图像生成任务上达到最先进性能，显著提升了提示遵从率和主体一致性，且在多主体组合控制方面更可靠。

Conclusion: 该文提出的分层概念—外观引导（CAG）方法，通过在概念层引入VAE dropout训练并在外观层加入对应感知掩码注意力，实现了多主体图像生成中更稳定的概念一致性和精确的属性绑定。实验表明在提示遵从性和主体一致性上均优于现有方法。

Abstract: Multi-subject image generation aims to synthesize images that faithfully preserve the identities of multiple reference subjects while following textual instructions. However, existing methods often suffer from identity inconsistency and limited compositional control, as they rely on diffusion models to implicitly associate text prompts with reference images. In this work, we propose Hierarchical Concept-to-Appearance Guidance (CAG), a framework that provides explicit, structured supervision from high-level concepts to fine-grained appearances. At the conceptual level, we introduce a VAE dropout training strategy that randomly omits reference VAE features, encouraging the model to rely more on robust semantic signals from a Visual Language Model (VLM) and thereby promoting consistent concept-level generation in the absence of complete appearance cues. At the appearance level, we integrate the VLM-derived correspondences into a correspondence-aware masked attention module within the Diffusion Transformer (DiT). This module restricts each text token to attend only to its matched reference regions, ensuring precise attribute binding and reliable multi-subject composition. Extensive experiments demonstrate that our method achieves state-of-the-art performance on the multi-subject image generation, substantially improving prompt following and subject consistency.

</details>


### [74] [Contextualized Visual Personalization in Vision-Language Models](https://arxiv.org/abs/2602.03454)
*Yeongtak Oh,Sangwon Yu,Junsung Park,Han Cheol Moon,Jisoo Mok,Sungroh Yoon*

Main category: cs.CV

TL;DR: 本文提出CoViP，通过强化学习后训练和描述增强生成，把个性化图像描述作为核心，显著改善VLM对用户视觉-文本上下文的利用，从而在情境化视觉个性化任务及其下游任务上均取得明显提升。


<details>
  <summary>Details</summary>
Motivation: 当前VLM缺乏将视觉输入与用户累积的视觉-文本经验关联的能力，难以基于用户具体经历生成个性化响应，因而需要情境化视觉个性化能力。

Method: 提出CoViP框架：1) 将个性化图像描述作为中心任务；2) 使用强化学习的后训练（reinforcement-learning-based post-training）来优化模型对个体化视觉-文本上下文的利用；3) 采用caption-augmented generation用生成的个性化描述增强下游任务；并设计诊断评估以排除文本捷径。

Result: 实验证明现有开源和专有VLM在此任务上有显著不足；CoViP在个性化图像描述上带来提升，并在多种下游个性化任务上取得整体增益，表明其促进了鲁棒且可推广的情境化视觉个性化。

Conclusion: CoViP通过将个性化图像描述作为核心任务，并结合强化学习后训练与基于描述的生成策略，能显著提升VLM在情境化视觉个性化上的表现。

Abstract: Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user's specific experiences, as they lack the ability to associate visual inputs with a user's accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.

</details>


### [75] [Inlier-Centric Post-Training Quantization for Object Detection Models](https://arxiv.org/abs/2602.03472)
*Minsu Kim,Dongyeun Lee,Jaemyung Yu,Jiwan Hur,Giseop Kim,Junmo Kim*

Main category: cs.CV

TL;DR: InlierQ是一种无标签、后训练、基于梯度显著性与EM分布建模的内点中心量化方法，通过识别并抑制异常激活，保留有用特征，从而减少量化误差并提升2D/3D及LiDAR目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 目标检测计算量大、能耗高，量化可以减轻负担，但背景杂波和传感噪声等任务无关形态会产生冗余/异常激活，这些异常拓宽激活范围并扭曲分布，导致位分配困难并损害信息保持；缺乏判别标准使得直接抑制可能丢弃有用信息。

Method: 提出基于梯度的体素/体积显著性评分，按评分将体积分类为内点或异常；利用EM拟合评分的后验分布以确定阈值/分类；对异常体积进行抑制以缩小激活范围并优化位宽分配。该方法是无标签、插拔式的，仅需64个校准样本，适用于2D/3D相机和LiDAR检测器的后训练量化。

Result: 在COCO和nuScenes数据集上验证，InlierQ在相机（2D/3D）与LiDAR（3D）检测器上均降低了量化误差，表现稳定提升。

Conclusion: InlierQ通过基于梯度的体素显著性评分并使用EM对评分分布建模，实现了在后训练量化中分离异常与有效分布，进而抑制异常激活并保留有用特征，从而降低量化误差并提升检测性能。

Abstract: Object detection is pivotal in computer vision, yet its immense computational demands make deployment slow and power-hungry, motivating quantization. However, task-irrelevant morphologies such as background clutter and sensor noise induce redundant activations (or anomalies). These anomalies expand activation ranges and skew activation distributions toward task-irrelevant responses, complicating bit allocation and weakening the preservation of informative features. Without a clear criterion to distinguish anomalies, suppressing them can inadvertently discard useful information. To address this, we present InlierQ, an inlier-centric post-training quantization approach that separates anomalies from informative inliers. InlierQ computes gradient-aware volume saliency scores, classifies each volume as an inlier or anomaly, and fits a posterior distribution over these scores using the Expectation-Maximization (EM) algorithm. This design suppresses anomalies while preserving informative features. InlierQ is label-free, drop-in, and requires only 64 calibration samples. Experiments on the COCO and nuScenes benchmarks show consistent reductions in quantization error for camera-based (2D and 3D) and LiDAR-based (3D) object detection.

</details>


### [76] [Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance](https://arxiv.org/abs/2602.03491)
*Yingjie Zhu,Xuefeng Bai,Kehai Chen,Yang Xiang,Youcheng Pan,Xiaoqiang Zhou,Min Zhang*

Main category: cs.CV

TL;DR: 提出DiSCo解耦结构与内容对齐，结合Table-GLS的全局到局部结构引导推理，在少量标注与无外部工具条件下，有效提升LVLM的表格图像推理与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 表格图像包含复杂布局与紧耦合的结构-内容信息，现有方法通常依赖大量有监督数据、外部工具或复杂训练手段，限制了效率与扩展性。研究目标是以最少注释、无需外部工具的方式，增强LVLM对表格结构与语义的联合理解与推理能力。

Method: 方法包括两部分：1) DiSCo（Disentangled Structure-Content alignment）：在多模态对齐阶段显式分离表格结构抽象（布局、单元格关系）与语义绑定（单元格内容），以更高效地适配LVLM到表格任务；2) Table-GLS（Global-to-Local Structure-guided reasoning）：采用自上而下的结构引导推理策略，首先进行全局结构探索以建立表格宏观理解，再在局部单元格层面进行证据驱动的细粒度推理与回答生成。训练策略强调最小标注、无需外部工具或强化学习，可能使用对比损失或结构感知损失进行弱监督优化。

Result: 作者在多个基准上进行广泛实验，结果表明所提框架在表格理解与推理任务上显著提升性能，尤其在泛化到未见表格结构时表现突出。同时展示了方法在数据与计算效率上的优势。

Conclusion: 该论文提出了DiSCo和Table-GLS两阶段框架，通过结构-内容解耦与全局到局部的结构引导推理，有效提升了LVLM在表格图像推理上的能力。作者展示了在极少标注且不依赖外部工具条件下，模型仍能泛化到未见表格结构并取得显著性能提升。

Abstract: Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM's table understanding and reasoning capabilities, particularly generalizing to unseen table structures.

</details>


### [77] [Semantic Routing: Exploring Multi-Layer LLM Feature Weighting for Diffusion Transformers](https://arxiv.org/abs/2602.03510)
*Bozhou Li,Yushuo Guan,Haolin Li,Bohan Zeng,Yiyan Ji,Yue Ding,Pengfei Wan,Kun Gai,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 为了解决DiT生成中静态文本条件与非平稳去噪过程的不匹配，本文提出带轻量门控的规范化凸融合框架，对LLM多层隐藏状态进行时间/深度/联合路由；深度路由效果最佳，纯时间路由会因训练-推理轨迹不匹配而损害生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有DiT基文本到图像生成大多静态地只利用单层LLM表示，但LLM层间存在显著语义层次性，且扩散去噪过程在时间和网络深度上呈非平稳性。因此需要动态且轨迹感知的条件注入机制以更好地匹配生成过程。

Method: 提出统一的标准化凸融合（normalized convex fusion）框架，利用轻量级门控在不同维度（time-wise、depth-wise、joint）对多层LLM隐藏向量进行加权融合，针对DiT的时间与网络深度非平稳性设计路由策略；同时分析了在classifier-free guidance下名义时间步与有效SNR不一致导致的语义注入时序错配问题。

Result: 实验显示深度维语义路由（Depth-wise Semantic Routing）在文本-图像对齐和复合场景生成上最为有效，例如在GenAI-Bench的计数任务上提升+9.97分；纯时间维融合在某些情况下会降低视觉生成保真度，并分析出原因。

Conclusion: 本文提出了一种规范化凸融合框架，结合轻量级门控机制，对多层LLM隐藏状态进行时间维、深度维与联合维度的系统组织；实验证明深度维语义路由是最优策略，显著提升文本-图像对齐与组合生成能力，并揭示了纯时间维融合在推理时因训练-推理轨迹不匹配而降低生成质量的原因。

Abstract: Recent DiT-based text-to-image models increasingly adopt LLMs as text encoders, yet text conditioning remains largely static and often utilizes only a single LLM layer, despite pronounced semantic hierarchy across LLM layers and non-stationary denoising dynamics over both diffusion time and network depth. To better match the dynamic process of DiT generation and thereby enhance the diffusion model's generative capability, we introduce a unified normalized convex fusion framework equipped with lightweight gates to systematically organize multi-layer LLM hidden states via time-wise, depth-wise, and joint fusion. Experiments establish Depth-wise Semantic Routing as the superior conditioning strategy, consistently improving text-image alignment and compositional generation (e.g., +9.97 on the GenAI-Bench Counting task). Conversely, we find that purely time-wise fusion can paradoxically degrade visual generation fidelity. We attribute this to a train-inference trajectory mismatch: under classifier-free guidance, nominal timesteps fail to track the effective SNR, causing semantically mistimed feature injection during inference. Overall, our results position depth-wise routing as a strong and effective baseline and highlight the critical need for trajectory-aware signals to enable robust time-dependent conditioning.

</details>


### [78] [Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning](https://arxiv.org/abs/2602.03530)
*Xufei Zhang,Xinjiao Zhou,Ziling Deng,Dongdong Geng,Jianxiong Wang*

Main category: cs.CV

TL;DR: 提出LAC任务与LogiCls框架，通过将逻辑约束分解为子查询并用CoT监督训练VLMs，结合难度感知重采样，实现了可解释且高性能的工业逻辑异常分类。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测多为二分类，不能指出违反了哪条逻辑规则，难以用于质量管控；因此需要统一检测与细粒度违规则别的任务，并提高模型在逻辑敏感场景下的可解释性与精度。

Method: 提出LogiCls框架：将逻辑约束分解为序列化子查询，使用数据中心的指令合成生成CoT监督，提供精确的grounding注释并通过图文增强扩展数据。训练时使用难度感知重采样以强调困难样本和长尾类别。模型在推理时输出违规则别并给出证据链。

Result: 在大量实验中，LogiCls在准确性、鲁棒性和可解释性方面均优于基线方法，能够为工业图像的逻辑异常提供类别预测与证据轨迹，训练也因难度重采样而更稳定，对长尾违规则别表现更好。

Conclusion: 本文提出了Logical Anomaly Classification (LAC)任务，并设计了LogiCls框架，通过将复杂逻辑约束分解为可验证的子查询，以在一次推理中同时完成异常检测与细粒度违规则别。作者还引入了一个数据中心的指令合成流水线，生成链式思维监督（CoT），结合精确的定位标注与多样的图文增强，以提高视觉语言模型在逻辑敏感推理上的表现。训练时采用基于难度的重采样策略，重点强化困难子查询与长尾约束类型，从而稳定训练过程。实验结果表明，LogiCls在工业逻辑异常分类上具有鲁棒性、可解释性和高精度，能够提供预测的违规则别及其证据路径。

Abstract: Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.

</details>


### [79] [PnP-U3D: Plug-and-Play 3D Framework Bridging Autoregression and Diffusion for Unified Understanding and Generation](https://arxiv.org/abs/2602.03533)
*Yongwei Chen,Tianyi Wei,Yushi Lan,Zhaoyang Lyu,Shangchen Zhou,Xudong Xu,Xingang Pan*

Main category: cs.CV

TL;DR: 提出首个AR+扩散混合的统一3D理解与生成框架，使用AR做理解、扩散做生成，轻量桥接器实现跨模态交互并保留预训练先验，显著提升多个3D任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有将3D任务统一为AR范式的方法因信号量化与高训练成本导致性能下降；关键问题在于如何在不显著牺牲各自固有能力且能利用预训练模型的前提下，使生成与理解模块有效交互。

Method: 采用AR的下一标记预测用于3D理解，采用连续扩散用于3D生成；设计轻量Transformer作为桥接器，将大型语言模型的特征映射到3D扩散模型的条件空间，实现信息交互而不破坏各自模型能力，利用预训练模型减少训练成本。

Result: 在多项3D理解与生成基准上取得SOTA，且在3D编辑任务中表现优异，展示了AR+扩散混合范式在构建通用3D智能方面的潜力。

Conclusion: 该论文提出了一种将自回归（AR）与扩散模型结合的统一3D理解与生成框架，避免了纯AR方法的量化与训练开销问题，通过轻量Transformer桥接大语言模型特征与3D扩散模型条件空间，实现跨模态信息交互，保持预训练模型先验。实验证明在3D理解、生成和编辑多个基准上取得了最先进性能。

Abstract: The rapid progress of large multimodal models has inspired efforts toward unified frameworks that couple understanding and generation. While such paradigms have shown remarkable success in 2D, extending them to 3D remains largely underexplored. Existing attempts to unify 3D tasks under a single autoregressive (AR) paradigm lead to significant performance degradation due to forced signal quantization and prohibitive training cost. Our key insight is that the essential challenge lies not in enforcing a unified autoregressive paradigm, but in enabling effective information interaction between generation and understanding while minimally compromising their inherent capabilities and leveraging pretrained models to reduce training cost. Guided by this perspective, we present the first unified framework for 3D understanding and generation that combines autoregression with diffusion. Specifically, we adopt an autoregressive next-token prediction paradigm for 3D understanding, and a continuous diffusion paradigm for 3D generation. A lightweight transformer bridges the feature space of large language models and the conditional space of 3D diffusion models, enabling effective cross-modal information exchange while preserving the priors learned by standalone models. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across diverse 3D understanding and generation benchmarks, while also excelling in 3D editing tasks. These results highlight the potential of unified AR+diffusion models as a promising direction for building more general-purpose 3D intelligence.

</details>


### [80] [Constrained Dynamic Gaussian Splatting](https://arxiv.org/abs/2602.03538)
*Zihan Zheng,Zhenglong Wu,Xuanxuan Wang,Houqiang Zhong,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: CDGS通过可微预算控制器、多模态重要性评分与静动态自适应分配，在训练中严格控制高斯预算，实现边缘友好的高质量动态高斯splatting，达成>3x压缩与更优率失真表现。


<details>
  <summary>Details</summary>
Motivation: 现有动态高斯splatting要么不受约束导致内存爆炸，不适合边缘设备，要么通过启发式剪枝在给定高斯预算下难以达到最优渲染质量，因此需要一种在训练时严格控制高斯数量同时最大化渲染质量的方法。

Method: 将动态高斯splatting建模为带预算约束的优化问题；引入可微预算控制器，融合几何、运动、感知线索生成统一重要性评分；静态/动态元素分离并用自适应分配机制分配预算；三阶段训练策略确保精确满足目标计数；结合双模式混合压缩提高压缩比和质量。

Result: 在严格硬件约束下（误差<2%）CDGS将模型压缩超过3倍，并在不同容量限制下取得最优渲染质量，推动了率-失真前沿。

Conclusion: CDGS在训练中通过可微预算控制器和多模态重要性评分，实现对高动态场景的高质量4D重建同时严格满足高斯数量预算，提升了存储/渲染效率并改进了率失真性能。

Abstract: While Dynamic Gaussian Splatting enables high-fidelity 4D reconstruction, its deployment is severely hindered by a fundamental dilemma: unconstrained densification leads to excessive memory consumption incompatible with edge devices, whereas heuristic pruning fails to achieve optimal rendering quality under preset Gaussian budgets. In this work, we propose Constrained Dynamic Gaussian Splatting (CDGS), a novel framework that formulates dynamic scene reconstruction as a budget-constrained optimization problem to enforce a strict, user-defined Gaussian budget during training. Our key insight is to introduce a differentiable budget controller as the core optimization driver. Guided by a multi-modal unified importance score, this controller fuses geometric, motion, and perceptual cues for precise capacity regulation. To maximize the utility of this fixed budget, we further decouple the optimization of static and dynamic elements, employing an adaptive allocation mechanism that dynamically distributes capacity based on motion complexity. Furthermore, we implement a three-phase training strategy to seamlessly integrate these constraints, ensuring precise adherence to the target count. Coupled with a dual-mode hybrid compression scheme, CDGS not only strictly adheres to hardware constraints (error < 2%}) but also pushes the Pareto frontier of rate-distortion performance. Extensive experiments demonstrate that CDGS delivers optimal rendering quality under varying capacity limits, achieving over 3x compression compared to state-of-the-art methods.

</details>


### [81] [Cut to the Mix: Simple Data Augmentation Outperforms Elaborate Ones in Limited Organ Segmentation Datasets](https://arxiv.org/abs/2602.03555)
*Chang Liu,Fuxin Fan,Annette Schwarz,Andreas Maier*

Main category: cs.CV

TL;DR: 研究表明图像间/对象级数据增强（尤其是CutMix）能在有限数据下显著提升多器官分割性能，且与传统增强结合效果更好，作者已开源实现供后续基准测试。


<details>
  <summary>Details</summary>
Motivation: 多器官分割深度学习模型需大量带注释数据，临床中数据匮乏。研究图像间和对象级数据增强能否在有限数据下提高多器官分割性能并作为有效正则化方法。

Method: 在两个器官分割数据集上，将CutMix、CarveMix、ObjectAug和AnatoMix四种图像间/对象级数据增强方法分别应用于nnUNet训练流程，测量平均Dice分数并与不使用增强的nnUNet基线以及加入传统数据增强的结果对比。

Result: 实验证明CutMix将平均Dice提升约4.9，CarveMix提升约2.0，AnatoMix提升约1.9（均相对于不使用增强的nnUNet）；加入传统数据增强后效果进一步提升。CutMix在多器官分割中表现稳健，即使生成直观上“错误”的图像也能改善性能。

Conclusion: 本文比较了四种基于图像间与对象级的数据增强策略在多器官分割任务上的效果，发现CutMix、CarveMix、AnatoMix均能显著提升分割性能，其中CutMix表现最稳健，结合传统增强效果进一步提升。

Abstract: Multi-organ segmentation is a widely applied clinical routine and automated organ segmentation tools dramatically improve the pipeline of the radiologists. Recently, deep learning (DL) based segmentation models have shown the capacity to accomplish such a task. However, the training of the segmentation networks requires large amount of data with manual annotations, which is a major concern due to the data scarcity from clinic. Working with limited data is still common for researches on novel imaging modalities. To enhance the effectiveness of DL models trained with limited data, data augmentation (DA) is a crucial regularization technique. Traditional DA (TDA) strategies focus on basic intra-image operations, i.e. generating images with different orientations and intensity distributions. In contrast, the interimage and object-level DA operations are able to create new images from separate individuals. However, such DA strategies are not well explored on the task of multi-organ segmentation. In this paper, we investigated four possible inter-image DA strategies: CutMix, CarveMix, ObjectAug and AnatoMix, on two organ segmentation datasets. The result shows that CutMix, CarveMix and AnatoMix can improve the average dice score by 4.9, 2.0 and 1.9, compared with the state-of-the-art nnUNet without DA strategies. These results can be further improved by adding TDA strategies. It is revealed in our experiments that Cut-Mix is a robust but simple DA strategy to drive up the segmentation performance for multi-organ segmentation, even when CutMix produces intuitively 'wrong' images. Our implementation is publicly available for future benchmarks.

</details>


### [82] [ELIQ: A Label-Free Framework for Quality Assessment of Evolving AI-Generated Images](https://arxiv.org/abs/2602.03558)
*Xinyue Li,Zhiming Xu,Zhichao Zhang,Zhaolin Cai,Sijing Wu,Xiongkuo Min,Yitong Chen,Guangtao Zhai*

Main category: cs.CV

TL;DR: ELIQ通过自动构造正/负样本并对多模态模型进行指令微调，实现无标签、可迁移的二维图像质量评估，适应快速演进的生成模型。


<details>
  <summary>Details</summary>
Motivation: 生成式文生图模型快速进化导致原有人工标注对新一代图像失效，需无标签且可迁移的质量评估方法。

Method: 自动构造正样本和多方面的负样本（涵盖传统失真和AIGC特有失真），基于多模态预训练模型通过指令微调为质量判定器，使用轻量门控融合与Quality Query Transformer预测二维质量（视觉质量与提示-图像一致性）。

Result: 在多项基准上优于现有无标签方法，同时能从AIGC泛化至UGC，无需修改。

Conclusion: ELIQ能在无标签条件下评估AI生成图像的质量，并优于现有无标签方法，且可迁移到用户生成内容场景。

Abstract: Generative text-to-image models are advancing at an unprecedented pace, continuously shifting the perceptual quality ceiling and rendering previously collected labels unreliable for newer generations. To address this, we present ELIQ, a Label-free Framework for Quality Assessment of Evolving AI-generated Images. Specifically, ELIQ focuses on visual quality and prompt-image alignment, automatically constructs positive and aspect-specific negative pairs to cover both conventional distortions and AIGC-specific distortion modes, enabling transferable supervision without human annotations. Building on these pairs, ELIQ adapts a pre-trained multimodal model into a quality-aware critic via instruction tuning and predicts two-dimensional quality using lightweight gated fusion and a Quality Query Transformer. Experiments across multiple benchmarks demonstrate that ELIQ consistently outperforms existing label-free methods, generalizes from AI-generated content (AIGC) to user-generated content (UGC) scenarios without modification, and paves the way for scalable and label-free quality assessment under continuously evolving generative models. The code will be released upon publication.

</details>


### [83] [SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM](https://arxiv.org/abs/2602.03589)
*Ming Nie,Dan Ding,Chunwei Wang,Yuanfan Guo,Jianhua Han,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: SlowFocus通过基于问题定位的局部稠密采样与多频率注意力融合，全方位提升Vid-LLMs在细粒度时序理解任务上的表现，并引入FineAction-CGR基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有Vid-LLMs难以同时兼顾每帧的高质量语义令牌数量和视频级足够采样帧数，限制了对细粒度时序理解的能力，需提高等效采样频率而不降低单帧令牌质量。

Method: SlowFocus先基于问题定位相关时间段，对该段进行局部高频稠密采样以获得高质量帧令牌；随后采用多频率混合注意力将局部高频细节与全局低频上下文融合；并配套训练策略以增强时序定位与细节推理能力。

Result: 在若干公开视频理解基准和新构建的FineAction-CGR细粒度时序理解基准上，SlowFocus表现优于现有方法，证明其在保持帧级质量的同时提升时序理解能力的有效性。

Conclusion: 本文提出SlowFocus机制，通过在问题相关时段内进行稠密采样并结合多频率混合注意力模块，实现高质量帧级语义与全面视频级时序信息的兼顾，从而提升Vid-LLMs对细粒度时序理解的能力。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.

</details>


### [84] [High-Resolution Underwater Camouflaged Object Detection: GBU-UCOD Dataset and Topology-Aware and Frequency-Decoupled Networks](https://arxiv.org/abs/2602.03591)
*Wenji Wu,Shuo Ye,Yiyu Liu,Jiguang He,Zhuo Wang,Zitong Yu*

Main category: cs.CV

TL;DR: 提出DeepTopo-Net，包含WCAP和ATRM，针对深海伪装目标检测提升拓扑连通性和特征提取，并构建2K的GBU-UCOD基准，实验显示方法效果良好。


<details>
  <summary>Details</summary>
Motivation: 海洋深度变化导致目标与背景在视觉上高度相似，现有方法在处理深海细长生物的拓扑断裂和透明生物的微弱特征提取上存在不足。

Method: 提出WCAP使用黎曼度量张量动态变形卷积采样域以适应水下物理退化；ATRM引入骨架先验来保持细长目标的结构连通性；并结合频率解耦的感知模块进行特征提取。

Result: 在MAS3K、RMAS及新建的高分辨率GBU-UCOD（2K）数据集上进行的大量实验表明，DeepTopo-Net在整体指标和形态完整性保留方面达到或优于现有最先进方法。

Conclusion: DeepTopo-Net通过结合拓扑感知和频率解耦感知，有效提升了对海洋伪装目标的检测能力，尤其在细长和透明生物的形态完整性保持上表现突出。

Abstract: Underwater Camouflaged Object Detection (UCOD) is a challenging task due to the extreme visual similarity between targets and backgrounds across varying marine depths. Existing methods often struggle with topological fragmentation of slender creatures in the deep sea and the subtle feature extraction of transparent organisms. In this paper, we propose DeepTopo-Net, a novel framework that integrates topology-aware modeling with frequency-decoupled perception. To address physical degradation, we design the Water-Conditioned Adaptive Perceptor (WCAP), which employs Riemannian metric tensors to dynamically deform convolutional sampling fields. Furthermore, the Abyssal-Topology Refinement Module (ATRM) is developed to maintain the structural connectivity of spindly targets through skeletal priors. Specifically, we first introduce GBU-UCOD, the first high-resolution (2K) benchmark tailored for marine vertical zonation, filling the data gap for hadal and abyssal zones. Extensive experiments on MAS3K, RMAS, and our proposed GBU-UCOD datasets demonstrate that DeepTopo-Net achieves state-of-the-art performance, particularly in preserving the morphological integrity of complex underwater patterns. The datasets and codes will be released at https://github.com/Wuwenji18/GBU-UCOD.

</details>


### [85] [TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection](https://arxiv.org/abs/2602.03594)
*Alireza Salehi,Ehsan Karami,Sepehr Noey,Sahand Noey,Makoto Yamada,Reshad Hosseini,Mohammad Sabokrou*

Main category: cs.CV

TL;DR: 用空间感知VLM（TIPS）替代CLIP并通过解耦提示与局部证据融合，简单管线即能提升零样本异常检测与定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的ZSAD受限于粗糙的图像-文本对齐，导致空间错配和对细粒度异常敏感性差；作者认为更换为空间感知训练的骨干并调整提示策略可直接改善检测与定位。

Method: 使用TIPS作为骨干，采用固定的图像级提示词与可学习的像素级提示词解耦全局与局部表示；计算局部得分并将其注入到全局得分以弥补分布差距；无需复杂的CLIP特定技巧或附加模块。

Result: 在七个工业数据集上，基于TIPS的管线在图像级性能上提高1.1–3.9%，在像素级上提高1.5–6.9%，模型结构精简且能良好泛化。

Conclusion: 本文提出基于空间感知视觉-语言模型TIPS的零样本异常检测方法，通过重新设计提示词以解耦全局与局部特征并将局部证据注入全局得分，显著提升图像级和像素级性能。

Abstract: Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP's coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP's issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.

</details>


### [86] [Refer-Agent: A Collaborative Multi-Agent System with Reasoning and Reflection for Referring Video Object Segmentation](https://arxiv.org/abs/2602.03595)
*Haichao Jiang,Tianming Liang,Wei-Shi Zheng,Jian-Fang Hu*

Main category: cs.CV

TL;DR: 提出Refer-Agent，通过多智能体交替推理与反思、粗到细帧选择与动态关注布局，在零样本场景大幅提升了文本驱动的视频目标分割性能，并保持良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前SFT范式依赖大量标注数据且难以适配快速演进的多模态大模型；现有零样本方法尽管灵活但因流程设计简单性能欠佳，因而需要一种既不依赖微调又能提升性能的方法。

Method: 系统将RVOS分解为多步推理流程，采用Coarse-to-Fine帧选择保证帧多样性与文本相关性，使用Dynamic Focus Layout自适应调整视觉关注区域，并引入Questioner-Responder的Chain-of-Reflection用于自我验证与生成反馈以迭代优化结果。

Result: 在五个基准数据集上进行的大量实验表明，Refer-Agent在性能上显著超过现有最先进的方法（包括SFT与零样本方法），并且无需额外微调即可快速集成新的MLLMs。

Conclusion: Refer-Agent通过协作多智能体与交替推理-反思机制，实现了在无微调条件下提升RVOS性能，从而缓解了对大规模监督微调的依赖，具备良好可扩展性和快速集成新MLLMs的能力。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment objects in videos based on textual queries. Current methods mainly rely on large-scale supervised fine-tuning (SFT) of Multi-modal Large Language Models (MLLMs). However, this paradigm suffers from heavy data dependence and limited scalability against the rapid evolution of MLLMs. Although recent zero-shot approaches offer a flexible alternative, their performance remains significantly behind SFT-based methods, due to the straightforward workflow designs. To address these limitations, we propose \textbf{Refer-Agent}, a collaborative multi-agent system with alternating reasoning-reflection mechanisms. This system decomposes RVOS into step-by-step reasoning process. During reasoning, we introduce a Coarse-to-Fine frame selection strategy to ensure the frame diversity and textual relevance, along with a Dynamic Focus Layout that adaptively adjusts the agent's visual focus. Furthermore, we propose a Chain-of-Reflection mechanism, which employs a Questioner-Responder pair to generate a self-reflection chain, enabling the system to verify intermediate results and generates feedback for next-round reasoning refinement. Extensive experiments on five challenging benchmarks demonstrate that Refer-Agent significantly outperforms state-of-the-art methods, including both SFT-based models and zero-shot approaches. Moreover, Refer-Agent is flexible and enables fast integration of new MLLMs without any additional fine-tuning costs. Code will be released.

</details>


### [87] [A Lightweight Library for Energy-Based Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.03604)
*Basile Terver,Randall Balestriero,Megi Dervishi,David Fan,Quentin Garrido,Tushar Nagarajan,Koustuv Sinha,Wancong Zhang,Mike Rabbat,Yann LeCun,Amir Bar*

Main category: cs.CV

TL;DR: EB-JEPA 是一个单 GPU 可复现的开源库，演示在表示空间的能量基 JEPA 方法从图像扩展到视频和动作条件世界模型，提供强表征（CIFAR-10 91%）、可伸缩时间建模与高成功率规划（Two Rooms 97%），并通过消融强调正则化的重要性。


<details>
  <summary>Details</summary>
Motivation: 希望把图像级自监督表示学习技术扩展到视频及动作条件世界模型，使得研究者能用更高效的表示（而非像素级生成）做多步预测与规划，同时降低计算资源门槛并便于教学与复现。

Method: 实现模块化的能量基（EB）自监督 JEPA 框架：在表示空间做预测，采用对比/能量式损失与正则化项（防止 collapse），并通过图像（CIFAR-10）、视频（Moving MNIST）和动作条件世界模型（Two Rooms）三个示例展示扩展性；每个示例可单 GPU 在数小时内训练。

Result: CIFAR-10 表征探测达 91% 准确率；Moving MNIST 展示了多步时间预测能力；Two Rooms 任务在使用该表示的规划中达成 97% 成功率；消融实验表明各正则化项对防止表示塌缩至关重要。

Conclusion: EB-JEPA 提供了一套基于 Joint-Embedding Predictive Architectures 的开源实现，能在表示空间而非像素空间进行预测，从而获得语义化表示并避免生成建模问题。库覆盖图像、视频与受控世界模型的示例，适合单 GPU 快速复现，并提供了 CIFAR-10、Moving MNIST 与 Two Rooms 上的实验结果与消融分析，证明正则化组件对防止表示塌缩至关重要。

Abstract: We present EB-JEPA, an open-source library for learning representations and world models using Joint-Embedding Predictive Architectures (JEPAs). JEPAs learn to predict in representation space rather than pixel space, avoiding the pitfalls of generative modeling while capturing semantically meaningful features suitable for downstream tasks. Our library provides modular, self-contained implementations that illustrate how representation learning techniques developed for image-level self-supervised learning can transfer to video, where temporal dynamics add complexity, and ultimately to action-conditioned world models, where the model must additionally learn to predict the effects of control inputs. Each example is designed for single-GPU training within a few hours, making energy-based self-supervised learning accessible for research and education. We provide ablations of JEA components on CIFAR-10. Probing these representations yields 91% accuracy, indicating that the model learns useful features. Extending to video, we include a multi-step prediction example on Moving MNIST that demonstrates how the same principles scale to temporal modeling. Finally, we show how these representations can drive action-conditioned world models, achieving a 97% planning success rate on the Two Rooms navigation task. Comprehensive ablations reveal the critical importance of each regularization component for preventing representation collapse. Code is available at https://github.com/facebookresearch/eb_jepa.

</details>


### [88] [KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs](https://arxiv.org/abs/2602.03615)
*Baiyang Song,Jun Peng,Yuxin Zhang,Guangyao Chen,Feidiao Yang,Jianyuan Guo*

Main category: cs.CV

TL;DR: KTV通过帧聚类与帧内token裁剪的两阶段策略，解决冗余与偏差问题，在节省计算的同时提升训练免费视频理解效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练免费方法在长视频处理上受视觉冗余和计算开销限制，且基于CLIP相似度的关键帧选择可能有偏差，遗漏关键信息，导致理解效果下降。

Method: 两阶段方法：1) 问题无关的关键帧选择，通过聚类帧级视觉特征得到多样且具代表性的帧子集；2) 关键视觉token选择，根据token重要性与冗余性在每帧内裁剪不重要或冗余的token，以减少输入到LLM的token数量。

Result: 在Multiple-Choice VideoQA任务上，KTV在使用显著更少视觉token的情况下优于最先进的训练免费基线：例如对一段60分钟、10800帧的视频仅用504个视觉token，在MLVU-Test上达44.8%准确率，并在部分基准上超越若干训练型方法。

Conclusion: KTV有效解决了训练免费视频理解中的冗余与计算开销问题，显著提高了多选VideoQA任务上的性能，同时大幅减少视觉token数量。

Abstract: Training-free video understanding leverages the strong image comprehension capabilities of pre-trained vision language models (VLMs) by treating a video as a sequence of static frames, thus obviating the need for costly video-specific training. However, this paradigm often suffers from severe visual redundancy and high computational overhead, especially when processing long videos. Crucially, existing keyframe selection strategies, especially those based on CLIP similarity, are prone to biases and may inadvertently overlook critical frames, resulting in suboptimal video comprehension. To address these significant challenges, we propose \textbf{KTV}, a novel two-stage framework for efficient and effective training-free video understanding. In the first stage, KTV performs question-agnostic keyframe selection by clustering frame-level visual features, yielding a compact, diverse, and representative subset of frames that mitigates temporal redundancy. In the second stage, KTV applies key visual token selection, pruning redundant or less informative tokens from each selected keyframe based on token importance and redundancy, which significantly reduces the number of tokens fed into the LLM. Extensive experiments on the Multiple-Choice VideoQA task demonstrate that KTV outperforms state-of-the-art training-free baselines while using significantly fewer visual tokens, \emph{e.g.}, only 504 visual tokens for a 60-min video with 10800 frames, achieving $44.8\%$ accuracy on the MLVU-Test benchmark. In particular, KTV also exceeds several training-based approaches on certain benchmarks.

</details>


### [89] [Quasi-multimodal-based pathophysiological feature learning for retinal disease diagnosis](https://arxiv.org/abs/2602.03622)
*Lu Zhang,Huizhen Yu,Zuowei Wang,Fu Gui,Yatu Guo,Wei Zhang,Mengyu Jia*

Main category: cs.CV

TL;DR: 提出通过合成FFA/MSI/显著图并并行学习与自适应融合的统一多模态框架，提高了视网膜疾病分类与分级的性能并具备可解释性与扩展性。


<details>
  <summary>Details</summary>
Motivation: 多模态视网膜影像能互补提供病变信息，但在临床应用中受限于数据异质性、侵入性和配准难题，需一个能合成与融合多模态数据的统一框架以提高诊断鲁棒性与泛化性。

Method: 通过合成多模态输入（FFA、MSI、显著图）并并行训练各模态专属模型获取特征，采用模态内与模态间的自适应校准机制进行信息剪枝与柔性融合，最后通过可视化解释模型。

Result: 在两个公开数据集上，方法在多标签分类（F1:0.683, AUC:0.953）和糖尿病视网膜病变分级（Acc:0.842, Kappa:0.861）任务上优于现有方法，并通过图像与特征空间可视化解释模型行为。

Conclusion: 该论文提出了一个统一的多模态数据合成与融合框架，用于视网膜疾病的分类和分级，解决了数据异质性和配准复杂性问题，提升了筛查准确性和效率。

Abstract: Retinal diseases spanning a broad spectrum can be effectively identified and diagnosed using complementary signals from multimodal data. However, multimodal diagnosis in ophthalmic practice is typically challenged in terms of data heterogeneity, potential invasiveness, registration complexity, and so on. As such, a unified framework that integrates multimodal data synthesis and fusion is proposed for retinal disease classification and grading. Specifically, the synthesized multimodal data incorporates fundus fluorescein angiography (FFA), multispectral imaging (MSI), and saliency maps that emphasize latent lesions as well as optic disc/cup regions. Parallel models are independently trained to learn modality-specific representations that capture cross-pathophysiological signatures. These features are then adaptively calibrated within and across modalities to perform information pruning and flexible integration according to downstream tasks. The proposed learning system is thoroughly interpreted through visualizations in both image and feature spaces. Extensive experiments on two public datasets demonstrated the superiority of our approach over state-of-the-art ones in the tasks of multi-label classification (F1-score: 0.683, AUC: 0.953) and diabetic retinopathy grading (Accuracy:0.842, Kappa: 0.861). This work not only enhances the accuracy and efficiency of retinal disease screening but also offers a scalable framework for data augmentation across various medical imaging modalities.

</details>


### [90] [Multi-Objective Optimization for Synthetic-to-Real Style Transfer](https://arxiv.org/abs/2602.03625)
*Estelle Chigot,Thomas Oberlin,Manon Huguenin,Dennis Wilson*

Main category: cs.CV

TL;DR: 把风格迁移看成算子序列的多目标优化问题，用遗传算法在风格相似与结构一致间寻找折中，利用成对图像指标加速搜索，实验在GTA5到Cityscapes/ACDC上验证有效。


<details>
  <summary>Details</summary>
Motivation: 真实像素级标注昂贵，需用合成数据训练语义分割；但合成到真实存在域差，风格迁移可减小差距；因风格算子及其顺序组合庞大，需自动化搜索高效流水线。

Method: 构建由多种风格变换算子组成的流水线，将其序列化为个体，用多目标进化算法优化；在进化过程中采用成对图像度量（每个合成图像与目标域样本）作为快速评估指标，最终用分布层面度量及语义分割性能评估Pareto前沿。

Result: 在GTA5->Cityscapes及GTA5->ACDC（逆境条件）任务上，进化算法生成的流水线在风格相似性与结构保持之间表现出多样化折中，最终提升了目标域上的分割性能，证明该方法可行且生成的流水线适应不同目标。

Conclusion: 本文将风格迁移建模为运算序列优化问题，利用多目标遗传算法在结构一致性与风格相似性之间找到平衡，能自动生成适应不同目标域的多样化增强流水线。

Abstract: Semantic segmentation networks require large amounts of pixel-level annotated data, which are costly to obtain for real-world images. Computer graphics engines can generate synthetic images alongside their ground-truth annotations. However, models trained on such images can perform poorly on real images due to the domain gap between real and synthetic images. Style transfer methods can reduce this difference by applying a realistic style to synthetic images. Choosing effective data transformations and their sequence is difficult due to the large combinatorial search space of style transfer operators. Using multi-objective genetic algorithms, we optimize pipelines to balance structural coherence and style similarity to target domains. We study the use of paired-image metrics on individual image samples during evolution to enable rapid pipeline evaluation, as opposed to standard distributional metrics that require the generation of many images. After optimization, we evaluate the resulting Pareto front using distributional metrics and segmentation performance. We apply this approach to standard datasets in synthetic-to-real domain adaptation: from the video game GTA5 to real image datasets Cityscapes and ACDC, focusing on adverse conditions. Results demonstrate that evolutionary algorithms can propose diverse augmentation pipelines adapted to different objectives. The contribution of this work is the formulation of style transfer as a sequencing problem suitable for evolutionary optimization and the study of efficient metrics that enable feasible search in this space. The source code is available at: https://github.com/echigot/MOOSS.

</details>


### [91] [SPWOOD: Sparse Partial Weakly-Supervised Oriented Object Detection](https://arxiv.org/abs/2602.03634)
*Wei Zhang,Xiang Liu,Ningjing Liu,Mingxin Liu,Wei Liao,Chunyan Xu,Xue Yang*

Main category: cs.CV

TL;DR: 针对遥感方向目标检测的高标注成本，提出SPWOOD框架：SOS-Student模型、多层伪标签过滤与稀疏划分策略，在稀疏弱监督下显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 在遥感图像中目标密集且类别多，完整标注代价高昂，需研究在极少标注下仍能保持较好性能的方法。

Method: 提出SOS-Student模型用于从稀疏弱标注中分离目标与背景并学习方向与尺度信息；设计多层伪标签过滤策略，利用模型多层预测分布去除错误伪标签；引入稀疏划分策略，确保各类别样本平衡。

Result: 在DOTA与DIOR数据集上，所提框架相比传统方法有显著性能提升，证明了其在节省标注成本的同时保持检测效果的有效性。

Conclusion: 本文提出了首个用于稀疏部分弱监督方向目标检测的框架SPWOOD，通过在稀疏弱标注+大量未标注数据条件下，显著提升检测性能并降低标注成本。

Abstract: A consistent trend throughout the research of oriented object detection has been the pursuit of maintaining comparable performance with fewer and weaker annotations. This is particularly crucial in the remote sensing domain, where the dense object distribution and a wide variety of categories contribute to prohibitively high costs. Based on the supervision level, existing oriented object detection algorithms can be broadly grouped into fully supervised, semi-supervised, and weakly supervised methods. Within the scope of this work, we further categorize them to include sparsely supervised and partially weakly-supervised methods. To address the challenges of large-scale labeling, we introduce the first Sparse Partial Weakly-Supervised Oriented Object Detection framework, designed to efficiently leverage only a few sparse weakly-labeled data and plenty of unlabeled data. Our framework incorporates three key innovations: (1) We design a Sparse-annotation-Orientation-and-Scale-aware Student (SOS-Student) model to separate unlabeled objects from the background in a sparsely-labeled setting, and learn orientation and scale information from orientation-agnostic or scale-agnostic weak annotations. (2) We construct a novel Multi-level Pseudo-label Filtering strategy that leverages the distribution of model predictions, which is informed by the model's multi-layer predictions. (3) We propose a unique sparse partitioning approach, ensuring equal treatment for each category. Extensive experiments on the DOTA and DIOR datasets show that our framework achieves a significant performance gain over traditional oriented object detection methods mentioned above, offering a highly cost-effective solution. Our code is publicly available at https://github.com/VisionXLab/SPWOOD.

</details>


### [92] [MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment](https://arxiv.org/abs/2602.03665)
*Eunkyu Park,Wesley Hanwen Deng,Cheyon Jin,Matheus Kunzler Maldaner,Jordan Wheeler,Jason I. Hong,Hong Shen,Adam Perer,Ken Holstein,Motahhare Eslami,Gunhee Kim*

Main category: cs.CV

TL;DR: MM-SCALE通过5点标度与模态归因标签为VLM提供更细粒度的道德监督，显著提升模型的排序能力与安全校准。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在多模态、社会模糊的场景中难以做出道德判断，且二元或成对监督无法捕捉人类道德推理的连续性和多样性，因此需要标量和模态归因更细粒度的监督数据。

Method: 构建大规模标注界面收集图像-情境对的5点道德可接受性评分和基于模态的推理标签，使用列表式排序偏好优化（listwise preference optimization）对VLM进行微调，比二元监督提供更丰富的信号。

Result: 在实验中，使用MM-SCALE微调的VLM在排序保真度（ranking fidelity）和安全性校准上均优于使用二元信号训练的模型。

Conclusion: 该论文提出MM-SCALE数据集，通过5点标度和模态归因标签对图像-情境对进行道德可接受性打分，以提高VLM在多模态伦理判断上的表现。

Abstract: Vision-Language Models (VLMs) continue to struggle to make morally salient judgments in multimodal and socially ambiguous contexts. Prior works typically rely on binary or pairwise supervision, which often fail to capture the continuous and pluralistic nature of human moral reasoning. We present MM-SCALE (Multimodal Moral Scale), a large-scale dataset for aligning VLMs with human moral preferences through 5-point scalar ratings and explicit modality grounding. Each image-scenario pair is annotated with moral acceptability scores and grounded reasoning labels by humans using an interface we tailored for data collection, enabling listwise preference optimization over ranked scenario sets. By moving from discrete to scalar supervision, our framework provides richer alignment signals and finer calibration of multimodal moral reasoning. Experiments show that VLMs fine-tuned on MM-SCALE achieve higher ranking fidelity and more stable safety calibration than those trained with binary signals.

</details>


### [93] [Efficient Sequential Neural Network with Spatial-Temporal Attention and Linear LSTM for Robust Lane Detection Using Multi-Frame Images](https://arxiv.org/abs/2602.03669)
*Sandeep Patil,Yongqi Dong,Haneen Farah,Hans Hellendoorn*

Main category: cs.CV

TL;DR: 本文提出一种引入时空注意力的序列神经网络，能在遮挡和眩光等困难情形下更准确、鲁棒且高效地检测车道线，且在三个大规模数据集上优于现有方法，开源了代码与模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉车道检测方法对图像关键区域及其时空显著性关注不足，导致在混合交通与恶劣视觉条件下性能下降，需提升准确性、鲁棒性与实时性。

Method: 基于标准encoder-decoder结构，采用常见骨干网并在序列模型中引入时空注意力模块以突出重要图像区域与连续帧之间的显著相关性；在三个大规模公开数据集上训练与评估。

Result: 在多种测试场景下优于最先进方法；模型参数和MACs均低于基线序列模型；发布了数据、代码与模型。

Conclusion: 提出的含时空注意力机制的序列神经网络能在复杂场景（遮挡、强光）下更鲁棒地检测车道线，同时参数与MACs低于基线序列模型，具有计算效率和实际部署潜力。

Abstract: Lane detection is a crucial perception task for all levels of automated vehicles (AVs) and Advanced Driver Assistance Systems, particularly in mixed-traffic environments where AVs must interact with human-driven vehicles (HDVs) and challenging traffic scenarios. Current methods lack versatility in delivering accurate, robust, and real-time compatible lane detection, especially vision-based methods often neglect critical regions of the image and their spatial-temporal (ST) salience, leading to poor performance in difficult circumstances such as serious occlusion and dazzle lighting. This study introduces a novel sequential neural network model with a spatial-temporal attention mechanism to focus on key features of lane lines and exploit salient ST correlations among continuous image frames. The proposed model, built on a standard encoder-decoder structure and common neural network backbones, is trained and evaluated on three large-scale open-source datasets. Extensive experiments demonstrate the strength and robustness of the proposed model, outperforming state-of-the-art methods in various testing scenarios. Furthermore, with the ST attention mechanism, the developed sequential neural network models exhibit fewer parameters and reduced Multiply-Accumulate Operations (MACs) compared to baseline sequential models, highlighting their computational efficiency. Relevant data, code, and models are released at https://doi.org/10.4121/4619cab6-ae4a-40d5-af77-582a77f3d821.

</details>


### [94] [Referring Industrial Anomaly Segmentation](https://arxiv.org/abs/2602.03673)
*Pengfei Yue,Xiaokang Jiang,Yilin Lu,Jianghang Lin,Shengchuan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 本工作提出将文本提示引入工业异常分割（RIAS），构建MVTec-Ref数据集，并提出高效的DQFormer+LMA模型，用仅两个查询token实现文本指导的多尺度异常分割，推动IAD向开放集方向发展。


<details>
  <summary>Details</summary>
Motivation: 传统IAD方法在定位精度、阈值依赖、数据稀缺和模型泛化（单异常类一模型）方面存在不足，期望通过语言提示统一检测多类异常并免除人工阈值设定。

Method: 构建MVTec-Ref数据集并设计DQFormer网络：使用Dual Query Token（仅“异常”与“背景”两token）与Mask Group Transformer，结合Language-Gated Multi-Level Aggregation（LMA）实现多尺度视觉-文本融合，生成从文本到精确异常掩码的映射。

Result: 在MVTec-Ref上实验表明RIAS能高效生成从文本描述到异常掩码的精确分割，95%小尺寸异常的数据集验证了方法在小目标上的鲁棒性，展示了向开放集IAD能力迈进的潜力。

Conclusion: 提出利用语言引导实现工业异常分割（RIAS），解决无监督阈值依赖和有监督过拟合及“一异常类一模型”限制，推动开放集检测。

Abstract: Industrial Anomaly Detection (IAD) is vital for manufacturing, yet traditional methods face significant challenges: unsupervised approaches yield rough localizations requiring manual thresholds, while supervised methods overfit due to scarce, imbalanced data. Both suffer from the "One Anomaly Class, One Model" limitation. To address this, we propose Referring Industrial Anomaly Segmentation (RIAS), a paradigm leveraging language to guide detection. RIAS generates precise masks from text descriptions without manual thresholds and uses universal prompts to detect diverse anomalies with a single model. We introduce the MVTec-Ref dataset to support this, designed with diverse referring expressions and focusing on anomaly patterns, notably with 95% small anomalies. We also propose the Dual Query Token with Mask Group Transformer (DQFormer) benchmark, enhanced by Language-Gated Multi-Level Aggregation (LMA) to improve multi-scale segmentation. Unlike traditional methods using redundant queries, DQFormer employs only "Anomaly" and "Background" tokens for efficient visual-textual integration. Experiments demonstrate RIAS's effectiveness in advancing IAD toward open-set capabilities. Code: https://github.com/swagger-coder/RIAS-MVTec-Ref.

</details>


### [95] [RegionReasoner: Region-Grounded Multi-Round Visual Reasoning](https://arxiv.org/abs/2602.03733)
*Wenfang Sun,Hao Chen,Yingjun Du,Yefeng Zheng,Cees G. M. Snoek*

Main category: cs.CV

TL;DR: 该工作提出了多轮视觉推理基准和基于强化学习的RegionReasoner，通过要求推理过程引用边界框并使用全局-局部一致性奖励，提升了多轮场景下的定位与语义一致性，RegionReasoner-7B在检测与分割任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型多依赖单轮或仅文本推理，难以在多视觉上下文间迭代细化理解；需要评估和提升模型在多轮场景下的定位与语义一致性。

Method: 构建多轮视觉推理基准（RegionDial-Bench），并提出RegionReasoner框架：在推理轨迹中强制引用参考边界框（grounded reasoning），并通过全局-局部一致性奖励（从场景与区域字幕中提取关键对象/名词并与推理轨迹对齐）维持语义一致性；使用结合了定位保真度与全局-局部语义对齐的结构化奖励进行强化学习优化。

Result: 在检测与分割任务上，RegionReasoner-7B配合RegionDial-Bench在多轮推理准确率、空间定位精度和全局-局部一致性上显著优于对比方法，确立了该任务的新基线。

Conclusion: 本文提出了RegionReasoner，通过强化学习和结构化奖励将多轮视觉推理与区域级绑定相结合，能显著提升多轮推理准确率与空间定位精度，为多轮视觉-语言推理提供强基线。

Abstract: Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.

</details>


### [96] [Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment](https://arxiv.org/abs/2602.03742)
*Johny J. Lopez,Md Meftahul Ferdaus,Mahdi Abdelguerfi*

Main category: cs.CV

TL;DR: 论文提出一个结合轻量分割模型与微调VLM的边缘可部署管道缺陷检测与自动报告生成系统，在保证性能的同时实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 当前地下管道/涵洞巡检依赖人工生成的报告，自动从图像检测转成可读报告在资源受限的边缘设备上仍具挑战，因而需要轻量、高效且能输出人类可理解摘要的系统。

Method: 两阶段方法：第一阶段使用RAPID-SCAN进行缺陷分割（0.64M参数，F1=0.834）；第二阶段使用微调的Phi-3.5 VLM从分割结果生成领域化自然语言摘要，并通过后训练量化与硬件优化在边缘设备部署。

Result: RAPID-SCAN在缺陷分割上达到0.834 F1、参数量仅0.64M；微调的Phi-3.5能基于分割输出生成简洁领域化描述；后训练量化显著减小模型大小与延迟；在移动机器人平台上完成部署并在实际场景中验证有效性。

Conclusion: 该论文提出了一个面向地下基础设施缺陷的端到端轻量化检测与自然语言总结流水线，通过紧凑的分割模型RAPID-SCAN和微调的视觉语言模型在边缘设备上实现实用的自动巡检报告生成。

Abstract: Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.

</details>


### [97] [LIVE: Long-horizon Interactive Video World Modeling](https://arxiv.org/abs/2602.03747)
*Junchao Huang,Ziyang Ye,Xinting Hu,Tianyu He,Guiyu Zhang,Shaoshuai Shi,Jiang Bian,Li Jiang*

Main category: cs.CV

TL;DR: 提出LIVE，通过前向回滚与逆向重建的循环一致性损失，显式约束长时误差累积，取代教师蒸馏并结合渐进训练，显著提升长时视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 自回归视频世界模型在短时有效但会随时间累积小误差导致长时生成失败，现有基于教师蒸馏的方法代价高且不能阻止训练长度外的误差传播，需新的方法直接限制误差累积。

Method: LIVE先从真实帧做前向回滚生成，然后执行逆向生成以重建初始状态，在重建的终态上计算扩散损失，从而在训练中显式约束长时序误差传播；同时提供统一视角并采用渐进训练课程以稳定训练。

Result: 在长时基准上达到最先进性能，能生成远超训练回滚长度的稳定高质量视频。

Conclusion: LIVE通过引入循环一致性目标，有效限制了长时序误差累积，使得无需教师蒸馏即可实现长时序稳定生成。

Abstract: Autoregressive video world models predict future visual observations conditioned on actions. While effective over short horizons, these models often struggle with long-horizon generation, as small prediction errors accumulate over time. Prior methods alleviate this by introducing pre-trained teacher models and sequence-level distribution matching, which incur additional computational cost and fail to prevent error propagation beyond the training horizon. In this work, we propose LIVE, a Long-horizon Interactive Video world modEl that enforces bounded error accumulation via a novel cycle-consistency objective, thereby eliminating the need for teacher-based distillation. Specifically, LIVE first performs a forward rollout from ground-truth frames and then applies a reverse generation process to reconstruct the initial state. The diffusion loss is subsequently computed on the reconstructed terminal state, providing an explicit constraint on long-horizon error propagation. Moreover, we provide an unified view that encompasses different approaches and introduce progressive training curriculum to stabilize training. Experiments demonstrate that LIVE achieves state-of-the-art performance on long-horizon benchmarks, generating stable, high-quality videos far beyond training rollout lengths.

</details>


### [98] [See-through: Single-image Layer Decomposition for Anime Characters](https://arxiv.org/abs/2602.03749)
*Jian Lin,Chengze Li,Haoyun Qin,Kwun Wang Chan,Yanghua Jin,Hanyuan Liu,Stephen Chun Wang Choy,Xueting Liu*

Main category: cs.CV

TL;DR: 论文提出一种从单张静态二次元插画生成可操控2.5D模型的自动化框架；核心是用Live2D数据自监督、扩散模型保证部件一致性和像素级伪深度推断，实现高保真层级重建，适合专业实时动画。


<details>
  <summary>Details</summary>
Motivation: 专业工作流程需要手动分割和填补被遮挡区域，耗时且要求艺术经验。作者旨在通过自动化流程减少人工成本、提高制作效率，并生成可实时用于专业动画的软件级模型。

Method: 方法包括：1）从商业Live2D模型中自动引导生成高质量监督数据的引擎；2）基于扩散模型的Body Part Consistency Module以保证全局几何一致性；3）像素级伪深度推断模块；4）语义分层与隐含区域的修复（inpainting）以及绘制顺序推断，最终输出可编辑的分层和深度图。

Result: 实验表明，方法能生成高保真、语义清晰且具有正确遮挡顺序的图层化模型，适用于实时动画应用；在处理复杂发丝、服饰重叠等层级交错问题上效果显著，并能从商业Live2D模型中学习到像素级语义与隐藏几何。

Conclusion: 该论文提出了一个端到端框架，将静态二次元插画自动分解为可操控的2.5D图层模型，实现从单张图像生成带有深度和绘制顺序的可动画化结构。

Abstract: We introduce a framework that automates the transformation of static anime illustrations into manipulatable 2.5D models. Current professional workflows require tedious manual segmentation and the artistic ``hallucination'' of occluded regions to enable motion. Our approach overcomes this by decomposing a single image into fully inpainted, semantically distinct layers with inferred drawing orders. To address the scarcity of training data, we introduce a scalable engine that bootstraps high-quality supervision from commercial Live2D models, capturing pixel-perfect semantics and hidden geometry. Our methodology couples a diffusion-based Body Part Consistency Module, which enforces global geometric coherence, with a pixel-level pseudo-depth inference mechanism. This combination resolves the intricate stratification of anime characters, e.g., interleaving hair strands, allowing for dynamic layer reconstruction. We demonstrate that our approach yields high-fidelity, manipulatable models suitable for professional, real-time animation applications.

</details>


### [99] [Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives](https://arxiv.org/abs/2602.03750)
*Owen Dong,Lily Gao,Manish Kota,Bennett A. Landmana,Jelena Bekvalac,Gaynor Western,Katherine D. Van Schaik*

Main category: cs.CV

TL;DR: 利用LVLM的零样本提示方法，可自动标注古放射学影像的骨种、投影视图和左右侧，大幅提高初筛效率，准确率高且能标注不确定样本以供专家复核。


<details>
  <summary>Details</summary>
Motivation: 古放射学影像在野外采集时存在非标准化、骨骼分离、侧别标记缺失及设备/个体差异等问题，使得基于内容的检索与初筛成为瓶颈，亟需自动化工具提高专家工作效率。

Method: 将原始DICOM转为骨窗PNG，向LVLM提交精心设计的提示（prompt），接收结构化JSON输出并导出至电子表格供专家验证；在随机100幅影像上与一名认证古放射科医师对比评估准确性。

Result: 在100幅随机样本上，系统在主要骨骼识别上达到92%准确率、投影视图80%准确率、左右侧识别100%准确率；对不确定案例返回低/中置信度标记以便人工复核。

Conclusion: 本研究表明，基于大型视觉语言模型（LVLM）的零样本提示策略能够在古放射学影像中高效识别主要骨骼、投影视图和左右侧，显著加速大规模数据集的内容导航和初筛工作。

Abstract: Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.

</details>


### [100] [Test-Time Conditioning with Representation-Aligned Visual Features](https://arxiv.org/abs/2602.03753)
*Nicolas Sereyjol-Garros,Ellington Kirby,Victor Letzelter,Victor Besnier,Nermin Samet*

Main category: cs.CV

TL;DR: REPA-G是在推理时利用自监督对齐表示，通过优化相似度势函数引导扩散模型生成，实现从局部纹理到全局语义的多尺度条件控制并支持多概念组合，替代模糊的文本提示，实验证明在ImageNet和COCO上生成质量和多样性良好。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明自监督模型学习的表示在训练阶段能够对齐并改善扩散模型训练，但如何在推理阶段利用这些对齐表示来进行更精确、灵活的条件控制尚未充分探索。作者旨在利用这些语义丰富的对齐表示，在不依赖文本提示或标签的情况下，通过特征级约束实现更精细的生成控制。

Method: REPA-G在推理阶段将预训练特征提取器（自监督对齐表示）提取的目标表示作为条件，通过定义一个相似度势函数并在每个扩散步骤中优化该势函数来调整模型的去噪方向。可使用不同尺度的特征（单个图像补丁到全局特征token）来实现细粒度或宏观语义控制，并可以将多个概念的特征合成用于复合条件。该方法无需在训练时修改扩散模型，完全在采样时通过梯度或相似度优化实现控制。

Result: 在ImageNet和COCO上的定量评估显示，REPA-G在保持生成质量的同时实现了高度多样性，并能进行细粒度纹理匹配、全局语义控制及多概念组合。代码已公开。

Conclusion: 该论文提出了REPA-G，一种在推理阶段利用自监督模型对齐表示进行条件控制的框架，通过在推理时优化相似度目标引导扩散模型的去噪过程，实现在多尺度上从局部纹理到全局语义的灵活控制，并扩展到多概念组合。方法在推理时无需微调，能够替代模糊的文本提示或粗粒度标签，理论上证明了这种引导对应于从一个势函数导致的倾斜分布进行采样。实验在ImageNet和COCO上展示了高质量和多样性的生成结果。

Abstract: While representation alignment with self-supervised models has been shown to improve diffusion model training, its potential for enhancing inference-time conditioning remains largely unexplored. We introduce Representation-Aligned Guidance (REPA-G), a framework that leverages these aligned representations, with rich semantic properties, to enable test-time conditioning from features in generation. By optimizing a similarity objective (the potential) at inference, we steer the denoising process toward a conditioned representation extracted from a pre-trained feature extractor. Our method provides versatile control at multiple scales, ranging from fine-grained texture matching via single patches to broad semantic guidance using global image feature tokens. We further extend this to multi-concept composition, allowing for the faithful combination of distinct concepts. REPA-G operates entirely at inference time, offering a flexible and precise alternative to often ambiguous text prompts or coarse class labels. We theoretically justify how this guidance enables sampling from the potential-induced tilted distribution. Quantitative results on ImageNet and COCO demonstrate that our approach achieves high-quality, diverse generations. Code is available at https://github.com/valeoai/REPA-G.

</details>


### [101] [RAWDet-7: A Multi-Scenario Benchmark for Object Detection and Description on Quantized RAW Images](https://arxiv.org/abs/2602.03760)
*Mishal Fatima,Shashank Agnihotri,Kanchana Vaishnavi Gandikota,Michael Moeller,Margret Keuper*

Main category: cs.CV

TL;DR: RAWDet-7: a large RAW image dataset (~32.6k total) with annotations and object descriptions for 7 categories, enabling study of detection and description under low-bit RAW quantization and diverse capture conditions.


<details>
  <summary>Details</summary>
Motivation: ISPs optimize for human viewing and can discard information useful for machine reasoning. Studying RAW allows leveraging richer scene cues and understanding robustness under low-bit quantization and varied sensor constraints.

Method: Collected ~25k train and 7.6k test RAW images from diverse cameras/environments; annotated object bounding boxes for seven categories per MS-COCO/LVIS rules; generated object-level descriptions from high-res sRGB counterparts; evaluated effects of simulated 4/6/8-bit quantization on detection and description benchmarks.

Result: Introduced RAWDet-7 dataset with dense annotations and object descriptions, enabling benchmarks for detection, description quality, and generalization under simulated low-bit RAW processing. Dataset/code to be released upon acceptance.

Conclusion: RAW images offer significant advantages for machine vision by preserving sensor-level cues lost in ISP-processed sRGB, improving detection and description under diverse conditions and low-bit quantization.

Abstract: Most vision models are trained on RGB images processed through ISP pipelines optimized for human perception, which can discard sensor-level information useful for machine reasoning. RAW images preserve unprocessed scene data, enabling models to leverage richer cues for both object detection and object description, capturing fine-grained details, spatial relationships, and contextual information often lost in processed images. To support research in this domain, we introduce RAWDet-7, a large-scale dataset of ~25k training and 7.6k test RAW images collected across diverse cameras, lighting conditions, and environments, densely annotated for seven object categories following MS-COCO and LVIS conventions. In addition, we provide object-level descriptions derived from the corresponding high-resolution sRGB images, facilitating the study of object-level information preservation under RAW image processing and low-bit quantization. The dataset allows evaluation under simulated 4-bit, 6-bit, and 8-bit quantization, reflecting realistic sensor constraints, and provides a benchmark for studying detection performance, description quality & detail, and generalization in low-bit RAW image processing. Dataset & code upon acceptance.

</details>


### [102] [FOVI: A biologically-inspired foveated interface for deep vision models](https://arxiv.org/abs/2602.03766)
*Nicholas M. Blauch,George A. Alvarez,Talia Konkle*

Main category: cs.CV

TL;DR: 引入FOVI：把可变分辨率的视网膜样式传感器重构成均匀V1样式流形，通过kNN感受野和核映射实现kNN-卷积，能显著降低高分辨率视觉处理的计算开销，同时保持竞争性能。


<details>
  <summary>Details</summary>
Motivation: 动机是人类视觉通过眼动在大视场内用高分辨率中心与低分辨率周边的折衷实现高效主动感知，而传统计算机视觉普遍采用均匀分辨率，导致高分辨率全视场图像处理效率低下。

Method: 方法包括：将传感器点位嵌入到一个统一的传感器流形；在该流形上定义感受野为k近邻（kNN）；提出一种基于核映射的新技术实现kNN-卷积；并展示两种应用——端到端的kNN-卷积架构和对DINOv3 ViT的foveated适配（使用LoRA）。

Result: 实验表明，所提模型在计算成本远低于非foveated基线的情况下仍能取得具有竞争力的性能，适用于高分辨率自我视角视觉的高效可扩展主动感知。代码与预训练模型已公开。

Conclusion: 该论文提出了一种受人类视网膜与初级视觉皮层启发的foveated视觉接口（FOVI），将可变分辨率的视网膜样式传感器重构为均匀稠密的V1样式传感器流形，从而在效率上优于统一分辨率的计算机视觉。

Abstract: Human vision is foveated, with variable resolution peaking at the center of a large field of view; this reflects an efficient trade-off for active sensing, allowing eye-movements to bring different parts of the world into focus with other parts of the world in context. In contrast, most computer vision systems encode the visual world at a uniform resolution, raising challenges for processing full-field high-resolution images efficiently. We propose a foveated vision interface (FOVI) based on the human retina and primary visual cortex, that reformats a variable-resolution retina-like sensor array into a uniformly dense, V1-like sensor manifold. Receptive fields are defined as k-nearest-neighborhoods (kNNs) on the sensor manifold, enabling kNN-convolution via a novel kernel mapping technique. We demonstrate two use cases: (1) an end-to-end kNN-convolutional architecture, and (2) a foveated adaptation of the foundational DINOv3 ViT model, leveraging low-rank adaptation (LoRA). These models provide competitive performance at a fraction of the computational cost of non-foveated baselines, opening pathways for efficient and scalable active sensing for high-resolution egocentric vision. Code and pre-trained models are available at https://github.com/nblauch/fovi and https://huggingface.co/fovi-pytorch.

</details>


### [103] [QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization](https://arxiv.org/abs/2602.03782)
*Yuhao Xu,Yantai Yang,Zhenyang Fan,Yufan Liu,Yuming Li,Bing Li,Zhipeng Zhang*

Main category: cs.CV

TL;DR: QVLA通过通道级动作敏感性评估分配非均匀位宽，并将量化与剪枝统一，显著压缩VLA模型且保持或提升控制性能，便于在资源受限机器人上部署。


<details>
  <summary>Details</summary>
Motivation: 现有把LLM的均匀比特量化方法直接套用于机器人VLA模型存在问题：这些方法关注被动的数据保真度，但在机器人控制中，微小的动作偏差会累积导致任务失败，因此需要以动作敏感性为中心的量化方法。

Method: 提出按通道的细粒度位宽分配策略：对每个通道在不同比特宽下评估其对动作输出的敏感性，基于该重要性度量进行全局优化，同时将量化与剪枝(0-bit)统一。实现了针对动作误差累积的直接度量而非仅关注数据重建误差。

Result: 在LIBERO数据集上，将OpenVLA-OFT量化后仅使用29.2%原始VRAM、保持98.9%性能并获得1.49x加速；相比SmoothQuant提升22.6%性能。总体实验显示QVLA在多种基线下优越。

Conclusion: QVLA提出了一种针对视觉-语言-动作（VLA）模型的动作中心化量化框架，通过按通道分配非均匀比特位并直接评估量化对最终动作空间的敏感性，显著减少模型显存并保持性能，优于基于LLM的方法。

Abstract: The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model's quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model's VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.

</details>


### [104] [From Pre- to Intra-operative MRI: Predicting Brain Shift in Temporal Lobe Resection for Epilepsy Surgery](https://arxiv.org/abs/2602.03785)
*Jingjing Peng,Giorgio Fiore,Yang Liu,Ksenia Ellum,Debayan Daspupta,Keyoumars Ashkan,Andrew McEvoy,Anna Miserocchi,Sebastien Ourselin,John Duncan,Alejandro Granados*

Main category: cs.CV

TL;DR: NeuralShift用U-Net从术前MRI预测颞叶切除术中脑位移，达到了高DICE（0.97）和低TRE（1.12 mm），有望替代部分术中MRI更新以提高导航精度。


<details>
  <summary>Details</summary>
Motivation: 颅脑外科术中脑移位会使术前MRI失效，实时更新影像对提高导航精度和手术效果至关重要，但术中MRI获取受限，需用术前图像预测位移。

Method: 提出基于U-Net的深度学习模型，从术前MRI直接预测脑位移，评估指标包括基于解剖标志点的TRE和基于掩模的DICE。

Result: 模型在全局形变（掩模DICE=0.97）和局部位移（标志点TRE最低1.12 mm）上表现良好，能补偿较大脑移位。

Conclusion: NeuralShift能在颞叶切除术中仅基于术前MRI预测大范围脑位移，显著改善配准精度并潜在提升神经导航安全性。

Abstract: Introduction: In neurosurgery, image-guided Neurosurgery Systems (IGNS) highly rely on preoperative brain magnetic resonance images (MRI) to assist surgeons in locating surgical targets and determining surgical paths. However, brain shift invalidates the preoperative MRI after dural opening. Updated intraoperative brain MRI with brain shift compensation is crucial for enhancing the precision of neuronavigation systems and ensuring the optimal outcome of surgical interventions. Methodology: We propose NeuralShift, a U-Net-based model that predicts brain shift entirely from pre-operative MRI for patients undergoing temporal lobe resection. We evaluated our results using Target Registration Errors (TREs) computed on anatomical landmarks located on the resection side and along the midline, and DICE scores comparing predicted intraoperative masks with masks derived from intraoperative MRI. Results: Our experimental results show that our model can predict the global deformation of the brain (DICE of 0.97) with accurate local displacements (achieve landmark TRE as low as 1.12 mm), compensating for large brain shifts during temporal lobe removal neurosurgery. Conclusion: Our proposed model is capable of predicting the global deformation of the brain during temporal lobe resection using only preoperative images, providing potential opportunities to the surgical team to increase safety and efficiency of neurosurgery and better outcomes to patients. Our contributions will be publicly available after acceptance in https://github.com/SurgicalDataScienceKCL/NeuralShift.

</details>


### [105] [3D-Aware Implicit Motion Control for View-Adaptive Human Video Generation](https://arxiv.org/abs/2602.03796)
*Zhixue Fang,Xu He,Songlin Tang,Haoxian Zhang,Qingfeng Li,Xiaoqiang Liu,Pengfei Wan,Kun Gai*

Main category: cs.CV

TL;DR: 提出3DiMo：一种视角无关的隐式运动令牌，通过与预训练视频生成器联合训练并辅以视角丰富监督与退火的SMPL初始化，实现更好视角泛化和高质量运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么用2D姿态导致视角绑定，要么用显式3D模型带来深度歧义和不准导致覆盖生成器的内在3D感知。需要一种与生成器空间先验自然对齐、具备视角泛化能力的隐式运动表示。

Method: 设计一个运动编码器，输出视角无关的运动令牌，通过交叉注意力注入到预训练视频生成器；训练采用视角丰富的监督（单视角、多视角和移动相机视频）确保跨视角的一致性；使用辅助几何监督并仅在早期用SMPL初始化，随后退火至零，使模型从外部3D指导过渡到从数据和生成器先验中学习真实的3D运动理解。

Result: 实验表明3DiMo在运动保真度和视觉质量上明显优于现有方法，能够忠实重现驱动动作并提供灵活的文本驱动相机控制。

Conclusion: 提出一种基于隐式、视角无关的运动表示（3DiMo），通过与预训练视频生成器联合训练的运动编码器，将驱动帧蒸馏为紧凑的、语义注入的运动令牌，从而实现更好的视角泛化和运动保真度。

Abstract: Existing methods for human motion control in video generation typically rely on either 2D poses or explicit 3D parametric models (e.g., SMPL) as control signals. However, 2D poses rigidly bind motion to the driving viewpoint, precluding novel-view synthesis. Explicit 3D models, though structurally informative, suffer from inherent inaccuracies (e.g., depth ambiguity and inaccurate dynamics) which, when used as a strong constraint, override the powerful intrinsic 3D awareness of large-scale video generators. In this work, we revisit motion control from a 3D-aware perspective, advocating for an implicit, view-agnostic motion representation that naturally aligns with the generator's spatial priors rather than depending on externally reconstructed constraints. We introduce 3DiMo, which jointly trains a motion encoder with a pretrained video generator to distill driving frames into compact, view-agnostic motion tokens, injected semantically via cross-attention. To foster 3D awareness, we train with view-rich supervision (i.e., single-view, multi-view, and moving-camera videos), forcing motion consistency across diverse viewpoints. Additionally, we use auxiliary geometric supervision that leverages SMPL only for early initialization and is annealed to zero, enabling the model to transition from external 3D guidance to learning genuine 3D spatial motion understanding from the data and the generator's priors. Experiments confirm that 3DiMo faithfully reproduces driving motions with flexible, text-driven camera control, significantly surpassing existing methods in both motion fidelity and visual quality.

</details>


### [106] [Progressive Checkerboards for Autoregressive Multiscale Image Generation](https://arxiv.org/abs/2602.03811)
*David Eigen*

Main category: cs.CV

TL;DR: 提出渐进棋盘的多尺度自回归采样顺序，平衡四叉树各层并在尺度内外实现有效条件化，在ImageNet上用更少步数达到与同容量模型竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 目标是在自回归图像生成中实现高效并行采样（在独立位置并行采样）同时保持通过序列条件化刻画位置间的相互依赖，弥合多尺度方法与单图并行化方法的优点。

Method: 提出固定的渐进棋盘采样顺序：在多尺度金字塔中按四叉树分层，每步从各尺度中均匀间隔的区域并行采样，保证在四叉树每一层的平衡性，从而结合尺度间的条件化和尺度内的并行化。分析了不同尺度放大因子与序列步骤数的关系，并在类条件ImageNet上进行评估。

Result: 在类条件ImageNet上，相对于同等模型容量的最新自回归系统，所提方法在使用更少采样步骤的情况下取得了竞争性性能；并发现只要总的序列步骤数不变，不同的尺度放大因子产生相近的效果。

Conclusion: 本文提出一种基于渐进棋盘（progressive checkerboards）的多尺度自回归图像生成排序，通过在每个尺度上并行采样均匀间隔区域并在四叉树细分的各层保持平衡，从而在尺度间和尺度内实现有效条件化。实验在ImageNet上表明在保持序列步骤总数恒定的情况下，不同的放大因子表现相近，且在相似模型容量下采样步骤更少时能达竞争性性能。

Abstract: A key challenge in autoregressive image generation is to efficiently sample independent locations in parallel, while still modeling mutual dependencies with serial conditioning. Some recent works have addressed this by conditioning between scales in a multiscale pyramid. Others have looked at parallelizing samples in a single image using regular partitions or randomized orders. In this work we examine a flexible, fixed ordering based on progressive checkerboards for multiscale autoregressive image generation. Our ordering draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision at each step. This enables effective conditioning both between and within scales. Intriguingly, we find evidence that in our balanced setting, a wide range of scale-up factors lead to similar results, so long as the total number of serial steps is constant. On class-conditional ImageNet, our method achieves competitive performance compared to recent state-of-the-art autoregressive systems with like model capacity, using fewer sampling steps.

</details>


### [107] [Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://arxiv.org/abs/2602.03815)
*Dingkun Zhang,Shuhan Qi,Yulin Wu,Xinyu Xiao,Xuan Wang,Long Chen*

Main category: cs.CV

TL;DR: DualSpeed通过快模式剪枝与慢模式完整输入结合自蒸馏，解决VTP用于训练时的训练-推理不一致，在显著加速MLLM训练的同时保持几乎无损性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM训练效率低，主要因模型巨大与视觉token数量多。视觉token剪枝能改善推理效率，但直接用于训练会造成训练-推理不一致，影响在完整视觉序列下的表现，因而需要新的训练方法以减少视觉token同时保持一致性。

Method: 提出DualSpeed框架：快模式接入现有视觉token剪枝方法并使用模式隔离器；慢模式在完整视觉token上训练并通过自蒸馏从快模式获取知识；两模式并行协同以兼顾高效训练与训练-推理一致性。

Result: 在LLaVA-1.5上训练加速约2.1×、在LLaVA-NeXT上加速约4.0×，性能保持在99%以上；框架支持将现有VTP方法作为插件，代码开源。

Conclusion: DualSpeed在训练阶段通过快慢两种模式并存、快模式结合视觉token剪枝并隔离行为、慢模式保持完整视觉序列并利用自蒸馏从快模式学习，实现训练效率显著提升且性能不降。

Abstract: Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model's behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed

</details>


### [108] [Continuous Control of Editing Models via Adaptive-Origin Guidance](https://arxiv.org/abs/2602.03826)
*Alon Wolf,Chen Katzir,Kfir Aberman,Or Patashnik*

Main category: cs.CV

TL;DR: 通过在CFG的无条件项引入一个身份条件化预测并按编辑强度插值，AdaOr实现了扩散编辑中从原图到编辑结果的连续可控过渡。


<details>
  <summary>Details</summary>
Motivation: 现有扩散编辑模型缺乏平滑可控的编辑强度；直接缩放Classifier-Free Guidance (CFG) 无法在输入与编辑结果间产生连贯平滑的过渡，原因是无条件预测作为指导起点在低引导强度下主导生成但其本身对输入内容有任意变换。

Method: 在训练框架中加入一个身份指令，使模型在给定身份操作时预测一个表示“保持原始内容”的起点；推理时根据编辑强度在该身份预测和标准无条件预测之间插值，替换CFG中的无条件项，得到连续的过渡。

Result: 在图像与视频编辑任务上的实验表明，AdaOr在控制平滑性和一致性方面优于现有基于滑块的编辑方法，无需为每次编辑设计专门流程或依赖特殊数据集。

Conclusion: 本文提出了Adaptive-Origin Guidance (AdaOr)，通过引入身份（identity）条件化的自适应起点，并在推理时与无条件预测按编辑强度插值，从而实现对基于扩散的图像/视频编辑模型的平滑强度控制。

Abstract: Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.

</details>


### [109] [EventNeuS: 3D Mesh Reconstruction from a Single Event Camera](https://arxiv.org/abs/2602.03847)
*Shreyas Sachan,Viktor Rudnev,Mohamed Elgharib,Christian Theobalt,Vladislav Golyanik*

Main category: cs.CV

TL;DR: EventNeuS: first to combine SDF and density learning with event supervision and spherical harmonics, greatly improving event-based monocular 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing event-based novel-view synthesis exists but dense 3D mesh reconstruction from events is underexplored and current methods have low accuracy; the paper aims to close this gap.

Method: EventNeuS trains a self-supervised neural representation from monocular color event streams by learning a 3D signed distance function and a density field, and integrates spherical harmonics encodings to model view-dependent effects; supervision comes from event data rather than dense RGB frames.

Result: EventNeuS achieves on average 34% lower Chamfer distance and 31% lower mean absolute error compared to the best previous method.

Conclusion: EventNeuS demonstrates that combining signed distance functions (SDF) and density field learning with event-based supervision and spherical harmonics improves monocular event-camera 3D reconstruction, yielding substantially better accuracy than prior methods.

Abstract: Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [110] [ResQ: Realistic Performance-Aware Query Generation](https://arxiv.org/abs/2602.02999)
*Zhengle Wang,Yanfei Zhang,Chunwei Liu*

Main category: cs.DB

TL;DR: ResQ用执行感知图+贝叶斯谓词搜索生成高保真、可执行的模拟工作负载，显著优于现有方法，兼具精度与效率。


<details>
  <summary>Details</summary>
Motivation: 真实SQL工作负载对数据库研究至关重要，但受隐私限制难以获取；现有匿名性能追踪仅含高层统计信息，无法满足需要可执行查询的场景，且现有工具无法高保真、高效地重现这些追踪。

Method: 构建execution-aware query graphs，采用Bayesian Optimization驱动的谓词搜索将图实例化为SQL，并通过在精确查询与参数化模板级别重用来建模重复性；为提升可扩展性，结合搜索空间约束与轻量本地成本模型加速优化。

Result: 在Snowset、Redset和新发布的Bendset工业追踪上，ResQ相比最先进基线实现了96.71% token节省、86.97%运行时减少，最大Q-error在CPU时间上降低14.8倍、扫描字节上降低997.7倍，并且操作符组成高度匹配。

Conclusion: ResQ通过构建执行感知查询图并结合贝叶斯优化的谓词搜索，生成可执行SQL工作负载，能在保持高保真度的同时显著提高效率。

Abstract: Database research and development rely heavily on realistic user workloads for benchmarking, instance optimization, migration testing, and database tuning. However, acquiring real-world SQL queries is notoriously challenging due to strict privacy regulations. While cloud database vendors have begun releasing anonymized performance traces to the research community, these traces typi- cally provide only high-level execution statistics without the origi- nal query text or data, which is insufficient for scenarios that require actual execution. Existing tools fail to capture fine-grained perfor- mance patterns or generate runnable workloads that reproduce these public traces with both high fidelity and efficiency. To bridge this gap, we propose ResQ, a fine-grained workload synthesis sys- tem designed to generate executable SQL workloads that faithfully match the per-query execution targets and operator distributions of production traces. ResQ constructs execution-aware query graphs, instantiates them into SQL via Bayesian Optimization-driven pred- icate search, and explicitly models workload repetition through reuse at both exact-query and parameterized-template levels. To ensure practical scalability, ResQ combines search-space bounding with lightweight local cost models to accelerate optimization. Ex- periments on public cloud traces (Snowset, Redset) and a newly released industrial trace (Bendset) demonstrate that ResQ signif- icantly outperforms state-of-the-art baselines, achieving 96.71% token savings and a 86.97% reduction in runtime, while lowering maximum Q-error by 14.8x on CPU time and 997.7x on scanned bytes, and closely matching operator composition.

</details>


### [111] [Skill-Based Autonomous Agents for Material Creep Database Construction](https://arxiv.org/abs/2602.03069)
*Yue Wu,Tianhao Su,Shunbo Hu,Deng Pan*

Main category: cs.DB

TL;DR: 作者提出一个LLM驱动的代理式多模态抽取框架，能在无人工干预下从文献PDF中提取并验证蠕变力学数据，243篇文献上图表数字化成功率>90%，跨模态一致性R^2>0.99。


<details>
  <summary>Details</summary>
Motivation: 历史实验数据被非结构化文本和栅格图像锁定，人工整理耗时且易出错，因此需要自动化、可扩展的数据抽取方法以推动数据驱动材料科学。

Method: 采用模块化“技能”架构，代理执行语义过滤、多模态信息提取（文本与图像）和物理约束验证；包含图形数据数字化、文本参数提取与跨模态验证流程。

Result: 在243篇文献上构建的蠕变力学数据库中，图形数据数字化的验证成功率超过90%；提出的跨模态验证使视觉提取的数据与文本提取的本构参数高度一致（R^2>0.99），保证物理自洽性。

Conclusion: 该论文提出了一个基于代理与大型语言模型的自动化框架，用于从科学PDF中提取高保真材料实验数据，尤其是蠕变力学领域，目标是无人工干预地构建物理自洽数据库。

Abstract: The advancement of data-driven materials science is currently constrained by a fundamental bottleneck: the vast majority of historical experimental data remains locked within the unstructured text and rasterized figures of legacy scientific literature. Manual curation of this knowledge is prohibitively labor-intensive and prone to human error. To address this challenge, we introduce an autonomous, agent-based framework powered by Large Language Models (LLMs) designed to excavate high-fidelity datasets from scientific PDFs without human intervention. By deploying a modular "skill-based" architecture, the agent orchestrates complex cognitive tasks - including semantic filtering, multi-modal information extraction, and physics-informed validation. We demonstrate the efficacy of this framework by constructing a physically self-consistent database for material creep mechanics, a domain characterized by complex graphical trajectories and heterogeneous constitutive models. Applying the pipeline to 243 publications, the agent achieved a verified extraction success rate exceeding 90% for graphical data digitization. Crucially, we introduce a cross-modal verification protocol, demonstrating that the agent can autonomously align visually extracted data points with textually extracted constitutive parameters ($R^2 > 0.99$), ensuring the physical self-consistency of the database. This work not only provides a critical resource for investigating time-dependent deformation across diverse material systems but also establishes a scalable paradigm for autonomous knowledge acquisition, paving the way for the next generation of self-driving laboratories.

</details>


### [112] [StreamShield: A Production-Proven Resiliency Solution for Apache Flink at ByteDance](https://arxiv.org/abs/2602.03189)
*Yong Fang,Yuxing Han,Meng Wang,Yifan Zhang,Yue Ma,Chi Zhang*

Main category: cs.DB

TL;DR: StreamShield：通过引擎与集群层面的多项改进（运行时优化、细粒度容错、混合复制、高可用支持及严谨发布流水线），在 ByteDance 的 Flink 生产集群中显著提升弹性与稳定性，并已通过大规模生产评估。


<details>
  <summary>Details</summary>
Motivation: 在 ByteDance 大规模 Flink 生产集群中，需满足严格 SLO，但集群规模大、业务多样且运维开销高，使得实现弹性与稳定性具有挑战性。

Method: 提出运行时优化、细粒度容错、混合复制策略以及对外部系统高可用支持；并构建了完整的测试与发布流水线以保障生产可靠性。

Result: 在生产集群上的大规模评估表明，StreamShield 的技术在效率和效果上均有显著提升，验证了其在真实环境中的可行性与价值。

Conclusion: StreamShield 是面向大规模生产环境的实用弹性解决方案，通过引擎与集群两个互补视角提出多项关键技术，显著提升 Flink 集群的容错与稳定性。

Abstract: Distributed Stream Processing Systems (DSPSs) form the backbone of real-time processing and analytics at ByteDance, where Apache Flink powers one of the largest production clusters worldwide. Ensuring resiliency, the ability to withstand and rapidly recover from failures, together with operational stability, which provides consistent and predictable performance under normal conditions, is essential for meeting strict Service Level Objectives (SLOs). However, achieving resiliency and stability in large-scale production environments remains challenging due to the cluster scale, business diversity, and significant operational overhead. In this work, we present StreamShield, a production-proven resiliency solution deployed in ByteDance's Flink clusters. Designed along complementary perspectives of the engine and cluster, StreamShield introduces key techniques to enhance resiliency, covering runtime optimization, fine-grained fault-tolerance, hybrid replication strategy, and high availability under external systems. Furthermore, StreamShield proposes a robust testing and deployment pipeline that ensures reliability and robustness in production releases. Extensive evaluations on a production cluster demonstrate the efficiency and effectiveness of techniques proposed by StreamShield.

</details>


### [113] [A Pipeline for ADNI Resting-State Functional MRI Processing and Quality Control](https://arxiv.org/abs/2602.03278)
*Saige Rutherford,Zeshawn Zahid,Robert C. Welsh,Andrea Avena-Koenigsberger,Vincent Koppelmans,Amanda F. Mejia*

Main category: cs.DB

TL;DR: 作者构建了一个透明、可扩展的ADNI rs-fMRI数据整理与预处理流水线，整合开源工具与自定义脚本，解决多中心数据异质性与时间对齐问题，产出符合BIDS-derivatives的高质量时序数据，促进大规模功能影像学研究。


<details>
  <summary>Details</summary>
Motivation: ADNI rs-fMRI数据具有价值但因协议差异、数据质量和时间对齐问题被大量研究弃用，导致统计能力和可重复性受限，无法大规模研究纵向功能变化。

Method: 采用Clinica整合下载与数据整理、fMRIPrep进行标准化预处理、MRIQC用于质量评估，并开发定制工具完成临床与成像时间对齐、质量报告生成以及BIDS-derivatives格式输出。对ADNI-GO/ADNI-2/ADNI-3数据进行了支持和处理。

Result: 生成了可复现的脚本和配置文件，输出高质量的rs-fMRI时间序列（BIDS-derivatives），并为每个被试和会话提供质量指标与报告，便于严格筛选和下游分析。

Conclusion: 文章介绍了一个用于处理ADNI多中心静息态fMRI数据的开放且可复现的流水线，解决了采集协议差异、数据质量波动、缺失会话和临床时间对齐不一致等问题，从而提高了利用ADNI rs-fMRI进行大规模功能生物标志物发现的可行性。

Abstract: The Alzheimer's Disease Neuroimaging Initiative (ADNI) provides a comprehensive multimodal neuroimaging resource for studying aging and Alzheimer's disease (AD). Since its second wave, ADNI has increasingly collected resting-state functional MRI (rs-fMRI), a valuable resource for discovering brain connectivity changes predictive of cognitive decline and AD. A major barrier to its use is the considerable variability in acquisition protocols and data quality, compounded by missing imaging sessions and inconsistencies in how functional scans temporally align with clinical assessments. As a result, many studies only utilize a small subset of the total rs-fMRI data, limiting statistical power, reproducibility, and the ability to study longitudinal functional brain changes at scale. Here, we describe a pipeline for ADNI rs-fMRI data that encompasses the download of necessary imaging and clinical data, temporally aligning the clinical and imaging data, preprocessing, and quality control. We integrate data curation and preprocessing across all ADNI sites and scanner types using a combination of open-source software (Clinica, fMRIPrep, and MRIQC) and bespoke tools. Quality metrics and reports are generated for each subject and session to facilitate rigorous data screening. All scripts and configuration files are available to enable reproducibility. The pipeline, which currently supports ADNI-GO, ADNI-2, and ADNI-3 data releases, outputs high-quality rs-fMRI time series data adhering to the BIDS-derivatives specification. This protocol provides a transparent and scalable framework for curating and utilizing ADNI fMRI data, empowering large-scale functional biomarker discovery and integrative multimodal analyses in Alzheimer's disease research.

</details>
