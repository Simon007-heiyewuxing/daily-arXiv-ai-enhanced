<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: 提出面向非铺装/越野环境的CMSNet和Kamino数据集，通过模块化网络设计与层裁剪+TensorRT加速，实现低延迟的实时语义分割，能在夜、雨、尘等恶劣能见度下分割可通行区域与障碍物并通过实车验证。


<details>
  <summary>Details</summary>
Motivation: 在采矿场和发展中国家的非铺装、非均匀地形上，自动驾驶需要低延迟感知能力；现有道路分割方法对铺装路和明确车道依赖较多，难以适应无固定行驶轨迹的越野环境，因此提出针对越野和恶劣能见度的感知系统与数据集。

Method: 提出Configurable Modular Segmentation Network（CMSNet），支持多种架构配置；利用八路同步相机采集Kamino数据集的近1.2万张带标签图像；对网络进行层级裁剪并用TensorRT、C++、CUDA进行融合与加速，实现实时推理；在不同能见度（夜、雨、尘）条件下对模型进行训练与测试，并在实地车辆上进行在线语义分割验证。

Result: 在Kamino数据集及另一个公开数据集上进行的实验验证了CMSNet的有效性：在恶劣能见度条件下仍能准确分割可通行区域与障碍物；经过裁剪与TensorRT加速后，推理达到了实时要求；并在实车测试中表现出稳定的在线分割能力。

Conclusion: 该论文提出了CMSNet框架及Kamino数据集，旨在实现对非铺装路面和越野环境的实时语义分割以支持自动驾驶。通过模块化配置和网络层裁剪融合提高推理速度，并在恶劣能见度条件下验证了性能。实验表明，CMSNet在两个数据集上能有效分割可通行区域与障碍物，满足低延迟需求。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [2] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 通过LifeCLEF 2020 Plant挑战，研究证明将数十万数字化标本与少量野外照片联合用于跨域训练，能在南美热带物种的野外照片识别上带来可量化提升，但仍面临域差异与数据偏斜问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习植物识别受限于数据多集中在北美与西欧，热带高生物多样性地区数据匮乏。历史上大量的标本馆数据已被数字化，能否利用这些标本补充训练以提升热带区域自动识别是主要动机。

Method: 在约1,000个物种的数据集中，训练集以数十万张数字化植物标本图（herbarium sheets）为主，并辅以数千张野外照片，采用跨域分类（domain adaptation/transfer learning）策略训练模型，测试集仅由野外照片构成。

Result: 评测表明使用大量标本图与少量照片共同训练可改善在野外照片上的识别表现，但依然受到域差异（标本与照片外观差异）、物种长尾分布和地区性数据偏差的限制。挑战推动了多种有效方法出现（例如域对齐、数据增强和利用元信息），并提供了详细的资源与基准供后续研究。

Conclusion: 结合标本与野外照片能显著提升对数据贫乏地区植物的自动识别能力，但存在域差异、数据偏斜与长尾问题需进一步解决。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [3] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder把驾驶视频转成层级的可解释语义结构，辅以三段式提示，使LLM能在零样本情况下对事故推理与视频解释显著优于端到端V-VLM，提升高达39%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有V-VLM在仅有视频输入的驾驶后分析中，缺乏结构化归纳偏置，导致空间推理、因果判定和可解释性薄弱；因此需将视觉线索以领域特定且可解释的方式提供给LLM，以提升推理可靠性。

Method: iFinder为无训练、模块化管道：使用预训练视觉模型提取对象姿态、车道位置和轨迹等关键线索，组织成帧级与视频级的层级结构；配合三段式提示（three-block）策略，支持逐步、可溯源的LLM推理以修正并增强V-VLM的输出。

Result: 在四个公开行车记录仪基准数据集上，iFinder在零样本事故推理准确率上比端到端V-VLM最高提升约39%，整体在多个驾驶视频理解任务上显著优于无结构化输入的V-VLM。

Conclusion: iFinder通过将驾驶视频解析为层级且可解释的结构化语义表示，有效地将感知与推理解耦，从而显著提升LLM在事故推理和事件解释任务上的性能。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [4] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 提出CURE：首个无监督人脸识别模型遗忘框架，配合UES指标，在无身份标签下有效移除目标样本并保持性能，且支持基于图像质量的可解释性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸识别机器遗忘方法依赖身份标签，但在隐私受限或大规模噪声数据场景中往往不可用。需要一种无监督的遗忘方法，能在无标签环境下有效移除用户数据，满足隐私法规，同时保留系统性能。

Method: CURE基于中心点引导的表示抹除机制：在无标签设置下，通过聚类或中心估计构建每个样本的表示中心（centroid），并设计表示抹除策略将目标样本的表示从模型中移除，同时采用保护策略保持非目标样本的表示稳定。研究过程中还比较了若干无监督变体的现有消除方法作为基线，并在多个数据集上进行评估。引入了Unlearning Efficiency Score（UES）来综合衡量遗忘效果与保留稳定性。

Result: CURE在无监督设置下显著优于现有无监督变体的遗忘方法；使用UES指标展示了在遗忘率和保留稳定性之间的平衡优势；质量感知遗忘实验表明将低质量图像作为遗忘集合可以提高模型整体质量或其他性能指标。

Conclusion: 该论文提出了无监督的人脸识别模型遗忘框架CURE，能够在无身份标签的情况下从模型中移除指定样本影响，同时保持整体性能。论文还提出了新的评估指标UES来平衡遗忘与保留稳定性，并在质量感知的遗忘实验中展示了低质量图像作为遗忘集合的有效性。

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [5] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: 通过向合成高质量图像注入预定义区域的人工制品生成像素级伪标签，显著提升了artifact检测器性能，减少了人工标注需求。


<details>
  <summary>Details</summary>
Motivation: 像素级人工制品标注昂贵且稀缺，现有的伪标签方法噪声较大、效果受限，因此需要一种可扩展且可靠的自动标注方法来提高检测器性能。

Method: 作者设计了一个artifact corruption pipeline，在干净高质量合成图像上按预定区域自动注入多种人工制品，生成像素级标注。随后使用这些合成带标注数据训练检测器，避免了噪声伪标签的影响，并与基线方法在人工标注数据上进行验证。

Result: 在人工标注数据集上，使用该方法训练的检测器在ConvNeXt上性能提升13.2%，在Swin-T上提升3.7%，证明了方法在提高检测性能方面的有效性。

Conclusion: 本文提出了一种通过自动注入人工制品（artifacts）到高质量合成图像中的方法，生成像素级伪标签，从而缓解了人工像素级标注成本高的问题。该方法能够在预定区域内注入伪造的失真，为训练人工制品检测器提供大量带标签的数据，进而在ConvNeXt和Swin-T上分别取得了13.2%和3.7%的性能提升。

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [6] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: 在Swin Transformer上用前共享后特定的adapter并结合轻量级基于梯度的任务相似度分配，提出了渐进式参数高效多任务微调，在密集预测任务上以约1/5可训练参数超过全量微调并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多任务学习中参数受限会加剧任务干扰和负迁移，作者希望通过在模型中渐进地从共享到特定的参数设计来在共享学习和任务特定学习之间取得平衡，既能利用任务间的正迁移又能减小冲突。

Method: 在Swin Transformer上引入adapter模块，设计为前层共享、后层逐步变得任务特定；提出基于梯度的任务相似度度量，用于将相似任务分配到共享adapter；该相似度计算具有很小的额外开销。

Result: 在PASCAL和NYUD-v2的密集预测任务上，方法在相对单任务微调改进上表现更好，且只需约20%可训练参数就能超越全量微调的多任务模型，并领先于当前参数高效的多任务学习方法。

Conclusion: 该论文提出了一种渐进式任务特定的参数高效多任务微调方法，通过在预训练模型中插入adapter，使前层共享、后层逐步任务特定，以减轻任务间冲突和负迁移。实验表明在PASCAL和NYUD-v2数据集上，该方法在仅使用约五分之一可训练参数的情况下超过了全量微调的多任务模型，并优于现有参数高效多任务学习方法。

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [7] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出将可逆、可学习的预处理变换及其紧凑参数嵌入JPEG注释域的方法，使JPEG既兼具高压缩与兼容性，又能实现高质量原始图像重建。


<details>
  <summary>Details</summary>
Motivation: 原始图像保留了传感器的全部信息，对编辑和视觉任务非常有用，但标准原始格式（如DNG）体积大，在存储受限场景中不实用；而JPEG虽然压缩率高且兼容性好，但不适合直接存储原始数据。研究动机是如何在不牺牲兼容性和压缩效率的前提下，支持高质量的原始重建。

Method: 方法在空间域和可选的频域上对原始图像施加可逆变换，并将少量变换参数以紧凑形式存储在JPEG的注释字段（comment）中。该变换是轻量且可学习的，使得经过JPEG压缩/解压后的数据可以准确反向映射回原始格式。

Result: 在多个数据集上的实验表明，RawJPEG Adapter相比直接将原始图像以JPEG存储，在重建保真度上表现更好；方法也支持其他编码器（codecs），并在压缩率与重建精度之间提供了更优的折中。

Conclusion: 该论文提出了一种名为RawJPEG Adapter的轻量可学习可逆预处理管线，用于将原始（raw）图像适配到标准JPEG压缩，从而在保持高压缩效率和兼容性的同时实现原始图像的高保真重建。

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [8] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 通过在现有雷达到类LiDAR点云生成框架中代入不同容量的分割骨干，找到一个容量折衷点，可将点云质量提升约23.7%，但过度增加模型容量会有害。


<details>
  <summary>Details</summary>
Motivation: 降成本地用4D雷达替代高成本LiDAR，训练神经网络将雷达数据映射为LiDAR式稠密点云以提升自动驾驶感知性能。

Method: 在已有基于2D CNN骨干和时序一致性网络的框架上，替换或比较更高容量的分割主干网络，对比不同骨干对生成点云质量的影响，使用RaDelft数据集进行训练与评估。

Result: 发现最佳容量的分割主干能在指标上比现有SOTA提升23.7%，而极高容量模型反而降低性能。

Conclusion: 使用合适容量的分割主干网络能显著提升基于4D雷达生成类LiDAR点云的质量，但过高容量会有负面影响。

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [9] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 作者构建并公开了一个1,343张新闻图片的多模态偏见基准，评估多种VLM，发现视觉信息会系统性地影响模型输出，性别和职业偏见最显著，且忠实度与低偏见并非正相关。


<details>
  <summary>Details</summary>
Motivation: 研究者担心大规模视觉-语言模型在解释包含人口属性的图像时会吸收并再现有害社会刻板印象，因此需要系统性评估这些模型在现实新闻图片场景中的偏见表现。

Method: 构建了包含1,343个图片-问题对的新闻图片基准，标注了标准答案和人口属性；评估了多种最先进的VLM，并使用大型语言模型作为裁判进行评估，辅以人工验证；公开基准的提示、评估标准和代码以支持可复现性。

Result: 结果显示：（i）视觉上下文在开放式回答中系统性地改变模型输出；（ii）偏见存在于不同属性与模型之间，性别和职业偏见尤为严重；（iii）模型更高的忠实度并不一定意味着更低的偏见。

Conclusion: 本文提出了一个新闻图片基准用于评估大规模视觉-语言模型（VLM）在含有年龄、性别、种族、服装和职业等人口属性时的偏见和可信度问题，发现视觉上下文会显著影响模型输出，不同属性和模型间的偏见程度差异明显，性别和职业偏见风险尤其高，并且更高的忠实度并不保证更低的偏见。

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [10] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: 通过贝叶斯先验对齐、对比学习、动量自监督与虚拟类别构成的MoTiC框架，减少新类原型偏差并提升FSCIL表现。


<details>
  <summary>Details</summary>
Motivation: FSCIL在新类样本稀缺时新类原型估计偏差大，导致灾难性遗忘与过拟合；利用旧类丰富统计信息可作为先验以改善新类估计。

Method: 基于贝叶斯分析调整新类先验以降低原型方差；引入大规模对比学习增强跨类特征紧致性；结合动量自监督和虚拟类别构建MoTiC框架以丰富特征多样性并注入先验信息。

Result: 在三个FSCIL基准数据集上达到SOTA，尤其在细粒度CUB-200任务上表现显著，证明方法能减少估计偏差并提升增量学习鲁棒性。

Conclusion: 提出通过贝叶斯对齐新类先验与旧类统计量、结合大规模对比学习、动量自监督与虚拟类别来减少新类原型估计偏差并提高特征紧致性，最终提升FSCIL性能。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [11] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 针对MethaneSAT/AIR高分辨率高光谱数据，深度学习（UNet、SCAN）显著优于ILR/MLP，SCAN在卫星数据上表现最佳，提升了云/云影筛选以支持更准确的甲烷排放量估算。


<details>
  <summary>Details</summary>
Motivation: 云和云影会显著影响高光谱甲烷浓度反演结果，需要可靠的自动化筛除方法以提升排放量估算精度，特别针对MethaneSAT和MethaneAIR高分辨率传感器。

Method: 比较传统机器学习（迭代逻辑回归ILR、多层感知机MLP）与深度学习（UNet、SCAN）在高空间分辨率高光谱数据上的检测性能，使用MethaneSAT与MethaneAIR相关数据集进行训练和评估。

Result: 传统方法在空间连贯性和边界定义上表现差；UNet在保持空间结构方面表现最好；SCAN在边界细节捕捉上优于UNet，并在MethaneSAT数据上取得更好效果。

Conclusion: 深度学习模型明显优于传统方法，在云和云影检测上提供更好的空间一致性和边界细节，从而可提高甲烷遥感反演的准确性。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [12] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 通过在分词阶段加入空间扰动和在层间加入可学习去噪，作者提出了两种轻量、架构无关的方法（STA与ANF），改善了ViT特征图噪声伪影并提升多个任务性能。


<details>
  <summary>Details</summary>
Motivation: 观察到ViT特征图中存在结构性噪声伪影，影响下游任务（如分割、深度估计）性能与可解释性，因此提出旨在改善token表示与抑制噪声的模块。

Method: 提出Structured Token Augmentation (STA)：在分词(tokenisation)阶段通过空间扰动增加token多样性；提出Adaptive Noise Filtering (ANF)：在transformer层间加入可学习的在线去噪模块。两者为架构无关的轻量设计，可插入现有ViT。

Result: 在ImageNet、ADE20K、NYUv2等基准上进行了评估，结果显示在视觉质量和任务性能上均有一致改善。

Conclusion: 该论文提出了两种轻量级优化方法（STA和ANF），提高了ViT特征图的可解释性并降低了结构性噪声伪影，实验在多个基准上显示了性能提升。

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [13] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文通过帧级去噪引导构造逐帧属性过渡方向，使生成视频实现平滑一致的属性渐变，提出CAT-Bench和两项评价指标，实验验证了方法优于基线并开源代码与基准。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成和提示插值方法在处理渐进式属性变换时表现不足，容易产生不一致性，需一种能保持运动一致性且实现连贯属性过渡的简单有效方法。

Method: 在扩散/去噪生成过程中为每个噪声潜变量构造数据驱动的过渡方向（frame-wise guidance），逐帧引导从初始属性到目标属性的平滑转变，同时保留视频原有的运动动力学。

Result: 在提出的CAT-Bench与两个新度量（用于衡量准确性与平滑性）上，所提方法在视觉保真度、文本提示对齐和过渡平滑度方面均超越现有基线。并公开了代码与基准。

Conclusion: 该工作提出了一种通过在去噪过程中对每帧施加引导方向以实现平滑且一致的属性过渡的方法，并构建了CAT-Bench基准和两类评估指标，实验表明方法在视觉质量、文本对齐性和过渡连贯性上优于基线。

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [14] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: 限制 transformer 与自监督任务到心肌区域，使分类更聚焦可信的临床区域，从而提升 CA 检测性能并支持注意力可视化。


<details>
  <summary>Details</summary>
Motivation: 传统视频分类模型虽能处理整个回声视频，但无法保证分类依据的是与 CA 相关的临床特征；使用定量特征（如应变）可保证临床相关性，因而作者希望通过解剖约束让深度模型的判别依据聚焦在已知病变发生的心肌上。

Method: 将心肌表示为一组随时间变形的点和对应的图像补丁，将其嵌入为 transformer 的输入 token；同时提出在 masked autoencoder 预训练中仅对解剖补丁进行遮掩与重建（解剖局部 MAE），以便自监督学习专注于心肌区域。

Result: 在 CA 分类任务上，受解剖约束的 transformer（包括解剖限定的 MAE 预训练）相比完整视频 transformer 提升了性能；并实现了注意力在变形心肌上的可视化，证明模型确实关注解剖相关信息。

Conclusion: 通过将 transformer 的输入和自监督预训练任务都限制在心肌这一解剖区域，模型在心脏淀粉样变性（CA）分类任务上优于对整段回声视频建模的 transformer，并且能够可视化注意力集中于变形心肌上，从而提供分类集中于解剖相关区域的保证。

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [15] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: 提出利用强化学习选择最优心脏超声片段子集并用注意力聚合，以在计算/效率受限下保持或提升疾病分类性能；在心脏淀粉样变性检测上用30%片段达AUC 0.91。


<details>
  <summary>Details</summary>
Motivation: 胸部超声检查产生大量不同视角的视频片段，使用全部片段进行自动疾病分类计算昂贵且影响临床可采纳性；仅使用单个片段则可能忽略互补信息。因此需要一种能够在性能与效率间权衡的方法，选择最小数量的有用片段。

Method: 设计一个强学习代理(agent)：在处理每个视图的剪辑后，代理决定继续获取更多剪辑还是停止；代理的目标是通过减少分类不确定性来最大化任务性能。另提出一个可学习的基于注意力的聚合模块，用于将多个片段的信息灵活融合，替代简单平均。

Result: 在诊断心脏淀粉样变性(cardiac amyloidosis)的任务中，方法在仅使用30%片段的情况下达到AUC 0.91，性能超过使用全部片段及其他基准方法。

Conclusion: 该论文提出了一种基于强化学习的视图选择与可学习注意力融合相结合的方法，用于在心脏超声视频中选择最优子集以进行疾病分类，旨在用更少的回波片段达到或超过利用全部片段的分类性能。

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [16] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: SynthICL用域随机化+解剖先验与个体变异建模生成高多样性且域适配的合成医疗影像，有效提升ICL分割模型性能与泛化。


<details>
  <summary>Details</summary>
Motivation: ICL在通用医疗影像分割中对大规模、多样化数据的需求激增，但真实数据稀缺；现有合成方法要么多样性不足，要么分布不符合医疗场景，需提出兼顾两者的方法。

Method: 提出基于域随机化的数据合成框架，利用真实数据的解剖先验保证真实感，生成多样化的解剖结构并明确建模个体间差异以形成适合ICL的数据队列。

Result: 在四个独立测试集上，使用SynthICL生成数据训练的模型在平均Dice上提升最高达63%，并显著提高对未见解剖域的泛化能力。

Conclusion: SynthICL通过域随机化结合解剖先验与个体间变异建模，能生成多样且适配医疗影像分割的合成数据，从而缓解ICL训练的数据稀缺问题。

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [17] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: VIMD结合MSCKF视觉-惯性跟踪与多视图逐像素尺度优化，提供了一种模块化、鲁棒且在稀疏度量点下仍高效准确的密集度量深度估计方案，具有良好泛化和实用部署价值。


<details>
  <summary>Details</summary>
Motivation: 动机是为资源受限场景（机器人、XR）提供一种既准确又高效的密集度量深度估计方法，克服纯学习方法尺度不确定性以及需要大量标注深度的数据问题，利用廉价的少量度量点和惯性信息提升深度尺度恢复和泛化能力。

Method: 方法为模块化框架：1) 使用MSCKF实现高效准确的单目视觉-惯性运动估计；2) 将多视图几何信息用于逐像素尺度迭代优化，而不是全局拟合不变仿射模型；3) 可插拔地兼容多种深度估计骨干网络，支持稀疏度量深度点引导的联合优化。

Result: 在TartanAir和VOID数据集上进行了大量评估，并在AR Table上展现了零样本泛化能力；结果表明VIMD在精度和鲁棒性上表现优异，甚至在每张图像仅有10-20个度量深度点时仍能维持高性能，适合资源受限部署。

Conclusion: 该论文提出了基于单目视觉-惯性（VIMD）的密集度量深度估计框架，通过结合MSCKF的视觉-惯性跟踪，利用多视图信息逐像素迭代精化尺度，从而在极少量稀疏度量深度点下仍能实现高精度和鲁棒性。

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [18] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: 提出基于频域的多模态交互模型FMISeg，通过FFBI和LFFI模块在解码器中融合频域视觉与语言信息，改善语义无关干扰并提升医学图像分割效果，在两个数据集上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 临床文本可作为语义引导提高医学分割精度，但现有方法受病灶形态复杂性和视觉-语言模态间语义鸿沟限制，导致视觉特征表示不足且包含无关信息，影响分割效果。

Method: 提出了一个晚期融合框架FMISeg；引入频域特征双向交互模块(FFBI)以融合不同频域视觉表示；在解码器中加入语言引导的频域特征交互模块(LFFI)，用于在语言指导下抑制语义无关的视觉特征，从而增强分割表示。

Result: 在两个新冠胸部CT相关数据集(QaTa-COV19与MosMedData+)上，FMISeg在定性和定量指标上均超过现有最先进方法，说明频域交互与语言引导能有效提升分割性能。

Conclusion: 本文提出的FMISeg通过在解码器中将语言特征与频域视觉特征进行交互，有效提升了基于文本引导的医学图像分割性能，实验证明在QaTa-COV19和MosMedData+数据集上优于现有方法。

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [19] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: PolGS：将偏振约束融入3D Gaussian Splatting，快速且更准确地重建复杂反射表面。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS方法虽然渲染快，但对复杂反射材料的重建效果不如隐式神经表示，因而需要一种既快又能处理复杂反射的重建方法。

Method: 在3DGS基础上加入偏振信息，利用偏振约束区分镜面和漫反射分量，并在重建流程中融合该分离信息以提升表面细节恢复和反射特性重现，保持快速渲染特性。

Result: 在合成与真实数据集上的实验表明，PolGS在反射表面重建质量上优于基线3DGS方法，且能在约10分钟内完成重建，验证了方法的有效性。

Conclusion: PolGS能在10分钟内实现对复杂反射表面的高效重建，通过在3D Gaussian Splatting框架中引入偏振约束，改善了镜面与漫反射成分的分离，从而提升了重建质量。

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [20] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: 提出CAMILA——一种基于上下文感知掩码的文本引导图像编辑方法，能识别并忽略不可执行指令，在单/多指令数据集上比SOTA表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本的图像编辑模型会盲从用户指令，执行不可行或矛盾的请求，导致不合理的生成结果，需引入上下文感知机制以提升可靠性。

Method: 通过在编辑前对指令与图像进行上下文验证并基于掩码选择性应用编辑，忽略不可执行的指令；构建包含不可行请求的单指令与多指令数据集进行评估。

Result: 在构建的含不可行请求的数据集上，CAMILA在性能与语义对齐度上均优于现有最先进模型，且更好地保持原图完整性。

Conclusion: CAMILA能有效判断指令与图像的上下文一致性，避免执行不可行或矛盾的编辑，提高语义对齐和图像完整性。

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [21] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: 论文通过将空间提示与基于FFT的频率域提示结合，并引入模态融合提示生成器，实现高效的RGB-T跨模态提示学习与交互，显著提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数高效微调的RGB-T跟踪方法主要只利用空间域提示，忽视了频率域信息在提示学习中的重要性，导致性能受限。

Method: 使用冻结的对称特征提取编码器提取RGB和TIR特征；在空间域引入可学习视觉提示，并在频率域通过FFT生成频域提示；设计模态融合提示生成器，将不同模态特征融合生成双向交互提示，与各模态特征交互以增强信息传递。

Result: 在三个主流RGB-T跟踪基准上进行大量实验，结果表明VFPTrack性能优异，优于现有PEFT方法（论文声称优势显著）。

Conclusion: 该论文提出了一种新颖的视觉傅里叶提示跟踪方法（VFPTrack），通过结合空间域和频率域提示，改进RGB-T跟踪的提示学习，提升跨模态特征交互与融合效果。

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [22] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 作者发现去耦合数据蒸馏领域存在评估协议不一致问题，提出RD^3并给出统一、严谨的后评估流程与改进策略，从而实现公平可复现的性能比较。


<details>
  <summary>Details</summary>
Motivation: 早期基于双层优化的方法（如MTT）虽在小数据集上表现优异但计算开销大；去耦合方法（如SRe^2L）通过分离教师模型预训练与合成数据生成降低开销，但由于后评估协议不一致，难以公平比较方法优劣。故需纠正评估流程，保证结论可靠。

Method: 提出RD^3框架：1) 系统变更并对比多种后评估设置（例如随机数据增强、epoch-wise软标签、模型前训练等）；2) 分析不同评估协议对最终测试准确率的影响；3) 识别并推荐若干通用策略以提升蒸馏数据在多种设置下的有效性；4) 构建标准化基准并实现严格的评估流程以复现并公平比较方法。

Result: 实验证明：1) 很多方法间的性能差异主要由后评估协议不一致造成；2) 通过统一和优化后评估策略，合成数据在不同设置下的泛化性能均可提升；3) RD^3 提供的标准化评估基准能更公平地反映方法本质性能。

Conclusion: 本文指出现有去耦合数据集蒸馏方法在后评估（post-evaluation）协议上存在不一致，导致不同方法间性能差异很大一部分来源于评估流程差异而非合成数据本身质量。作者提出RD^3（Rectified Decoupled Dataset Distillation），通过系统研究各种后评估设置对测试精度的影响，建立标准化基准和严格评估协议，从而实现公平可复现的比较。

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [23] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: 提出nnFilterMatch：将熵基伪标签过滤融入单次训练的nnU-Net，实现无循环的SSL+AL式学习，在低标注比例下仍可达到或超越全监督性能。


<details>
  <summary>Details</summary>
Motivation: 现有的SSL与AL结合方法通常需要在每轮人工标注后进行循环重训练，计算开销高且不利于临床可扩展性。目标是设计一个在单次训练中就能利用不确定性信息进行伪标签筛选，从而降低标注量并节省计算资源。

Method: 在nnU-Net的单次训练流程中引入熵阈值伪标签过滤机制；在每个训练步骤中，计算未标注样本的预测熵并根据阈值排除高置信（低熵或高熵？需要核实）伪标签，进而避免每轮标注后的重训练循环，从而实现AL思想的选择性学习而不引入迭代开销。

Result: 在多个医学影像分割基准上，使用5%–20%标注数据时，nnFilterMatch达到了与全监督模型可比甚至更好的性能，证明了其在降低标注需求和保持精度方面的有效性。

Conclusion: 该论文提出了一种将半监督学习(SSL)与基于熵的伪标签过滤(FilterMatch)相结合，集成到单次训练的nnU-Net框架中的方法（nnFilterMatch），以减少标注需求并避免基于循环的重复训练。实验在多个临床分割基准上验证，结果显示在仅使用5%–20%标注数据时，性能能与甚至超越全监督模型。

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [24] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出显式AU到2D关键点的两阶段方法：变分生成关键点序列+扩散合成器生成视频，提升表情可控性与生成质量，在MEAD上优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 动机是现有方法大多依赖情感标签或隐式的AU条件，无法提供物理约束的逐帧表情控制；通过显式AU到关键点的映射，可提高表情准确性和可控性。

Method: 方法上分为两阶段：第一阶段使用变分运动生成器（variational motion generator），以音频和AU强度为输入，生成时间一致的关键点序列；第二阶段使用基于扩散模型的图像合成器，结合生成的关键点序列和参考图像，生成逼真且唇形同步的视频帧。

Result: 在MEAD数据集上，论文方法在多项指标上优于现有最先进基线，表现出更高的表情准确性、时间稳定性和视觉真实度。

Conclusion: 该论文提出了一个两阶段的音频驱动口型同步人脸生成框架，通过显式的面部动作单元（AU）到2D人脸关键点映射，实现了逐帧、可控的表情合成。

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [25] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: 作者提出ExpFace：在角度空间使用指数角度margin，增强对中心（干净）样本的惩罚、抑制外围（噪声）样本，兼具训练稳定性和良好相似度/梯度性质，实验显示优于现有margin-based损失。


<details>
  <summary>Details</summary>
Motivation: 传统的margin-based softmax忽视了噪声样本的影响；通过角度空间观察样本分布，期望设计一个能强调干净样本、抑制噪声样本的margin。

Method: 在角度空间对样本分布进行分析，观察干净样本集中在中心区域而噪声样本偏向外围，基于此设计了指数角度margin，使得中心区域惩罚更大、外围惩罚更小。并通过数学分析（margin嵌入、相似度曲线、梯度曲线）与SphereFace/CosFace/ArcFace比较，保证训练稳定性和相似度单调性。

Result: 理论上避免了SphereFace训练不稳和ArcFace非单调性问题，并在大量实验中取得了SOTA成绩，作者并开源了代码。

Conclusion: 该论文提出了ExpFace，通过在角度空间引入指数角度项作为margin，增强对干净样本的惩罚并抑制噪声样本，从而提升人脸识别在开放集下的判别能力。

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [26] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: 提出将强化学习和定制奖励机制融入LVLM的Logics-Parsing，联合监督微调与新数据集评估，提升复杂文档的布局与阅读顺序解析，实验达SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 当前端到端LVLM在将PDF图像转结构化输出方面优于传统流水线方法，但缺乏显式布局与阅读顺序分析模块，导致处理复杂文档（多栏、非线性布局）时表现受限。为此需要在端到端模型中引入能驱动逻辑推理的机制。

Method: 在LVLM框架中集成强化学习，设计多维奖励函数以优化布局分析与阅读顺序推断；同时在监督微调阶段加入化学公式和手写中文数据以扩展模型输入类型的覆盖；构建并用于评估的LogicsParsingBench数据集（1,078页，九大类、二十余子类）。

Result: 在LogicsParsingBench上的全面实验表明，Logics-Parsing在多种文档分析场景下取得了优异且达到SOTA的性能，有效改善了多栏布局和阅读顺序推断的准确率，且对化学公式与手写中文的识别能力也有提升。

Conclusion: 本文提出的Logics-Parsing通过在端到端的大视觉语言模型（LVLM）中引入强化学习和专门设计的奖励机制，显著提升了复杂文档（如多栏报纸、海报）布局解析与阅读顺序推断的能力。结合对化学公式与手写中文的监督微调，以及新构建的LogicsParsingBench数据集，模型在多样文档分析任务上达到了SOTA性能。

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [27] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: 由于器官尺寸差异，DSC在没有模型差异的情况下也会对女性产生较低评分，研究公平性时应警惕这个指标偏差


<details>
  <summary>Details</summary>
Motivation: 检验DSC作为重叠指标是否本身会因器官尺寸差异导致性别偏差，从而混淆模型公平性评估

Method: 在50名受试者的手工MRI标注上施加等大小的合成边界偏移（如1mm），比较男女的DSC和归一化DSC差异，独立于模型影响

Result: 即便是极小的边界偏移，也在小/中等结构上产生平均DSC差异（小结构≈0.03，中等≈0.01），大器官差异接近零

Conclusion: DSC对性别存在固有偏差，特别是在小器官上，会导致相同大小错误在女性上产生更低的DSC

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [28] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 提出针对边缘设备的轻量级Vision Transformer HDR方法：YCbCr分离、IAAF抑鬼影、IRE/DyT/E-MSDC降成本；在保证图像质量下大幅降低FLOPS并显著加速推理，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现高质量HDR成像，解决现有多曝光融合方法计算开销大与鬼影伪影问题，满足智能监控与自动驾驶等下游任务的实际部署需求。

Method: 基于Context-Aware Vision Transformer框架，先将输入转为YCbCr分离亮度/色度；采用Intersection-Aware Adaptive Fusion(IAAF)进行动态融合以抑制鬼影；引入Inverted Residual Embedding(IRE)与Dynamic Tanh(DyT)以减少参数与计算；提出Enhanced Multi-Scale Dilated Convolution(E-MSDC)做多尺度高效感受野。提供主模型与轻量模型两种版本以兼顾质量与效率。

Result: 相比基线，主模型FLOPS降低约67%，CPU上推理速度提升5倍以上，边缘设备上提升约2.5倍；在多种动态场景下保持无鬼影且视觉质量优异，兼具实用性与通用性。

Conclusion: 本文提出了一种面向边缘设备的轻量级视觉Transformer用于HDR重建，通过色彩空间分离亮度和色度、引入IAAF模块抑制重影、并结合IRE、DyT与E-MSDC等模块在多层次上降低计算复杂度，最终在保持图像质量的同时显著提升推理效率。

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [29] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: BiTAA用3D Gaussian Splatting生成能同时破坏检测与可控偏置单目深度的对抗扰动，提出复合损失与统一评估协议，揭示跨任务传递与实际风险。


<details>
  <summary>Details</summary>
Motivation: 现有2D/3D攻击多为任务孤立，缺乏可控深度偏置机制与跨任务转移量化协议，需要研究检测与深度估计之间的相互影响。

Method: 提出基于3D Gaussian Splatting的双任务对抗攻击框架，支持全图与补丁设置，兼容多种检测器和深度估计器，并可选EOT；设计复合损失耦合检测抑制与带符号、可控幅度的对数深度偏置在ROI内。

Result: 在标准与真实场景评估中，BiTAA在两任务上均稳定降级，显示Det→Depth与Depth→Det存在不对称传递，并提供统一评估协议和物理世界验证。

Conclusion: BiTAA展示了单一摄像机扰动能同时破坏目标检测与单目深度估计，强调了多任务感知系统易受联合攻击的风险。

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [30] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN通过3D卷积、多光谱融合和天体物理正则化，使低分辨率天文图像重建更清晰且物理一致。


<details>
  <summary>Details</summary>
Motivation: 小型望远镜如MobilTelesco采集的影像分辨率和质量有限，传统2D图像到图像翻译方法（如CycleGAN）常导致恒星和星系形态失真，因此需要一种能同时保持物理一致性与视觉清晰度的增强方法。

Method: 在CycleGAN框架基础上引入3D卷积以捕捉体积空间相关性，结合光学与近红外（NIR）多光谱融合，并加入天体物理正则化模块以保留恒星和星系形态；训练使用跨使命的全天空多波段参考作为监督。

Result: 在基准测试中，StrCGAN生成的重建在主观视觉锐利度和客观物理一致性指标（如形状保真、光度一致性、多波段一致性）上优于标准GAN模型。

Conclusion: StrCGAN通过将CycleGAN扩展为包含3D卷积、多光谱融合和天体物理正则化模块，有效提高了低分辨率天文影像的重建质量，生成的图像在视觉和物理一致性上优于传统GAN模型。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [31] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: 用元学习生成逐层混合系数，自适应融合多任务模型参数，缓解任务与层级知识冲突，提升连续学习的抗遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 现有模型集成方法在连续学习中虽能通过参数插值融合知识，但存在任务级和层级的知识冲突，导致新旧任务性能受损。为此需要一种能够自适应处理不同任务与不同层之间冲突的融合策略。

Method: 设计一个混合系数生成器（mixing coefficient generator），通过元学习训练该生成器以为每一层生成适当的混合系数，用于对不同任务的模型参数进行加权融合；该生成器被整合到现有的连续学习方法中以提升抗遗忘能力。

Result: 在多个连续学习数据集上的实验表明，meta-weight-ensembler能有效缓解灾难性遗忘，并在多项评测中达到或超过当前最优方法的性能。

Conclusion: 提出的meta-weight-ensembler通过元学习生成逐层混合系数，自适应融合不同任务的模型参数，从而缓解任务级与层级的知识冲突，实现对旧任务与新任务的有效兼容；实验表明该方法能提高连续学习性能，达到或超越现有最佳水平。

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [32] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 提出ThinkFake：结合MLLM与GRPO的推理式、可解释AI图像伪造检测框架，在GenImage和LOKI上表现优异，泛化性强。


<details>
  <summary>Details</summary>
Motivation: 针对AI生成图像在现实感提升带来的误导与隐私风险，现有二分类且缺乏解释性或高度依赖监督微调导致泛化差的问题，提出可解释且泛化性强的检测方法。

Method: 该方法使用多模态大模型（MLLM）作为基础，通过设计伪造推理提示引导模型进行分步推理；使用Group Relative Policy Optimization（GRPO）强化学习并设计专门的奖励函数以优化推理策略；此外构建了结构化检测流水线以提升推理质量与适应性。

Result: 在GenImage基准上优于现有最先进方法，并在更具挑战性的LOKI基准上展示了强零样本泛化能力，表明方法有效且稳健。

Conclusion: ThinkFake提出了一种基于推理的通用AI生成图像检测框架，通过结合多模态大模型、伪造推理提示与GRPO强化学习，实现了可解释的逐步推理和结构化输出，从而提高了检测性能与泛化能力。

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [33] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: 作者提出PersONAL基准，包含2000+个性化对象导航任务，要求智能体理解对象与用户的关联，在实时导航和场景定位两种模式下评估，结果显示现有方法仍远不及人类。


<details>
  <summary>Details</summary>
Motivation: 现实人居场景中部署具身智能体的挑战在于难以建模个人偏好与行为，需研究能记忆和推理用户特定信息的智能体。

Method: 构建了包含2000+高质量情节的数据集，基于HM3D的30+真实感家庭场景，设计自然语言场景描述并标注对象-用户关联；提供两种评估模式：在未见环境中主动导航和已建图场景中的对象定位。对比多种最先进基线方法进行实验评估。

Result: 基准测试显示当前最先进方法在任务上远落后于人类，表明需要更强的感知、推理和记忆能力来处理个性化语义和关联推理。

Conclusion: 本文提出了PersONAL基准，用于研究具身智能中的个性化对象导航与定位，强调个体偏好和语义关联的重要性，并指出现有方法与人类表现存在显著差距。

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [34] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该工作发现并形式化了VLA系统中能使模型忽视后续指令的“冻结”对抗脆弱性，提出FreezeVLA通过双层优化生成可迁移的对抗图像，实验证明其高成功率，提示需要紧急防御研究。


<details>
  <summary>Details</summary>
Motivation: 随着VLA模型在机器人控制中被广泛采用，其对安全性和鲁棒性的威胁尚未充分研究。作者关注一种严重且被忽视的攻击模式：对抗图像导致模型“冻结”而忽视后续指令，可能在关键时刻引发危险。

Method: 提出FreezeVLA框架，通过最小-最大双层优化生成对抗图像，优化目标是使模型在遭遇对抗图像后对后续语言指令无响应（行动冻结）。在三种SOTA VLA模型和四个机器人任务基准上进行评估，并测试了对抗图像的迁移性。

Result: FreezeVLA在三种VLA模型与四个基准上的平均攻击成功率为76.2%，显著优于现有方法；生成的对抗图像具有强迁移性，可在不同语言提示下复现冻结效果。

Conclusion: 该论文揭示了VLA模型对“冻结攻击”的脆弱性，证明攻击可使模型忽略后续指令，造成机器人失能，提出的FreezeVLA方法能高效生成此类对抗图像并在多模型多基准上成功率显著高于现有方法。

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [35] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: 利用微调后的MLLM在边缘-云协同系统中生成结构化语义指导，通过自适应映射即时调整边缘检测器参数，显著提升复杂场景下检测准确性并大幅减少延迟和计算资源。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测在低光、强遮挡等复杂场景中缺乏高层语义理解，导致性能下降；因此引入MLLM进行场景语义理解以增强边缘检测器。

Method: 方法包括对MLLM进行指令微调以生成结构化场景描述、设计自适应映射机制将语义信息动态转换为边缘检测器的参数调节信号、并在边缘-云协同推理框架中基于置信度自动选择是否调用云端语义指导。

Result: 在低光和强遮挡场景下，方法在保持准确性的同时能够将延迟降低超过79%，计算成本降低约70%，并提升检测性能与实时性。

Conclusion: 该方法通过在边缘-云协同框架中利用多模态大语言模型（MLLM），实现了在复杂场景下对目标检测的语义增强，从而在保证准确性的同时大幅降低延迟和计算开销。

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [36] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: 提出SphSPS：基于球面最短路径的超像素方法，兼顾精度与规则性，并提出球面全局规则性度量，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 常见超像素方法针对平面图像设计，不能正确处理360°全景或鱼眼等宽视角图像的几何畸变；因此需要考虑球面几何来获得更准确且规则的超像素分割以服务下游任务。

Method: 方法基于球面最短路径的超像素聚类：在球面采集空间上定义像素到超像素中心的最短路径并用于快速提取聚类特征，尊重三维几何，改进距离度量以提升分割与形状规则性，同时对超像素生成过程进行高效实现；另外推广并定义了球面上的全局规则性度量。

Result: 在参考的360°全景分割数据集与合成道路全景图像上，SphSPS在分割准确率、对噪声的鲁棒性和超像素规则性指标上均显著超越当前平面与球面最先进方法。

Conclusion: 该论文提出了一种针对360°球面图像的新超像素分割方法SphSPS，能同时提高分割精度与超像素形状规则性，并提出了球面全局规则性评价指标。实验显示在基准球面全景和合成道路全景数据集上，该方法在精度、抗噪和规则性方面显著优于平面与现有球面方法。

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [37] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: CWA-MSN通过跨孔对齐结合掩码孪生学习，解决批次效应问题，能在更少数据与更小模型下学到更好、更鲁棒的细胞表型表示。


<details>
  <summary>Details</summary>
Motivation: 现有自监督和对比学习方法需大规模模型或海量精心标注数据，且仍受批次效应困扰，难以获得生物学上有意义且批次稳健的细胞表型表征。

Method: 将跨孔对齐目标融入掩码孪生网络架构，通过对来自不同孔但施加相同扰动的细胞样本施加对齐损失，结合掩码重建与孪生一致性训练，实现对批次效应的鲁棒性，同时保持参数与数据高效。

Result: 在基因-基因关系检索基准上，CWA-MSN分别比OpenPhenom和CellCLIP提升约29%和9%，同时使用更少数据（0.2M vs 2.2M图像）和更小模型（22M vs 1.48B参数），显示出在受限数据与参数预算下的优越性。

Conclusion: 提出一种跨孔对齐掩码孪生网络(CWA-MSN)，通过对相同处理的细胞在不同孔位的嵌入对齐，增强语义一致性以减弱批次效应，从而学习细粒度且高效的表型表示。

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [38] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 通过利用航拍稀疏重建与3D Gaussian Splatting渲染中间视图，缓解视角差异，显著提升航拍—地面影像的特征匹配与3D重建与渲染效果。


<details>
  <summary>Details</summary>
Motivation: 航拍与地面影像在视角上差异大，直接匹配可靠对应点困难。本研究动机是通过生成视角居中的中间视图以桥接两者视角差异，从而提高匹配可靠性与重建完整性。

Method: 先仅用航拍影像通过增量SfM恢复稀疏模型，再利用稀疏点与带方向信息的影像输入3D Gaussian Splatting进行场景渲染。设计渲染视点确定算法基于航拍相机位姿生成高质量中间影像；随后在渲染-航拍与渲染-地面图像对上进行特征匹配，并通过中间视图传播对应关系生成最终匹配。

Result: 在真实航拍与地面数据集上实验表明：与常用方法相比，提出方法在初始与精化匹配数目上有明显提升，能提供足够匹配以实现准确的增量SfM重建，并完成基于3D Gaussian Splatting的完整场景渲染。

Conclusion: 提出的方法通过生成中间视图有效缓解航拍与地面影像间的视角畸变，从而显著提升跨视角特征匹配质量，最终支持准确的ISfM重建与基于3D Gaussian Splatting的完整场景渲染。

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [39] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: CapStARE将胶囊网络与双流GRU时序解码相结合，在保持低延迟的同时实现SOTA凝视估计和良好泛化。


<details>
  <summary>Details</summary>
Motivation: 提升凝视估计在实时交互场景下的精度、鲁棒性与可解释性，同时在时序建模上区分不同速率的目光变化，以提高性能和泛化能力。

Method: 采用ConvNeXt骨干提取特征，随后通过注意力路由形成胶囊以实现部件-整体推理，最后用两个GRU解码器分别建模慢速与快速凝视动态；模块化设计使参数更少且易解释。

Result: 在ETH-XGaze上误差3.36，MPIIFaceGaze上2.65，实时推理低于10ms；在Gaze360和RT-GENE上也表现优异（9.06与4.76），参数更少且具更好可解释性。

Conclusion: CapStARE通过胶囊网络与时空解码器结合，实现了在保持实时性的同时提升凝视估计精度，适合交互式系统部署。

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [40] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching在3DGS上通过补丁匹配与替换-融合优化，实现了高效、结构一致的驾驶场景修补，优于基线且无需重训练或依赖2D生成模型。


<details>
  <summary>Details</summary>
Motivation: 驾驶场景中大量重复的模式在隐式3DGS特征空间中具有多模态相似性，适合通过结构匹配实现替换式修补，从而避免依赖2D生成模型的局限并提升效率与一致性。

Method: 构建带特征嵌入的3DGS场景，设计多尺度补丁测量抽象局部上下文，提出结构化搜索在3D空间中匹配候选补丁，最后采用替换与融合的优化策略以提升视觉和谐性。

Result: 在多个公开数据集上，方法在质量和可互操作性方面均优于基线，实验还证明该3D修补策略在通用场景下的适用性，且代码与项目页面已公开。

Conclusion: 本文提出的GS-RoadPatching通过在3D Gaussian Splatting（3DGS）表示下进行替换式修补，实现了驾驶场景的高效完整化，不依赖耗时的2D跨模态一致性或重训练高斯，能在3D层面直接编辑并保持结构一致性。

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [41] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 通过分析神经元与注意力头的计算路径，作者发现可解释且稀疏的“神经元-头”单元，并将其用于无训练语义分割与分布漂移检测。


<details>
  <summary>Details</summary>
Motivation: 传统对神经网络内部可解释性研究通常关注单一神经元或整层激活，作者希望更细粒度地沿计算路径（神经元到注意力头）发现更可解释且可用的结构，以便既解释模型也支持下游任务。

Method: 枚举并分析神经元与注意力池化层注意力头的所有两两组合，将每对（神经元-头）在图文嵌入空间投影为方向并用文本关联进行解释；统计贡献稀疏性与多义性子概念；基于这些对直接构建零训练语义分割和基于贡献的分布漂移监测。

Result: 发现多数神经元-头对可用单一嵌入方向解释，只有稀疏子集对输出有显著贡献，部分对尽管多义但对应神经元的子概念；在CLIP-ResNet上，基于这些对的无训练语义分割优于先前方法，且基于贡献的监测能检测数据分布偏移。

Conclusion: 本文提出通过分解计算路径解释CLIP-ResNet中神经元的贡献，发现神经元-注意力头对可近似为图文嵌入空间中的单一方向，并且只有稀疏子集有显著贡献，从而可用于无训练语义分割与数据分布监测。

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [42] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [43] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa通过融合文本与音频的多模态情感嵌入、音频驱动动作模块及LLM生成的场景描述，克服单模态情感和单图限制，实现更情感丰富、动作自然且时间一致的音频驱动人脸视频生成，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感感知方法通常只使用单一模态（音频或图像）提取情绪，限制了情绪表达的细腻程度；且多依赖单张参考图，难以表示随时间变化的动作和属性。因此需要多模态情感融合与多参考/文本条件以提升表达和时序一致性。

Method: 方法包括：1) 多模态情感嵌入模块，融合文本（情感分析）、音频（情感识别+VA特征）以丰富情绪表示；2) A2M模块将音频转为与音频对齐的头部动作帧，保证头动与唇动自然一致；3) 利用LLM生成场景描述作为额外文本条件，结合视觉条件（多张参考图而非单图）以捕捉动态动作和语义属性；4) 整体生成网络在视觉和文本提示下合成视频帧，并在基准数据集上对比评估。

Result: 在基准数据集的定量评测中，SynchroRaMa在图像质量、表情保留和动作真实性指标上均优于最先进方法；主观用户研究显示在总体自然性、动作多样性和视频平滑性方面得分更高。

Conclusion: 该论文提出了SynchroRaMa框架，通过多模态情感嵌入（文本情感分析、语音情感识别及声学情感特征）以及音频到动作（A2M）模块和LLM生成的场景描述，实现更丰富逼真的情绪驱动口型同步人脸视频生成。实验证明在图像质量、表情保真和动作真实度上优于现有方法，并在主观评估中获得更高评分。

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [44] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: 提出结合视觉-语言和知识蒸馏的类人4D场景理解框架OmniScene，及其层次化多模态融合策略，在nuScenes上实现多任务超越性表现。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶侧重于基于深度的3D重建，缺乏类似人类的场景理解能力；目标是引入语言语义与人类驾驶行为对齐，打造从感知到行动的类人场景理解体系。

Method: 构建OmniVLM（多视角+时间感知的视觉-语言模型）；采用教师-学生结构进行知识蒸馏，将文本语义嵌入到3D实例特征作为语义监督；提出层次化融合策略（HFS），在多个抽象层次上自适应校准几何与语义特征的相对重要性，以动态加权实现多模态信息协同。

Result: 在nuScenes数据集上对比十余个SOTA模型，OmniScene在感知、预测、规划和视觉问答任务上均取得领先，建立了新的基准。

Conclusion: OmniScene提出了一种类人全景感知-理解-决策框架，通过视觉-语言模型、教师-学生知识蒸馏以及分层模态融合，显著提升自动驾驶系统在感知、预测、规划与视问答等任务上的表现。

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


### [45] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: 提出CamPVG：通过全景Plücker嵌入和球面极线自适应注意力模块，在扩散框架内实现精确相机位姿引导的几何一致全景视频生成，性能大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作侧重透视投影视频的相机控制，全景（equirectangular）投影下的几何一致性生成尚未解决，主要困难在于全景位姿表示和球面投影导致的几何复杂性。

Method: 提出全景Plücker嵌入用于编码相机外参（通过球面坐标变换），以及球面极线模块通过沿极线的自适应注意力掩码实现跨视图特征聚合，集成于扩散生成流程。

Result: 实验表明CamPVG在保持与相机轨迹一致性和视频质量方面明显优于现有方法，生成高质量、几何一致的全景视频。

Conclusion: 本文提出CamPVG，一种首创的基于扩散模型的全景视频生成框架，通过精确相机位姿引导，实现几何一致的全景视频生成。

Abstract: Recently, camera-controlled video generation has seen rapid development,
offering more precise control over video generation. However, existing methods
predominantly focus on camera control in perspective projection video
generation, while geometrically consistent panoramic video generation remains
challenging. This limitation is primarily due to the inherent complexities in
panoramic pose representation and spherical projection. To address this issue,
we propose CamPVG, the first diffusion-based framework for panoramic video
generation guided by precise camera poses. We achieve camera position encoding
for panoramic images and cross-view feature aggregation based on spherical
projection. Specifically, we propose a panoramic Pl\"ucker embedding that
encodes camera extrinsic parameters through spherical coordinate
transformation. This pose encoder effectively captures panoramic geometry,
overcoming the limitations of traditional methods when applied to
equirectangular projections. Additionally, we introduce a spherical epipolar
module that enforces geometric constraints through adaptive attention masking
along epipolar lines. This module enables fine-grained cross-view feature
aggregation, substantially enhancing the quality and consistency of generated
panoramic videos. Extensive experiments demonstrate that our method generates
high-quality panoramic videos consistent with camera trajectories, far
surpassing existing methods in panoramic video generation.

</details>


### [46] [SDE-DET: A Precision Network for Shatian Pomelo Detection in Complex Orchard Environments](https://arxiv.org/abs/2509.19990)
*Yihao Hu,Pan Wang,Xiaodong Bai,Shijie Cai,Hang Wang,Huazhong Liu,Aiping Yang,Xiangxiang Li,Meiping Ding,Hongyan Liu,Jianguo Yao*

Main category: cs.CV

TL;DR: 为解决果园中沙田柚多尺度与遮挡等难题，本文构建数据集并提出结合Star Block、可变形注意力与高效多尺度注意力的SDE-DET，达成了在STP-AgriData上的SOTA检测效果。


<details>
  <summary>Details</summary>
Motivation: 沙田柚在果园场景存在尺度变化大、果树干叶遮挡、目标较小等挑战，现有目标检测方法在这种复杂自然场景中效果有限，需提出轻量且能处理遮挡与多尺度问题的专用检测模型。

Method: 构建STP-AgriData数据集；设计SDE-DET检测模型，关键模块包括：Star Block用于高维信息获取而不增加计算量；在主干引入可变形注意力（Deformable Attention）提升遮挡下的检测能力；集成多路高效多尺度注意力（Efficient Multi-Scale Attention）以降低计算开销并加强小目标特征提取。与Yolo系列等主流模型在该数据集上对比评估。

Result: 在STP-AgriData上，SDE-DET取得Precision=0.883、Recall=0.771、mAP@0.5=0.838、mAP@0.5:0.95=0.497、F1=0.823，超过Yolo系列及其他主流检测器，表现为小目标检测与遮挡处理能力提升同时保持较低计算开销。

Conclusion: 该文提出的SDE-DET模型针对沙田柚在复杂果园环境下的检测问题，取得了在自建数据集STP-AgriData上的SOTA性能，特别在精度和小目标检测方面有明显提升，验证了其在自动化采摘等应用中的可行性。

Abstract: Pomelo detection is an essential process for their localization, automated
robotic harvesting, and maturity analysis. However, detecting Shatian pomelo in
complex orchard environments poses significant challenges, including
multi-scale issues, obstructions from trunks and leaves, small object
detection, etc. To address these issues, this study constructs a custom dataset
STP-AgriData and proposes the SDE-DET model for Shatian pomelo detection.
SDE-DET first utilizes the Star Block to effectively acquire high-dimensional
information without increasing the computational overhead. Furthermore, the
presented model adopts Deformable Attention in its backbone, to enhance its
ability to detect pomelos under occluded conditions. Finally, multiple
Efficient Multi-Scale Attention mechanisms are integrated into our model to
reduce the computational overhead and extract deep visual representations,
thereby improving the capacity for small object detection. In the experiment,
we compared SDE-DET with the Yolo series and other mainstream detection models
in Shatian pomelo detection. The presented SDE-DET model achieved scores of
0.883, 0.771, 0.838, 0.497, and 0.823 in Precision, Recall, mAP@0.5,
mAP@0.5:0.95 and F1-score, respectively. SDE-DET has achieved state-of-the-art
performance on the STP-AgriData dataset. Experiments indicate that the SDE-DET
provides a reliable method for Shatian pomelo detection, laying the foundation
for the further development of automatic harvest robots.

</details>


### [47] [Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](https://arxiv.org/abs/2509.19994)
*Zhifang Zhang,Jiahan Zhang,Shengjie Zhou,Qi Wei,Shuo He,Feng Liu,Lei Feng*

Main category: cs.CV

TL;DR: 提出PTA，通过多代理优化定向对抗样本，显著提升对多模态预训练模型攻击的泛化性与不可检测性，理论与实验均支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对多模态预训练模型（如ImageBind）的定向对抗攻击在面对部分已知或语义相似目标时泛化性不足，且容易被简单异常检测方法识别，存在安全风险，需要提升攻击的泛化性与不可检测性。

Method: 提出Proxy Targeted Attack (PTA)，通过使用多个源模态与目标模态代理（proxies）联合优化目标化对抗样本，使其同时对多个潜在目标对齐并对抗检测器。包含理论分析以平衡泛化性与不可检测性的权衡，并在不同异常检测器上进行实验证明。

Result: PTA在多种相关目标上达到较高攻击成功率，并能有效规避多种异常检测方法的检测，显示出良好的泛化性与不可检测性。

Conclusion: 该论文提出的Proxy Targeted Attack (PTA) 有效提升了对多模态预训练模型的定向对抗攻击在泛化性和不可检测性方面的表现，通过多源模态和多目标模态代理进行优化，使对抗样本对相关目标保持高命中率并绕过异常检测器。理论分析给出泛化性与不可检测性之间的关系，并指导达到最优泛化性的条件。实验在若干异常检测方法和相关目标上验证了PTA的有效性。

Abstract: Multimodal pre-trained models (e.g., ImageBind), which align distinct data
modalities into a shared embedding space, have shown remarkable success across
downstream tasks. However, their increasing adoption raises serious security
concerns, especially regarding targeted adversarial attacks. In this paper, we
show that existing targeted adversarial attacks on multimodal pre-trained
models still have limitations in two aspects: generalizability and
undetectability. Specifically, the crafted targeted adversarial examples (AEs)
exhibit limited generalization to partially known or semantically similar
targets in cross-modal alignment tasks (i.e., limited generalizability) and can
be easily detected by simple anomaly detection methods (i.e., limited
undetectability). To address these limitations, we propose a novel method
called Proxy Targeted Attack (PTA), which leverages multiple source-modal and
target-modal proxies to optimize targeted AEs, ensuring they remain evasive to
defenses while aligning with multiple potential targets. We also provide
theoretical analyses to highlight the relationship between generalizability and
undetectability and to ensure optimal generalizability while meeting the
specified requirements for undetectability. Furthermore, experimental results
demonstrate that our PTA can achieve a high success rate across various related
targets and remain undetectable against multiple anomaly detection methods.

</details>


### [48] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 文章用DINOv2特征+DPMM替代内存库进行医学影像无监督异常检测，效果优且更快。


<details>
  <summary>Details</summary>
Motivation: 传统基于内存库的特征匹配在小数据集有效，但在大规模医学影像数据上计算代价高昂；因此希望用可自适应组件数量的非参数混合模型压缩和建模正常特征分布，减少存储和计算开销。

Method: 提取DINOv2预训练模型的嵌入，采用DPMM对正常样本的嵌入分布建模，使用每个输入嵌入与DPMM成分中心相似度作为异常评分，生成粗略异常分割掩码；比较归一化与非归一化嵌入的效果，并评估计算性能与检测性能。

Result: 使用DINOv2嵌入配合DPMM在医学影像基准上取得了有竞争力的异常检测效果，同时推理时间至少减少一半；归一化的DINOv2嵌入对解剖结构更敏感，有利于定位异常。

Conclusion: 该论文提出使用DINOv2特征与Dirichlet过程混合模型（DPMM）结合，用于医学影像的无监督异常检测，替代内存库方法，从而显著降低大规模数据集推理时的计算开销，同时保持竞争性的检测性能。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [49] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: 结合不确定性与多样性的主动学习在表格检测中能减少标注成本并提升mAP，优于随机采样并接近全监督性能。


<details>
  <summary>Details</summary>
Motivation: 对象检测尤其是表格检测需要大量标注，标注成本高昂；传统主动学习多依赖不确定性导致样本冗余，结合多样性可以选择更具代表性的样本，从而提高标注效率与模型泛化能力。

Method: 在两种基准数据集（TableBank-LaTeX、TableBank-Word）上，使用CascadeTabNet与YOLOv9两种表格检测架构，采用基于不确定性评估与样本多样性度量的混合采样策略来选择待标注样本；随后用选中样本对模型进行迭代训练与评估，并与随机采样及全监督基线比较。

Result: 在相同的标注预算下，主动学习策略相比随机采样能显著提高mAP，实验在两个数据集与两种检测器上均验证了该结论，且性能接近全监督模型。

Conclusion: 本文提出的将不确定性与多样性结合的主动学习方法在表格检测任务中有效，能在有限标注预算下达到接近全监督模型的性能并优于随机采样。

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [50] [Does the Manipulation Process Matter? RITA: Reasoning Composite Image Manipulations via Reversely-Ordered Incremental-Transition Autoregression](https://arxiv.org/abs/2509.20006)
*Xuekang Zhu,Ji-Zhe Zhou,Kaiwen Feng,Chenfan Qu,Yunfei Wang,Liting Zhou,Jian liu*

Main category: cs.CV

TL;DR: 提出将图像篡改定位视为条件序列预测的RITA，逐层建模编辑过程并配套HSIM数据集与HSS评测，解决一镜到底的维度塌缩问题并达成SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用一次性二值掩码预测，忽视篡改过程的顺序性与层次性，导致高维组合空间被压缩并产生维度塌缩，与任务本质不符。

Method: RITA以逐层有序预测被篡改区域为核心，每一步的输出作为下一步的条件输入，显式建模编辑操作之间的时序依赖与层次关系；为训练构建多步篡改合成数据并设计新基准HSIM，提出评估顺序与层次对齐的HSS指标。

Result: 在传统基准上取得SOTA结果，并在新提出的层次定位任务上表现良好，证明RITA作为通用有效范式的潜力。

Conclusion: 本文将图像篡改定位重构为条件序列预测，提出RITA框架，通过逐步预测编辑层实现对时序和层次结构的建模，有效缓解一镜到底的维度塌缩问题。

Abstract: Image manipulations often entail a complex manipulation process, comprising a
series of editing operations to create a deceptive image, exhibiting
sequentiality and hierarchical characteristics. However, existing IML methods
remain manipulation-process-agnostic, directly producing localization masks in
a one-shot prediction paradigm without modeling the underlying editing steps.
This one-shot paradigm compresses the high-dimensional compositional space into
a single binary mask, inducing severe dimensional collapse, thereby creating a
fundamental mismatch with the intrinsic nature of the IML task.
  To address this, we are the first to reformulate image manipulation
localization as a conditional sequence prediction task, proposing the RITA
framework. RITA predicts manipulated regions layer-by-layer in an ordered
manner, using each step's prediction as the condition for the next, thereby
explicitly modeling temporal dependencies and hierarchical structures among
editing operations.
  To enable training and evaluation, we synthesize multi-step manipulation data
and construct a new benchmark HSIM. We further propose the HSS metric to assess
sequential order and hierarchical alignment. Extensive experiments show RITA
achieves SOTA on traditional benchmarks and provides a solid foundation for the
novel hierarchical localization task, validating its potential as a general and
effective paradigm. The code and dataset will be publicly available.

</details>


### [51] [PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](https://arxiv.org/abs/2509.20022)
*Manahil Raza,Ayesha Azam,Talha Qaiser,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 提出PS3：用诊断/组织学/生物通路三类原型将病理报告、WSI与转录组标准化为可融合Token，送入Transformer建模交互，实现更准确的生存预测并优于TCGA基线。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法多整合WSI与基因/转录数据，病理报告作为临床中易得且含诊断与背景信息的补充尚未充分利用，因模态异质性（图像超高维 vs 文本简洁）导致融合不平衡，需要方法平衡表征从而提升预后预测。

Method: 构建三类原型：诊断原型（自注意力提取报告中诊断相关段落并标准化文本表征）、组织学原型（压缩WSI中的关键形态模式）、生物通路原型（编码转录组以反映细胞功能）；将原型作为Token输入三模态Transformer，建模模内与跨模态交互进行生存风险预测。

Result: 在TCGA六个数据集上，PS3优于临床、单模态及其他多模态基线方法，实验证明原型化表征有助于缓解模态不平衡并提升预测性能。

Conclusion: 本文提出PS3，通过将病理报告、WSI与转录组的原型表示送入Transformer融合，提高了生存预测性能，优于多项TCGA基线方法。

Abstract: Current multimodal fusion approaches in computational oncology primarily
focus on integrating multi-gigapixel histology whole slide images (WSIs) with
genomic or transcriptomic data, demonstrating improved survival prediction. We
hypothesize that incorporating pathology reports can further enhance prognostic
performance. Pathology reports, as essential components of clinical workflows,
offer readily available complementary information by summarizing
histopathological findings and integrating expert interpretations and clinical
context. However, fusing these modalities poses challenges due to their
heterogeneous nature. WSIs are high-dimensional, each containing several
billion pixels, whereas pathology reports consist of concise text summaries of
varying lengths, leading to potential modality imbalance. To address this, we
propose a prototype-based approach to generate balanced representations, which
are then integrated using a Transformer-based fusion model for survival
prediction that we term PS3 (Predicting Survival from Three Modalities).
Specifically, we present: (1) Diagnostic prototypes from pathology reports,
leveraging self-attention to extract diagnostically relevant sections and
standardize text representation; (2) Histological prototypes to compactly
represent key morphological patterns in WSIs; and (3) Biological pathway
prototypes to encode transcriptomic expressions, accurately capturing cellular
functions. PS3, the three-modal transformer model, processes the resulting
prototype-based multimodal tokens and models intra-modal and cross-modal
interactions across pathology reports, WSIs and transcriptomic data. The
proposed model outperforms state-of-the-art methods when evaluated against
clinical, unimodal and multimodal baselines on six datasets from The Cancer
Genome Atlas (TCGA). The code is available at: https://github.com/manahilr/PS3.

</details>


### [52] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 用GAN把人脸变成花朵/鞋子等视觉私密域，训练分类器在该域进行认证，兼顾隐私保护与识别性能，并抵抗攻击。


<details>
  <summary>Details</summary>
Motivation: 现有生物特征认证系统无法让用户控制其数据使用，且生物特征数据泄露会被滥用；因此需要一种既保护隐私又保持认证功能的方法。

Method: 利用生成对抗网络（GAN）进行图像转换：将人脸图像翻译成视觉私密域图像；在翻译后图像上训练分类器用于认证；评估攻击鲁棒性和认证效用。

Result: 实验表明，该方法在保护隐私的同时仍能提供有意义的认证效果，并对攻击表现出鲁棒性。

Conclusion: 该方法通过使用GAN将人脸映射到视觉上无关的域（如花朵、鞋子），在保护隐私同时保持认证可用性，展示了对攻击的鲁棒性和合理的识别性能。

Abstract: Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.

</details>


### [53] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 提出任务感知的轻量级帧质量预测，用于筛选送入oracle的帧并提高图形防伪验证鲁棒性；实验证明冻结的预训练主干在跨印刷域泛化上优于全微调。


<details>
  <summary>Details</summary>
Motivation: 手机上对高熵防伪图形的随意拍摄导致高误拒率，传统感知图像质量评价（IQA）不足以反映帧对验证任务的实际效用，需要一种任务感知的质量评估方法来弥补“可靠性差距”。

Method: 构建一个轻量级质量预测模型，为视频帧打分以决定是否送入资源密集的oracle验证模型；采用重新定义的FNMR和ISRR指标在包含105款手机、32000+图像的大规模数据集上进行验证；进行跨域实验，将冻结的ImageNet预训练主干与全量微调模型比较。

Result: 在大规模数据集和跨印刷技术实验中，所提出的轻量级探针能够有效筛选出有用帧，降低误拒率；重要发现是：在跨印刷域转移情形下，冻结的ImageNet预训练主干配合轻量探针比完全微调模型具有更好的泛化能力，避免对源域制造工艺伪影的过拟合。

Conclusion: 本文提出了一种用于安全图形验证的视频帧质量预测框架，通过轻量级模型评估帧对下游验证器的实用性，从而降低因手机拍摄质量差导致的误识率。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [54] [SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads](https://arxiv.org/abs/2509.20073)
*Yuxi Zheng,Jianhui Feng,Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: 本文首次将Mixture of Experts机制引入可变形图像配准，编码器用MoA增强特征专门化，解码器用SHMoE实现方向上异质位移预测，显著提升Dice并提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有编码器-解码器DIR方法缺乏针对配准任务的专门化特征提取，且在三方向上以同质方式联合预测位移，限制了性能与解释性。

Method: 在编码器层引入Mixture of Attention (MoA)，为每个图像token动态选择最优注意力头组合；在解码器层引入Spatial Heterogeneous Mixture of Experts (SHMoE)，使用不同卷核尺寸的专家为每个体素在三个方向上分别预测位移。

Result: 在两个公开数据集上进行大量实验，均优于多种方法；在腹部CT数据集上Dice从60.58%提升至65.58%。模型还能通过专家在不同分辨率层的差异化使用提高可解释性。

Conclusion: 提出的SHMoAReg通过在编码器和解码器中引入MoE机制，改善了特征提取专门化与位移场预测的异质性，从而提升了可变形图像配准性能与可解释性。

Abstract: Encoder-Decoder architectures are widely used in deep learning-based
Deformable Image Registration (DIR), where the encoder extracts multi-scale
features and the decoder predicts deformation fields by recovering spatial
locations. However, current methods lack specialized extraction of features
(that are useful for registration) and predict deformation jointly and
homogeneously in all three directions. In this paper, we propose a novel
expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in
both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture
of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous
Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the
specialization of feature extraction by dynamically selecting the optimal
combination of attention heads for each image token. Meanwhile, the SHMoE
predicts deformation fields heterogeneously in three directions for each voxel
using experts with varying kernel sizes. Extensive experiments conducted on two
publicly available datasets show consistent improvements over various methods,
with a notable increase from 60.58% to 65.58% in Dice score for the abdominal
CT dataset. Furthermore, SHMoAReg enhances model interpretability by
differentiating experts' utilities across/within different resolution layers.
To the best of our knowledge, we are the first to introduce MoE mechanism into
DIR tasks. The code will be released soon.

</details>


### [55] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 利用冻结预训练扩散模型不同时间步的潜在语义表示作为去雾网络的指导，提出了无需重训练与迭代采样的DiffLI^2D，实验证明其在性能与效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在图像去雾方面有强大的建模能力，但重新训练扩散模型代价高且推理需大量采样步骤，限制了实际应用；因此希望利用预训练扩散模型的潜在表示来规避这些问题。

Method: 提取冻结预训练扩散模型在不同时间步的语义潜在表示，分析其随时间步变化所承载的雾与图像内容信息，然后将这些多时刻潜在表示输入到设计的去雾网络（DiffLI^2D）作为指导，从而实现去雾而无需重训练扩散模型或迭代采样。

Result: 通过在多个去雾数据集上的大量实验，DiffLI^2D在定量指标和视觉效果上均优于现有图像去雾方法，同时显著减少了计算开销（无需重训练与采样迭代）。

Conclusion: 本文提出利用冻结的预训练扩散模型的语义潜在空间特性，构建无需重新训练扩散模型且免去迭代采样的图像去雾方法DiffLI^2D，证明了不同扩散时间步的潜在表示能编码雾与内容信息，并将其整合进专门设计的去雾网络，从而在多数据集上超越现有方法。

Abstract: Diffusion models have recently been investigated as powerful generative
solvers for image dehazing, owing to their remarkable capability to model the
data distribution. However, the massive computational burden imposed by the
retraining of diffusion models, coupled with the extensive sampling steps
during the inference, limit the broader application of diffusion models in
image dehazing. To address these issues, we explore the properties of hazy
images in the semantic latent space of frozen pre-trained diffusion models, and
propose a Diffusion Latent Inspired network for Image Dehazing, dubbed
DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of
pre-trained diffusion models can represent the content and haze characteristics
of hazy images, as the diffusion time-step changes. Building upon this insight,
we integrate the diffusion latent representations at different time-steps into
a delicately designed dehazing network to provide instructions for image
dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative
sampling process by effectively utilizing the informative representations
derived from the pre-trained diffusion models, which also offers a novel
perspective for introducing diffusion models to image dehazing. Extensive
experiments on multiple datasets demonstrate that the proposed method achieves
superior performance to existing image dehazing methods. Code is available at
https://github.com/aaaasan111/difflid.

</details>


### [56] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出了一种高光谱适配器，结合光谱Transformer、空间先验和模态交互模块，利用冻结的预训练视觉Transformer实现HSI语义分割的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有高光谱语义分割方法大多沿用为RGB设计的架构和训练范式，不能充分利用高光谱富含的信息，导致性能不足。论文旨在借助预训练的视觉模型弥补数据和架构的不足，从而提升HSI分割效果。

Method: 构建了包含光谱Transformer、光谱感知空间先验模块和模态感知交互模块的网络。光谱Transformer用于提取光谱信息，空间先验模块整合光谱与空间上下文，交互模块通过抽取与注入机制将高光谱特征与冻结的视觉Transformer特征对齐和融合。

Result: 在三个自动驾驶基准数据集上进行了广泛评估，结果显示该方法在直接以高光谱输入的条件下，超过了现有基于视觉和高光谱的分割方法，达到了最先进水平。

Conclusion: 该论文提出了一个针对高光谱输入的适配器架构，通过利用预训练视觉基础模型和专门设计的光谱-空间模块，实现了对高光谱数据的有效建模和语义分割性能提升。

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [57] [A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](https://arxiv.org/abs/2509.20119)
*Belal Shoer,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 通过合成text-in-image训练数据并对小型多模态模型进行微调，显著提升了科学可视化问答在多语言上的表现。


<details>
  <summary>Details</summary>
Motivation: EXAMS-V将视觉与文本内容合并为单一图像，但零样本下即便是最先进模型也表现不佳，说明需要任务特定的微调；然而text-in-image格式训练数据稀缺，因此通过合成数据来填补这一空白。

Method: 将现有的图像-文本配对数据转换为嵌入文本的图像（text-in-image）以生成合成数据集，并用这些合成数据与EXAMS-V混合微调小型多语种多模态模型。

Result: 在13种语言上均有显著提升，平均性能和跨语种迁移效果良好。

Conclusion: 本论文提出通过将原本分离的图像与文本合成为单一"text-in-image"图像并合成训练数据，来改进科学可视化问答在多语言场景下的表现。

Abstract: Scientific visual question answering poses significant challenges for
vision-language models due to the complexity of scientific figures and their
multimodal context. Traditional approaches treat the figure and accompanying
text (e.g., questions and answer options) as separate inputs. EXAMS-V
introduced a new paradigm by embedding both visual and textual content into a
single image. However, even state-of-the-art proprietary models perform poorly
on this setup in zero-shot settings, underscoring the need for task-specific
fine-tuning. To address the scarcity of training data in this "text-in-image"
format, we synthesize a new dataset by converting existing separate image-text
pairs into unified images. Fine-tuning a small multilingual multimodal model on
a mix of our synthetic data and EXAMS-V yields notable gains across 13
languages, demonstrating strong average improvements and cross-lingual
transfer.

</details>


### [58] [EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](https://arxiv.org/abs/2509.20146)
*Botai Yuan,Yutian Zhou,Yingjie Wang,Fushuo Huo,Yongcheng Jing,Li Shen,Ying Wei,Zhiqi Shen,Ziwei Liu,Tianwei Zhang,Jie Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: EchoBench揭示医疗LVLM普遍存在盲目附和用户偏置信息的问题，并提供数据与提示层面的可行缓解策略，强调评测需超越准确率以保证安全可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前医疗LVLM评测侧重榜单准确率，忽视了模型在安全性和可靠性方面（如盲目附和用户错误信息）的风险，作者希望通过标准化基准暴露并改善这一问题。

Method: 构建包含2,122张图像、18个科室、20种模态和90个模拟偏倚输入的基准，针对患者、医学生和医生三类偏倚来源设计提示，评估多类医疗专用、开源和闭源LVLM；并通过细分偏倚类型、科室、感知粒度和模态做深度分析，同时测试提示工程（负面提示、one-shot、few-shot）作为缓解手段。

Result: 发现所有评估模型均存在显著附和性：最好闭源模型Claude 3.7 Sonnet附和率45.98%，GPT-4.1为59.15%，而许多医疗专用模型超过95%附和率且准确率仅中等。提高训练数据质量/多样性和领域知识能降低附和性，不影响无偏准确率；提示级干预可稳定减少附和行为。

Conclusion: 作者提出了EchoBench，系统评估医疗大视觉-语言模型(LVLM)在临床高风险场景中的“附和性”(sycophancy)，并发现多种模型普遍存在问题。

Abstract: Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize
leaderboard accuracy, overlooking reliability and safety. We study sycophancy
-- models' tendency to uncritically echo user-provided information -- in
high-stakes clinical settings. We introduce EchoBench, a benchmark to
systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images
across 18 departments and 20 modalities with 90 prompts that simulate biased
inputs from patients, medical students, and physicians. We evaluate
medical-specific, open-source, and proprietary LVLMs. All exhibit substantial
sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%
sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%
sycophancy despite only moderate accuracy. Fine-grained analyses by bias type,
department, perceptual granularity, and modality identify factors that increase
susceptibility. We further show that higher data quality/diversity and stronger
domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench
also serves as a testbed for mitigation: simple prompt-level interventions
(negative prompting, one-shot, few-shot) produce consistent reductions and
motivate training- and decoding-time strategies. Our findings highlight the
need for robust evaluation beyond accuracy and provide actionable guidance
toward safer, more trustworthy medical LVLMs.

</details>


### [59] [Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning](https://arxiv.org/abs/2509.20148)
*Sanish Suwal,Shaurya Garg,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文系统比较了自然训练、对抗训练和剪枝对交通标志分类器后验解释质量的影响，结论是剪枝能显著提高显著性图的可解释性与忠实性，并提高模型效率，适合资源受限的车辆AI。


<details>
  <summary>Details</summary>
Motivation: CAVs依赖黑箱式AI，后验解释的可靠性不足，质疑解释一致性和忠实性；研究意在理解训练方法对解释质量的影响，从而提高透明性与安全性。

Method: 作者对比了自然训练、对抗训练和剪枝三种方法，使用广泛的定量指标和基于显著性图的后验解释技术，在交通标志分类任务上进行大规模实证评估。

Result: 实验证明剪枝在显著性图的可理解性与忠实性指标上明显优于自然训练与对抗训练；剪枝同时带来模型稀疏性和推理效率提升。

Conclusion: 剪枝能显著提升交通标志分类器的后验可解释性和忠实度，同时提高效率，是构建资源受限车辆AI系统透明模型的有效策略。

Abstract: Connected and autonomous vehicles continue to heavily rely on AI systems,
where transparency and security are critical for trust and operational safety.
Post-hoc explanations provide transparency to these black-box like AI models
but the quality and reliability of these explanations is often questioned due
to inconsistencies and lack of faithfulness in representing model decisions.
This paper systematically examines the impact of three widely used training
approaches, namely natural training, adversarial training, and pruning, affect
the quality of post-hoc explanations for traffic sign classifiers. Through
extensive empirical evaluation, we demonstrate that pruning significantly
enhances the comprehensibility and faithfulness of explanations (using saliency
maps). Our findings reveal that pruning not only improves model efficiency but
also enforces sparsity in learned representation, leading to more interpretable
and reliable decisions. Additionally, these insights suggest that pruning is a
promising strategy for developing transparent deep learning models, especially
in resource-constrained vehicular AI systems.

</details>


### [60] [C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://arxiv.org/abs/2509.20152)
*Min Cen,Zhenfeng Zhuang,Yuzhe Zhang,Min Zeng,Baptiste Magnier,Lequan Yu,Hong Zhang,Liansheng Wang*

Main category: cs.CV

TL;DR: 提出C^2MIL：通过语义解缠和伯努利可微子图采样的双因果框架，改进WSI上基于图的MIL在生存分析中的泛化与可解释性。


<details>
  <summary>Details</summary>
Motivation: H&E染色与扫描变异引入语义偏差，且与因果关系无关的拓扑子图增加噪声，导致图基MIL在WSI生存分析中可解释性和泛化性受限。

Method: 构建双重结构因果模型为理论基础；提出跨尺度自适应特征解缠模块用于语义因果干预；提出伯努利可微分子图采样用于拓扑因果发现；结合解缠监督与对比学习的联合优化策略同时精炼语义和拓扑因果性。

Result: 实验表明C^2MIL在泛化性与可解释性上优于现有方法，并可作为对多种MIL基线的因果增强模块。作者公开了代码。

Conclusion: 该文提出了C^2MIL，一种结合语义与拓扑因果发现的可解释图表征多实例学习方法，旨在消除染色/扫描语义偏差和无关子图噪声，从而提升WSI病理图像的生存分析的泛化与可解释性。

Abstract: Graph-based Multiple Instance Learning (MIL) is widely used in survival
analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs)
due to its ability to capture topological information. However, variations in
staining and scanning can introduce semantic bias, while topological subgraphs
that are not relevant to the causal relationships can create noise, resulting
in biased slide-level representations. These issues can hinder both the
interpretability and generalization of the analysis. To tackle this, we
introduce a dual structural causal model as the theoretical foundation and
propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.
C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module
for semantic causal intervention and a new Bernoulli differentiable causal
subgraph sampling method for topological causal discovery. A joint optimization
strategy combining disentangling supervision and contrastive learning enables
simultaneous refinement of both semantic and topological causalities.
Experiments demonstrate that C$^2$MIL consistently improves generalization and
interpretability over existing methods and can serve as a causal enhancement
for diverse MIL baselines. The code is available at
https://github.com/mimic0127/C2MIL.

</details>


### [61] [U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT](https://arxiv.org/abs/2509.20154)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出U-Mamba2-SSL：自监督预训练+一致性正则化+降权伪标签的半监督框架，用于CBCT牙齿与牙髓分割，在验证集上达DSC 0.969，效果显著。


<details>
  <summary>Details</summary>
Motivation: 标注CBCT牙齿与牙髓切分需要大量专业知识且耗时，故希望通过半监督方法有效利用未标注数据以减少对人工标注的依赖。

Method: 方法为多阶段半监督学习：1) 使用破坏性自编码器对U-Mamba2进行自监督预训练；2) 利用一致性正则化结合输入扰动与特征扰动在未标注数据上训练以稳定输出；3) 采用伪标签并对其损失进行降低权重以减少错误影响。

Result: 在验证集上取得平均分0.872和DSC 0.969，表现优越；并公布代码实现。

Conclusion: 该论文提出的U-Mamba2-SSL在牙齿和牙髓CBCT分割任务中通过自监督预训练、输入与特征扰动的一致性正则化以及降权伪标签策略有效利用未标注数据，实现了优越性能。

Abstract: Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography
(CBCT) is vital for clinical applications like treatment planning and
diagnosis. However, this process requires extensive expertise and is
exceptionally time-consuming, highlighting the critical need for automated
algorithms that can effectively utilize unlabeled data. In this paper, we
propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on
the U-Mamba2 model and employs a multi-stage training strategy. The framework
first pre-trains U-Mamba2 in a self-supervised manner using a disruptive
autoencoder. It then leverages unlabeled data through consistency
regularization, where we introduce input and feature perturbations to ensure
stable model outputs. Finally, a pseudo-labeling strategy is implemented with a
reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL
achieved an average score of 0.872 and a DSC of 0.969 on the validation
dataset, demonstrating the superior performance of our approach. The code is
available at https://github.com/zhiqin1998/UMamba2.

</details>


### [62] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 提出一种在受控水槽中通过校准添加剂复现实测水下光学条件的框架，生成带地面真值的可重复数据，便于系统评估和比较水下视觉算法的性能和泛化。


<details>
  <summary>Details</summary>
Motivation: 现有水下机器视觉评估依赖特定开放水域或合成数据，难以涵盖不同水体光学特性且缺乏可重复性，导致方法在真实场景中泛化性差。需要可控、带地面真值的测试环境来系统研究水体成分对图像外观及视觉任务的影响。

Method: 在受控环境（水槽）中调配不同光学成分（色度和散射剂），通过系统化配方生成不同海水类型和成像条件下的图像，采集带有地面真值的数据用于多种视觉任务评估；提供示范数据集并演示两个视觉任务的应用。

Result: 构建了可控的光学海洋配方系统，生成了示范数据集并展示了在水参数估计、图像复原、语义分割、SLAM及图像合成等任务中的应用潜力；将公开数据集与评估代码。

Conclusion: 本文提出了“Optical Ocean Recipes”框架，通过在受控水槽中使用校准的颜色和散射添加剂，生成真实且可重复的水下图像数据集，从而提升水下机器视觉研究的可控性与可比性。

Abstract: The development and evaluation of machine vision in underwater environments
remains challenging, often relying on trial-and-error-based testing tailored to
specific applications. This is partly due to the lack of controlled,
ground-truthed testing environments that account for the optical challenges,
such as color distortion from spectrally variant light attenuation, reduced
contrast and blur from backscatter and volume scattering, and dynamic light
patterns from natural or artificial illumination. Additionally, the appearance
of ocean water in images varies significantly across regions, depths, and
seasons. However, most machine vision evaluations are conducted under specific
optical water types and imaging conditions, therefore often lack
generalizability. Exhaustive testing across diverse open-water scenarios is
technically impractical. To address this, we introduce the \textit{Optical
Ocean Recipes}, a framework for creating realistic datasets under controlled
underwater conditions. Unlike synthetic or open-water data, these recipes,
using calibrated color and scattering additives, enable repeatable and
controlled testing of the impact of water composition on image appearance.
Hence, this provides a unique framework for analyzing machine vision in
realistic, yet controlled underwater scenarios. The controlled environment
enables the creation of ground-truth data for a range of vision tasks,
including water parameter estimation, image restoration, segmentation, visual
SLAM, and underwater image synthesis. We provide a demonstration dataset
generated using the Optical Ocean Recipes and briefly demonstrate the use of
our system for two underwater vision tasks. The dataset and evaluation code
will be made available.

</details>


### [63] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出UCA：一种基于特征空间与特征散度损失的通用伪装攻击，产生物理可实现的纹理，对VLM-AD有强泛化性与鲁棒性，攻击成功率显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击多集中在数字域或针对低层视觉模块，难以直接转移到视觉语言自动驾驶系统；因此需要一种物理可实现且能跨命令、跨模型泛化的攻击方法。

Method: UCA在特征空间进行优化，设计了特征散度损失（FDL）以扩大干净与对抗样本的表示差异；同时采用多尺度学习与采样比例调整以提高对尺度和视角变化的适应性，训练生成可物理打印的伪装纹理。

Result: 实验表明UCA在多种VLM-AD模型与驾驶场景中能显著提高攻击成功率，相较于现有最先进方法在3-P指标上提升约30%，并在多视角与动态条件下表现出稳健性。

Conclusion: 论文提出了首个面向自动驾驶视觉语言模型（VLM-AD）的通用伪装攻击（UCA）框架，通过在特征空间中最大化干净图像与对抗图像的表示差异，生成可物理实现且具强泛化能力的伪装纹理，能够在多模型、多指令和多场景下诱导错误驾驶决策。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [64] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: 提出将局部邻域建模为各向异性高斯并在几何域直接采样的上采样网络，结合细化网络实现SOTA点云上采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法在隐式特征上采样或距离函数学习间常牺牲几何可解释性或对稀疏输入的鲁棒性，需一种既具可解释性又对稀疏输入稳健的上采样方法。

Method: 使用各向异性3D高斯分布拟合每个点的局部邻域，在几何域内直接采样生成密集粗糙点云，随后用细化网络提升均匀性和边缘锐利度。

Result: 在PU1K和PUGAN数据集上进行了广泛测试，PU-Gaussian取得了最先进的性能，并公开了代码与模型权重。

Conclusion: PU-Gaussian通过对局部邻域建模为各向异性3D高斯分布，在几何上更具解释性并对稀疏输入更鲁棒，能生成高质量上采样点云。

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing
challenges for tasks requiring dense and high-fidelity 3D representations.
Prior work has explored both implicit feature-based upsampling and
distance-function learning to address this, but often at the expense of
geometric interpretability or robustness to input sparsity. To overcome these
limitations, we propose PU-Gaussian, a novel upsampling network that models the
local neighborhood around each point using anisotropic 3D Gaussian
distributions. These Gaussians capture the underlying geometric structure,
allowing us to perform upsampling explicitly in the local geometric domain by
direct point sampling. The sampling process generates a dense, but coarse,
point cloud. A subsequent refinement network adjusts the coarse output to
produce a more uniform distribution and sharper edges. We perform extensive
testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves
state-of-the-art performance. We make code and model weights publicly available
at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [65] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 作者提出一种通过系统抑制形状/纹理/颜色线索的无域框架来量化模型特征依赖，结果表明CNN主要依赖局部形状而非纹理，且依赖模式随训练策略、架构和应用领域而异。


<details>
  <summary>Details</summary>
Motivation: 质疑并重审Geirhos等人提出的‘CNN纹理偏好’假设，纠正其cue-conflict实验的局限，提供更可靠的依赖性测量方法。

Method: 设计域无关的特征依赖量化框架，通过在图像中系统地抑制形状、纹理、颜色线索（非强制冲突）并在受控条件下评估人类与模型，比较不同训练策略与架构（如ConvNeXt、ViT）对依赖性的影响，同时跨三个领域做广泛实验。

Result: 发现CNN并非天生纹理偏好，而是主要依赖局部形状特征；现代训练方法和先进架构能显著降低该依赖；且不同领域模型依赖模式不同：计算机视觉偏好形状，医疗影像偏好颜色，遥感偏好纹理。

Conclusion: 作者否定CNN天生偏向纹理的观点，提出系统性抑制形状、纹理、颜色线索的无域框架，结论是CNN更依赖局部形状且训练与架构能减弱该依赖，不同行业（CV、医疗、遥感）依赖模式不同。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


### [66] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 利用高分辨率平面纹理通过多参考非局部注意力模块迁移到低分辨率通过面图像，从而显著提升CT体数据的切片插值效果。


<details>
  <summary>Details</summary>
Motivation: 临床CT常因存储和扫描时间成本采用较大层厚，导致体数据在纵向分辨率远低于平面分辨率，影响诊断；现有超分或插值方法未充分利用3D体数据的各向异性特征，故提出利用高分辨率平面信息改善通过面分辨率。

Method: 构建了一个独特的框架，利用多参考非局部注意力模块从多个平面图像中提取有意义的特征，并将高频纹理信息传递到通过面切片以重建细节。整体为一种基于深度学习的跨视图纹理迁移策略用于切片插值。

Result: 在多个公开CT数据集（包括真实配对基准）上做了广泛实验，结果显示该方法在CT切片插值任务上显著优于现有竞争方法，验证了所提框架的有效性。

Conclusion: 本文提出了一种基于跨视图纹理迁移的CT切片插值方法，通过将高分辨率平面纹理作为参考并迁移到低分辨率的纵向切片，有效提升了体数据的通过面分辨率，从而改善了由厚层切片引起的各向异性问题。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging
modalities for medical diagnosis. In clinical practice, CT images are usually
acquired with large slice thicknesses due to the high cost of memory storage
and operation time, resulting in an anisotropic CT volume with much lower
inter-slice resolution than in-plane resolution. Since such inconsistent
resolution may lead to difficulties in disease diagnosis, deep learning-based
volumetric super-resolution methods have been developed to improve inter-slice
resolution. Most existing methods conduct single-image super-resolution on the
through-plane or synthesize intermediate slices from adjacent slices; however,
the anisotropic characteristic of 3D CT volume has not been well explored. In
this paper, we propose a novel cross-view texture transfer approach for CT
slice interpolation by fully utilizing the anisotropic nature of 3D CT volume.
Specifically, we design a unique framework that takes high-resolution in-plane
texture details as a reference and transfers them to low-resolution
through-plane images. To this end, we introduce a multi-reference non-local
attention module that extracts meaningful features for reconstructing
through-plane high-frequency details from multiple in-plane images. Through
extensive experiments, we demonstrate that our method performs significantly
better in CT slice interpolation than existing competing methods on public CT
datasets including a real-paired benchmark, verifying the effectiveness of the
proposed framework. The source code of this work is available at
https://github.com/khuhm/ACVTT.

</details>


### [67] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis 提出一种两阶段、几何感知的4D生成框架，通过视频VAE前馈重建和几何引导扩散（含Stereo Forcing）实现高质量时空一致的4D驾驶场景生成与新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以同时支持时间外推和空间NVS且无需每场景优化，需统一生成与新视角合成能力，保持几何与时间一致性。

Method: 两阶段框架：第一阶段用预训练视频VAE加range-view适配器实现从多视角图像的前馈4D重建，生成含几何、语义、运动的4D高斯点云；第二阶段用几何引导的视频扩散模型，以历史4D渲染为先验，结合轨迹条件生成未来视图，并提出Stereo Forcing在去噪阶段融入几何不确定性以提升时序一致性。

Result: 在外观与几何重建、时序生成和NVS任务上达到SOTA，并在下游任务中表现竞争力。

Conclusion: PhiGenesis 成功将视频生成与几何一致性相结合，实现了无需场景微调的4D驾驶场景生成，兼顾时间外推与空间新视角合成。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [68] [A Versatile Foundation Model for AI-enabled Mammogram Interpretation](https://arxiv.org/abs/2509.20271)
*Fuxiang Huang,Jiayi Zhu,Yunfang Yu,Yu Xie,Yuan Guo,Qingcong Kong,Mingxiang Wu,Xinrui Jiang,Shu Yang,Jiabo Ma,Ziyi Liu,Zhe Xu,Zhixuan Chen,Yujie Tan,Zifan He,Luhui Mao,Xi Wang,Junlin Hou,Lei Zhang,Qiong Luo,Zhenhui Li,Herui Yao,Hao Chen*

Main category: cs.CV

TL;DR: VersaMammo基于706k多机构钼靶图像，采用自监督+有监督/蒸馏的两阶段预训练，构建并在92项涵盖临床关键任务的基准上评估，显著提升跨机构泛化，在多数内部与外部任务中达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有钼靶基础模型在训练数据多样性、模型泛化性以及临床任务评估覆盖上存在不足，限制了临床转化。为此需构建更大更多样的数据集并采用能同时吸收无标签影像特征与有标签临床知识的预训练策略，确保模型在多任务、跨机构场景下的稳健性。

Method: 构建规模最大的多机构钼靶数据集（706,239张，来自21个来源）；采用两阶段预训练：先用自监督学习训练教师模型以从无标签钼靶中提取可迁移特征；再通过有监督学习结合知识蒸馏将特征与临床知识迁移到目标基础模型VersaMammo；设计覆盖5类临床任务（检测、分割、分类、图像检索、视觉问答）的92项基准任务进行评估。

Result: 在内部68项任务中位列第一50项，平均排名1.5；在24项外部验证任务中位列第一20项，平均排名1.2，显示出在检测、分割、分类、检索和视觉问答等多类任务上的领先性能与优越泛化能力。

Conclusion: VersaMammo显著提升了乳腺钼靶影像的泛化能力和临床适用性，通过大规模多院数据与两阶段预训练策略，取得多项任务的领先性能，推动了乳腺癌筛查与诊断的可靠性与可扩展性。

Abstract: Breast cancer is the most commonly diagnosed cancer and the leading cause of
cancer-related mortality in women globally. Mammography is essential for the
early detection and diagnosis of breast lesions. Despite recent progress in
foundation models (FMs) for mammogram analysis, their clinical translation
remains constrained by several fundamental limitations, including insufficient
diversity in training data, limited model generalizability, and a lack of
comprehensive evaluation across clinically relevant tasks. Here, we introduce
VersaMammo, a versatile foundation model for mammograms, designed to overcome
these limitations. We curated the largest multi-institutional mammogram dataset
to date, comprising 706,239 images from 21 sources. To improve generalization,
we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram
foundation model. First, a teacher model is trained via self-supervised
learning to extract transferable features from unlabeled mammograms. Then,
supervised learning combined with knowledge distillation transfers both
features and clinical knowledge into VersaMammo. To ensure a comprehensive
evaluation, we established a benchmark comprising 92 specific tasks, including
68 internal tasks and 24 external validation tasks, spanning 5 major clinical
task categories: lesion detection, segmentation, classification, image
retrieval, and visual question answering. VersaMammo achieves state-of-the-art
performance, ranking first in 50 out of 68 specific internal tasks and 20 out
of 24 external validation tasks, with average ranks of 1.5 and 1.2,
respectively. These results demonstrate its superior generalization and
clinical utility, offering a substantial advancement toward reliable and
scalable breast cancer screening and diagnosis.

</details>


### [69] [A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279)
*Songhao Li,Jonathan Xu,Tiancheng Bao,Yuxuan Liu,Yuchen Liu,Yihang Liu,Lilin Wang,Wenhui Lei,Sheng Wang,Yinuo Xu,Yan Cui,Jialu Yao,Shunsuke Koga,Zhi Huang*

Main category: cs.CV

TL;DR: TissueLab是一个将多学科工具标准化并联动的可交互、可持续学习的代理化医学图像AI系统，强调可解释工作流与专家即时反馈，在多项临床任务上超越现有VLM和代理系统，支持快速少样本适应并开源发布。


<details>
  <summary>Details</summary>
Motivation: 当前代理化AI在医疗图像领域受限于工具生态稀缺、工具集不足以及缺乏实时互动与专家反馈回路，导致性能和临床采纳受阻。TissueLab旨在构建可交互、可解释且可持续演化的工具生态以推动临床应用。

Method: TissueLab构建了标准化的工具接口（输入/输出/能力），并通过计划生成器自动选择和调用工具链，提供可视化的中间结果以便专家实时干预。系统集成了病理、影像和空间组学的工具工厂，并利用主动学习与在线微调让模型在少量样本或新领域中快速收敛。

Result: 在多项临床相关量化任务（用于分期、预后和治疗规划）上，TissueLab在性能上优于端到端视觉语言模型和其他代理系统（例如GPT-5），并能在无大规模训练数据或长时间再训练的情况下，通过主动学习在几分钟内适应未见病种。系统以开源生态方式发布。

Conclusion: TissueLab提出了一种面向医学图像分析的可共进化（co-evolving）代理化AI系统，通过工具工厂标准化并组合病理、放射和空间组学等多源工具，实现可解释工作流生成、实时交互与专家反馈，提升了在临床量化任务上的性能和可用性，并支持持续在线学习与主动学习以快速适应新病种。

Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research.
However, in medical image analysis, their performance and adoption remain
limited due to the lack of a robust ecosystem, insufficient toolsets, and the
absence of real-time interactive expert feedback. Here we present "TissueLab",
a co-evolving agentic AI system that allows researchers to ask direct
questions, automatically plan and generate explainable workflows, and conduct
real-time analyses where experts can visualize intermediate results and refine
them. TissueLab integrates tool factories across pathology, radiology, and
spatial omics domains. By standardizing inputs, outputs, and capabilities of
diverse tools, the system determines when and how to invoke them to address
research and clinical questions. Across diverse tasks with clinically
meaningful quantifications that inform staging, prognosis, and treatment
planning, TissueLab achieves state-of-the-art performance compared with
end-to-end vision-language models (VLMs) and other agentic AI systems such as
GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward
improved classifiers and more effective decision strategies. With active
learning, it delivers accurate results in unseen disease contexts within
minutes, without requiring massive datasets or prolonged retraining. Released
as a sustainable open-source ecosystem, TissueLab aims to accelerate
computational research and translational adoption in medical imaging while
establishing a foundation for the next generation of medical AI.

</details>


### [70] [HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy](https://arxiv.org/abs/2509.20280)
*Dayu Tan,Zhenpeng Xu,Yansen Su,Xin Peng,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: HiPerformer通过并行模块化层级编码器、LGFF和PPA模块改善局部与全局特征融合，减少特征冲突并提升医学图像分割的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有CNN-Transformer混合结构通常采用简单的特征融合（串联、端点拼接或点加），难以处理特征不一致，易导致信息冲突和丢失，影响分割精度，故提出更有效的融合机制。

Method: 提出模块化层级编码器在并行分支中逐层深度融合异构特征；设计局部-全局特征融合(LGFF)模块用于高效精确融合细节与语义信息；提出渐进金字塔聚合(PPA)模块替代传统跳跃连接以增强多尺度表示并抑制噪声。

Result: 在11个公开数据集上的实验表明，HiPerformer优于现有分割方法，在精度和鲁棒性上均有提升。

Conclusion: HiPerformer通过模块化并行层级编码器、LGFF融合模块和PPA跳跃替代，有效整合局部与全局信息，减少特征冲突和信息丢失，从而提升医学图像分割性能。

Abstract: Both local details and global context are crucial in medical image
segmentation, and effectively integrating them is essential for achieving high
accuracy. However, existing mainstream methods based on CNN-Transformer hybrid
architectures typically employ simple feature fusion techniques such as serial
stacking, endpoint concatenation, or pointwise addition, which struggle to
address the inconsistencies between features and are prone to information
conflict and loss. To address the aforementioned challenges, we innovatively
propose HiPerformer. The encoder of HiPerformer employs a novel modular
hierarchical architecture that dynamically fuses multi-source features in
parallel, enabling layer-wise deep integration of heterogeneous information.
The modular hierarchical design not only retains the independent modeling
capability of each branch in the encoder, but also ensures sufficient
information transfer between layers, effectively avoiding the degradation of
features and information loss that come with traditional stacking methods.
Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve
precise and efficient integration of local details and global semantic
information, effectively alleviating the feature inconsistency problem and
resulting in a more comprehensive feature representation. To further enhance
multi-scale feature representation capabilities and suppress noise
interference, we also propose a Progressive Pyramid Aggregation (PPA) module to
replace traditional skip connections. Experiments on eleven public datasets
demonstrate that the proposed method outperforms existing segmentation
techniques, demonstrating higher segmentation accuracy and robustness. The code
is available at https://github.com/xzphappy/HiPerformer.

</details>


### [71] [PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization](https://arxiv.org/abs/2509.20281)
*Haruka Kumagai,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: 提出一种基于人类感知的面部相似度度量；构建6400三元组数据集并用度量学习训练；在相似度预测和属性分类上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有人脸识别模型偏向二元判断（是否为同一人），无法量化“完全不同”与“高度相似但不同”等细粒度相似程度，导致在面部匿名化与自然性之间难以权衡。因此需一种更接近人类感知的相似度度量用于选择替换身份。

Method: 构建了包含6400个三元组标注的数据集，基于人类感知定义相似度，并采用度量学习方法训练模型以预测人类感知的面部相似度。

Result: 实验表明，该方法在面部相似度预测任务及基于属性的人脸分类任务上，相较于现有方法有显著提升，证明了所提度量的有效性。

Conclusion: 本论文结论指出，引入基于人类感知的面部相似度度量能有效解决现有方法只能进行二分类（同一人与否）带来的局限，从而在面部替换应用中更好地权衡匿名性与自然性。

Abstract: In response to rising societal awareness of privacy concerns, face
anonymization techniques have advanced, including the emergence of
face-swapping methods that replace one identity with another. Achieving a
balance between anonymity and naturalness in face swapping requires careful
selection of identities: overly similar faces compromise anonymity, while
dissimilar ones reduce naturalness. Existing models, however, focus on binary
identity classification "the same person or not", making it difficult to
measure nuanced similarities such as "completely different" versus "highly
similar but different." This paper proposes a human-perception-based face
similarity metric, creating a dataset of 6,400 triplet annotations and metric
learning to predict the similarity. Experimental results demonstrate
significant improvements in both face similarity prediction and attribute-based
face classification tasks over existing methods.

</details>


### [72] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: FAST是一种前景感知的快速扩散合成框架（AIAS+FARM），能在约10步内高效生成面向分割的高质量工业异常，显著提升下游分割性能。


<details>
  <summary>Details</summary>
Motivation: 像素级标注昂贵且异常样本稀缺，现有合成方法在采样效率与生成质量间难以权衡，且忽略前景与背景的统计差异，导致无法合成可控的分割专用异常。

Method: 提出了AIAS（训练自由的快速采样算法）与FARM（前景感知重构模块）；AIAS使用粗到细聚合加速反向过程，可在约10步内生成高质量分割导向异常；FARM在每步采样中对前景掩码内的噪声进行自适应调整，以在去噪轨迹中保留局部异常信号。

Result: 在多个工业基准上，FAST在下游分割任务上持续优于现有异常合成方法，且在低步数采样下仍能保持高质量。

Conclusion: FAST通过前景感知扩散和两大模块显著提升了合成异常的分割效果，兼顾效率与质量。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations,
yet real-world anomalies are often scarce, diverse, and costly to label.
Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a
promising alternative; however, existing methods struggle to balance sampling
efficiency and generation quality. Moreover, most approaches treat all spatial
regions uniformly, overlooking the distinct statistical differences between
anomaly and background areas. This uniform treatment hinders the synthesis of
controllable, structure-specific anomalies tailored for segmentation tasks. In
this paper, we propose FAST, a foreground-aware diffusion framework featuring
two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the
Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling
algorithm specifically designed for segmentation-oriented industrial anomaly
synthesis, which accelerates the reverse process through coarse-to-fine
aggregation and enables the synthesis of state-of-the-art segmentation-oriented
anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the
anomaly-aware noise within the masked foreground regions at each sampling step,
preserving localized anomaly signals throughout the denoising trajectory.
Extensive experiments on multiple industrial benchmarks demonstrate that FAST
consistently outperforms existing anomaly synthesis methods in downstream
segmentation tasks. We release the code at:
https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [73] [A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices](https://arxiv.org/abs/2509.20318)
*Bishal Adhikari,Jiajia Li,Eric S. Michel,Jacob Dykes,Te-Ming Paul Tseng,Mary Love Tagert,Dong Chen*

Main category: cs.CV

TL;DR: 作者发布了一个3,095张鹿图像数据集，比较12个YOLOv8–v11模型变体，结论是YOLOv11n/YOLOv8s/YOLOv9s在精度与速度上最佳，Jetson可实现实时检测，Raspberry Pi需优化。


<details>
  <summary>Details</summary>
Motivation: 农业因鹿入侵造成数亿美元损失，传统防护措施费时且效果有限，迫切需要面向现场部署的智能、自治式鹿检测系统，但现有研究缺乏领域专用数据集与部署可行性分析。

Method: 作者从Idaho Cameratraps项目收集并筛选出3,095张带边界框标注的鹿图像，训练并比较了12个模型变体，覆盖YOLOv8–v11系列，使用NVIDIA RTX 5090进行训练和基准测试，并在Raspberry Pi 5与Jetson AGX Xavier上进行推理速度评估。

Result: 实验表明：1) 数据集与模型在复杂野外场景下能达到高检测性能（AP@.5 > 0.85）；2) 小型先进模型在保持高精度的同时能在Jetson上实现实时推理；3) Raspberry Pi 5需进行硬件特定的模型优化才能实时运行。

Conclusion: 本文贡献了一个实用的鹿检测数据集并系统评估了多种YOLO变体，结论是小型但先进的模型（如YOLOv11n、YOLOv8s、YOLOv9s）在精度与效率上取得最佳平衡；在边缘平台上，Raspberry Pi 5在未优化情况下无法实现实时检测，而NVIDIA Jetson AGX Xavier在GPU加速下对's'和'n'系列模型可达>30 FPS。

Abstract: The escalating economic losses in agriculture due to deer intrusion,
estimated to be in the hundreds of millions of dollars annually in the U.S.,
highlight the inadequacy of traditional mitigation strategies since these
methods are often labor-intensive, costly, and ineffective for modern farming
systems. To overcome this, there is a critical need for intelligent, autonomous
solutions which require accurate and efficient deer detection. But the progress
in this field is impeded by a significant gap in the literature, mainly the
lack of a domain-specific, practical dataset and limited study on the on-field
deployability of deer detection systems. Addressing this gap, this study
presents a comprehensive evaluation of state-of-the-art deep learning models
for deer detection in challenging real-world scenarios. The contributions of
this work are threefold. First, we introduce a curated, publicly available
dataset of 3,095 annotated images with bounding-box annotations of deer,
derived from the Idaho Cameratraps project. Second, we provide an extensive
comparative analysis of 12 model variants across four recent YOLO
architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a
high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing
platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the
real-time detection is not feasible in Raspberry Pi without hardware-specific
model optimization, while NVIDIA Jetson provides greater than 30 FPS with
GPU-accelerated inference on 's' and 'n' series models. This study also reveals
that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and
YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and
computational efficiency (FPS > 30). To support further research, both the
source code and datasets are publicly available at
https://github.com/WinnerBishal/track-the-deer.

</details>


### [74] [Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On](https://arxiv.org/abs/2509.20343)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 在不增加额外参数的纯拼接VTON框架中，将姿态图空间拼接到输入并配合细粒度/边界框混合掩码训练，能在保持姿态一致性同时提升生成图像的真实感和商品融合灵活性。


<details>
  <summary>Details</summary>
Motivation: 在线购物推动了虚拟试穿需求，关键问题是如何在保持商品与人体精确对齐的同时支持多样化姿态；现有方法引入额外模块或参数，增加复杂性，故提出在不增加模型复杂度下实现姿态控制的方法。

Method: 在纯拼接范式下，比较了将姿态以骨架（skeleton）或姿态图（pose maps）形式进行空间拼接到输入的效果；不使用外部编码器、控制网络或复杂注意力层，保持基线模型参数不变；并设计混合掩码训练策略，交替使用细粒度掩码和边界框掩码训练模型以增强鲁棒性。

Result: 实验表明：1) 将姿态以姿态图形式拼接到输入比骨架线条表现更好，能够更好地保留目标姿态并提升生成图像的真实感；2) 混合掩码训练提升了模型在不同遮挡和姿态变化下的商品融合能力；3) 整体方法在不增加参数的前提下实现了有效的姿态控制。

Conclusion: 本文在不增加额外参数或模块的前提下，将姿态信息通过空间拼接方式融入基线VTON模型，结论是使用姿态图（pose maps）进行拼接能在保持姿态一致性和提升生成图像真实感方面取得最佳效果；同时引入的混合掩码训练策略（细粒度掩码与边界框掩码结合）提高了对不同姿态和条件下商品融合的灵活性。

Abstract: As online shopping continues to grow, the demand for Virtual Try-On (VTON)
technology has surged, allowing customers to visualize products on themselves
by overlaying product images onto their own photos. An essential yet
challenging condition for effective VTON is pose control, which ensures
accurate alignment of products with the user's body while supporting diverse
orientations for a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges, including selecting
the optimal pose representation, integrating poses without additional
parameters, and balancing pose preservation with flexible pose control.
  In this work, we build upon a baseline VTON model that concatenates the
reference image condition without external encoder, control network, or complex
attention layers. We investigate methods to incorporate pose control into this
pure concatenation paradigm by spatially concatenating pose data, comparing
performance using pose maps and skeletons, without adding any additional
parameters or module to the baseline model. Our experiments reveal that pose
stitching with pose maps yields the best results, enhancing both pose
preservation and output realism. Additionally, we introduce a mixed-mask
training strategy using fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses and conditions.

</details>


### [75] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: 提出PhysCtrl：基于扩散的生成物理网络，学习四种材料的3D点轨迹并加入时空注意力与物理约束，实现物理参数与力可控的图像到视频生成，数据集55万条仿真动画，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本/图像到视频模型在真实感方面表现良好，但缺乏物理合理性和三维可控性，需引入物理参数和力控以提高物理拟合和可控性。

Method: 设计了一个生成式物理网络，用扩散模型学习3D点轨迹（表征物理动力学），引入新的时空注意力模块以模拟粒子交互，并在训练中加入基于物理的约束。训练数据为55万条由物理仿真器生成的动画。

Result: PhysCtrl能生成真实且符合物理的运动轨迹，并用以驱动图像到视频模型生成高保真且可控的视频，在视觉质量和物理合理性上优于现有方法。

Conclusion: PhysCtrl提出了一个以物理参数和外力可控的图像到视频生成框架，通过学习四种材料的物理动力学分布并结合扩散模型实现了物理可控的视频生成。

Abstract: Existing video generation models excel at producing photo-realistic videos
from text or images, but often lack physical plausibility and 3D
controllability. To overcome these limitations, we introduce PhysCtrl, a novel
framework for physics-grounded image-to-video generation with physical
parameters and force control. At its core is a generative physics network that
learns the distribution of physical dynamics across four materials (elastic,
sand, plasticine, and rigid) via a diffusion model conditioned on physics
parameters and applied forces. We represent physical dynamics as 3D point
trajectories and train on a large-scale synthetic dataset of 550K animations
generated by physics simulators. We enhance the diffusion model with a novel
spatiotemporal attention block that emulates particle interactions and
incorporates physics-based constraints during training to enforce physical
plausibility. Experiments show that PhysCtrl generates realistic,
physics-grounded motion trajectories which, when used to drive image-to-video
models, yield high-fidelity, controllable videos that outperform existing
methods in both visual quality and physical plausibility. Project Page:
https://cwchenwang.github.io/physctrl

</details>


### [76] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: 提出EditVerse，一个将文本/图像/视频统一为token序列并用自注意力Transformer训练的单模型，配合232K视频编辑数据与新基准EditVerseBench，实现了领先的视频/图像生成与编辑性能与跨模态能力。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成/编辑已向统一框架发展，但视频生成/编辑仍碎片化，受架构限制与数据稀缺影响；目标是实现单模型跨模态统一的生成与编辑能力。

Method: 将多模态（文本、图像、视频）编码为统一的token序列，利用自注意力Transformer架构进行联合训练；构建了规模化数据管道（232K视频编辑样本+大规模图像/视频数据）进行混合训练；设计EditVerseBench作为指令式视频编辑评测集。

Result: 在大规模训练后，EditVerse在多项任务上超过先前开源与商用模型，用户研究与实验表明其在编辑与生成任务中表现优异，并展现出跨模态的 emergent 能力。

Conclusion: EditVerse提出了一个统一的图像和视频生成/编辑框架，通过将文本、图像、视频统一为token序列并使用自注意力实现上下文学习与跨模态迁移，解决数据稀缺问题并构建了视频编辑数据集与基准。

Abstract: Recent advances in foundation models highlight a clear trend toward
unification and scaling, showing emergent capabilities across diverse domains.
While image generation and editing have rapidly transitioned from task-specific
to unified frameworks, video generation and editing remain fragmented due to
architectural limitations and data scarcity. In this work, we introduce
EditVerse, a unified framework for image and video generation and editing
within a single model. By representing all modalities, i.e., text, image, and
video, as a unified token sequence, EditVerse leverages self-attention to
achieve robust in-context learning, natural cross-modal knowledge transfer, and
flexible handling of inputs and outputs with arbitrary resolutions and
durations. To address the lack of video editing training data, we design a
scalable data pipeline that curates 232K video editing samples and combines
them with large-scale image and video datasets for joint training. Furthermore,
we present EditVerseBench, the first benchmark for instruction-based video
editing covering diverse tasks and resolutions. Extensive experiments and user
studies demonstrate that EditVerse achieves state-of-the-art performance,
surpassing existing open-source and commercial models, while exhibiting
emergent editing and generation abilities across modalities.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [77] [About the Multi-Head Linear Restricted Chase Termination](https://arxiv.org/abs/2509.19400)
*Lukas Gerlach,Lucas Larroque,Jerzy Marcinkowski,Piotr Ostropolski-Nalewaja*

Main category: cs.DB

TL;DR: 本文解决了一个长期开放的问题：证明了线性多头存在性规则下的所有实例受限chase终止性是可判定的，方法是通过识别和检测导致无限触发的循环模板实现的。


<details>
  <summary>Details</summary>
Motivation: 受限chase的终止性对数据库推理和查询重写等应用至关重要，但在存在性规则下终止性通常不可保证且在许多情形下不可判定。线性或有保护（guarded）多头规则等良好类的受限chase终止性问题长期未解，本文旨在推进这一理论前沿。

Method: 作者通过构造性方法分析受限chase的触发顺序和无限触发链的结构，识别出能够导致非终止的循环模式，并基于这些模式设计了判定过程。关键在于利用线性规则的性质将无限衍生归约为可检测的循环模板，从而使判定变为有限检查问题。

Result: 针对线性多头规则，作者给出了一个确定性算法来判定是否对所有实例受限chase都会终止，并证明了该算法的正确性与完备性，从而填补了该类规则下受限chase可判定性的空白。

Conclusion: 本文证明了在线性多头存在性规则（linear multi-headed TGDs）下，针对所有输入实例的受限（restricted）chase算法终止性是可判定的。

Abstract: The chase is a ubiquitous algorithm in database theory. However, for
existential rules (aka tuple-generating dependencies), its termination is not
guaranteed, and even undecidable in general. The problem of termination becomes
particularly difficult for the restricted (or standard) chase, for which the
order of rule application matters. Thus, decidability of restricted chase
termination is still open for many well-behaved classes such as linear or
guarded multi-headed rules. We make a step forward by showing that
all-instances restricted chase termination is decidable in the linear
multi-headed case.

</details>


### [78] [STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases](https://arxiv.org/abs/2509.19508)
*Mounica Maddela,Lingjue Xie,Daniel Preotiuc-Pietro,Mausam*

Main category: cs.DB

TL;DR: 提出 STARQA，首个专人创建的复杂分析推理文本到 SQL 数据集；并引入 Text2SQLCode（SQL+Python）方法，结果优于纯 SQL，但仍很难。


<details>
  <summary>Details</summary>
Motivation: 现有文本到 SQL 基准问题复杂度受限，缺乏涉及聚合计算、时间序列分析和情景理解等复杂分析推理的问题，因而需要专门的数据集与方法来衡量和推动该方向的进步。

Method: 构建人类标注的 STARQA 数据集（三个专用域），并比较直接用 LLM 生成 SQL 与提出的 Text2SQLCode 方法（SQL 用于数据提取，Python 用于复杂推理），在多模型上进行评估。

Result: 实验显示将 SQL 与 Python 能力结合的策略优于仅生成 SQL，但总体性能仍不高，说明 STARQA 的问题对当前最先进 LLMs 非常具有挑战性。

Conclusion: STARQA 提出并验证了在专用领域数据库上进行复杂分析推理的文本到 SQL 问答的新基准，表明将 SQL 与 Python 结合（Text2SQLCode）优于仅用 SQL，但任务对现有大型模型仍具挑战性。

Abstract: Semantic parsing methods for converting text to SQL queries enable question
answering over structured data and can greatly benefit analysts who routinely
perform complex analytics on vast data stored in specialized relational
databases. Although several benchmarks measure the abilities of text to SQL,
the complexity of their questions is inherently limited by the level of
expressiveness in query languages and none focus explicitly on questions
involving complex analytical reasoning which require operations such as
calculations over aggregate analytics, time series analysis or scenario
understanding. In this paper, we introduce STARQA, the first public
human-created dataset of complex analytical reasoning questions and answers on
three specialized-domain databases. In addition to generating SQL directly
using LLMs, we evaluate a novel approach (Text2SQLCode) that decomposes the
task into a combination of SQL and Python: SQL is responsible for data
fetching, and Python more naturally performs reasoning. Our results demonstrate
that identifying and combining the abilities of SQL and Python is beneficial
compared to using SQL alone, yet the dataset still remains quite challenging
for the existing state-of-the-art LLMs.

</details>


### [79] [Gamma Acyclicity, Annotated Relations, and Consistency Witness Functions](https://arxiv.org/abs/2509.19621)
*Albert Atserias,Phokion G. Kolaitis*

Main category: cs.DB

TL;DR: 若注释来自满足运输性的正交换幺半群，则γ-无环数据库模式的良好语义性质可扩展到注释关系；标准关系的连接的关键作用可抽象为一致性见证函数，该抽象在注释框架中被系统研究并用以证明主要结果。


<details>
  <summary>Details</summary>
Motivation: 动机是：经典关系数据库理论中γ-无环模式拥有若干良好的语义性质（如分解的可重构性、查询评估简化等），希望将这些性质推广到更一般的注释关系模型（annotations from semirings/monoids），从而将数据库完整性与查询优化的理论推广到带权重或注释的数据（例如概率、成本、访问权限等）；此外，研究标准连接在抽象注释模型中的作用，寻找一个更一般的“见证”概念。

Method: 方法上，作者先回顾α、β、γ-无环性的经典定义与性质，随后将α-无环性的已有扩展（针对来自正交换幺半群的注释）推广到β、γ-无环性场景。核心技术包括引入并形式化“一致性见证函数”以抽象标准连接的关键性质，证明在具有运输性的半群上这些见证函数存在并保持所需的语义性质，同时利用子模式、投影与连接操作的代数性质进行构造性证明。

Result: 主要结果包括：1) 证明若注释来自正交换幺半群且满足运输性，则γ-无环模式在注释关系下仍保持所宣称的语义性质；2) 给出一致性见证函数的形式化定义并系统研究其性质与构造条件；3) 识别出标准连接在保持一致性方面的核心作用，并说明在注释化的抽象框架下一致性见证函数可以替代标准连接作为一致性证明工具；4) 通过构造性证明与反例阐明了运输性条件的必要性与充分性范围。

Conclusion: 本文结论是：当注释来自具有运输性（transportation property）的正交换幺半群时，γ-无环模式的若干良好语义性质可以扩展到注释关系（annotated relations）；此外，标准关系的连接（join）在该语境下的唯一相关性质是作为两个关系一致性的见证（consistency witness），这一性质在注释关系中由一致性见证函数（consistency witness function）抽象并系统研究。

Abstract: During the early days of relational database theory it was realized that
"acyclic" database schemas possess a number of desirable semantic properties.
In fact, three different notions of "acyclicity" were identified and
extensively investigated during the 1980s, namely, alpha-acyclicity,
beta-acyclicity, and gamma-acyclicity. Much more recently, the study of
alpha-acyclicity was extended to annotated relations, where the annotations are
values from some positive commutative monoid. The recent results about
alpha-acyclic schemas and annotated relations give rise to results about
beta-acyclic schemas and annotated relations, since a schema is beta-acyclic if
and only if every sub-schema of it is alpha-acyclic. Here, we study
gamma-acyclic schemas and annotated relations. Our main finding is that the
desirable semantic properties of gamma-acyclic schemas extend to annotated
relations, provided the annotations come from a positive commutative monoid
that has the transportation property. Furthermore, the results reported here
shed light on the role of the join of two standard relations, Specifically, our
results reveal that the only relevant property of the join of two standard
relations is that it is a witness to the consistency of the two relations,
provided that these two relations are consistent. For the more abstract setting
of annotated relations, this property of the standard join is captured by the
notion of a consistency witness function, a notion which we systematically
investigate in this work.

</details>


### [80] [ARCADE: A Real-Time Data System for Hybrid and Continuous Query Processing across Diverse Data Modalities](https://arxiv.org/abs/2509.19757)
*Jingyi Yang,Songsong Mo,Jiachen Shi,Zihao Yu,Kunhao Shi,Xuchen Ding,Gao Cong*

Main category: cs.DB

TL;DR: ARCADE 是一个基于 LSM 存储和 MySQL 的实时多模态数据库系统，通过统一磁盘索引、代价优化器与增量物化视图，实现高效摄取与表达性混合/连续查询，显著提升读写性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态与实时数据库要么难以高效摄取与持续查询，要么无法支持表达性强的混合分析，无法满足多模态实时语义检索需求。

Method: 在 RocksDB（LSM 存储）和 MySQL 查询引擎之上实现：1) 统一的磁盘二级索引结构，支持向量、空间与文本模态；2) 基于代价的优化器用于混合查询计划选择；3) 增量物化视图框架用于高效连续查询维护。

Result: 在基准测试上，ARCADE 在读密集型工作负载上最多比已知系统快 7.4 倍，在写密集型工作负载上快 1.4 倍。

Conclusion: ARCADE 提出了一种支持高吞吐量写入与复杂混合、连续查询的实时多模态数据系统，实现统一的磁盘级二级索引、基于代价的混合查询优化器和增量物化视图框架，从而在读写性能上显著优于现有系统。

Abstract: The explosive growth of multimodal data - spanning text, image, video,
spatial, and relational modalities, coupled with the need for real-time
semantic search and retrieval over these data - has outpaced the capabilities
of existing multimodal and real-time database systems, which either lack
efficient ingestion and continuous query capability, or fall short in
supporting expressive hybrid analytics. We introduce ARCADE, a real-time data
system that efficiently supports high-throughput ingestion and expressive
hybrid and continuous query processing across diverse data types. ARCADE
introduces unified disk-based secondary index on LSM-based storage for vector,
spatial, and text data modalities, a comprehensive cost-based query optimizer
for hybrid queries, and an incremental materialized view framework for
efficient continuous queries. Built on open-source RocksDB storage and MySQL
query engine, ARCADE outperforms leading multimodal data systems by up to 7.4x
on read-heavy and 1.4x on write-heavy workloads.

</details>


### [81] [Output-Sensitive Evaluation of Acyclic Conjunctive Regular Path Queries](https://arxiv.org/abs/2509.20204)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu,Zilu Tian*

Main category: cs.DB

TL;DR: 为无环CRPQ设计了一个基于“收缩宽度”的两阶段输出敏感评估算法，复杂度依赖于输入图和实际查询输出，而非正则表达式的输出规模。


<details>
  <summary>Details</summary>
Motivation: 传统方法在某些情况下依赖于出现在查询中正则表达式的输出规模，可能导致复杂度高于实际查询输出需要的场景；因此需要一个复杂度与实际输出大小相关的算法。

Method: 算法分两阶段：第一阶段将查询压缩为free-connex无环CRPQ，通过合成正则表达式或将绑定变量提升为自由变量来移除绑定变量，并定义了新的参数“收缩宽度”；第二阶段评估该free-connex无环CRPQ并投影去除提升的绑定变量列，同时按比例计算正则表达式的标定输出以保证输出敏感性。

Result: 提出的算法在两类实例上优于现有方法：(i) 查询输出明显小于最坏情况输出，(ii) 查询中任一正则表达式的最大输出大小较大时。

Conclusion: 本文提出了首个对输出敏感的可用于评估无环CRPQ的算法，复杂度与输入图和查询输出大小相关，而与正则表达式本身的输出规模无关。

Abstract: Conjunctive Regular Path Queries, or CRPQs for short, are an essential
construct in graph query languages. In this paper, we propose the first
output-sensitive algorithm for evaluating acyclic CRPQs. It is output-sensitive
in the sense that its complexity is a function of the sizes of the input graph
and of the query output. In particular, it does not depend on the output sizes
of the regular expressions that appear in the query, as these sizes can be much
larger than the query output size.
  Our algorithm proceeds in two stages. In the first stage, it contracts the
given query into a free-connex acyclic one such that the output of the original
query can be obtained from the output of the contracted one. This contraction
removes bound variables by composing regular expressions or by promoting bound
variables to free ones. The minimum necessary number of promoted bound
variables gives the contraction width, which is a novel parameter specific to
CRPQs. In the second stage, our algorithm evaluates the free-connex acyclic
CRPQ and projects away the columns of the promoted bound variables. It ensures
output-sensitivity by computing the calibrated outputs of the regular
expressions appearing in the free-connex acyclic CRPQ in time proportional to
their sizes.
  Our algorithm has lower complexity than the state-of-the-art approaches for
problem instances where (i) the query output is asymptotically smaller than the
worst-case output size or (ii) the largest output size of any of the regular
expression in the query.

</details>
