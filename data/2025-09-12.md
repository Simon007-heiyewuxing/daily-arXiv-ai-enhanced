<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 78]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2 是一个支持图文混合查询和文档的多模态检索模型，使用多层表征与带门控的循环 Transformer 动态融合跨层跨模态信息，在多个基准上实现 SOTA 与资源效率提升，并改善检索增强生成任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖任务特定微调且通常仅限单模态查询或文档，难以应对日益复杂的图文混合检索任务，故提出支持多模态查询与多模态文档的统一检索模型。

Method: 采用多层表示与循环 Transformer 架构，结合 LSTM 风格的门控机制，在每层动态融合视觉与文本信息以构建多尺度表征；训练评估在 M2KR 与 M-BEIR 基准上，并与检索增强生成管道整合评估下游任务性能。

Result: 在 M2KR 与 M-BEIR 的多种检索配置下，ReT-2 在多项指标上达到或超越现有最先进水平，同时推理更快、显存占用更低；在检索增强生成任务（Encyclopedic-VQA、InfoSeek）中也带来下游性能提升。

Conclusion: ReT-2 提出了一种统一的多模态检索模型，支持图文混合查询与检索多模态文档集合，通过多层表征与类 LSTM 门控的循环 Transformer 动态整合跨层与跨模态信息，实现细粒度视觉与文本信息捕捉。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [2] [Diffusion-Based Action Recognition Generalizes to Untrained Domains](https://arxiv.org/abs/2509.08908)
*Rogerio Guimaraes,Frank Xiao,Pietro Perona,Markus Marks*

Main category: cs.CV

TL;DR: 用扩散模型生成的语义化特征+Transformer聚合，实现对动作的跨物种、视角和场景强泛化，三项基准均获SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在面对大幅上下文、视角和物种差异时泛化较差，而人类能保持一致识别，同一动作跨这些变化表现稳健；因此希望借助VDM的语义化特征改善模型泛化。

Method: 使用VDM在扩散过程的早期时间步条件下提取特征，以强调语义信息而非像素细节；将这些特征作为时序输入，通过Transformer架构进行聚合并分类。实验在跨物种（动物）、跨视角（第一人称与第三人称）、跨上下文（真实与电影）等设置上评估。

Result: 在三个泛化基准上均达到新的国标（state-of-the-art），证明早期扩散时步条件的VDM特征与Transformer聚合能提升对语义级动作的跨条件泛化。

Conclusion: 本文提出用基于扩散模型（Vision Diffusion Model, VDM）生成的特征，通过Transformer聚合来实现对动作的跨物种、跨视角和跨场景鲁棒识别，显著提升泛化能力并在三个基准上创下新SOTA。

Abstract: Humans can recognize the same actions despite large context and viewpoint
variations, such as differences between species (walking in spiders vs.
horses), viewpoints (egocentric vs. third-person), and contexts (real life vs
movies). Current deep learning models struggle with such generalization. We
propose using features generated by a Vision Diffusion Model (VDM), aggregated
via a transformer, to achieve human-like action recognition across these
challenging conditions. We find that generalization is enhanced by the use of a
model conditioned on earlier timesteps of the diffusion process to highlight
semantic information over pixel level details in the extracted features. We
experimentally explore the generalization properties of our approach in
classifying actions across animal species, across different viewing angles, and
different recording contexts. Our model sets a new state-of-the-art across all
three generalization benchmarks, bringing machine action recognition closer to
human-like robustness. Project page:
$\href{https://www.vision.caltech.edu/actiondiff/}{\texttt{vision.caltech.edu/actiondiff}}$
Code:
$\href{https://github.com/frankyaoxiao/ActionDiff}{\texttt{github.com/frankyaoxiao/ActionDiff}}$

</details>


### [3] [PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability](https://arxiv.org/abs/2509.08910)
*Tung Vu,Lam Nguyen,Quynh Dao*

Main category: cs.CV

TL;DR: 提出PromptGuard与VulnGuard Prompt，通过少量GitHub示例、伦理链式思考和多目标信息论形式化，声称在理论上能减少25-30%对弱势群体的有害生成，并设计六模块系统用于实时防护。


<details>
  <summary>Details</summary>
Motivation: 动机是当前LLM在真实应用中可能向LGBTQ+、单亲及边缘化群体生成有害或误导性信息，现有事后过滤或通用对齐不足以在生成源头预防伤害，因此提出源头级防护机制。

Method: 方法包括：1）从GitHub精选代码库采集少量示例用于对比学习；2）设计VulnGuard Prompt结合few-shot示例、伦理链式思考和自适应角色提示；3）构建包含输入分类、外部工具交互、输出验证等六模块的系统；4）用信息论和多目标优化给出形式化证明并用GitHub数据集做理论验证。

Result: 论文宣称提供了数学证明和理论验证，证明可通过熵界与帕累托最优性达成25-30%的分析伤害降低，且系统模块化可实现实时防护；并用GitHub来源的数据集进行了脆弱性分析与理论验证。

Conclusion: 本文提出了PromptGuard，一种通过VulnGuard Prompt实现的模块化提示框架，声称能基于数据驱动的对比学习和伦理链式思考阻止对弱势群体的有害生成。作者主张在理论上通过熵界和帕累托最优性证明可实现25-30%的分析性伤害减少，并提供六个核心模块与数学形式化及收敛证明。

Abstract: The proliferation of Large Language Models (LLMs) in real-world applications
poses unprecedented risks of generating harmful, biased, or misleading
information to vulnerable populations including LGBTQ+ individuals, single
parents, and marginalized communities. While existing safety approaches rely on
post-hoc filtering or generic alignment techniques, they fail to proactively
prevent harmful outputs at the generation source. This paper introduces
PromptGuard, a novel modular prompting framework with our breakthrough
contribution: VulnGuard Prompt, a hybrid technique that prevents harmful
information generation using real-world data-driven contrastive learning.
VulnGuard integrates few-shot examples from curated GitHub repositories,
ethical chain-of-thought reasoning, and adaptive role-prompting to create
population-specific protective barriers. Our framework employs theoretical
multi-objective optimization with formal proofs demonstrating 25-30% analytical
harm reduction through entropy bounds and Pareto optimality. PromptGuard
orchestrates six core modules: Input Classification, VulnGuard Prompting,
Ethical Principles Integration, External Tool Interaction, Output Validation,
and User-System Interaction, creating an intelligent expert system for
real-time harm prevention. We provide comprehensive mathematical formalization
including convergence proofs, vulnerability analysis using information theory,
and theoretical validation framework using GitHub-sourced datasets,
establishing mathematical foundations for systematic empirical research.

</details>


### [4] [Similarity-based Outlier Detection for Noisy Object Re-Identification Using Beta Mixtures](https://arxiv.org/abs/2509.08926)
*Waqar Ahmad,Evan Murphy,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 将Re-ID视作成对相似性学习，使用Siamese网络并用两成分Beta混合模型对余弦相似度做异常检测（Beta-SOD）以去噪，从而在有噪标签场景下提升Re-ID性能。


<details>
  <summary>Details</summary>
Motivation: 现有Re-ID对标签噪声敏感，导致性能下降。通过直接学习成对相似性并检测噪声对训练进行去噪，可以提升鲁棒性。

Method: 构建Siamese架构，结合二元交叉熵、对比损失和余弦嵌入损失训练；对成对嵌入的余弦相似度建模为两成分Beta混合分布并估计其参数用于区分内点/异常点（噪声标签）。

Result: 在带噪的人体Re-ID（CUHK03、Market-1501）和车辆Re-ID（VeRi-776）数据集上，Beta-SOD在10-30%噪声水平下优于现有方法，展示了更强的鲁棒性和广泛适用性。

Conclusion: 该文提出将Re-ID重构为监督图像相似性任务，使用Siamese网络并引入基于相似度分布的异常值检测Beta-SOD以应对标签噪声。

Abstract: Object re-identification (Re-ID) methods are highly sensitive to label noise,
which typically leads to significant performance degradation. We address this
challenge by reframing Re-ID as a supervised image similarity task and adopting
a Siamese network architecture trained to capture discriminative pairwise
relationships. Central to our approach is a novel statistical outlier detection
(OD) framework, termed Beta-SOD (Beta mixture Similarity-based Outlier
Detection), which models the distribution of cosine similarities between
embedding pairs using a two-component Beta distribution mixture model. We
establish a novel identifiability result for mixtures of two Beta
distributions, ensuring that our learning task is well-posed.The proposed OD
step complements the Re-ID architecture combining binary cross-entropy,
contrastive, and cosine embedding losses that jointly optimize feature-level
similarity learning.We demonstrate the effectiveness of Beta-SOD in de-noising
and Re-ID tasks for person Re-ID, on CUHK03 and Market-1501 datasets, and
vehicle Re-ID, on VeRi-776 dataset. Our method shows superior performance
compared to the state-of-the-art methods across various noise levels (10-30\%),
demonstrating both robustness and broad applicability in noisy Re-ID scenarios.
The implementation of Beta-SOD is available at:
https://github.com/waqar3411/Beta-SOD

</details>


### [5] [SFD-Mamba2Net: Strcture-Guided Frequency-Enhanced Dual-Stream Mamba2 Network for Coronary Artery Segmentation](https://arxiv.org/abs/2509.08934)
*Nan Mu,Ruiqi Song,Zhihui Xu,Jingfeng Jiang,Chen Zhao*

Main category: cs.CV

TL;DR: SFD-Mamba2Net通过曲率感知的多尺度结构增强和基于小波的高频逐步重建，有效提升了ICA血管分割与狭窄检测性能，实验显示全面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: ICA图像低对比、高噪声与复杂细小血管结构使得传统分割与检测方法难以准确提取血管和狭窄；因此需要引入多尺度结构先验、长程依赖建模和频域高频增强以提升性能。

Method: 方法包括编码器中的CASE模块（多尺度曲率感知结构增强）用于突出细长管状血管并抑制背景干扰；解码器中的PHFP模块基于多层小波分解逐步细化高频细节并融合低频全局结构；整体采用状态空间长程依赖建模与频域细节增强，端到端训练。

Result: 在八项分割指标上均优于最先进方法，并在狭窄检测任务上取得最高的真正例率（TPR）和阳性预测值（PPV），表明方法在检测敏感性与精确性上都有提升。

Conclusion: 本文提出的SFD-Mamba2Net针对ICA图像血管分割与狭窄检测具有显著改进效果，方法合理且实验结果优于现有方法，结论可信。

Abstract: Background: Coronary Artery Disease (CAD) is one of the leading causes of
death worldwide. Invasive Coronary Angiography (ICA), regarded as the gold
standard for CAD diagnosis, necessitates precise vessel segmentation and
stenosis detection. However, ICA images are typically characterized by low
contrast, high noise levels, and complex, fine-grained vascular structures,
which pose significant challenges to the clinical adoption of existing
segmentation and detection methods. Objective: This study aims to improve the
accuracy of coronary artery segmentation and stenosis detection in ICA images
by integrating multi-scale structural priors, state-space-based long-range
dependency modeling, and frequency-domain detail enhancement strategies.
Methods: We propose SFD-Mamba2Net, an end-to-end framework tailored for
ICA-based vascular segmentation and stenosis detection. In the encoder, a
Curvature-Aware Structural Enhancement (CASE) module is embedded to leverage
multi-scale responses for highlighting slender tubular vascular structures,
suppressing background interference, and directing attention toward vascular
regions. In the decoder, we introduce a Progressive High-Frequency Perception
(PHFP) module that employs multi-level wavelet decomposition to progressively
refine high-frequency details while integrating low-frequency global
structures. Results and Conclusions: SFD-Mamba2Net consistently outperformed
state-of-the-art methods across eight segmentation metrics, and achieved the
highest true positive rate and positive predictive value in stenosis detection.

</details>


### [6] [Live(r) Die: Predicting Survival in Colorectal Liver Metastasis](https://arxiv.org/abs/2509.08935)
*Muhammad Alberb,Helen Cheung,Anne Martel*

Main category: cs.CV

TL;DR: 提出结合SAMONAI与SurvAMINN的全自动MRI影像组学框架，实现对CRLM术后生存的高效准确预测，C-index提升>10%。


<details>
  <summary>Details</summary>
Motivation: 多发性CRLM病例中肿瘤异质性和注释代价使得基于有限临床或分子特征的现有预后模型预测能力不足，需开发自动化、注释高效且能处理多病灶信息的影像组学预测方法。

Method: 两大模块：1) 分割管线：利用可提示的基础模型补全缺失标签，提出SAMONAI——一种零样本的3D提示传播算法，基于Segment Anything Model从单点提示推广到3D分割；训练分割肝脏、肿瘤、脾脏。2) 影像组学管线：对每个肿瘤在造影前后分割结果上提取特征，并用SurvAMINN（一种自编码器式的多实例神经网络）对右删失生存数据联合学习降维与风险预测，强调识别最具侵袭性的病灶。

Result: 在包含227例患者的机构数据集上进行评估，与现有临床和基因组生物标志物相比，框架在生存预测的C-index上提升超过10%；提出的SAMONAI显著提高了3D分割的准确性和效率；SurvAMINN能够定位并利用最具侵袭性的肿瘤来改善风险估计。

Conclusion: 该论文提出了一个端到端的基于MRI的自动化框架，用于预测结直肠肝转移（CRLM）患者术后生存，结合了可提示基础模型完成部分注释的分割管线与基于自编码器的多实例生存分析网络SurvAMINN，实验显示比现有临床和基因组生物标志物的C-index提升超过10%。

Abstract: Colorectal cancer frequently metastasizes to the liver, significantly
reducing long-term survival. While surgical resection is the only potentially
curative treatment for colorectal liver metastasis (CRLM), patient outcomes
vary widely depending on tumor characteristics along with clinical and genomic
factors. Current prognostic models, often based on limited clinical or
molecular features, lack sufficient predictive power, especially in multifocal
CRLM cases. We present a fully automated framework for surgical outcome
prediction from pre- and post-contrast MRI acquired before surgery. Our
framework consists of a segmentation pipeline and a radiomics pipeline. The
segmentation pipeline learns to segment the liver, tumors, and spleen from
partially annotated data by leveraging promptable foundation models to complete
missing labels. Also, we propose SAMONAI, a novel zero-shot 3D prompt
propagation algorithm that leverages the Segment Anything Model to segment 3D
regions of interest from a single point prompt, significantly improving our
segmentation pipeline's accuracy and efficiency. The predicted pre- and
post-contrast segmentations are then fed into our radiomics pipeline, which
extracts features from each tumor and predicts survival using SurvAMINN, a
novel autoencoder-based multiple instance neural network for survival analysis.
SurvAMINN jointly learns dimensionality reduction and hazard prediction from
right-censored survival data, focusing on the most aggressive tumors. Extensive
evaluation on an institutional dataset comprising 227 patients demonstrates
that our framework surpasses existing clinical and genomic biomarkers,
delivering a C-index improvement exceeding 10%. Our results demonstrate the
potential of integrating automated segmentation algorithms and radiomics-based
survival analysis to deliver accurate, annotation-efficient, and interpretable
outcome prediction in CRLM.

</details>


### [7] [Discovering Divergent Representations between Text-to-Image Models](https://arxiv.org/abs/2509.08940)
*Lisa Dunlap,Joseph E. Gonzalez,Trevor Darrell,Fabian Caba Heilbron,Josef Sivic,Bryan Russell*

Main category: cs.CV

TL;DR: 提出CompCon算法和ID2基准，自动发现文本到图像模型间的视觉表示差异，能揭示模型的语义偏差与提示依赖性。


<details>
  <summary>Details</summary>
Motivation: 研究不同生成模型在学习视觉表示时何时以及如何产生分歧，自动发现两模型在某些输入下出现特定视觉属性的倾向差异，以便理解与缓解潜在偏差。

Method: 提出CompCon：基于进化搜索的算法在提示空间中探索，生成驱动差异更显著的提示变体；同时设计自动化的数据生成流程用于构建ID2基准；比较多种LLM和VLM驱动基线方法以验证效果。

Result: 构建了ID2数据集（60个输入相关差异），CompCon在发现模型间差异上优于多个基线；在实际模型比较中发现例如PixArt将“孤独”相关提示生成湿街场景，Stable Diffusion 3.5将提到媒体职业的非裔美国人更频繁出现等差异案例。

Conclusion: 本文提出CompCon，一种进化搜索算法，用于发现两个文本到图像生成模型在视觉表示上的差异属性及触发这些差异的提示词概念。作者构建了自动数据生成流水线并制作了ID2数据集（包含60个输入相关差异）用于评估，并与多种基于大模型的基线方法比较。最终在流行模型间发现了语义偏差示例。

Abstract: In this paper, we investigate when and how visual representations learned by
two different generative models diverge. Given two text-to-image models, our
goal is to discover visual attributes that appear in images generated by one
model but not the other, along with the types of prompts that trigger these
attribute differences. For example, "flames" might appear in one model's
outputs when given prompts expressing strong emotions, while the other model
does not produce this attribute given the same prompts. We introduce CompCon
(Comparing Concepts), an evolutionary search algorithm that discovers visual
attributes more prevalent in one model's output than the other, and uncovers
the prompt concepts linked to these visual differences. To evaluate CompCon's
ability to find diverging representations, we create an automated data
generation pipeline to produce ID2, a dataset of 60 input-dependent
differences, and compare our approach to several LLM- and VLM-powered
baselines. Finally, we use CompCon to compare popular text-to-image models,
finding divergent representations such as how PixArt depicts prompts mentioning
loneliness with wet streets and Stable Diffusion 3.5 depicts African American
people in media professions. Code at: https://github.com/adobe-research/CompCon

</details>


### [8] [An U-Net-Based Deep Neural Network for Cloud Shadow and Sun-Glint Correction of Unmanned Aerial System (UAS) Imagery](https://arxiv.org/abs/2509.08949)
*Yibin Wang,Wondimagegn Beshah,Padmanava Dash,Haifeng Wang*

Main category: cs.CV

TL;DR: 使用像素级U-Net模型检测并纠正UAS图像中的云影与太阳眩光，从而恢复受影响区域，提高水体遥感分析的可靠性。


<details>
  <summary>Details</summary>
Motivation: UAS影像可在多云条件下采集但常受云影影响，水面图像还会受太阳眩光干扰，二者会严重影响基于影像的水质参数估算，因此需要有效方法来识别并修复这些干扰区域以提升检测精度。

Method: 在像素级别从UAS图像中提取训练数据，使用U-Net深度学习模型进行云影与太阳眩光的分割与检测；依据多种评估指标（来自测试集）调整训练参数以确定最佳训练设置；随后基于该模型进行图像校正，恢复被遮挡或受眩光影响的区域。

Result: 通过测试评估指标筛选出高质量的图像校正模型，能够较好地识别并恢复云影和太阳眩光区域，使得校正后图像更适合用于水质参数估算（具体定量结果在摘要中未给出）。

Conclusion: 提出了一种基于深度学习的U-Net方法，用于识别、分离并修复UAS影像中的云影和太阳眩光区域，从而提高水体遥感图像的质量和水质参数估计的可靠性。

Abstract: The use of unmanned aerial systems (UASs) has increased tremendously in the
current decade. They have significantly advanced remote sensing with the
capability to deploy and image the terrain as per required spatial, spectral,
temporal, and radiometric resolutions for various remote sensing applications.
One of the major advantages of UAS imagery is that images can be acquired in
cloudy conditions by flying the UAS under the clouds. The limitation to the
technology is that the imagery is often sullied by cloud shadows. Images taken
over water are additionally affected by sun glint. These are two pose serious
issues for estimating water quality parameters from the UAS images. This study
proposes a novel machine learning approach first to identify and extract
regions with cloud shadows and sun glint and separate such regions from
non-obstructed clear sky regions and sun-glint unaffected regions. The data was
extracted from the images at pixel level to train an U-Net based deep learning
model and best settings for model training was identified based on the various
evaluation metrics from test cases. Using this evaluation, a high-quality image
correction model was determined, which was used to recover the cloud shadow and
sun glint areas in the images.

</details>


### [9] [CoSwin: Convolution Enhanced Hierarchical Shifted Window Attention For Small-Scale Vision](https://arxiv.org/abs/2509.08959)
*Puskal Khadka,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.CV

TL;DR: CoSwin通过在Swin Transformer中嵌入可学习卷积局部增强模块，实现局部与全局特征融合，在小数据集上显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: ViT/Swin等Transformer在小数据集上因缺乏局部性与平移不变等归纳偏置，导致对局部特征的提取不足。为此提出将卷积局部偏置与Transformer全局建模结合。

Method: 在每个注意力模块中加入可学习的局部特征增强模块（基于卷积），与移位窗口自注意力并行或串联融合，以同时捕捉细粒度空间信息与全局语义。

Result: 在多个小尺度数据集（CIFAR-10/100、MNIST、SVHN、Tiny ImageNet）上，相较于基线Swin，CoSwin分别获得约+2.17%、+4.92%、+0.10%、+0.26%、+4.47%的准确度提升，表现稳定优于现有卷积与Transformer模型。

Conclusion: CoSwin在小规模图像数据集上通过将局部卷积特征与Swin Transformer的层次化移位窗口注意力融合，有效提升了局部细节建模能力，从而改善了泛化性与鲁棒性。

Abstract: Vision Transformers (ViTs) have achieved impressive results in computer
vision by leveraging self-attention to model long-range dependencies. However,
their emphasis on global context often comes at the expense of local feature
extraction in small datasets, particularly due to the lack of key inductive
biases such as locality and translation equivariance. To mitigate this, we
propose CoSwin, a novel feature-fusion architecture that augments the
hierarchical shifted window attention with localized convolutional feature
learning. Specifically, CoSwin integrates a learnable local feature enhancement
module into each attention block, enabling the model to simultaneously capture
fine-grained spatial details and global semantic structure. We evaluate CoSwin
on multiple image classification benchmarks including CIFAR-10, CIFAR-100,
MNIST, SVHN, and Tiny ImageNet. Our experimental results show consistent
performance gains over state-of-the-art convolutional and transformer-based
models. Notably, CoSwin achieves improvements of 2.17% on CIFAR-10, 4.92% on
CIFAR-100, 0.10% on MNIST, 0.26% on SVHN, and 4.47% on Tiny ImageNet over the
baseline Swin Transformer. These improvements underscore the effectiveness of
local-global feature fusion in enhancing the generalization and robustness of
transformers for small-scale vision. Code and pretrained weights available at
https://github.com/puskal-khadka/coswin

</details>


### [10] [iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning](https://arxiv.org/abs/2509.08982)
*Karim Slimani,Catherine Achard,Brahim Tamadazte*

Main category: cs.CV

TL;DR: iMatcher是一个端到端可微的点云匹配框架，结合局部图嵌入与全局几何一致性学习，通过双向最近邻重定位精炼匹配，显著提升内点率并在多种数据集上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 提高点云配准中特征匹配的精确性与几何一致性，以获得更高的内点率和更稳健的刚性配准结果。

Method: 基于学习的特征提取→局部图嵌入生成初始得分矩阵→在3D空间实施双向最近邻重新定位以精炼得分→将配对特征堆叠并通过全局几何一致性学习预测点级匹配概率；端到端可微。

Result: 在KITTI、KITTI-360、3DMatch等室外/室内数据集以及TUD-L、MVP-RG任务上取得领先性能；内点率分别达95%–97%（KITTI）、94%–97%（KITTI-360）、最高81.1%（3DMatch）。

Conclusion: iMatcher通过联合局部与全局几何一致性，显著提升点云配准中的特征匹配质量与鲁棒性。

Abstract: This paper presents iMatcher, a fully differentiable framework for feature
matching in point cloud registration. The proposed method leverages learned
features to predict a geometrically consistent confidence matrix, incorporating
both local and global consistency. First, a local graph embedding module leads
to an initialization of the score matrix. A subsequent repositioning step
refines this matrix by considering bilateral source-to-target and
target-to-source matching via nearest neighbor search in 3D space. The paired
point features are then stacked together to be refined through global geometric
consistency learning to predict a point-wise matching probability. Extensive
experiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)
datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partial
matching (MVP-RG), demonstrate that iMatcher significantly improves rigid
registration performance. The method achieves state-of-the-art inlier ratios,
scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,
highlighting its robustness across diverse settings.

</details>


### [11] [UltrON: Ultrasound Occupancy Networks](https://arxiv.org/abs/2509.08991)
*Magdalena Wysocki,Felix Duelmer,Ananya Bal,Nassir Navab,Mohammad Farid Azampour*

Main category: cs.CV

TL;DR: UltrON通过占据场与B-mode声学特征的结合，提出视角补偿损失，实现弱监督多视角超声三维重建，抗遮挡、少标注且具泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统基于SDF或网格/体素的方法依赖精确分割标注且难以处理超声的视角相关性和声影遮挡，作者希望利用B-mode中未被充分利用的声学信息来提升重建鲁棒性与泛化能力。

Method: 采用占据表示替代SDF，设计一种从B-mode图像自提取的声学特征，并提出新型损失函数以补偿B-mode的视角依赖性，在弱监督多视角优化中训练占据场以实现形状重建。

Result: UltrON在弱监督设置下能从多视角B-mode图像中重建相同解剖结构的三维形状，缓解遮挡与稀疏标注带来的性能下降，并展示对同一解剖的泛化能力。

Conclusion: 本论文提出基于占据函数的隐式表示UltrON，利用B-mode超声图像的声学特征进行弱监督优化，从而提高多视角超声重建的几何一致性并缓解遮挡与稀疏标注问题。

Abstract: In free-hand ultrasound imaging, sonographers rely on expertise to mentally
integrate partial 2D views into 3D anatomical shapes. Shape reconstruction can
assist clinicians in this process. Central to this task is the choice of shape
representation, as it determines how accurately and efficiently the structure
can be visualized, analyzed, and interpreted. Implicit representations, such as
SDF and occupancy function, offer a powerful alternative to traditional voxel-
or mesh-based methods by modeling continuous, smooth surfaces with compact
storage, avoiding explicit discretization. Recent studies demonstrate that SDF
can be effectively optimized using annotations derived from segmented B-mode
ultrasound images. Yet, these approaches hinge on precise annotations,
overlooking the rich acoustic information embedded in B-mode intensity.
Moreover, implicit representation approaches struggle with the ultrasound's
view-dependent nature and acoustic shadowing artifacts, which impair
reconstruction. To address the problems resulting from occlusions and
annotation dependency, we propose an occupancy-based representation and
introduce \gls{UltrON} that leverages acoustic features to improve geometric
consistency in weakly-supervised optimization regime. We show that these
features can be obtained from B-mode images without additional annotation cost.
Moreover, we propose a novel loss function that compensates for view-dependency
in the B-mode images and facilitates occupancy optimization from multiview
ultrasound. By incorporating acoustic properties, \gls{UltrON} generalizes to
shapes of the same anatomy. We show that \gls{UltrON} mitigates the limitations
of occlusions and sparse labeling and paves the way for more accurate 3D
reconstruction. Code and dataset will be available at
https://github.com/magdalena-wysocki/ultron.

</details>


### [12] [Implicit Neural Representations of Intramyocardial Motion and Strain](https://arxiv.org/abs/2509.09004)
*Andrew Bell,Yan Kit Choi,Steffen Peterson,Andrew King,Muhummad Sohaib Nazir,Alistair Young*

Main category: cs.CV

TL;DR: 本文用条件隐式神经表示直接回归连续心室位移，既比三种基线更准（2.14 mm RMSE，环向2.86%，径向6.42%），又快约380×，适合大规模CMR应变分析。


<details>
  <summary>Details</summary>
Motivation: 需要在心肌标记MRI中自动准确量化肌内运动与应变，但传统方法在精度或计算效率上存在挑战；希望得到既精确又可扩展到大型数据集的方案。

Method: 使用条件隐式神经表示（INR），将图像/标签信息编码为潜在代码，网络直接输出连续的LV位移场，训练完毕后推理无需额外优化，从而实现快速预测。与三种深度学习基线比较，评估指标包括位移RMSE和全局环向、径向应变误差。

Result: 在452例UK Biobank测试集中，方法达到最佳跟踪精度：位移RMSE 2.14 mm；全局环向应变误差2.86%，径向应变误差6.42%；比最准确的基线快约380倍。

Conclusion: 提出的基于隐式神经表示（INR）并以学习到的潜在编码为条件的方法，能在无需推理时优化的情况下，连续预测左心室位移，在UK Biobank 452例测试中表现优异；方法兼顾高精度与高速度，适合大规模心肌应变分析。

Abstract: Automatic quantification of intramyocardial motion and strain from tagging
MRI remains an important but challenging task. We propose a method using
implicit neural representations (INRs), conditioned on learned latent codes, to
predict continuous left ventricular (LV) displacement -- without requiring
inference-time optimisation. Evaluated on 452 UK Biobank test cases, our method
achieved the best tracking accuracy (2.14 mm RMSE) and the lowest combined
error in global circumferential (2.86%) and radial (6.42%) strain compared to
three deep learning baselines. In addition, our method is $\sim$380$\times$
faster than the most accurate baseline. These results highlight the suitability
of INR-based models for accurate and scalable analysis of myocardial strain in
large CMR datasets.

</details>


### [13] [E-MLNet: Enhanced Mutual Learning for Universal Domain Adaptation with Sample-Specific Weighting](https://arxiv.org/abs/2509.09006)
*Samuel Felipe dos Santos,Tiago Agostinho de Almeida,Jurandy Almeida*

Main category: cs.CV

TL;DR: E-MLNet通过闭集预测驱动的动态权重增强OEM，实现了更有针对性的领域适配，显著提升已知/未知类区分能力，实验证明优于并稳健超越MLNet。


<details>
  <summary>Details</summary>
Motivation: MLNet使用的一对多分类器和OEM在适配过程中对所有分类器一视同仁，导致学习信号被稀释；通过动态聚焦于与样本最相关的类边界可以提高已知/未知类的区分能力。

Method: 在MLNet的基础上，引入动态权重机制，将闭集分类器对目标样本的预测用于对Open-set Entropy Minimization的权重分配，使训练重点放在最相关的一对多分类器上，从而避免平等对待所有一对多分类器而稀释学习信号。

Result: 在Office-31、Office-Home、VisDA-2017和ImageCLEF四个基准上进行广泛实验，E-MLNet在VisDA和ImageCLEF上取得最高平均H-score，并在Open-Partial DA和Open-Set DA设置下分别在31个任务中的22个和19个任务上优于强基线MLNet，显示出更好的性能和稳健性。

Conclusion: 本文提出E-MLNet，通过基于闭集分类器预测的动态加权策略增强OEM，有针对性地强化对每个目标样本最相关的类边界，从而在已知/未知类区分上更为清晰，实验证明在多个基准上优于MLNet并更稳健。

Abstract: Universal Domain Adaptation (UniDA) seeks to transfer knowledge from a
labeled source to an unlabeled target domain without assuming any relationship
between their label sets, requiring models to classify known samples while
rejecting unknown ones. Advanced methods like Mutual Learning Network (MLNet)
use a bank of one-vs-all classifiers adapted via Open-set Entropy Minimization
(OEM). However, this strategy treats all classifiers equally, diluting the
learning signal. We propose the Enhanced Mutual Learning Network (E-MLNet),
which integrates a dynamic weighting strategy to OEM. By leveraging the
closed-set classifier's predictions, E-MLNet focuses adaptation on the most
relevant class boundaries for each target sample, sharpening the distinction
between known and unknown classes. We conduct extensive experiments on four
challenging benchmarks: Office-31, Office-Home, VisDA-2017, and ImageCLEF. The
results demonstrate that E-MLNet achieves the highest average H-scores on VisDA
and ImageCLEF and exhibits superior robustness over its predecessor. E-MLNet
outperforms the strong MLNet baseline in the majority of individual adaptation
tasks -- 22 out of 31 in the challenging Open-Partial DA setting and 19 out of
31 in the Open-Set DA setting -- confirming the benefits of our focused
adaptation strategy.

</details>


### [14] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 作者从 MS COCO 翻译并筛选出 COCO-Urdu（59k 图像、319k 乌尔都语字幕），用多模态质量评估（COMET-Kiwi、CLIP、BERTScore+反译）与开源大模型修正，发布数据与评估管线以减少语言偏见并推动乌尔都语视觉-语言研究。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语用户众多但缺乏大规模高质量多模态数据，导致现有多模态模型在低资源语言上表现欠佳并存在偏见，需要构建公开数据集与质量评估流程以促进包容性研究。

Method: 从 MS COCO 中采用分层抽样保留原始分布，使用 SeamlessM4T v2 翻译生成乌尔都文字幕，采用混合多模态质量评估框架（COMET-Kiwi、CLIP 相似度、BERTScore+反向翻译）进行验证，并对低分字幕用开源大模型迭代修正；基于 BLEU、SacreBLEU 和 chrF 进行基准评测。

Result: 构建出包含59,000张图片与319,000条乌尔都文字幕的 COCO-Urdu 数据集，并公开数据与质量评估管线；基准评测显示在 BLEU、SacreBLEU、chrF 指标上表现稳健，成为迄今最大的公开乌尔都语字幕数据集。

Conclusion: 本文通过构建 COCO-Urdu 大规模图像-字幕数据集，填补了乌尔都语在多模态与视觉-语言研究中的数据空白，对抗高资源语言偏向。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [15] [VoxelFormer: Parameter-Efficient Multi-Subject Visual Decoding from fMRI](https://arxiv.org/abs/2509.09015)
*Chenqian Le,Yilin Zhao,Nikasadat Emami,Kushagra Yadav,Xujin "Chris" Liu,Xupeng Chen,Yao Wang*

Main category: cs.CV

TL;DR: VoxelFormer通过ToMer和Q-Former实现多受试者fMRI视觉解码的参数高效方案，在7T数据集上以更少参数达到竞争性检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有fMRI到图像重建/检索大多依赖受试者专用训练，导致可扩展性差和部署困难。目标是设计一个能在多受试者上训练且参数高效的模型，以提高泛化性和实用性。

Method: 方法包括两部分：1) 使用Token Merging Transformer(ToMer)对高维体素数据进行有效压缩，减少输入token数与计算量；2) 采用query-driven Q-Former生成固定大小的神经表征，这些表征与CLIP图像嵌入空间对齐，便于检索与重建。整体架构轻量化，参数量显著低于现有方法。

Result: 在7T Natural Scenes Dataset上评估，VoxelFormer在包含训练受试者上实现了具有竞争力的检索性能（检索指标与现有方法相近或更好），并使用了更少的参数，证明了token合并和query驱动Transformer在参数效率上的有效性。

Conclusion: 该论文提出VoxelFormer，一种轻量级Transformer架构，支持多受试者fMRI视觉解码，通过Token Merging和query-driven Q-Former实现参数高效的体素压缩与固定尺寸表征，能与CLIP图像嵌入对齐，从而在多受试者训练场景下保持竞争性检索性能。

Abstract: Recent advances in fMRI-based visual decoding have enabled compelling
reconstructions of perceived images. However, most approaches rely on
subject-specific training, limiting scalability and practical deployment. We
introduce \textbf{VoxelFormer}, a lightweight transformer architecture that
enables multi-subject training for visual decoding from fMRI. VoxelFormer
integrates a Token Merging Transformer (ToMer) for efficient voxel compression
and a query-driven Q-Former that produces fixed-size neural representations
aligned with the CLIP image embedding space. Evaluated on the 7T Natural Scenes
Dataset, VoxelFormer achieves competitive retrieval performance on subjects
included during training with significantly fewer parameters than existing
methods. These results highlight token merging and query-based transformers as
promising strategies for parameter-efficient neural decoding.

</details>


### [16] [Integrating Anatomical Priors into a Causal Diffusion Model](https://arxiv.org/abs/2509.09054)
*Binxu Li,Wei Peng,Mingjie Li,Ehsan Adeli,Kilian M. Pohl*

Main category: cs.CV

TL;DR: 提出PCGM：以概率因果图为先验、用体素级掩码约束3D反事实扩散生成，从而在保留细微解剖差异的同时生成高质量脑MRI，验证能复制已知疾病相关微小脑区变化。


<details>
  <summary>Details</summary>
Motivation: 传统反事实生成模型缺乏显式解剖学归纳偏置，难以保留细粒度、医学相关的局部变异，影响生成影像的解剖学可信度。

Method: 提出将基于概率因果图的解剖学约束编码为体素级二值空间掩码，通过3D扩展的ControlNet输入约束反事实去噪UNet，再由3D扩散解码器生成高质量MRI。

Result: 在多数据集上，PCGM生成的结构MRI在质量上优于若干基线方法，并且从合成数据提取的脑区测量能够再现文献中关于疾病的微小形态学效应。

Conclusion: PCGM能生成更具解剖学合理性的脑MRI反事实图像，且其测量结果能复制已报道的疾病对皮层区域的细微影响。

Abstract: 3D brain MRI studies often examine subtle morphometric differences between
cohorts that are hard to detect visually. Given the high cost of MRI
acquisition, these studies could greatly benefit from image syntheses,
particularly counterfactual image generation, as seen in other domains, such as
computer vision. However, counterfactual models struggle to produce
anatomically plausible MRIs due to the lack of explicit inductive biases to
preserve fine-grained anatomical details. This shortcoming arises from the
training of the models aiming to optimize for the overall appearance of the
images (e.g., via cross-entropy) rather than preserving subtle, yet medically
relevant, local variations across subjects. To preserve subtle variations, we
propose to explicitly integrate anatomical constraints on a voxel-level as
prior into a generative diffusion framework. Called Probabilistic Causal Graph
Model (PCGM), the approach captures anatomical constraints via a probabilistic
graph module and translates those constraints into spatial binary masks of
regions where subtle variations occur. The masks (encoded by a 3D extension of
ControlNet) constrain a novel counterfactual denoising UNet, whose encodings
are then transferred into high-quality brain MRIs via our 3D diffusion decoder.
Extensive experiments on multiple datasets demonstrate that PCGM generates
structural brain MRIs of higher quality than several baseline approaches.
Furthermore, we show for the first time that brain measurements extracted from
counterfactuals (generated by PCGM) replicate the subtle effects of a disease
on cortical brain regions previously reported in the neuroscience literature.
This achievement is an important milestone in the use of synthetic MRIs in
studies investigating subtle morphological differences.

</details>


### [17] [Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models](https://arxiv.org/abs/2509.09064)
*Qiuhui Chen,Xuancheng Yao,Huping Ye,Yi Hong*

Main category: cs.CV

TL;DR: 将2D多模态大语言模型引入3D医学影像无监督预训练，通过plane-slice-aware Transformer与部分最优传输对齐，提高语义理解并在分割、分类任务上超越现有SSL方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学影像的卷积和Transformer自监督方法语义理解不足，而2D多模态大语言模型在图像-文本语义上具备优势，作者希望将2D MLLM的语义能力迁移到3D影像表示学习中，提高语义理解和下游任务性能。

Method: 设计一个平面-切片感知的Transformer模块来将3D体数据的切片信息与2D MLLM的文本/图像语义对齐；采用部分最优传输来做鲁棒对齐以容忍LLM生成内容中的噪声；无监督预训练框架可无人工注释地生成标注信号并用于下游分割与分类任务。

Result: 在多个公开CT与MRI数据集上进行预训练并微调，Med3DInsight在分割和分类任务上均达到或超过当前自监督学习（SSL）方法的最先进水平。作者还将开源代码、生成的数据集和预训练模型。

Conclusion: 该论文提出Med3DInsight，通过将3D图像编码器与2D多模态大语言模型(MLLM)连接，利用平面-切片感知（plane-slice-aware）变换模块和部分最优传输（partial optimal transport）对齐，提升3D医学影像的自监督语义理解能力，且在分割与分类任务上优于现有SSL方法。

Abstract: Understanding 3D medical image volumes is critical in the medical field, yet
existing 3D medical convolution and transformer-based self-supervised learning
(SSL) methods often lack deep semantic comprehension. Recent advancements in
multimodal large language models (MLLMs) provide a promising approach to
enhance image understanding through text descriptions. To leverage these 2D
MLLMs for improved 3D medical image understanding, we propose Med3DInsight, a
novel pretraining framework that integrates 3D image encoders with 2D MLLMs via
a specially designed plane-slice-aware transformer module. Additionally, our
model employs a partial optimal transport based alignment, demonstrating
greater tolerance to noise introduced by potential noises in LLM-generated
content. Med3DInsight introduces a new paradigm for scalable multimodal 3D
medical representation learning without requiring human annotations. Extensive
experiments demonstrate our state-of-the-art performance on two downstream
tasks, i.e., segmentation and classification, across various public datasets
with CT and MRI modalities, outperforming current SSL methods. Med3DInsight can
be seamlessly integrated into existing 3D medical image understanding networks,
potentially enhancing their performance. Our source code, generated datasets,
and pre-trained models will be available at
https://github.com/Qybc/Med3DInsight.

</details>


### [18] [Improvement of Human-Object Interaction Action Recognition Using Scene Information and Multi-Task Learning Approach](https://arxiv.org/abs/2509.09067)
*Hesham M. Shehata,Mohammad Abdolrahmani*

Main category: cs.CV

TL;DR: 将固定物体（交互区域）信息嵌入骨架GCN并采用多任务学习，能显著改善人-物交互动作识别，实测准确率从基线提升2.75%至99.25%。


<details>
  <summary>Details</summary>
Motivation: 现有基于骨架的GCN在一般动作识别上表现良好，但在需要场景物体信息的人-物交互识别上效果不足，主要因缺乏场景物体表示和合适的学习结构。作者意图通过引入固定物体信息和多任务学习来弥补这一缺陷。

Method: 在基础的图卷积网络（GCN）骨架动作识别模型上，加入表示固定物体位置/交互区域的输入，并采用多任务学习框架同时学习动作类别与交互判定（或交互区域相关任务）。数据集为作者在公共环境采集的包含固定物体交互（如自动取票机、办理机）和非交互（走路、站立）类别的骨架序列。评估通过比较基线（仅骨架）与提出方法的分类准确率进行。

Result: 在作者构建的数据集上，提出方法的总体识别准确率为99.25%，较仅使用骨架的基线方法提高了2.75%。说明交互区域信息与多任务学习能够有效提升人-物交互动作识别性能。

Conclusion: 本文提出将固定场景中物体信息与多任务学习相结合，以提升基于骨架的动作识别在人-物交互场景下的性能。实验表明加入交互区域信息和多任务损失后，准确率从基线提高了2.75%，达到99.25%。

Abstract: Recent graph convolutional neural networks (GCNs) have shown high performance
in the field of human action recognition by using human skeleton poses.
However, it fails to detect human-object interaction cases successfully due to
the lack of effective representation of the scene information and appropriate
learning architectures. In this context, we propose a methodology to utilize
human action recognition performance by considering fixed object information in
the environment and following a multi-task learning approach. In order to
evaluate the proposed method, we collected real data from public environments
and prepared our data set, which includes interaction classes of hands-on fixed
objects (e.g., ATM ticketing machines, check-in/out machines, etc.) and
non-interaction classes of walking and standing. The multi-task learning
approach, along with interaction area information, succeeds in recognizing the
studied interaction and non-interaction actions with an accuracy of 99.25%,
outperforming the accuracy of the base model using only human skeleton poses by
2.75%.

</details>


### [19] [IRDFusion: Iterative Relation-Map Difference guided Feature Fusion for Multispectral Object Detection](https://arxiv.org/abs/2509.09085)
*Jifeng Shen,Haibo Zhan,Xin Zuo,Heng Fan,Xiaohui Yuan,Jun Li,Wankou Yang*

Main category: cs.CV

TL;DR: 提出基于互补差分反馈的迭代式融合（IRDFusion），通过MFRM与DFFM协同抑噪增强显著结构，显著提升多光谱目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多光谱检测在特征融合过程中保留了大量背景或噪声，限制了感知性能；因此需提出一种能自适应增强目标显著结构并抑制共享背景干扰的融合策略。

Method: 设计了两个模块：Mutual Feature Refinement Module (MFRM)用于建模模态间与模态内关系以提升表征和对齐；Differential Feature Feedback Module (DFFM)计算模态差分特征作为反馈引导，结合MFRM做迭代式的Relation-Map Differential Guided Feature Fusion。整体形成迭代反馈机制逐步放大显著关系信号并抑制共模噪声。

Result: 在FLIR、LLVIP和M^3FD等数据集上，IRDFusion达到了或优于现有最先进方法，且在多种挑战性场景下表现稳健。公开代码可复现。

Conclusion: 该论文提出了一种名为IRDFusion的跨模态特征融合框架，通过互补性特征对比与筛选策略，抑制背景噪声并增强目标结构，从而提升多光谱目标检测性能。

Abstract: Current multispectral object detection methods often retain extraneous
background or noise during feature fusion, limiting perceptual performance.To
address this, we propose an innovative feature fusion framework based on
cross-modal feature contrastive and screening strategy, diverging from
conventional approaches. The proposed method adaptively enhances salient
structures by fusing object-aware complementary cross-modal features while
suppressing shared background interference.Our solution centers on two novel,
specially designed modules: the Mutual Feature Refinement Module (MFRM) and the
Differential Feature Feedback Module (DFFM). The MFRM enhances intra- and
inter-modal feature representations by modeling their relationships, thereby
improving cross-modal alignment and discriminative power.Inspired by feedback
differential amplifiers, the DFFM dynamically computes inter-modal differential
features as guidance signals and feeds them back to the MFRM, enabling adaptive
fusion of complementary information while suppressing common-mode noise across
modalities. To enable robust feature learning, the MFRM and DFFM are integrated
into a unified framework, which is formally formulated as an Iterative
Relation-Map Differential Guided Feature Fusion mechanism, termed IRDFusion.
IRDFusion enables high-quality cross-modal fusion by progressively amplifying
salient relational signals through iterative feedback, while suppressing
feature noise, leading to significant performance gains.In extensive
experiments on FLIR, LLVIP and M$^3$FD datasets, IRDFusion achieves
state-of-the-art performance and consistently outperforms existing methods
across diverse challenging scenarios, demonstrating its robustness and
effectiveness. Code will be available at
https://github.com/61s61min/IRDFusion.git.

</details>


### [20] [SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models](https://arxiv.org/abs/2509.09090)
*Hengyu Fang,Yijiang Liu,Yuan Du,Li Du,Huanrui Yang*

Main category: cs.CV

TL;DR: SQAP-VLA通过协同量化与剪枝（量化感知剪枝标准与改进量化器）实现训练-free的VLA推理加速，带来近2×速度up且保持甚至提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型计算和内存开销巨大，现有压缩方法（量化或token剪枝）通常孤立进行且两者不兼容，导致难以实现综合效率提升。为此需要一种能同时支持两者且不需重新训练的方案。

Method: 通过协同设计量化和token剪枝流程，提出了量化感知的token剪枝判据，允许在高度量化的模型上有效剪枝；同时改进量化器设计以提升剪枝效果，整体为训练-free推理加速策略。

Result: 在标准VLA模型上，SQAP-VLA实现了显著的计算效率与推理速度提升，同时保留核心性能；报告了1.93×的加速以及相比原模型最高4.5%的平均成功率提升。

Conclusion: 本文提出了SQAP-VLA，一个面向Vision-Language-Action模型的结构化、无需训练的推理加速框架，能够同时实现量化与token剪枝，从而显著提升推理效率并保持性能。

Abstract: Vision-Language-Action (VLA) models exhibit unprecedented capabilities for
embodied intelligence. However, their extensive computational and memory costs
hinder their practical deployment. Existing VLA compression and acceleration
approaches conduct quantization or token pruning in an ad-hoc manner but fail
to enable both for a holistic efficiency improvement due to an observed
incompatibility. This work introduces SQAP-VLA, the first structured,
training-free VLA inference acceleration framework that simultaneously enables
state-of-the-art quantization and token pruning. We overcome the
incompatibility by co-designing the quantization and token pruning pipeline,
where we propose new quantization-aware token pruning criteria that work on an
aggressively quantized model while improving the quantizer design to enhance
pruning effectiveness. When applied to standard VLA models, SQAP-VLA yields
significant gains in computational efficiency and inference speed while
successfully preserving core model performance, achieving a $\times$1.93
speedup and up to a 4.5\% average success rate enhancement compared to the
original model.

</details>


### [21] [S-BEVLoc: BEV-based Self-supervised Framework for Large-scale LiDAR Global Localization](https://arxiv.org/abs/2509.09110)
*Chenghao Zhang,Lun Luo,Si-Yuan Cao,Xiaokai Bai,Yuncheng Jin,Zhu Yu,Beinan Yu,Yisen Wang,Hui-Liang Shen*

Main category: cs.CV

TL;DR: S-BEVLoc：一个基于BEV的自监督LiDAR全局定位框架，通过关键点中心BEV补丁构造三元组并用CNN+NetVLAD+SoftCos损失训练，省去位姿标签，在KITTI与NCLT上取得了SOTA效果并具备良好可扩展性。


<details>
  <summary>Details</summary>
Motivation: 动机是现有LiDAR全局定位方法依赖GPS或SLAM里程计提供的高精度位姿标签进行监督训练，获取这些标签成本高且费力，因此希望设计一个不依赖位姿标签且可扩展的自监督方法。

Method: 方法包括：1) 从单帧BEV图像中基于地理距离生成训练三元组（正样本和负样本）；2) 使用CNN提取局部特征，NetVLAD聚合全局描述子；3) 引入SoftCos损失函数以增强来自生成三元组的学习信号。

Result: 在大规模KITTI和NCLT数据集上的实验表明，S-BEVLoc在地点识别、回环闭合和全局定位任务上达到了最先进的性能，同时在可扩展性上优于需要额外标注工作的监督方法。

Conclusion: 该论文提出了一个无监督的BEV（俯视图）LiDAR全局定位框架S-BEVLoc，通过构造基于关键点中心BEV补丁的三元组并利用已知地理距离进行自监督训练，避免了对高精度位姿标签的依赖，具备良好的可扩展性。

Abstract: LiDAR-based global localization is an essential component of simultaneous
localization and mapping (SLAM), which helps loop closure and re-localization.
Current approaches rely on ground-truth poses obtained from GPS or SLAM
odometry to supervise network training. Despite the great success of these
supervised approaches, substantial cost and effort are required for
high-precision ground-truth pose acquisition. In this work, we propose
S-BEVLoc, a novel self-supervised framework based on bird's-eye view (BEV) for
LiDAR global localization, which eliminates the need for ground-truth poses and
is highly scalable. We construct training triplets from single BEV images by
leveraging the known geographic distances between keypoint-centered BEV
patches. Convolutional neural network (CNN) is used to extract local features,
and NetVLAD is employed to aggregate global descriptors. Moreover, we introduce
SoftCos loss to enhance learning from the generated triplets. Experimental
results on the large-scale KITTI and NCLT datasets show that S-BEVLoc achieves
state-of-the-art performance in place recognition, loop closure, and global
localization tasks, while offering scalability that would require extra effort
for supervised approaches.

</details>


### [22] [FPI-Det: a face--phone Interaction Dataset for phone-use detection and understanding](https://arxiv.org/abs/2509.09111)
*Jianqin Gao,Tianqi Wang,Yu Zhang,Yishu Zhang,Chenyuan Wang,Allan Dong,Zihao Wang*

Main category: cs.CV

TL;DR: FPI-Det：包含22,879张标注人脸与手机的多场景数据集，用于研究细粒度人-手机交互检测；提供YOLO/DETR基线与性能分析。


<details>
  <summary>Details</summary>
Motivation: 移动设备普及导致安全监控、生产力评估与注意力管理等场景中需要判断人物是否在使用手机，这要求识别物体并理解人、手与设备之间的行为上下文。现有通用基准无法充分覆盖此类细粒度交互场景，因此需要专门的数据集与基线评估。

Method: 构建大规模数据集FPI-Det，收集多场景图像并进行人脸与手机的精确同步标注；使用现有目标检测模型（YOLO系列、DETR）在数据集上训练与测试，统计不同条件（尺度、遮挡、场景）的检测性能指标，形成基线。

Result: 发布FPI-Det数据集并提供基线实验：YOLO和DETR在该任务上给出基线性能，实验显示在小目标、严重遮挡及复杂环境下性能显著下降，指出了未来研究在细粒度交互理解与鲁棒检测模型方向的挑战。

Conclusion: 本文提出了FPI-Det数据集以弥补现有基准在细粒度人-设备交互检测上的不足，数据集包含22,879张带有面部和手机同步标注的图片，覆盖工作场所、教育、交通和公共场景，具有大尺度变化、频繁遮挡和多样拍摄条件。作者对YOLO和DETR等代表性检测器进行了评估，给出了基线结果并分析了不同目标尺寸、遮挡程度与环境下的性能。

Abstract: The widespread use of mobile devices has created new challenges for vision
systems in safety monitoring, workplace productivity assessment, and attention
management. Detecting whether a person is using a phone requires not only
object recognition but also an understanding of behavioral context, which
involves reasoning about the relationship between faces, hands, and devices
under diverse conditions. Existing generic benchmarks do not fully capture such
fine-grained human--device interactions. To address this gap, we introduce the
FPI-Det, containing 22{,}879 images with synchronized annotations for faces and
phones across workplace, education, transportation, and public scenarios. The
dataset features extreme scale variation, frequent occlusions, and varied
capture conditions. We evaluate representative YOLO and DETR detectors,
providing baseline results and an analysis of performance across object sizes,
occlusion levels, and environments. Source code and dataset is available at
https://github.com/KvCgRv/FPI-Det.

</details>


### [23] [Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention](https://arxiv.org/abs/2509.09116)
*Junhao Xing,Ryohei Miyakawa,Yang Yang,Xinpeng Liu,Risa Shinoda,Hiroaki Santo,Yosuke Toda,Fumio Okura*

Main category: cs.CV

TL;DR: ZeroPlantSeg结合基础分割与视觉-语言推理实现罗塞特植物个体的零样本层次化分割，性能优于其他零次方法，并具备较好跨域泛化。


<details>
  <summary>Details</summary>
Motivation: 现有零次基础分割能提取叶片但难以恢复多片叶重叠的整株植物个体；而监督方法需大量物种特异标注，亟需无需标注的层次化分割方法。

Method: 先用基础分割模型零次提取叶片实例，再结合视觉-语言模型利用植物结构知识进行推理，将叶片聚类归属于各植物个体，形成层次化分割输出。

Result: 在多物种、多生长阶段和不同拍摄环境的数据集上，ZeroPlantSeg优于现有零次方法，并在跨域表现上超过某些监督方法。

Conclusion: 本文提出ZeroPlantSeg，通过融合基础分割模型和视觉-语言推理，实现了从顶视图图像中对罗塞特形植物个体的零样本分割，无需额外训练。

Abstract: Foundation segmentation models achieve reasonable leaf instance extraction
from top-view crop images without training (i.e., zero-shot). However,
segmenting entire plant individuals with each consisting of multiple
overlapping leaves remains challenging. This problem is referred to as a
hierarchical segmentation task, typically requiring annotated training
datasets, which are often species-specific and require notable human labor. To
address this, we introduce ZeroPlantSeg, a zero-shot segmentation for
rosette-shaped plant individuals from top-view images. We integrate a
foundation segmentation model, extracting leaf instances, and a vision-language
model, reasoning about plants' structures to extract plant individuals without
additional training. Evaluations on datasets with multiple plant species,
growth stages, and shooting environments demonstrate that our method surpasses
existing zero-shot methods and achieves better cross-domain performance than
supervised methods. Implementations are available at
https://github.com/JunhaoXing/ZeroPlantSeg.

</details>


### [24] [Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval](https://arxiv.org/abs/2509.09118)
*Tianlu Zheng,Yifan Zhang,Xiang An,Ziyong Feng,Kaicheng Yang,Qichuan Ding*

Main category: cs.CV

TL;DR: 通过构建大规模清洗的行人图文数据(WebPerson)并引入基于梯度-注意力的自适应掩码与掩码预测目标，改进CLIP以获得更好的细粒度行人表征与抗噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP在行人表征上受限于缺乏大规模高质量行人语义数据，且全局对比学习难以保留细粒度局部特征，易受文本噪声影响。

Method: 构建5M规模的WebPerson数据集：利用大模型(in-context learning)自动过滤与生成行人图像文本对；提出GA-DMS框架：基于梯度-注意力相似度自适应掩码筛除噪文本token，并加入掩码token预测任务以强化局部语义对齐。

Result: 在多个基准上达到SOTA性能，实验表明所构数据集与GA-DMS能有效提升细粒度检索与鲁棒性。

Conclusion: 该工作通过数据与模型两方面改进，使CLIP更适用于行人表征学习，显著提升了细粒度匹配能力与抗噪性。

Abstract: Although Contrastive Language-Image Pre-training (CLIP) exhibits strong
performance across diverse vision tasks, its application to person
representation learning faces two critical challenges: (i) the scarcity of
large-scale annotated vision-language data focused on person-centric images,
and (ii) the inherent limitations of global contrastive learning, which
struggles to maintain discriminative local features crucial for fine-grained
matching while remaining vulnerable to noisy text tokens. This work advances
CLIP for person representation learning through synergistic improvements in
data curation and model architecture. First, we develop a noise-resistant data
construction pipeline that leverages the in-context learning capabilities of
MLLMs to automatically filter and caption web-sourced images. This yields
WebPerson, a large-scale dataset of 5M high-quality person-centric image-text
pairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-Masking
Synergetic) framework, which improves cross-modal alignment by adaptively
masking noisy textual tokens based on the gradient-attention similarity score.
Additionally, we incorporate masked token prediction objectives that compel the
model to predict informative text tokens, enhancing fine-grained semantic
representation learning. Extensive experiments show that GA-DMS achieves
state-of-the-art performance across multiple benchmarks.

</details>


### [25] [ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain](https://arxiv.org/abs/2509.09130)
*Bin Huang,Kang Chen,Bingxuan Li,Huafeng Liu,Qiegen Liu*

Main category: cs.CV

TL;DR: A projection-domain latent diffusion model (ALL-PET) uses Radon-based mask augmentation, geometry-constrained masks, and segmentation-driven sinogram attention to enable low-shot, low-resource PET foundation modeling with strong generalization across tasks.


<details>
  <summary>Details</summary>
Motivation: To overcome scarcity of labeled PET data and limited computational resources for building foundation models, by exploiting the projection (sinogram) domain and physics-driven augmentations/attention to improve generalization in low-shot settings.

Method: They build a latent diffusion model operating in sinogram space with three innovations: Radon mask augmentation strategy (RMAS) including dynamic multi-mask (DMM) to create diverse training samples; positive/negative mask constraints to enforce geometric consistency; and transparent medical attention (TMA) using coarse segmentation-projected attention maps, plus clinician-adjustable ROI. Training uses only ~500 samples and runs within 24GB memory.

Result: ALL-PET achieves high-quality sinogram generation and competitive performance on multiple downstream tasks (low-dose reconstruction, attenuation correction, delayed-frame prediction, tracer separation) with only 500 training samples and memory <24GB, matching models trained on larger datasets.

Conclusion: ALL-PET demonstrates that high-quality sinogram generation and downstream PET tasks can be achieved with very limited labeled data and modest compute by leveraging projection-domain diffusion modeling and geometry-aware augmentations/attention.

Abstract: Building large-scale foundation model for PET imaging is hindered by limited
access to labeled data and insufficient computational resources. To overcome
data scarcity and efficiency limitations, we propose ALL-PET, a low-resource,
low-shot PET foundation model operating directly in the projection domain.
ALL-PET leverages a latent diffusion model (LDM) with three key innovations.
First, we design a Radon mask augmentation strategy (RMAS) that generates over
200,000 structurally diverse training samples by projecting randomized
image-domain masks into sinogram space, significantly improving generalization
with minimal data. This is extended by a dynamic multi-mask (DMM) mechanism
that varies mask quantity and distribution, enhancing data diversity without
added model complexity. Second, we implement positive/negative mask constraints
to embed strict geometric consistency, reducing parameter burden while
preserving generation quality. Third, we introduce transparent medical
attention (TMA), a parameter-free, geometry-driven mechanism that enhances
lesion-related regions in raw projection data. Lesion-focused attention maps
are derived from coarse segmentation, covering both hypermetabolic and
hypometabolic areas, and projected into sinogram space for physically
consistent guidance. The system supports clinician-defined ROI adjustments,
ensuring flexible, interpretable, and task-adaptive emphasis aligned with PET
acquisition physics. Experimental results show ALL-PET achieves high-quality
sinogram generation using only 500 samples, with performance comparable to
models trained on larger datasets. ALL-PET generalizes across tasks including
low-dose reconstruction, attenuation correction, delayed-frame prediction, and
tracer separation, operating efficiently with memory use under 24GB.

</details>


### [26] [Noise-Robust Topology Estimation of 2D Image Data via Neural Networks and Persistent Homology](https://arxiv.org/abs/2509.09140)
*Dylan Peek,Matthew P. Skerritt,Stephan Chalup*

Main category: cs.CV

TL;DR: 在结构噪声下，训练好的ANN比基于SEDT的PH方法更能准确预测2D二值图像的Betti数，原因在于其能学习到上下文和几何先验。


<details>
  <summary>Details</summary>
Motivation: 探索在结构噪声存在时，数据驱动的ANN能否作为持久同源性在拓扑估计上的替代或互补方法。

Method: 比较了监督学习的神经网络与基于cubical complexes和SEDT的持久同源性(PH)管道，在一个合成数据集和两个真实世界数据集上的性能，使用不同噪声水平评估Betti数预测准确率。

Result: 实验显示在大部分噪声条件下，ANN的Betti数预测准确率高于PH管道，表明其对结构噪声更鲁棒。

Conclusion: ANN在带噪二值图像的Betti数预测任务中，较之基于PH的SEDT+立方复形方法，对结构噪声表现出更高的鲁棒性，原因可能是ANN能够从训练数据中学习到上下文和几何先验。

Abstract: Persistent Homology (PH) and Artificial Neural Networks (ANNs) offer
contrasting approaches to inferring topological structure from data. In this
study, we examine the noise robustness of a supervised neural network trained
to predict Betti numbers in 2D binary images. We compare an ANN approach
against a PH pipeline based on cubical complexes and the Signed Euclidean
Distance Transform (SEDT), which is a widely adopted strategy for noise-robust
topological analysis. Using one synthetic and two real-world datasets, we show
that ANNs can outperform this PH approach under noise, likely due to their
capacity to learn contextual and geometric priors from training data. Though
still emerging, the use of ANNs for topology estimation offers a compelling
alternative to PH under structural noise.

</details>


### [27] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出面向对象的3D场景评估指标OSIM，利用目标检测特征计算对象相似性，主观实验表明其更符合人类感知，且对现有模型进行了重新评估。


<details>
  <summary>Details</summary>
Motivation: 现有指标更侧重整体图像质量，与人类对3D场景的感知存在偏差；受认知神经学启发，人类识别3D场景依赖对单个对象的关注，因此需要对象中心的评估方法。

Method: 利用目标检测模型及其特征表示来衡量场景中每个对象的"objectness"，并基于对象级别相似性构建评估分数；通过用户研究验证与人类感知的相关性，并对指标特性进行多角度分析。

Result: OSIM在用户研究中比现有通用指标（如PSNR、SSIM等或其它3D专用指标）与人类主观评价更一致；在统一实验设置下，使用OSIM对近作的重建与生成模型进行了重新评估以澄清领域进展。

Conclusion: 本文提出了OSIM，一种基于“对象”概念的3D场景评估指标，能够更贴近人类视觉认知。

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [28] [Video Understanding by Design: How Datasets Shape Architectures and Insights](https://arxiv.org/abs/2509.09151)
*Lei Wang,Piotr Koniusz,Yongsheng Gao*

Main category: cs.CV

TL;DR: 本文首次从数据集驱动的角度审视视频理解领域，识别出四类数据集维度及其对模型归纳偏置的要求，重构了模型发展脉络并给出对齐设计与可扩展性之间权衡的实践建议。


<details>
  <summary>Details</summary>
Motivation: 现有综述多按任务或模型家族分类，忽视了数据集本身对模型结构演化的驱动作用。作者希望通过数据驱动视角揭示不同数据集特点如何要求不同归纳偏置，从而更有针对性地指导模型设计和未来研究方向。

Method: 论文通过综述与分析的方式，将大量视频数据集按四个维度（运动复杂性、时间跨度、层次结构、模态丰富性）进行分类，分析每类数据对模型应编码的归纳偏置，并回顾模型发展（从两流、3D CNN 到序列模型、Transformer、基础多模态模型）如何回应这些偏置，最后给出设计指导原则。

Result: 论文的主要结果是提出了一个统一框架，明确四类数据集维度与相应应编码的归纳偏置之间的对应关系，并将历史模型作为这些压力的实例性回应，最后总结出一系列实践建议和研究路线图以推动通用视频理解的发展。

Conclusion: 该论文提出了以数据集为中心的视角，认为视频任务的发展和模型演进是对数据集中结构性挑战（如运动复杂性、时间跨度、层次组成、多模态丰富性）的回应。通过将历代模型重新解释为对不同数据驱动压力的有针对性设计，论文构建了一个统一框架并提供了对齐模型设计与数据不变性、权衡可扩展性与任务需求的实用建议。

Abstract: Video understanding has advanced rapidly, fueled by increasingly complex
datasets and powerful architectures. Yet existing surveys largely classify
models by task or family, overlooking the structural pressures through which
datasets guide architectural evolution. This survey is the first to adopt a
dataset-driven perspective, showing how motion complexity, temporal span,
hierarchical composition, and multimodal richness impose inductive biases that
models should encode. We reinterpret milestones, from two-stream and 3D CNNs to
sequential, transformer, and multimodal foundation models, as concrete
responses to these dataset-driven pressures. Building on this synthesis, we
offer practical guidance for aligning model design with dataset invariances
while balancing scalability and task demands. By unifying datasets, inductive
biases, and architectures into a coherent framework, this survey provides both
a comprehensive retrospective and a prescriptive roadmap for advancing
general-purpose video understanding.

</details>


### [29] [OCELOT 2023: Cell Detection from Cell-Tissue Interaction Challenge](https://arxiv.org/abs/2509.09153)
*JaeWoong Shin,Jeongun Ryu,Aaron Valero Puche,Jinhee Lee,Biagio Brattoli,Wonkyung Jung,Soo Ick Cho,Kyunghyun Paeng,Chan-Young Ock,Donggeun Yoo,Zhaoyang Li,Wangkai Li,Huayu Mai,Joshua Millward,Zhen He,Aiden Nibali,Lydia Anette Schoenpflug,Viktor Hendrik Koelzer,Xu Shuoyu,Ji Zheng,Hu Bin,Yu-Wen Lo,Ching-Hui Yang,Sérgio Pereira*

Main category: cs.CV

TL;DR: OCELOT 2023 提供首批多尺度重叠细胞-组织注释，参赛方法通过多任务与跨尺度融合显著提升细胞检测，最高比基线提升近8 个F1点，证明了建模细胞—组织关系的必要性。


<details>
  <summary>Details</summary>
Motivation: 病理学家在观察WSI时会在不同放大倍数间切换以判断宏观组织结构与微观细胞细节，现有模型多聚焦单一尺度细胞检测，无法捕捉细胞与组织间的相互语义依赖。缺乏带有重叠细胞与组织标注的数据集，阻碍了研究进展。OCELOT 2023 旨在提供多尺度重叠注释数据并推动相关方法发展。

Method: 主流方法包括联合多任务网络（同时做细胞检测与组织分割）、跨尺度特征融合（如跨层注意力、特征金字塔与多尺度Transformer）、利用重叠标注的监督策略以及后处理融合细胞-组织关系（例如上下文增强的NMS或条件推断）。数据增强与几十张WSI的局部采样策略也常被采用以平衡类不平衡。

Result: OCELOT 2023 数据集包含来自6个器官的673对重叠细胞检测与组织分割标注（306 张 TCGA WSI），参赛模型在测试集上最高比不使用组织信息的基线模型提升了最多7.99 的F1-score，显示出多尺度语义建模带来的实质性收益。论文还对各参赛队伍方法进行比较，归纳出成功策略与未来研究方向。

Conclusion: 研究表明在全片病理图像中同时建模细胞与组织的多尺度语义关系，能显著提升细胞检测性能。OCELOT 2023 挑战验证了这一点，参赛方法比基线提升明显，证明了多尺度信息的重要性。

Abstract: Pathologists routinely alternate between different magnifications when
examining Whole-Slide Images, allowing them to evaluate both broad tissue
morphology and intricate cellular details to form comprehensive diagnoses.
However, existing deep learning-based cell detection models struggle to
replicate these behaviors and learn the interdependent semantics between
structures at different magnifications. A key barrier in the field is the lack
of datasets with multi-scale overlapping cell and tissue annotations. The
OCELOT 2023 challenge was initiated to gather insights from the community to
validate the hypothesis that understanding cell and tissue (cell-tissue)
interactions is crucial for achieving human-level performance, and to
accelerate the research in this field. The challenge dataset includes
overlapping cell detection and tissue segmentation annotations from six organs,
comprising 673 pairs sourced from 306 The Cancer Genome Atlas (TCGA)
Whole-Slide Images with hematoxylin and eosin staining, divided into training,
validation, and test subsets. Participants presented models that significantly
enhanced the understanding of cell-tissue relationships. Top entries achieved
up to a 7.99 increase in F1-score on the test set compared to the baseline
cell-only model that did not incorporate cell-tissue relationships. This is a
substantial improvement in performance over traditional cell-only detection
methods, demonstrating the need for incorporating multi-scale semantics into
the models. This paper provides a comparative analysis of the methods used by
participants, highlighting innovative strategies implemented in the OCELOT 2023
challenge.

</details>


### [30] [RT-DETR++ for UAV Object Detection](https://arxiv.org/abs/2509.09157)
*Yuan Shufang*

Main category: cs.CV

TL;DR: 本文提出RT-DETR++：在encoder引入通道门控的上/下采样注意力（AU/AD），并在fusion中采用CSP-PAC并行空洞卷积，实现对无人机影像中小且密集目标更好的检测，保持实时性与不增复杂度。


<details>
  <summary>Details</summary>
Motivation: 无人机影像中存在小目标、密集排列、尺度变化与遮挡等挑战，传统特征传递和融合方法在保持细节与补偿上下文方面存在不足，需一种轻量且实时的neck/encoder设计提升检测性能。

Method: 在encoder中引入双路径的channel-gated attention upsampling/downsampling（AU/AD），以减少特征传递误差并保持细节；在neck融合阶段采用CSP-PAC，利用并行空洞卷积同时提取局部与上下文信息以更好整合多尺度特征。

Result: 实验表明，所提的neck设计在小目标与密集目标检测上取得了提高，同时维持实时检测速度，且未增加计算复杂度。

Conclusion: RT-DETR++通过改进RT-DETR的encoder，提出AU/AD通道门控注意力上/下采样和CSP-PAC并行空洞卷积的特征融合策略，从而在无人机影像中提高对小目标、密集目标的检测性能，并保持实时性与计算复杂度不增加。

Abstract: Object detection in unmanned aerial vehicle (UAV) imagery presents
significant challenges. Issues such as densely packed small objects, scale
variations, and occlusion are commonplace. This paper introduces RT-DETR++,
which enhances the encoder component of the RT-DETR model. Our improvements
focus on two key aspects. First, we introduce a channel-gated attention-based
upsampling/downsampling (AU/AD) mechanism. This dual-path system minimizes
errors and preserves details during feature layer propagation. Second, we
incorporate CSP-PAC during feature fusion. This technique employs parallel
hollow convolutions to process local and contextual information within the same
layer, facilitating the integration of multi-scale features. Evaluation
demonstrates that our novel neck design achieves superior performance in
detecting small and densely packed objects. The model maintains sufficient
speed for real-time detection without increasing computational complexity. This
study provides an effective approach for feature encoding design in real-time
detection systems.

</details>


### [31] [A Knowledge Noise Mitigation Framework for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2509.09159)
*Zhiyue Liu,Sihang Liu,Jinyuan Liu,Xinru Zhang*

Main category: cs.CV

TL;DR: 提出一个训练-free的知识聚焦框架，通过构建低噪查询、对检索知识进行大模型筛选及选择性融合，有效减少冗余噪声并提升KB-VQA表现。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法直接并入检索知识而忽视大量冗余，导致噪声干扰答案生成，故需一种减少噪声与冗余的无训练解决方案。

Method: 首先从图像-问题对提取关键信息构建低噪查询以提升检索相关性；其次使用大模型对检索到的知识进行筛选，提取对答案有益的片段；最后采用选择性知识融合策略，仅在模型对答案不自信时才引入外部知识。框架无需额外训练。

Result: 在广泛实验中，该方法在提升准确率和鲁棒性方面优于最新方法，显示出更高的知识相关性与更低的噪声影响。

Conclusion: 提出的训练-free框架通过聚焦知识能有效减少检索噪声与冗余，从而提升KB-VQA性能。

Abstract: Knowledge-based visual question answering (KB-VQA) requires a model to
understand images and utilize external knowledge to provide accurate answers.
Existing approaches often directly augment models with retrieved information
from knowledge sources while ignoring substantial knowledge redundancy, which
introduces noise into the answering process. To address this, we propose a
training-free framework with knowledge focusing for KB-VQA, that mitigates the
impact of noise by enhancing knowledge relevance and reducing redundancy.
First, for knowledge retrieval, our framework concludes essential parts from
the image-question pairs, creating low-noise queries that enhance the retrieval
of highly relevant knowledge. Considering that redundancy still persists in the
retrieved knowledge, we then prompt large models to identify and extract
answer-beneficial segments from knowledge. In addition, we introduce a
selective knowledge integration strategy, allowing the model to incorporate
knowledge only when it lacks confidence in answering the question, thereby
mitigating the influence of redundant information. Our framework enables the
acquisition of accurate and critical knowledge, and extensive experiments
demonstrate that it outperforms state-of-the-art methods.

</details>


### [32] [CWSSNet: Hyperspectral Image Classification Enhanced by Wavelet Domain Convolution](https://arxiv.org/abs/2509.09163)
*Yulin Tong,Fengzong Zhang,Haiqin Cheng*

Main category: cs.CV

TL;DR: CWSSNet在结合3D光谱-空间特征与小波域卷积的设计下，有效减少高光谱数据冗余并提升地物分类性能，对关键类别表现尤佳，且在小样本场景中表现稳定。


<details>
  <summary>Details</summary>
Motivation: 高光谱影像虽富含光谱信息但存在波段众多、高维及光谱混合导致的显著特征冗余，传统分类方法性能受限，需新方法提升精细地物识别能力并在小样本下保持鲁棒性。

Method: 构建了以ZY1F高光谱影像为数据源的分类框架CWSSNet，核心包含多尺度卷积注意模块用于融合多模态（3D光谱-空间）信息，并在小波域引入多波段分解与卷积操作以抑制冗余与增强特征表达；在不同训练集比例下进行了性能与时间开销评估。

Result: 在余干县实验中，CWSSNet分别取得mIoU=74.50%、mAcc=82.73%、mF1=84.94%；在水体、植被、裸地类别上IoU最高；当训练集比例为70%时训练时间增长有限且分类效果接近最优，表明小样本训练下性能可靠。

Conclusion: 该论文提出的CWSSNet通过在小样本条件下结合3D光谱-空间特征与小波域卷积，有效缓解高光谱图像的特征冗余并提升分类性能，在研究区（余干县）上取得了较高的mIoU、mAcc和mF1，且对水体、植被与裸地等类具有较好鲁棒性。

Abstract: Hyperspectral remote sensing technology has significant application value in
fields such as forestry ecology and precision agriculture, while also putting
forward higher requirements for fine ground object classification. However,
although hyperspectral images are rich in spectral information and can improve
recognition accuracy, they tend to cause prominent feature redundancy due to
their numerous bands, high dimensionality, and spectral mixing characteristics.
To address this, this study used hyperspectral images from the ZY1F satellite
as a data source and selected Yugan County, Shangrao City, Jiangxi Province as
the research area to perform ground object classification research. A
classification framework named CWSSNet was proposed, which integrates 3D
spectral-spatial features and wavelet convolution. This framework integrates
multimodal information us-ing a multiscale convolutional attention module and
breaks through the classification performance bottleneck of traditional methods
by introducing multi-band decomposition and convolution operations in the
wavelet domain. The experiments showed that CWSSNet achieved 74.50\%, 82.73\%,
and 84.94\% in mean Intersection over Union (mIoU), mean Accuracy (mAcc), and
mean F1-score (mF1) respectively in Yugan County. It also obtained the highest
Intersection over Union (IoU) in the classifica-tion of water bodies,
vegetation, and bare land, demonstrating good robustness. Additionally, when
the training set proportion was 70\%, the increase in training time was
limited, and the classification effect was close to the optimal level,
indicating that the model maintains reliable performance under small-sample
training conditions.

</details>


### [33] [Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios](https://arxiv.org/abs/2509.09172)
*Chunxiao Li,Xiaoxiao Wang,Meiling Li,Boming Miao,Peng Sun,Yunjian Zhang,Xiangyang Ji,Yao Zhu*

Main category: cs.CV

TL;DR: RRDataset用于评估合成图像检测在真实世界场景、网络传播与重数码化下的鲁棒性；基准测试（17个检测器、10个VLMs）与192人类参与者实验显示现有方法普遍脆弱，人类少样本能力可为算法改进提供方向。


<details>
  <summary>Details</summary>
Motivation: 尽管已有检测器能识别合成图像，但它们在现实世界复杂失真、跨场景泛化和网络传播导致的质量退化下的性能尚不清楚，因而需要一个更具现实代表性的数据集与系统评估。

Method: 构建了RRDataset，覆盖7类场景、模拟多轮社交媒体传播与4种重数码化方法；在此基准上评估了17个专用检测器和10个视觉-语言模型，并开展192人次的人类少样本检测研究。

Result: 在RRDataset上大部分检测器在新场景、经过多轮网络传输或重数码化后性能显著下降；视觉-语言模型亦表现有限；人类参与者在少量示例下能快速改善判断，部分情形下超过自动方法。

Conclusion: RRDataset揭示现有AI生成图像检测器在复杂真实世界条件下的显著脆弱性，当前方法在场景泛化、网络传播失真和重数码化三类现实因素下均表现下降。数据集中人类参与者显示出较强的少样本学习能力，提示结合人类适应性可提升检测鲁棒性。

Abstract: With the rapid advancement of generative models, highly realistic image
synthesis has posed new challenges to digital security and media credibility.
Although AI-generated image detection methods have partially addressed these
concerns, a substantial research gap remains in evaluating their performance
under complex real-world conditions. This paper introduces the Real-World
Robustness Dataset (RRDataset) for comprehensive evaluation of detection models
across three dimensions: 1) Scenario Generalization: RRDataset encompasses
high-quality images from seven major scenarios (War and Conflict, Disasters and
Accidents, Political and Social Events, Medical and Public Health, Culture and
Religion, Labor and Production, and everyday life), addressing existing dataset
gaps from a content perspective. 2) Internet Transmission Robustness: examining
detector performance on images that have undergone multiple rounds of sharing
across various social media platforms. 3) Re-digitization Robustness: assessing
model effectiveness on images altered through four distinct re-digitization
methods. We benchmarked 17 detectors and 10 vision-language models (VLMs) on
RRDataset and conducted a large-scale human study involving 192 participants to
investigate human few-shot learning capabilities in detecting AI-generated
images. The benchmarking results reveal the limitations of current AI detection
methods under real-world conditions and underscore the importance of drawing on
human adaptability to develop more robust detection algorithms.

</details>


### [34] [Dark-ISP: Enhancing RAW Image Processing for Low-Light Object Detection](https://arxiv.org/abs/2509.09183)
*Jiasheng Guo,Xin Gao,Yuxiang Yan,Guanghao Li,Jian Pu*

Main category: cs.CV

TL;DR: 提出可微分的轻量级RAW到RGB ISP插件Dark-ISP，拆分线性/非线性模块并加自适应与物理先验，辅以Self-Boost机制，能端到端训练并在低光检测中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有暗光目标检测在RGB图像上受图像退化影响，RAW数据有优势但现有方法要么造成信息损失要么过于复杂，需一种轻量且可端到端训练的RAW处理方法。

Method: 将传统ISP拆分为线性（传感器校准）与非线性（色调映射）子模块，做成可微分组件并加入内容自适应机制和物理先验，通过任务驱动损失优化；利用级联结构设计Self-Boost机制促进子模块协同。

Result: 在三个RAW图像数据集上，Dark-ISP在参数量极小的情况下优于现有RGB和RAW方法，在低光环境下实现更好检测性能。

Conclusion: 该论文提出了一种轻量级、可自适应的ISP插件Dark-ISP，直接处理Bayer RAW图像以提升暗光目标检测性能，并能与检测器端到端训练。

Abstract: Low-light Object detection is crucial for many real-world applications but
remains challenging due to degraded image quality. While recent studies have
shown that RAW images offer superior potential over RGB images, existing
approaches either use RAW-RGB images with information loss or employ complex
frameworks. To address these, we propose a lightweight and self-adaptive Image
Signal Processing (ISP) plugin, Dark-ISP, which directly processes Bayer RAW
images in dark environments, enabling seamless end-to-end training for object
detection. Our key innovations are: (1) We deconstruct conventional ISP
pipelines into sequential linear (sensor calibration) and nonlinear (tone
mapping) sub-modules, recasting them as differentiable components optimized
through task-driven losses. Each module is equipped with content-aware
adaptability and physics-informed priors, enabling automatic RAW-to-RGB
conversion aligned with detection objectives. (2) By exploiting the ISP
pipeline's intrinsic cascade structure, we devise a Self-Boost mechanism that
facilitates cooperation between sub-modules. Through extensive experiments on
three RAW image datasets, we demonstrate that our method outperforms
state-of-the-art RGB- and RAW-based detection approaches, achieving superior
results with minimal parameters in challenging low-light environments.

</details>


### [35] [VQualA 2025 Challenge on Visual Quality Comparison for Large Multimodal Models: Methods and Results](https://arxiv.org/abs/2509.09190)
*Hanwei Zhu,Haoning Wu,Zicheng Zhang,Lingyu Zhu,Yixuan Li,Peilin Chen,Shiqi Wang,Chris Wei Zhou,Linhan Cao,Wei Sun,Xiangyang Zhu,Weixia Zhang,Yucheng Zhu,Jing Liu,Dandan Zhu,Guangtao Zhai,Xiongkuo Min,Zhichao Zhang,Xinyue Li,Shubo Xu,Anh Dao,Yifan Li,Hongyuan Yu,Jiaojiao Yi,Yiding Tian,Yupeng Wu,Feiran Sun,Lijuan Liao,Song Jiang*

Main category: cs.CV

TL;DR: VQualA 2025推出了大规模细粒度视觉质量比较基准与评测协议，通过竞赛验证了指令微调LMM在质量推理上的潜力，但整体表现仍有限，促使未来研究加强可解释性与人类一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LMM在图像理解与生成评估方面缺乏开放域、可解释且细粒度的质量比较能力，需推动模型在视觉质量推理与比较上的进步，从而实现更人类一致的评估系统。

Method: 构建了包含数千个从粗到细的图像质量比较任务（单图、双图、多图），设计2AFC二元偏好和多选题的评测协议，组织线上竞赛并收集参赛模型输出进行定量与定性分析。

Result: 约100名参赛者提交，五个模型展示了指令微调LMM在质量评估上的新兴能力；基准提供了多项性能差异和失败模式分析，表明任务仍具挑战性并需要进一步研究。

Conclusion: 这篇论文总结了VQualA 2025挑战赛，提出并验证了一个大规模、多模态、细粒度的视觉质量比较基准，展示了指令微调的大型多模态模型在开放域质量推理上的初步能力。

Abstract: This paper presents a summary of the VQualA 2025 Challenge on Visual Quality
Comparison for Large Multimodal Models (LMMs), hosted as part of the ICCV 2025
Workshop on Visual Quality Assessment. The challenge aims to evaluate and
enhance the ability of state-of-the-art LMMs to perform open-ended and detailed
reasoning about visual quality differences across multiple images. To this end,
the competition introduces a novel benchmark comprising thousands of
coarse-to-fine grained visual quality comparison tasks, spanning single images,
pairs, and multi-image groups. Each task requires models to provide accurate
quality judgments. The competition emphasizes holistic evaluation protocols,
including 2AFC-based binary preference and multi-choice questions (MCQs).
Around 100 participants submitted entries, with five models demonstrating the
emerging capabilities of instruction-tuned LMMs on quality assessment. This
challenge marks a significant step toward open-domain visual quality reasoning
and comparison and serves as a catalyst for future research on interpretable
and human-aligned quality evaluation systems.

</details>


### [36] [MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network](https://arxiv.org/abs/2509.09200)
*Ge Sun,Jun Ma*

Main category: cs.CV

TL;DR: MGTraj引入多粒度、递归细化的目标引导轨迹预测方法，结合Transformer RRN、权重共享与速度辅助任务，在多个数据集上实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有目标引导方法通常在极端粒度上工作（粗粒度的目标预测和细粒度的逐时刻轨迹补全），忽视了中间时间粒度可能带来的性能提升。因此探究多粒度建模并将其与目标引导框架结合具有潜在收益。

Method: 提出MGTraj框架：自粗到细对轨迹候选进行递归编码。每层使用基于Transformer的递归细化网络（RRN）进行特征提取与逐步修正；不同粒度间采用权重共享来融合特征，并引入速度预测作为辅助任务。

Result: 在ETH/UCY和Stanford Drone Dataset上进行全面实验，MGTraj较基线方法有明显提升，并在目标引导类方法中达到最先进性能。

Conclusion: MGTraj通过多粒度目标引导的递归细化策略提高了行人轨迹预测的准确性，在标准数据集上优于现有目标引导方法，证实了中间时间粒度的价值。

Abstract: Accurate human trajectory prediction is crucial for robotics navigation and
autonomous driving. Recent research has demonstrated that incorporating goal
guidance significantly enhances prediction accuracy by reducing uncertainty and
leveraging prior knowledge. Most goal-guided approaches decouple the prediction
task into two stages: goal prediction and subsequent trajectory completion
based on the predicted goal, which operate at extreme granularities:
coarse-grained goal prediction forecasts the overall intention, while
fine-grained trajectory completion needs to generate the positions for all
future timesteps. The potential utility of intermediate temporal granularity
remains largely unexplored, which motivates multi-granularity trajectory
modeling. While prior work has shown that multi-granularity representations
capture diverse scales of human dynamics and motion patterns, effectively
integrating this concept into goal-guided frameworks remains challenging. In
this paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for
human Trajectory prediction. MGTraj recursively encodes trajectory proposals
from coarse to fine granularity levels. At each level, a transformer-based
recursive refinement network (RRN) captures features and predicts progressive
refinements. Features across different granularities are integrated using a
weight-sharing strategy, and velocity prediction is employed as an auxiliary
task to further enhance performance. Comprehensive experimental results in
EHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline
methods and achieves state-of-the-art performance among goal-guided methods.

</details>


### [37] [Medverse: A Universal Model for Full-Resolution 3D Medical Image Segmentation, Transformation and Enhancement](https://arxiv.org/abs/2509.09232)
*Jiesi Hu,Jianfeng Cao,Yanwu Yang,Chenfei Ye,Yixuan Zhang,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: Medverse是一种在22个数据集上训练的3D医学影像通用ICL模型，采用逐级自回归生成和块状交叉注意力实现高分辨率与全局解剖一致性，实验证明其在未见数据上的显著优越性。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像的ICL模型难以同时兼顾细节保真度与全局解剖一致性，且缺乏一个统一训练、适用于多任务多器官的模型，限制了ICL在医学影像的潜力。

Method: 在22个数据集上进行联合训练，采用next-scale自回归逐步从粗到细生成全分辨率体积预测，同时设计了块状交叉注意力以在保留空间稀疏性的同时实现长程上下文交互；训练任务涵盖分割、变换和增强等多种影像处理任务。

Result: 在多个未见临床中心、器官、物种和成像模态的保留测试集上，Medverse显著优于现有ICL基线模型，并展示了在多任务、跨域环境下的泛化能力。

Conclusion: 该论文提出了Medverse，一个面向3D医学影像的通用ICL模型，通过在多任务、多器官、多模态数据上训练并采用逐级细化的自回归框架与块状交叉注意力模块，实现了高分辨率体积输出与跨尺度解剖理解，从而在未见数据集上优于现有ICL基线。

Abstract: In-context learning (ICL) offers a promising paradigm for universal medical
image analysis, enabling models to perform diverse image processing tasks
without retraining. However, current ICL models for medical imaging remain
limited in two critical aspects: they cannot simultaneously achieve
high-fidelity predictions and global anatomical understanding, and there is no
unified model trained across diverse medical imaging tasks (e.g., segmentation
and enhancement) and anatomical regions. As a result, the full potential of ICL
in medical imaging remains underexplored. Thus, we present \textbf{Medverse}, a
universal ICL model for 3D medical imaging, trained on 22 datasets covering
diverse tasks in universal image segmentation, transformation, and enhancement
across multiple organs, imaging modalities, and clinical centers. Medverse
employs a next-scale autoregressive in-context learning framework that
progressively refines predictions from coarse to fine, generating consistent,
full-resolution volumetric outputs and enabling multi-scale anatomical
awareness. We further propose a blockwise cross-attention module that
facilitates long-range interactions between context and target inputs while
preserving computational efficiency through spatial sparsity. Medverse is
extensively evaluated on a broad collection of held-out datasets covering
previously unseen clinical centers, organs, species, and imaging modalities.
Results demonstrate that Medverse substantially outperforms existing ICL
baselines and establishes a novel paradigm for in-context learning. Code and
model weights will be made publicly available. Our model are publicly available
at https://github.com/jiesihu/Medverse.

</details>


### [38] [CoAtNeXt:An Attention-Enhanced ConvNeXtV2-Transformer Hybrid Model for Gastric Tissue Classification](https://arxiv.org/abs/2509.09242)
*Mustafa Yurdakul,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 提出CoAtNeXt，通过替换模块和加入CBAM提升局部特征提取，在两数据集上显著优于多种CNN/ViT，具备辅助病理诊断潜力。


<details>
  <summary>Details</summary>
Motivation: 传统病理组织学检查费时且主观性强，存在漏诊与一致性差的问题，亟需自动化、可靠的胃组织图像分析方法以辅助病理诊断。

Method: 在CoAtNet基础上，用改进的ConvNeXtV2模块替代MBConv层，并引入CBAM注意力模块，调整网络规模以平衡计算成本与性能；在两套公开数据集上与多种CNN和ViT模型进行比较验证。

Result: 在HMU-GC-HE-30K上：准确率96.47%、精确率96.60%、召回率96.47%、F1 96.45%、AUC 99.89%；在GasHisSDB上：准确率98.29%、精确率98.07%、召回率98.41%、F1 98.23%、AUC 99.90%，超越比较的CNN与ViT模型及文献方法。

Conclusion: 该研究提出的CoAtNeXt在两个公开胃组织病理图像数据集上均取得了优异分类性能，显示出较强的实际应用潜力。

Abstract: Background and objective Early diagnosis of gastric diseases is crucial to
prevent fatal outcomes. Although histopathologic examination remains the
diagnostic gold standard, it is performed entirely manually, making evaluations
labor-intensive and prone to variability among pathologists. Critical findings
may be missed, and lack of standard procedures reduces consistency. These
limitations highlight the need for automated, reliable, and efficient methods
for gastric tissue analysis. Methods In this study, a novel hybrid model named
CoAtNeXt was proposed for the classification of gastric tissue images. The
model is built upon the CoAtNet architecture by replacing its MBConv layers
with enhanced ConvNeXtV2 blocks. Additionally, the Convolutional Block
Attention Module (CBAM) is integrated to improve local feature extraction
through channel and spatial attention mechanisms. The architecture was scaled
to achieve a balance between computational efficiency and classification
performance. CoAtNeXt was evaluated on two publicly available datasets,
HMU-GC-HE-30K for eight-class classification and GasHisSDB for binary
classification, and was compared against 10 Convolutional Neural Networks
(CNNs) and ten Vision Transformer (ViT) models. Results CoAtNeXt achieved
96.47% accuracy, 96.60% precision, 96.47% recall, 96.45% F1 score, and 99.89%
AUC on HMU-GC-HE-30K. On GasHisSDB, it reached 98.29% accuracy, 98.07%
precision, 98.41% recall, 98.23% F1 score, and 99.90% AUC. It outperformed all
CNN and ViT models tested and surpassed previous studies in the literature.
Conclusion Experimental results show that CoAtNeXt is a robust architecture for
histopathological classification of gastric tissue images, providing
performance on binary and multiclass. Its highlights its potential to assist
pathologists by enhancing diagnostic accuracy and reducing workload.

</details>


### [39] [Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis](https://arxiv.org/abs/2509.09254)
*Jing Hao,Yuxuan Fan,Yanpeng Sun,Kaixin Guo,Lizhuo Lin,Jinrong Yang,Qi Yong H. Ai,Lun M. Wong,Hao Tang,Kuo Feng Hung*

Main category: cs.CV

TL;DR: 作者发布了面向牙科全景X光的大规模多模态指令数据集（MMOral）与基准，并通过SFT得到OralGPT，显著提升LVLM在该领域的表现，但总体模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM在专科影像（如牙科全景片）上的能力未知，且公开指令数据与基准缺乏对密集解剖与微小病变的覆盖，需构建专门数据集与评估以促进临床可用性。

Method: 收集20,563张全景X光并通过规则与人工标注扩展为约1.3M条指令式训练实例，设计多任务（属性抽取、报告生成、VQA、图像对话）和五维诊断评估套件（MMOral-Bench）；在Qwen2.5-VL-7B上进行单轮有监督微调得到OralGPT；对64个LVLM在基准上进行评估。

Result: 构建MMOral（20,563图像、1.3M指令实例）与MMOral-Bench，评估显示GPT-4o在基准上最高仅41.45%准确率，表明现有模型不足；通过一轮SFT，OralGPT提升显著（例如24.73%提升）。数据集与模型在GitHub公开。

Conclusion: 该论文构建了首个针对全景牙科X光的多模态指令数据集与基准，并提出微调模型以提升LVLM在口腔影像诊断上的性能。

Abstract: Recent advances in large vision-language models (LVLMs) have demonstrated
strong performance on general-purpose medical tasks. However, their
effectiveness in specialized domains such as dentistry remains underexplored.
In particular, panoramic X-rays, a widely used imaging modality in oral
radiology, pose interpretative challenges due to dense anatomical structures
and subtle pathological cues, which are not captured by existing medical
benchmarks or instruction datasets. To this end, we introduce MMOral, the first
large-scale multimodal instruction dataset and benchmark tailored for panoramic
X-ray interpretation. MMOral consists of 20,563 annotated images paired with
1.3 million instruction-following instances across diverse task types,
including attribute extraction, report generation, visual question answering,
and image-grounded dialogue. In addition, we present MMOral-Bench, a
comprehensive evaluation suite covering five key diagnostic dimensions in
dentistry. We evaluate 64 LVLMs on MMOral-Bench and find that even the
best-performing model, i.e., GPT-4o, only achieves 41.45% accuracy, revealing
significant limitations of current models in this domain. To promote the
progress of this specific domain, we also propose OralGPT, which conducts
supervised fine-tuning (SFT) upon Qwen2.5-VL-7B with our meticulously curated
MMOral instruction dataset. Remarkably, a single epoch of SFT yields
substantial performance enhancements for LVLMs, e.g., OralGPT demonstrates a
24.73% improvement. Both MMOral and OralGPT hold significant potential as a
critical foundation for intelligent dentistry and enable more clinically
impactful multimodal AI systems in the dental field. The dataset, model,
benchmark, and evaluation suite are available at
https://github.com/isbrycee/OralGPT.

</details>


### [40] [DATE: Dynamic Absolute Time Enhancement for Long Video Understanding](https://arxiv.org/abs/2509.09263)
*Chao Yuan,Yang Yang,Yehui Yang,Zach Cheng*

Main category: cs.CV

TL;DR: 通过时间戳注入和语义引导的相似性抽样，DATE显著增强MLLM对长视频绝对时间与关键事件的理解，并实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法对长距离时序依赖建模不足，统一帧抽样与隐式位置编码导致关键信息丢失和时序理解下降，需增强绝对时间感知与关键事件定位。

Method: 提出Timestamp Injection Mechanism (TIM)，在视频帧嵌入中交织文本时间戳token，构建连续时间参考；提出Temporal-Aware Similarity Sampling (TASS)，把抽样视为视文检索，先将查询扩展为描述性caption，再用相似性驱动且时间正则化的贪心策略采样关键事件。

Result: 在小时级视频基准上，DATE在7B和72B模型上均取得SOTA，显著提升绝对时间理解和关键事件定位性能；7B模型在部分基准上甚至超过许多72B模型。

Conclusion: DATE通过插入时间戳token和语义引导的抽样策略，有效提升了MLLM对长视频中绝对时间和事件定位的理解能力。

Abstract: Long video understanding remains a fundamental challenge for multimodal large
language models (MLLMs), particularly in tasks requiring precise temporal
reasoning and event localization. Existing approaches typically adopt uniform
frame sampling and rely on implicit position encodings to model temporal order.
However, these methods struggle with long-range dependencies, leading to
critical information loss and degraded temporal comprehension. In this paper,
we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporal
awareness in MLLMs through the Timestamp Injection Mechanism (TIM) and a
semantically guided Temporal-Aware Similarity Sampling (TASS) strategy.
Specifically, we interleave video frame embeddings with textual timestamp
tokens to construct a continuous temporal reference system. We further
reformulate the video sampling problem as a vision-language retrieval task and
introduce a two-stage algorithm to ensure both semantic relevance and temporal
coverage: enriching each query into a descriptive caption to better align with
the vision feature, and sampling key event with a similarity-driven temporally
regularized greedy strategy. Our method achieves remarkable improvements w.r.t.
absolute time understanding and key event localization, resulting in
state-of-the-art performance among 7B and 72B models on hour-long video
benchmarks. Particularly, our 7B model even exceeds many 72B models on some
benchmarks.

</details>


### [41] [Unified Start, Personalized End: Progressive Pruning for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2509.09267)
*Linhao Li,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: 提出基于逐块剪枝与功能解耦损失的渐进剪枝框架PSP-Seg，可动态生成高效3D分割模型，在多数据集上显著降低资源开销并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有高效分割模型多为静态、手工设计，难以在多任务和资源约束下灵活权衡性能与效率，需一种可动态适配且自动化的轻量化方法。

Method: 从冗余大模型出发，采用模块化的逐块（block-wise）剪枝并结合功能解耦（functional decoupling）损失来评估并去除冗余模块，迭代优化得到轻量化模型。

Result: 在五个公开数据集上对比七个SOTA模型和六个高效分割模型，PSP-Seg-S在保持与nnU-Net相当性能的同时，减少42-45% GPU显存、29-48%训练时间和83-87%参数量。

Conclusion: PSP-Seg通过渐进剪枝在保持分割性能的同时显著减少计算资源、训练时间和参数量，实现了高效且可动态适配的3D医学图像分割解决方案。

Abstract: 3D medical image segmentation often faces heavy resource and time
consumption, limiting its scalability and rapid deployment in clinical
environments. Existing efficient segmentation models are typically static and
manually designed prior to training, which restricts their adaptability across
diverse tasks and makes it difficult to balance performance with resource
efficiency. In this paper, we propose PSP-Seg, a progressive pruning framework
that enables dynamic and efficient 3D segmentation. PSP-Seg begins with a
redundant model and iteratively prunes redundant modules through a combination
of block-wise pruning and a functional decoupling loss. We evaluate PSP-Seg on
five public datasets, benchmarking it against seven state-of-the-art models and
six efficient segmentation models. Results demonstrate that the lightweight
variant, PSP-Seg-S, achieves performance on par with nnU-Net while reducing GPU
memory usage by 42-45%, training time by 29-48%, and parameter number by 83-87%
across all datasets. These findings underscore PSP-Seg's potential as a
cost-effective yet high-performing alternative for widespread clinical
application.

</details>


### [42] [Visual Programmability: A Guide for Code-as-Thought in Chart Understanding](https://arxiv.org/abs/2509.09286)
*Bohao Tang,Yan Ma,Fei Zhang,Jiadi Su,Ethan Chern,Zhulin Hu,Zhixin Wang,Pengfei Liu,Ya Zhang*

Main category: cs.CV

TL;DR: 提出可验证的Code-as-Thought并引入视觉可编程性，通过双奖励强化学习让VLM在代码与视觉推理间自适应切换，从而提升图表理解的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部工具或单一的文本链式思维，存在工具脆弱性或中间步骤不可验证、难以用强化学习改进的问题；因此需要一种既可验证又能自适应选择推理方式的方案。

Method: 构建一个包含两条通路的自适应框架：CaT通路将图表信息转换为可验证的符号化代码，直接视觉通路进行视觉推理；一个策略网络（由VLM实现）学习在两者间选择，策略通过强化学习训练，使用双重奖励（数据准确性奖励+决策奖励）。

Result: 在多种图表理解基准上表现出强健与高效的性能，能够动态选择最优推理路径，减少数值幻觉并提高事实准确性。

Conclusion: 本文提出将代码作为思维（Code-as-Thought, CaT）用于图表理解，并引入可学习的“视觉可编程性”以在代码与直接视觉推理之间自适应选择，从而提高推理可验证性与准确性。

Abstract: Chart understanding presents a critical test to the reasoning capabilities of
Vision-Language Models (VLMs). Prior approaches face critical limitations: some
rely on external tools, making them brittle and constrained by a predefined
toolkit, while others fine-tune specialist models that often adopt a single
reasoning strategy, such as text-based chain-of-thought (CoT). The intermediate
steps of text-based reasoning are difficult to verify, which complicates the
use of reinforcement-learning signals that reward factual accuracy. To address
this, we propose a Code-as-Thought (CaT) approach to represent the visual
information of a chart in a verifiable, symbolic format. Our key insight is
that this strategy must be adaptive: a fixed, code-only implementation
consistently fails on complex charts where symbolic representation is
unsuitable. This finding leads us to introduce Visual Programmability: a
learnable property that determines if a chart-question pair is better solved
with code or direct visual analysis. We implement this concept in an adaptive
framework where a VLM learns to choose between the CaT pathway and a direct
visual reasoning pathway. The selection policy of the model is trained with
reinforcement learning using a novel dual-reward system. This system combines a
data-accuracy reward to ground the model in facts and prevent numerical
hallucination, with a decision reward that teaches the model when to use each
strategy, preventing it from defaulting to a single reasoning mode. Experiments
demonstrate strong and robust performance across diverse chart-understanding
benchmarks. Our work shows that VLMs can be taught not only to reason but also
how to reason, dynamically selecting the optimal reasoning pathway for each
task.

</details>


### [43] [Modality-Agnostic Input Channels Enable Segmentation of Brain lesions in Multimodal MRI with Sequences Unavailable During Training](https://arxiv.org/abs/2509.09290)
*Anthony P. Addison,Felix Wagner,Wentian Xu,Natalie Voets,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 通过在U-net中加入模态不可知通道并用合成MRI模态增强训练，模型能同时处理已见和未见影像模态，提高多模态脑病灶分割的灵活性与性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态脑MRI分割模型通常受限于固定输入模态，无法在推断时处理未见或异构模态组合；需要一种既能泛化到新模态又能保留模态特异判别信息的方法。

Method: 在U-net结构中并行添加模态不可知路径和模态特定路径；设计一种图像增强策略，合成新的MRI对比：对病灶与健康组织做差异化外观变换以产生人工对比，同时保持解剖结构真实。训练时混合真实模态与合成模态。

Result: 在8个MRI数据库、5类病变、8种常见模态上评估，结果表明方法在保持对训练模态分割能力的同时，能有效处理未见模态并提升分割性能。

Conclusion: 本文提出在U-net中加入模态不可知（modality-agnostic）输入通道，结合模态特定通道，并用合成MRI模态的数据增强训练该不可知组件，从而实现对训练中未见模态的推断能力，同时保持对已见模态的性能。

Abstract: Segmentation models are important tools for the detection and analysis of
lesions in brain MRI. Depending on the type of brain pathology that is imaged,
MRI scanners can acquire multiple, different image modalities (contrasts). Most
segmentation models for multimodal brain MRI are restricted to fixed modalities
and cannot effectively process new ones at inference. Some models generalize to
unseen modalities but may lose discriminative modality-specific information.
This work aims to develop a model that can perform inference on data that
contain image modalities unseen during training, previously seen modalities,
and heterogeneous combinations of both, thus allowing a user to utilize any
available imaging modalities. We demonstrate this is possible with a simple,
thus practical alteration to the U-net architecture, by integrating a
modality-agnostic input channel or pathway, alongside modality-specific input
channels. To train this modality-agnostic component, we develop an image
augmentation scheme that synthesizes artificial MRI modalities. Augmentations
differentially alter the appearance of pathological and healthy brain tissue to
create artificial contrasts between them while maintaining realistic anatomical
integrity. We evaluate the method using 8 MRI databases that include 5 types of
pathologies (stroke, tumours, traumatic brain injury, multiple sclerosis and
white matter hyperintensities) and 8 modalities (T1, T1+contrast, T2, PD, SWI,
DWI, ADC and FLAIR). The results demonstrate that the approach preserves the
ability to effectively process MRI modalities encountered during training,
while being able to process new, unseen modalities to improve its segmentation.
Project code: https://github.com/Anthony-P-Addison/AGN-MOD-SEG

</details>


### [44] [Model-Agnostic Open-Set Air-to-Air Visual Object Detection for Reliable UAV Perception](https://arxiv.org/abs/2509.09297)
*Spyridon Loukovitis,Anastasios Arsenos,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出基于嵌入空间熵建模并辅以谱归一化与温度缩放的模型不可知开集检测框架，能更好地拒绝未知目标并抵抗飞行数据损坏，在AOT基准和真实飞行测试上显著优于YOLO基线。


<details>
  <summary>Details</summary>
Motivation: 传统的闭集检测器在领域漂移和飞行数据损坏（如噪声、模糊、遮挡）下性能急剧下降，无法可靠应对真实空对空场景中的未知目标和环境变化，进而影响无人机的安全性和自主性。

Method: 方法通过在嵌入空间对语义不确定性进行熵建模来显式估计未知目标的概率，并结合谱归一化（spectral normalization）与温度缩放（temperature scaling）提高嵌入表示的判别能力和开集识别性能；同时引入背景拒绝模块以增强对背景和扰动数据的鲁棒性。

Result: 在AOT空中基准与真实飞行测试中，作者的方法对比标准YOLO类检测器在AUROC上取得最多约10%的相对提升；消融实验表明谱归一化、温度缩放、熵建模和背景拒绝各贡献显著，且在不损失检测精度的前提下提高了未知目标的拒绝率与整体鲁棒性。

Conclusion: 该论文提出了一种针对嵌入式检测器的模型不可知（model-agnostic）开集检测框架，旨在在空对空飞行环境中提高无人机对未知目标和飞行数据扰动的鲁棒性。

Abstract: Open-set detection is crucial for robust UAV autonomy in air-to-air object
detection under real-world conditions. Traditional closed-set detectors degrade
significantly under domain shifts and flight data corruption, posing risks to
safety-critical applications. We propose a novel, model-agnostic open-set
detection framework designed specifically for embedding-based detectors. The
method explicitly handles unknown object rejection while maintaining robustness
against corrupted flight data. It estimates semantic uncertainty via entropy
modeling in the embedding space and incorporates spectral normalization and
temperature scaling to enhance open-set discrimination. We validate our
approach on the challenging AOT aerial benchmark and through extensive
real-world flight tests. Comprehensive ablation studies demonstrate consistent
improvements over baseline methods, achieving up to a 10\% relative AUROC gain
compared to standard YOLO-based detectors. Additionally, we show that
background rejection further strengthens robustness without compromising
detection accuracy, making our solution particularly well-suited for reliable
UAV perception in dynamic air-to-air environments.

</details>


### [45] [Learning Object-Centric Representations in SAR Images with Multi-Level Feature Fusion](https://arxiv.org/abs/2509.09298)
*Oh-Tae Jang,Min-Gon Cho,Kyung-Tae Kim*

Main category: cs.CV

TL;DR: SlotSAR通过结合高低层特征与多层槽注意力，在无标签掩码情况下实现目标-背景解耦，显著提升SAR目标表征与识别性能。


<details>
  <summary>Details</summary>
Motivation: SAR图像中复杂背景（地形反射与斑点噪声）常与目标呈现相似强度和模式，导致模型学习到混杂或伪特征，削弱目标表征能力，因此需要在无掩码注释下将目标与背景分离以提升识别性能。

Method: 使用SARATR-X提取高层语义特征，使用小波散射网络提取低层散射特征，形成互补多层表示；提出多层槽注意力模块融合这些特征以增强槽级表示的区分性，实现无监督的对象中心学习。

Result: 实验证明SlotSAR在SAR图像上相比现有对象中心学习方法保留了更多结构细节并取得了最先进的性能。

Conclusion: SlotSAR能在无掩码注释情况下将目标表征与背景混杂分离，通过多层特征融合与槽注意力提高目标表征区分性，从而增强SAR目标识别的鲁棒性。

Abstract: Synthetic aperture radar (SAR) images contain not only targets of interest
but also complex background clutter, including terrain reflections and speckle
noise. In many cases, such clutter exhibits intensity and patterns that
resemble targets, leading models to extract entangled or spurious features.
Such behavior undermines the ability to form clear target representations,
regardless of the classifier. To address this challenge, we propose a novel
object-centric learning (OCL) framework, named SlotSAR, that disentangles
target representations from background clutter in SAR images without mask
annotations. SlotSAR first extracts high-level semantic features from SARATR-X
and low-level scattering features from the wavelet scattering network in order
to obtain complementary multi-level representations for robust target
characterization. We further present a multi-level slot attention module that
integrates these low- and high-level features to enhance slot-wise
representation distinctiveness, enabling effective OCL. Experimental results
demonstrate that SlotSAR achieves state-of-the-art performance in SAR imagery
by preserving structural details compared to existing OCL methods.

</details>


### [46] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: MatCha：首个1500题材料表征图像理解基准，揭示现有MLLMs在真实材料表征场景中性能不足，促进后续研究。


<details>
  <summary>Details</summary>
Motivation: 弥补当前MLLMs在处理真实世界材料表征成像数据能力的评估空白，推动材料发现与自主科学代理研究。

Method: 构建包含1500道专家级问题、覆盖材料研究四个关键阶段和21个任务的基准数据集，并用最新MLLMs进行评测，测试few-shot和chain-of-thought提示策略的效果。

Result: 评测显示SOTA MLLMs在复杂视觉感知和高阶专业知识问题上表现明显落后，简单提示策略难以改善。

Conclusion: MatCha首次提出材料表征图像理解基准，揭示现有多模态大语言模型在真实材料表征任务上与专家存在显著差距。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [47] [You Share Beliefs, I Adapt: Progressive Heterogeneous Collaborative Perception](https://arxiv.org/abs/2509.09310)
*Hao Si,Ehsan Javanmardi,Manabu Tsukada*

Main category: cs.CV

TL;DR: PHCP通过推理时对adapter进行少样本无监督自训练，实现在异构车辆协同感知中无需联合训练也能达到接近SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 现实中不同车辆由不同厂商生产，模型多样且难以提前联合训练或存储所有模型；现有方法需要联合训练或保存模型，不适合实际部署，因此提出在推理时直接自适应的方法。

Method: 将问题建模为少样本无监督域自适应，在推理时自训练一个adapter以动态对齐特征；使用无标签数据进行自监督训练，逐步优化adapter并保持原模型参数固定或轻微调整，从而实现实时适配。

Result: 在OPV2V数据集上的大量实验显示，PHCP在多种异构场景下表现优异；在只使用少量无标签数据的情况下，性能可媲美在整个数据集上训练的最先进方法。

Conclusion: 本文提出PHCP，在推理阶段通过少样本无监督域自适应解决车辆协同感知中的模型异构问题，无需事先联合训练或存储所有合作者模型，解决了实用性瓶颈。

Abstract: Collaborative perception enables vehicles to overcome individual perception
limitations by sharing information, allowing them to see further and through
occlusions. In real-world scenarios, models on different vehicles are often
heterogeneous due to manufacturer variations. Existing methods for
heterogeneous collaborative perception address this challenge by fine-tuning
adapters or the entire network to bridge the domain gap. However, these methods
are impractical in real-world applications, as each new collaborator must
undergo joint training with the ego vehicle on a dataset before inference, or
the ego vehicle stores models for all potential collaborators in advance.
Therefore, we pose a new question: Can we tackle this challenge directly during
inference, eliminating the need for joint training? To answer this, we
introduce Progressive Heterogeneous Collaborative Perception (PHCP), a novel
framework that formulates the problem as few-shot unsupervised domain
adaptation. Unlike previous work, PHCP dynamically aligns features by
self-training an adapter during inference, eliminating the need for labeled
data and joint training. Extensive experiments on the OPV2V dataset demonstrate
that PHCP achieves strong performance across diverse heterogeneous scenarios.
Notably, PHCP achieves performance comparable to SOTA methods trained on the
entire dataset while using only a small amount of unlabeled data.

</details>


### [48] [Image Recognition with Vision and Language Embeddings of VLMs](https://arxiv.org/abs/2509.09311)
*Illia Volkov,Nikita Kisel,Klara Janouskova,Jiri Matas*

Main category: cs.CV

TL;DR: 系统比较了语言引导与视觉仅基线在多种双编码器VLM上的ImageNet分类表现，分析关键影响因素并用按类精度的无学习融合方法结合二者获得更好性能。


<details>
  <summary>Details</summary>
Motivation: 虽然VLMs在图像-文本对齐上用于零样本分类表现强劲，但其在不依赖文本提示的纯视觉分类能力尚未系统研究，且文本与视觉线索可能互补，值得探讨如何结合二者以提升图像识别。

Method: 在ImageNet-1k验证集及其标签修正版本上，比较了语言引导（基于文本提示的零样本）和纯视觉（k-NN基于参考集的图像相似度）两种方式的表现，分析提示设计、类别多样性、k值与参考集大小等因素，并提出按类精度融合规则进行推断结合。

Result: 实验表明：1) 文本提示与视觉相似性在不同类别上互为补充；2) 提示设计、参考集大小及k影响显著；3) 基于按类精度的无学习融合方法能带来性能提升；并给出不同模型（如SigLIP 2、RADIOv2.5等）在ImageNet上的比较结果。

Conclusion: 本文指出视觉-语言模型在纯视觉推理方面尚未充分探索，通过对多种双编码器VLM在ImageNet-1k上的评估，发现语言提示与视觉相似性在不同类别上各有优势，并提出基于每类精度的无学习融合方法提升分类性能。

Abstract: Vision-language models (VLMs) have enabled strong zero-shot classification
through image-text alignment. Yet, their purely visual inference capabilities
remain under-explored. In this work, we conduct a comprehensive evaluation of
both language-guided and vision-only image classification with a diverse set of
dual-encoder VLMs, including both well-established and recent models such as
SigLIP 2 and RADIOv2.5. The performance is compared in a standard setup on the
ImageNet-1k validation set and its label-corrected variant. The key factors
affecting accuracy are analysed, including prompt design, class diversity, the
number of neighbours in k-NN, and reference set size. We show that language and
vision offer complementary strengths, with some classes favouring textual
prompts and others better handled by visual similarity. To exploit this
complementarity, we introduce a simple, learning-free fusion method based on
per-class precision that improves classification performance. The code is
available at: https://github.com/gonikisgo/bmvc2025-vlm-image-recognition.

</details>


### [49] [Fine-Grained Customized Fashion Design with Image-into-Prompt benchmark and dataset from LMM](https://arxiv.org/abs/2509.09324)
*Hui Li,Yi You,Qiqi Chen,Bingfeng Zhang,George Q. Huang*

Main category: cs.CV

TL;DR: 提出BUG：结合LMM的image-into-prompt管线，自动从对话生成并细化服装设计，配合FashionEdit数据集评估，降低非专业用户的设计门槛。


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI能快速将头脑风暴转化为设计，但非专业用户在文本描述上的不确定性导致难以实现细粒度定制，需自动理解并补全用户意图。

Method: 构建基于大模态模型（LMM）的BUG管线：将用户与模型的聊天和输入图像转化为详细的编辑提示，自动生成并细化服装设计；同时设计了FashionEdit数据集用于模拟真实设计流程并评估生成相似度、用户满意度和质量。

Result: 提出的框架在FashionEdit数据集上的评估显示在生成相似度、用户满意度和图像质量上均有竞争力（具体数值未在摘要给出）；并公开了代码和数据集以便复现。

Conclusion: 该论文提出了BUG工作流，使用大模型和图像到提示（image-into-prompt）技术，实现从对话到服装设计的自动生成与细粒度定制，并降低非专业用户的设计门槛。

Abstract: Generative AI evolves the execution of complex workflows in industry, where
the large multimodal model empowers fashion design in the garment industry.
Current generation AI models magically transform brainstorming into fancy
designs easily, but the fine-grained customization still suffers from text
uncertainty without professional background knowledge from end-users. Thus, we
propose the Better Understanding Generation (BUG) workflow with LMM to
automatically create and fine-grain customize the cloth designs from chat with
image-into-prompt. Our framework unleashes users' creative potential beyond
words and also lowers the barriers of clothing design/editing without further
human involvement. To prove the effectiveness of our model, we propose a new
FashionEdit dataset that simulates the real-world clothing design workflow,
evaluated from generation similarity, user satisfaction, and quality. The code
and dataset: https://github.com/detectiveli/FashionEdit.

</details>


### [50] [Exploring Pre-training Across Domains for Few-Shot Surgical Skill Assessment](https://arxiv.org/abs/2509.09327)
*Dimitrios Anastasiou,Razvan Caramalau,Nazir Sirajudeen,Matthew Boal,Philip Edwards,Justin Collins,John Kelly,Ashwin Sridhar,Maxine Tran,Faiz Mumtaz,Nevil Pavithran,Nader Francis,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 对少样本外科技能评估，领域相关的小规模自监督预训练比大规模不相关预训练更有效；加入手术特定数据在相似域中有利，否则可能有害。


<details>
  <summary>Details</summary>
Motivation: 手术技能标注昂贵且稀缺，少样本学习能减少监督需求，但其成功依赖合适的预训练。此前对SSA的预训练研究有限，故需探究不同预训练源与域相似性如何影响少样本SSA。

Method: 作者在公开机器人手术数据集上标注了OSATS分数，采用多种自监督预训练源（包含小型领域相关数据集与大规模通用数据集），在1、2、5-shot设置下评估少样本分类性能，量化域相似性并分析将手术特定数据加入预训练的影响。

Result: 小而领域相关的数据集能优于大规模但不匹配的数据集，分别在1/2/5-shot下达成60.16%、66.03%、73.65%准确率。将手术特定数据并入预训练在域相关外部数据集上可提升平均+1.22%准确率和+2.28% F1；但若并入不相似的大规模源则可能导致性能下降。

Conclusion: 该论文提出将外科手术技能评估(SSA)建模为少样本学习任务，重点研究自监督预训练策略对下游少样本SSA性能的影响，并给出在不同预训练源与数据域相似性下的转移效果结论。

Abstract: Automated surgical skill assessment (SSA) is a central task in surgical
computer vision. Developing robust SSA models is challenging due to the
scarcity of skill annotations, which are time-consuming to produce and require
expert consensus. Few-shot learning (FSL) offers a scalable alternative
enabling model development with minimal supervision, though its success
critically depends on effective pre-training. While widely studied for several
surgical downstream tasks, pre-training has remained largely unexplored in SSA.
In this work, we formulate SSA as a few-shot task and investigate how
self-supervised pre-training strategies affect downstream few-shot SSA
performance. We annotate a publicly available robotic surgery dataset with
Objective Structured Assessment of Technical Skill (OSATS) scores, and evaluate
various pre-training sources across three few-shot settings. We quantify domain
similarity and analyze how domain gap and the inclusion of procedure-specific
data into pre-training influence transferability. Our results show that small
but domain-relevant datasets can outperform large scale, less aligned ones,
achieving accuracies of 60.16%, 66.03%, and 73.65% in the 1-, 2-, and 5-shot
settings, respectively. Moreover, incorporating procedure-specific data into
pre-training with a domain-relevant external dataset significantly boosts
downstream performance, with an average gain of +1.22% in accuracy and +2.28%
in F1-score; however, applying the same strategy with less similar but
large-scale sources can instead lead to performance degradation. Code and
models are available at https://github.com/anastadimi/ssa-fsl.

</details>


### [51] [Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles](https://arxiv.org/abs/2509.09349)
*Ian Nell,Shane Gilroy*

Main category: cs.CV

TL;DR: 该工作利用YOLO和定制车道估计，通过外部摄像头跟踪车辆轨迹与横向位移，实现对非联网车辆的分心与受损驾驶检测，实验验证了方法的可靠性与适应性。


<details>
  <summary>Details</summary>
Motivation: 道路交通事故中人为错误（尤其是分心与受损驾驶）是主要成因，现有基于车载或车际通信的方法无法覆盖非联网车辆，因而提出基于外部视觉的行为分析方法。

Method: 使用YOLO目标检测模型、实时目标跟踪、车辆横向位移分析和车道位置监测；自定义车道估计算法用于判断车道偏离与轨迹异常。

Result: 在多种道路和环境条件下的视频数据集上进行了实验评估，结果显示该框架能可靠识别过度横向移动和不稳定轨迹，具有较好的泛化能力。

Conclusion: 该论文提出了一种基于外部视觉观测的驾驶行为分类系统，旨在检测驾驶分心与受损驾驶的迹象。实验表明在多样化视频数据集上具有一定可靠性与适应性。

Abstract: Road traffic accidents remain a significant global concern, with human error,
particularly distracted and impaired driving, among the leading causes. This
study introduces a novel driver behavior classification system that uses
external observation techniques to detect indicators of distraction and
impairment. The proposed framework employs advanced computer vision
methodologies, including real-time object tracking, lateral displacement
analysis, and lane position monitoring. The system identifies unsafe driving
behaviors such as excessive lateral movement and erratic trajectory patterns by
implementing the YOLO object detection model and custom lane estimation
algorithms. Unlike systems reliant on inter-vehicular communication, this
vision-based approach enables behavioral analysis of non-connected vehicles.
Experimental evaluations on diverse video datasets demonstrate the framework's
reliability and adaptability across varying road and environmental conditions.

</details>


### [52] [Texture-aware Intrinsic Image Decomposition with Model- and Learning-based Priors](https://arxiv.org/abs/2509.09352)
*Xiaodong Wang,Zijun He,Xin Yuan*

Main category: cs.CV

TL;DR: 提出纹理引导正则化的优化框架，针对单图内在分解在复杂光照和丰富纹理场景下的不足，能更好保留纹理并分离光照，效果优于既有方法。


<details>
  <summary>Details</summary>
Motivation: 传统和学习方法在复杂场景（空间变化光照、丰富纹理）下难以准确分解，学习方法常导致纹理丢失或过度平滑，影响光照与材料的分离。作者希望通过显式纹理先验改善分解质量。

Method: 基于观察到学习方法存在过度平滑问题，设计纹理引导正则化，将问题构造成优化框架；利用原始RGB图像与纹理先验约束来区分材料纹理和光照影响，求解得到反射率与阴影层。

Result: 通过引入纹理感知先验，方法在实景图像上的内在分解效果优于现有方法，能保留材料纹理同时分离光照变化，主观和/或定量评估显示提升。

Conclusion: 本文提出一种结合纹理引导正则项的单张图像内在分解方法，能在复杂光照与丰富纹理场景下更好分离反射率与光照，生成质量更高的内在图像。

Abstract: This paper aims to recover the intrinsic reflectance layer and shading layer
given a single image. Though this intrinsic image decomposition problem has
been studied for decades, it remains a significant challenge in cases of
complex scenes, i.e. spatially-varying lighting effect and rich textures. In
this paper, we propose a novel method for handling severe lighting and rich
textures in intrinsic image decomposition, which enables to produce
high-quality intrinsic images for real-world images. Specifically, we observe
that previous learning-based methods tend to produce texture-less and
over-smoothing intrinsic images, which can be used to infer the lighting and
texture information given a RGB image. In this way, we design a texture-guided
regularization term and formulate the decomposition problem into an
optimization framework, to separate the material textures and lighting effect.
We demonstrate that combining the novel texture-aware prior can produce
superior results to existing approaches.

</details>


### [53] [Plug-and-play Diffusion Models for Image Compressive Sensing with Data Consistency Projection](https://arxiv.org/abs/2509.09365)
*Xiaodong Wang,Ping Wang,Zhangyuan Li,Xin Yuan*

Main category: cs.CV

TL;DR: 通过把DDIM分解为去噪、数据一致性与采样三阶段，并在去噪后施加线性组合的PnP式保真校正，提出一种混合数据一致性模块，提升了单像素成像的重建效果。


<details>
  <summary>Details</summary>
Motivation: 弥合PnP方法（以外部去噪器为先验）与扩散模型（基于连续随机过程的生成采样）在去噪机制和采样步骤上的差异，从而将学习到的先验更有原则地与物理前向模型结合以改善逆问题重建。

Method: 将DDIM的去噪步骤与PnP的多项保真项线性组合融合：在每次去噪后，直接对去噪估计施加混合线性校正（多重PnP风格的保真项加权和），同时不干扰扩散的采样轨迹。实验在单像素成像上验证。

Result: 在单像素成像任务上，所提方法在重建质量上优于基线方法（传统PnP或纯扩散模型），证明混合数据一致性校正能在不破坏扩散采样的前提下提高测量一致性和重建性能。

Conclusion: 本文将PnP与DDIM结合，提出将扩散过程分解为去噪、测量一致性和采样三阶段，并设计了一个混合数据一致性模块以提升单像素成像的重建质量。

Abstract: We explore the connection between Plug-and-Play (PnP) methods and Denoising
Diffusion Implicit Models (DDIM) for solving ill-posed inverse problems, with a
focus on single-pixel imaging. We begin by identifying key distinctions between
PnP and diffusion models-particularly in their denoising mechanisms and
sampling procedures. By decoupling the diffusion process into three
interpretable stages: denoising, data consistency enforcement, and sampling, we
provide a unified framework that integrates learned priors with physical
forward models in a principled manner. Building upon this insight, we propose a
hybrid data-consistency module that linearly combines multiple PnP-style
fidelity terms. This hybrid correction is applied directly to the denoised
estimate, improving measurement consistency without disrupting the diffusion
sampling trajectory. Experimental results on single-pixel imaging tasks
demonstrate that our method achieves better reconstruction quality.

</details>


### [54] [A Fully Automatic Framework for Intracranial Pressure Grading: Integrating Keyframe Identification, ONSD Measurement and Clinical Data](https://arxiv.org/abs/2509.09368)
*Pengxu Wen,Tingting Yu,Ziwei Nie,Cheng Jiang,Zhenyu Yin,Mingyang He,Bo Liao,Xiaoping Yang*

Main category: cs.CV

TL;DR: 提出自动化ONSD测量+临床数据融合的两阶段ICP分级框架，在准确性与稳定性上显著优于传统阈值方法，推进了非侵入性ICP临床评估的可应用性。


<details>
  <summary>Details</summary>
Motivation: 现有ONSD测量依赖手工操作、视图选择主观、阈值不一致，导致可靠性不足；因此需要自动化、可解释且能融合多源信息的非侵入性ICP评估方法。

Method: 方法包括：（1）视盘超声视频处理：基于帧级解剖分割和国际共识的规则化关键帧识别，提取合格视图并精确测量ONSD；（2）颅内压分级：将ONSD指标与临床数据融合，使用可解释模型预测ICP分级。五折交叉验证与独立测试用于性能评估，并与基于阈值的传统方法比较。

Result: 模型在五折交叉验证中取得0.845±0.071验证准确率，独立测试准确率0.786；显著优于传统阈值法（验证0.637±0.111，测试0.429）。显示出降低操作者变异、提高评估客观性的效果。

Conclusion: 该论文提出了一种全自动两阶段框架，通过视盘超声关键帧识别和ONSD测量，结合临床特征实现颅内压分级，证明了非侵入性ICP评估的可行性与优越性。

Abstract: Intracranial pressure (ICP) elevation poses severe threats to cerebral
function, thus necessitating monitoring for timely intervention. While lumbar
puncture is the gold standard for ICP measurement, its invasiveness and
associated risks drive the need for non-invasive alternatives. Optic nerve
sheath diameter (ONSD) has emerged as a promising biomarker, as elevated ICP
directly correlates with increased ONSD. However, current clinical practices
for ONSD measurement suffer from inconsistency in manual operation,
subjectivity in optimal view selection, and variability in thresholding,
limiting their reliability. To address these challenges, we introduce a fully
automatic two-stage framework for ICP grading, integrating keyframe
identification, ONSD measurement and clinical data. Specifically, the fundus
ultrasound video processing stage performs frame-level anatomical segmentation,
rule-based keyframe identification guided by an international consensus
statement, and precise ONSD measurement. The intracranial pressure grading
stage then fuses ONSD metrics with clinical features to enable the prediction
of ICP grades, thereby demonstrating an innovative blend of interpretable
ultrasound analysis and multi-source data integration for objective clinical
evaluation. Experimental results demonstrate that our method achieves a
validation accuracy of $0.845 \pm 0.071$ (with standard deviation from
five-fold cross-validation) and an independent test accuracy of 0.786,
significantly outperforming conventional threshold-based method ($0.637 \pm
0.111$ validation accuracy, $0.429$ test accuracy). Through effectively
reducing operator variability and integrating multi-source information, our
framework establishes a reliable non-invasive approach for clinical ICP
evaluation, holding promise for improving patient management in acute
neurological conditions.

</details>


### [55] [Unsupervised Integrated-Circuit Defect Segmentation via Image-Intrinsic Normality](https://arxiv.org/abs/2509.09375)
*Botong Zhao,Qijun Shi,Shujing Lyu,Yue Lu*

Main category: cs.CV

TL;DR: 无需外部正常样本的无监督IC缺陷分割：从单图像聚合正常特征、用一致性损失约束、仅重建正常内容并用残差定位缺陷，辅以伪异常增强，能提高分割性能并抗产品变异。


<details>
  <summary>Details</summary>
Motivation: 工业IC检测受产品布局变化和对齐难度影响，传统依赖外部正常集的方法在IC图像上不可靠。鉴于缺陷多为局部且单张图像内仍含丰富可重复的正常模式，提出在单图像上学习正常信息以避免外部支持。

Method: 设计一个可学习的正常信息提取器，从单张测试图像中聚合代表性正常特征；引入一致性损失以确保这些特征对应图像中的正常区域；利用提取的正常特征作为条件，训练解码器只重建正常内容；通过重建残差进行缺陷分割；使用伪异常数据增强训练鲁棒性。

Result: 在三个IC制程阶段的数据集上，所提方法在缺陷分割任务上较现有方法表现出一致提升，并对产品变异表现出较强鲁棒性。

Conclusion: 提出了一种无需外部正常样本的无监督IC缺陷分割框架，通过从测试图像中学习并聚合代表性正常特征，再用一致性损失约束其与正常区域的关联，从而指导解码器仅重建正常内容，残差用于分割缺陷，并结合伪异常增强稳定训练。

Abstract: Modern Integrated-Circuit(IC) manufacturing introduces diverse, fine-grained
defects that depress yield and reliability. Most industrial defect segmentation
compares a test image against an external normal set, a strategy that is
brittle for IC imagery where layouts vary across products and accurate
alignment is difficult. We observe that defects are predominantly local, while
each image still contains rich, repeatable normal patterns. We therefore
propose an unsupervised IC defect segmentation framework that requires no
external normal support. A learnable normal-information extractor aggregates
representative normal features from the test image, and a coherence loss
enforces their association with normal regions. Guided by these features, a
decoder reconstructs only normal content; the reconstruction residual then
segments defects. Pseudo-anomaly augmentation further stabilizes training.
Experiments on datasets from three IC process stages show consistent
improvements over existing approaches and strong robustness to product
variability.

</details>


### [56] [Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift](https://arxiv.org/abs/2509.09397)
*Umaima Rahman,Raza Imam,Mohammad Yaqub,Dwarikanath Mahapatra*

Main category: cs.CV

TL;DR: 提出DRiFt，通过LoRA+可学习prompt解耦特征并用合成高质量图文对齐，显著提升医疗VLM的原内性能并增强域外鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM受成像协议与自由文本报告变异影响，容易学习任务无关相关性，导致泛化差和在真实环境中失效的风险，因此需要方法显式分离相关信号与噪声以提升可靠性。

Method: 使用参数高效微调LoRA和可学习的prompt token实现特征解耦；通过为多样化医疗数据生成高质量、临床可信的图文配对来增强跨模态对齐与减少不确定性。

Result: 在内部数据上相较于先前prompt方法Top-1准确率提升+11.4%，Macro-F1提升+3.3%，同时在未见数据集上保持稳健性；消融实验表明特征解耦与对齐对泛化性与可预测性有显著影响。

Conclusion: DRiFt通过结构化特征解耦提高了医疗VLM在域外分布下的可靠性与泛化性，能够分离临床相关信号与任务无关噪声，从而降低模型在实际部署中的失败风险。

Abstract: Medical vision-language models (VLMs) offer promise for clinical decision
support, yet their reliability under distribution shifts remains a major
concern for safe deployment. These models often learn task-agnostic
correlations due to variability in imaging protocols and free-text reports,
limiting their generalizability and increasing the risk of failure in
real-world settings. We propose DRiFt, a structured feature decoupling
framework that explicitly separates clinically relevant signals from
task-agnostic noise using parameter-efficient tuning (LoRA) and learnable
prompt tokens. To enhance cross-modal alignment and reduce uncertainty, we
curate high-quality, clinically grounded image-text pairs by generating
captions for a diverse medical dataset. Our approach improves in-distribution
performance by +11.4% Top-1 accuracy and +3.3% Macro-F1 over prior prompt-based
methods, while maintaining strong robustness across unseen datasets. Ablation
studies reveal that disentangling task-relevant features and careful alignment
significantly enhance model generalization and reduce unpredictable behavior
under domain shift. These insights contribute toward building safer, more
trustworthy VLMs for clinical use. The code is available at
https://github.com/rumaima/DRiFt.

</details>


### [57] [FS-Diff: Semantic guidance and clarity-aware simultaneous multimodal image fusion and super-resolution](https://arxiv.org/abs/2509.09427)
*Yuchan Jie,Yushen Xu,Xiaosong Li,Fuqiang Zhou,Jianming Lv,Huafeng Li*

Main category: cs.CV

TL;DR: 提出基于扩散/迭代去噪的语义引导与清晰度感知联合图像融合与超分方法（FS-Diff），结合双向特征提取与多噪声级别训练，在公开及新建AVMS数据集上显著提升融合超分质量。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态图像常受低分辨率与语义信息弱化影响，导致现有融合或融合超分方法效果欠佳，需引入语义引导与清晰度感知来提升在腐蚀/远距场景下的融合超分性能。

Method: 将目标初始化为高斯噪声，使用双向特征Mamba提取多模态全局特征，并以源图像和语义作为条件，采用改进U-Net在多噪声水平上进行随机迭代去噪以生成高分辨率融合图像；并构建AVMS航拍多场景数据集用于评估。

Result: 在六个公开数据集及作者构建的AVMS基准上，多倍放大条件下FS-Diff优于现有最先进方法，能恢复更多细节与语义信息。并公开代码。

Conclusion: FS-Diff通过将图像融合与超分统一为条件生成问题，并引入清晰度感知的语义引导机制及双向特征提取（Mamba）和多噪声级别训练的改良U-Net随机迭代去噪流程，实现了在低分辨率、语义弱化场景下更丰富细节与语义的融合超分结果。

Abstract: As an influential information fusion and low-level vision technique, image
fusion integrates complementary information from source images to yield an
informative fused image. A few attempts have been made in recent years to
jointly realize image fusion and super-resolution. However, in real-world
applications such as military reconnaissance and long-range detection missions,
the target and background structures in multimodal images are easily corrupted,
with low resolution and weak semantic information, which leads to suboptimal
results in current fusion techniques. In response, we propose FS-Diff, a
semantic guidance and clarity-aware joint image fusion and super-resolution
method. FS-Diff unifies image fusion and super-resolution as a conditional
generation problem. It leverages semantic guidance from the proposed clarity
sensing mechanism for adaptive low-resolution perception and cross-modal
feature extraction. Specifically, we initialize the desired fused result as
pure Gaussian noise and introduce the bidirectional feature Mamba to extract
the global features of the multimodal images. Moreover, utilizing the source
images and semantics as conditions, we implement a random iterative denoising
process via a modified U-Net network. This network istrained for denoising at
multiple noise levels to produce high-resolution fusion results with
cross-modal features and abundant semantic information. We also construct a
powerful aerial view multiscene (AVMS) benchmark covering 600 pairs of images.
Extensive joint image fusion and super-resolution experiments on six public and
our AVMS datasets demonstrated that FS-Diff outperforms the state-of-the-art
methods at multiple magnifications and can recover richer details and semantics
in the fused images. The code is available at
https://github.com/XylonXu01/FS-Diff.

</details>


### [58] [Semantic Concentration for Self-Supervised Dense Representations Learning](https://arxiv.org/abs/2509.09429)
*Peisong Wen,Qianqian Xu,Siran Dai,Runmin Cong,Qingming Huang*

Main category: cs.CV

TL;DR: 为解决密集SSL中patch表征过度扩散，论文提出基于连续目标AP排序损失的噪声耐受蒸馏与基于对象原型的映射机制，实现显式语义聚焦，显著提升密集任务性能。


<details>
  <summary>Details</summary>
Motivation: 图像级SSL虽能隐式实现语义集中避免过度扩散，但密集SSL因空间敏感性和场景复杂性无法受益，导致同一实例/类别内patch表征分散，影响密集任务表现。需显式诱导语义聚焦以提高一致性。

Method: 方法包括三部分：1) 蒸馏patch对应关系以打破严格空间对齐；2) 提出噪声耐受的排序损失，将Average Precision损失扩展到连续目标以具备决策无关和自适应关注特性；3) 设计对象感知过滤器，用可学习的对象原型通过cross-attention把patch映射到对象空间，从而区分共享模式。并在多项下游密集任务上验证。

Result: 在多项密集任务上（文中所述）实证表明所提方法提升了下游性能，验证了噪声耐受AP损失及对象感知滤波对稳定学习密集表征的有效性。代码已开源。

Conclusion: 本文提出在密集自监督表示学习中引入显式语义聚焦以缓解过度扩散问题，并通过连续目标的AP排序损失和基于对象的输出映射（对象感知滤波）实现对噪声/不平衡伪标签与复杂场景的鲁棒训练，实验表明方法有效。

Abstract: Recent advances in image-level self-supervised learning (SSL) have made
significant progress, yet learning dense representations for patches remains
challenging. Mainstream methods encounter an over-dispersion phenomenon that
patches from the same instance/category scatter, harming downstream performance
on dense tasks. This work reveals that image-level SSL avoids over-dispersion
by involving implicit semantic concentration. Specifically, the non-strict
spatial alignment ensures intra-instance consistency, while shared patterns,
i.e., similar parts of within-class instances in the input space, ensure
inter-image consistency. Unfortunately, these approaches are infeasible for
dense SSL due to their spatial sensitivity and complicated scene-centric data.
These observations motivate us to explore explicit semantic concentration for
dense SSL. First, to break the strict spatial alignment, we propose to distill
the patch correspondences. Facing noisy and imbalanced pseudo labels, we
propose a noise-tolerant ranking loss. The core idea is extending the Average
Precision (AP) loss to continuous targets, such that its decision-agnostic and
adaptive focusing properties prevent the student model from being misled.
Second, to discriminate the shared patterns from complicated scenes, we propose
the object-aware filter to map the output space to an object-based space.
Specifically, patches are represented by learnable prototypes of objects via
cross-attention. Last but not least, empirical studies across various tasks
soundly support the effectiveness of our method. Code is available in
https://github.com/KID-7391/CoTAP.

</details>


### [59] [FlexiD-Fuse: Flexible number of inputs multi-modal medical image fusion based on diffusion model](https://arxiv.org/abs/2509.09456)
*Yushen Xu,Xiaosong Li,Yuchun Wang,Xiaoqi Cheng,Huafeng Li,Haishu Tan*

Main category: cs.CV

TL;DR: 提出基于扩散模型与EM算法的FlexiD-Fuse，实现同一权重下对任意数量模态的高质量图像融合，并在多项数据集与任务上超越现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 现有多模态医疗图像融合方法仅能处理固定数量的输入模态，难以适应临床上模态数目可变的需求，需提出能接受任意数量模态的融合方法。

Method: 将扩散模型的有条件采样问题转化为基于扩散过程与分层贝叶斯建模的最大似然估计问题，并将EM算法嵌入扩散采样迭代，从而实现端到端处理不同数量模态输入。

Result: 在Harvard数据集及红外-可见、多曝光、多焦点等扩展任务上，FlexiD-Fuse在九项常用指标上取得了优越表现，展示了效果稳定性和推广性。

Conclusion: 本文提出的FlexiD-Fuse能有效处理可变数量的多模态医疗图像融合，弥补了现有方法只能处理固定模态数的局限。

Abstract: Different modalities of medical images provide unique physiological and
anatomical information for diseases. Multi-modal medical image fusion
integrates useful information from different complementary medical images with
different modalities, producing a fused image that comprehensively and
objectively reflects lesion characteristics to assist doctors in clinical
diagnosis. However, existing fusion methods can only handle a fixed number of
modality inputs, such as accepting only two-modal or tri-modal inputs, and
cannot directly process varying input quantities, which hinders their
application in clinical settings. To tackle this issue, we introduce
FlexiD-Fuse, a diffusion-based image fusion network designed to accommodate
flexible quantities of input modalities. It can end-to-end process two-modal
and tri-modal medical image fusion under the same weight. FlexiD-Fuse
transforms the diffusion fusion problem, which supports only fixed-condition
inputs, into a maximum likelihood estimation problem based on the diffusion
process and hierarchical Bayesian modeling. By incorporating the
Expectation-Maximization algorithm into the diffusion sampling iteration
process, FlexiD-Fuse can generate high-quality fused images with cross-modal
information from source images, independently of the number of input images. We
compared the latest two and tri-modal medical image fusion methods, tested them
on Harvard datasets, and evaluated them using nine popular metrics. The
experimental results show that our method achieves the best performance in
medical image fusion with varying inputs. Meanwhile, we conducted extensive
extension experiments on infrared-visible, multi-exposure, and multi-focus
image fusion tasks with arbitrary numbers, and compared them with the
perspective SOTA methods. The results of the extension experiments consistently
demonstrate the effectiveness and superiority of our method.

</details>


### [60] [Resource-Efficient Glioma Segmentation on Sub-Saharan MRI](https://arxiv.org/abs/2509.09469)
*Freedmore Sidume,Oumayma Soula,Joseph Muthui Wacira,YunFei Zhu,Abbas Rabiu Muhammad,Abderrazek Zeraii,Oluwaseun Kalejaye,Hajer Ibrahim,Olfa Gaddour,Brain Halubanza,Dong Zhang,Udunna C Anazodo,Confidence Raymond*

Main category: cs.CV

TL;DR: 在资源受限的 SSA 场景，通过在 BraTS 2021 上预训练并微调的 3D Attention UNet+残差块，实现了对 BraTS-Africa 数据集上高效、紧凑且泛化良好的脑肿瘤分割（ET 0.76, NETC 0.80, SNFH 0.85）。


<details>
  <summary>Details</summary>
Motivation: 非洲撒哈拉以南地区高质量标注医学影像稀缺，计算资源受限，亟需一个准确且轻量的肿瘤分割模型以支持临床决策与部署。

Method: 采用3D Attention UNet 结构，加入残差块以改善特征学习，使用在 BraTS 2021 数据集上预训练的权重进行迁移学习；模型体积约90MB，推理时间小于1分钟/体积，评估使用95例 BraTS-Africa MRI。

Result: 在95例 BraTS-Africa 数据上取得 Dice：ET 0.76、NETC 0.80、SNFH 0.85，表明在低质量/少量数据下仍能保持较高性能；模型紧凑并能在消费级硬件上快速推理。

Conclusion: 该工作提出了一种针对资源受限环境的 3D Attention UNet+残差块的脑肿瘤分割框架，并通过在 BraTS 2021 上的迁移学习提升在 SSA（BraTS-Africa）数据集上的表现，展现出良好泛化性和实用性。

Abstract: Gliomas are the most prevalent type of primary brain tumors, and their
accurate segmentation from MRI is critical for diagnosis, treatment planning,
and longitudinal monitoring. However, the scarcity of high-quality annotated
imaging data in Sub-Saharan Africa (SSA) poses a significant challenge for
deploying advanced segmentation models in clinical workflows. This study
introduces a robust and computationally efficient deep learning framework
tailored for resource-constrained settings. We leveraged a 3D Attention UNet
architecture augmented with residual blocks and enhanced through transfer
learning from pre-trained weights on the BraTS 2021 dataset. Our model was
evaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for glioma
segmentation in SSA MRI data. Despite the limited data quality and quantity,
our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80
for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for Surrounding
Non-Functional Hemisphere (SNFH). These results demonstrate the
generalizability of the proposed model and its potential to support clinical
decision making in low-resource settings. The compact architecture,
approximately 90 MB, and sub-minute per-volume inference time on consumer-grade
hardware further underscore its practicality for deployment in SSA health
systems. This work contributes toward closing the gap in equitable AI for
global health by empowering underserved regions with high-performing and
accessible medical imaging solutions.

</details>


### [61] [OpenFake: An Open Dataset and Platform Toward Large-Scale Deepfake Detection](https://arxiv.org/abs/2509.09495)
*Victor Livernoche,Akshatha Arodi,Andreea Musulan,Zachary Yang,Adam Salvail,Gaétan Marceau Caron,Jean-François Godbout,Reihaneh Rabbany*

Main category: cs.CV

TL;DR: 作者发布了一个大规模、面向政治场景的真实-合成图像数据集（3M实图+963k合成），并通过人类感知研究、社媒分析与众包对抗平台，旨在提升和维持深度伪造检测在现代生成模型面前的有效性。


<details>
  <summary>Details</summary>
Motivation: 动机是现有深度伪造数据集过时、真实性低或单人脸图像限制了检测器的泛化能力；且新型生成模型越来越能骗过公众，需要专门面向政治敏感场景的数据与持续更新机制来提高检测鲁棒性。

Method: 通过分析社交媒体传播模式识别多模态传播路径，进行人工感知实验评估最新专有模型生成图像的逼真度，收集3百万真实图片及描述性字幕，并利用专有及开源模型生成963k高质量合成图像，还搭建了一个激励型众包平台让社区提交对抗性合成样本。

Result: 构建并发布了包含3M真实图像与963k对应高质量合成图像的政治相关数据集；实验证明最新生成模型能产生与真实图像难以区分的结果，且众包对抗平台能持续补充挑战性样本以提升检测方法的适应性。

Conclusion: 论文结论是提出并发布了一个面向政治领域的高质量大规模真实-合成图像数据集，并引入了众包对抗平台以持续更新和强化深度伪造检测方法。

Abstract: Deepfakes, synthetic media created using advanced AI techniques, have
intensified the spread of misinformation, particularly in politically sensitive
contexts. Existing deepfake detection datasets are often limited, relying on
outdated generation methods, low realism, or single-face imagery, restricting
the effectiveness for general synthetic image detection. By analyzing social
media posts, we identify multiple modalities through which deepfakes propagate
misinformation. Furthermore, our human perception study demonstrates that
recently developed proprietary models produce synthetic images increasingly
indistinguishable from real ones, complicating accurate identification by the
general public. Consequently, we present a comprehensive, politically-focused
dataset specifically crafted for benchmarking detection against modern
generative models. This dataset contains three million real images paired with
descriptive captions, which are used for generating 963k corresponding
high-quality synthetic images from a mix of proprietary and open-source models.
Recognizing the continual evolution of generative techniques, we introduce an
innovative crowdsourced adversarial platform, where participants are
incentivized to generate and submit challenging synthetic images. This ongoing
community-driven initiative ensures that deepfake detection methods remain
robust and adaptive, proactively safeguarding public discourse from
sophisticated misinformation threats.

</details>


### [62] [Improving Human Motion Plausibility with Body Momentum](https://arxiv.org/abs/2509.09496)
*Ha Linh Nguyen,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: 通过在损失中加入全身线性和角动量的一致性约束，将局部关节变化与全局位移物理性地联系起来，从而减少滑步与抖动、改善平衡且保持重建精度。


<details>
  <summary>Details</summary>
Motivation: 局部运动与根节点的全局运动并非独立，二者通过物理量（如线性与角动量）耦合，但现有模型常忽略这一点或直接基于力学推导成本高昂且复杂。

Method: 在损失函数中加入全身线性和角动量一致性项，强制生成运动的动量曲线与真实数据匹配，从而间接约束全局位移与局部关节配置的关系。

Result: 引入动量一致性损失后，减少了拖地滑步与抖动，提升了平衡性，同时保持了运动重建精度。

Conclusion: 使用动量作为约束可以有效耦合局部与全局运动，提升生成动作的物理一致性与运动质量。

Abstract: Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating
them separately. However, these two components are not independent. Global
movement arises from interactions with the environment, which are, in turn,
driven by changes in the body configuration. Motion models often fail to
precisely capture this physical coupling between local and global dynamics,
while deriving global trajectories from joint torques and external forces is
computationally expensive and complex. To address these challenges, we propose
using whole-body linear and angular momentum as a constraint to link local
motion with global movement. Since momentum reflects the aggregate effect of
joint-level dynamics on the body's movement through space, it provides a
physically grounded way to relate local joint behavior to global displacement.
Building on this insight, we introduce a new loss term that enforces
consistency between the generated momentum profiles and those observed in
ground-truth data. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion. Code and
data are available at the project page https://hlinhn.github.io/momentum_bmvc.

</details>


### [63] [Region-Wise Correspondence Prediction between Manga Line Art Images](https://arxiv.org/abs/2509.09501)
*Yingxuan Li,Jiafeng Mao,Qianru Qiu,Yusuke Matsui*

Main category: cs.CV

TL;DR: 提出一个无标注漫画线稿的区域对应预测任务；用Transformer学补丁相似性，结合边缘感知聚类与区域匹配；并构建数据集，实验显示补丁级准确率高且能生成一致的区域对应。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏在无预先分割或注释条件下处理漫画线稿区域对应的研究，而该任务对自动上色、生成中间帧等下游应用非常重要，因此提出一个实用的无监督/弱监督区域对应预测任务。

Method: 将每张线稿图像划分为补丁，使用Transformer学习图像内与图像间的补丁相似性，之后通过边缘敏感的聚类方法把补丁合并为连贯区域，最后用区域匹配算法生成区域级对应；同时构建了自动标注管线并人工精修子集作为基准数据集。

Result: 在多个数据集上取得了高补丁级准确率（如96.34%）并生成一致的区域级对应，表明方法在实际漫画应用中具有潜力。

Conclusion: 该论文提出了在无标注的原始漫画线稿图像上预测区域级对应关系的新任务，并设计了基于Transformer的补丁相似性学习、边缘感知聚类与区域匹配流程，实现从补丁级到区域级的对应推断。

Abstract: Understanding region-wise correspondence between manga line art images is a
fundamental task in manga processing, enabling downstream applications such as
automatic line art colorization and in-between frame generation. However, this
task remains largely unexplored, especially in realistic scenarios without
pre-existing segmentation or annotations. In this paper, we introduce a novel
and practical task: predicting region-wise correspondence between raw manga
line art images without any pre-existing labels or masks. To tackle this
problem, we divide each line art image into a set of patches and propose a
Transformer-based framework that learns patch-level similarities within and
across images. We then apply edge-aware clustering and a region matching
algorithm to convert patch-level predictions into coherent region-level
correspondences. To support training and evaluation, we develop an automatic
annotation pipeline and manually refine a subset of the data to construct
benchmark datasets. Experiments on multiple datasets demonstrate that our
method achieves high patch-level accuracy (e.g., 96.34%) and generates
consistent region-level correspondences, highlighting its potential for
real-world manga applications.

</details>


### [64] [Generative Diffusion Contrastive Network for Multi-View Clustering](https://arxiv.org/abs/2509.09527)
*Jian Zhu,Xin Zou,Xi Wang,Ning Zhang,Bian Wu,Yao Yang,Ying Zhou,Lingfang Zeng,Chang Tang,Cheng Luo*

Main category: cs.CV

TL;DR: 本文针对多视图聚类中噪声和缺失视图问题，提出SGDF生成扩散融合机制并构建GDCN对比网络，通过生成式多样化融合提升表示鲁棒性，实验验证了其SOTA性能并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 多视图融合在多视图聚类中至关重要，但实际数据常存在噪声或缺失视图，导致融合后的特征质量下降，影响聚类性能；因此需要一种对低质量视图具有鲁棒性的融合策略。

Method: 提出随机生成扩散融合（SGDF），利用多生成机制对每个样本的多视图特征进行生成与融合以提高对噪声或缺失视图的鲁棒性；在此基础上设计生成扩散对比网络（GDCN），结合生成式模型与对比学习以优化聚类表示。

Result: 实验表明GDCN在深度多视图聚类任务上优于现有方法，达到SOTA水平；并且作者公开了代码库以便复现。

Conclusion: 该文提出了一种面向多视图聚类中低质量数据鲁棒性的生成扩散融合方法（SGDF），并在此基础上构建了对比学习网络GDCN，声称在深度多视图聚类任务上达到最新的最好性能。

Abstract: In recent years, Multi-View Clustering (MVC) has been significantly advanced
under the influence of deep learning. By integrating heterogeneous data from
multiple views, MVC enhances clustering analysis, making multi-view fusion
critical to clustering performance. However, there is a problem of low-quality
data in multi-view fusion. This problem primarily arises from two reasons: 1)
Certain views are contaminated by noisy data. 2) Some views suffer from missing
data. This paper proposes a novel Stochastic Generative Diffusion Fusion (SGDF)
method to address this problem. SGDF leverages a multiple generative mechanism
for the multi-view feature of each sample. It is robust to low-quality data.
Building on SGDF, we further present the Generative Diffusion Contrastive
Network (GDCN). Extensive experiments show that GDCN achieves the
state-of-the-art results in deep MVC tasks. The source code is publicly
available at https://github.com/HackerHyper/GDCN.

</details>


### [65] [DualTrack: Sensorless 3D Ultrasound needs Local and Global Context](https://arxiv.org/abs/2509.09530)
*Paul F. R. Wilson,Matteo Ronchetti,Rüdiger Göbl,Viktoria Markova,Sebastian Rosenzweig,Raphael Prevost,Parvin Mousavi,Oliver Zettinig*

Main category: cs.CV

TL;DR: 提出DualTrack：解耦局部时空卷积与全局骨干+时间注意力的双编码器，用轻量融合估计轨迹，实验证明在公开数据集上平均重建误差<5 mm，优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D超声设备昂贵复杂，sensorless方法通过从序列2D图像预测探头轨迹提供低成本替代，但现有方法要么忽视全局解剖信息，要么将全局与局部特征紧耦合，限制了对不同尺度信息的建模能力。

Method: DualTrack采用两个专门编码器：局部编码器用稠密时空卷积提取帧间微小运动相关的局部特征；全局编码器基于二维CNN或大模型骨干并结合时间注意力层，编码全局解剖结构与长程依赖。随后用轻量融合模块将两类特征合并以回归探头轨迹并构建三维体积。

Result: 在大型公开基准上，DualTrack在准确性与全局一致性方面优于先前方法，平均重建误差低于5 mm，实现了最先进性能。

Conclusion: 本文提出DualTrack双编码器架构，通过解耦的局部与全局编码器分别捕捉细粒度斑点特征与高层解剖语义，进而提高无传感器三维超声（sensorless 3D US）轨迹估计和重建的一致性与精度。

Abstract: Three-dimensional ultrasound (US) offers many clinical advantages over
conventional 2D imaging, yet its widespread adoption is limited by the cost and
complexity of traditional 3D systems. Sensorless 3D US, which uses deep
learning to estimate a 3D probe trajectory from a sequence of 2D US images, is
a promising alternative. Local features, such as speckle patterns, can help
predict frame-to-frame motion, while global features, such as coarse shapes and
anatomical structures, can situate the scan relative to anatomy and help
predict its general shape. In prior approaches, global features are either
ignored or tightly coupled with local feature extraction, restricting the
ability to robustly model these two complementary aspects. We propose
DualTrack, a novel dual-encoder architecture that leverages decoupled local and
global encoders specialized for their respective scales of feature extraction.
The local encoder uses dense spatiotemporal convolutions to capture
fine-grained features, while the global encoder utilizes an image backbone
(e.g., a 2D CNN or foundation model) and temporal attention layers to embed
high-level anatomical features and long-range dependencies. A lightweight
fusion module then combines these features to estimate the trajectory.
Experimental results on a large public benchmark show that DualTrack achieves
state-of-the-art accuracy and globally consistent 3D reconstructions,
outperforming previous methods and yielding an average reconstruction error
below 5 mm.

</details>


### [66] [Improving Video Diffusion Transformer Training by Multi-Feature Fusion and Alignment from Self-Supervised Vision Encoders](https://arxiv.org/abs/2509.09547)
*Dohun Lee,Hyeonho Jeong,Jiwook Kim,Duygu Ceylan,Jong Chul Ye*

Main category: cs.CV

TL;DR: 提出Align4Gen，通过对齐生成器中间特征与预训练视觉编码器特征并进行多特征融合，提高了视频扩散模型的生成质量与时序一致性。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在架构与训练目标上已有诸多改进，但对提升特征表示能力关注较少，作者 hypothesize 对齐预训练视觉编码器的特征能帮助生成器学习更好的表征，从而改善视频生成质量。

Method: 通过设计新的度量评估视觉编码器的判别力和时序一致性，选择合适的编码器，并提出Align4Gen框架，包含多特征融合与特征对齐损失，集成到视频扩散模型训练中。

Result: 在无条件与有条件视频生成任务上，Align4Gen在多项评价指标上均优于基线，表明特征对齐能有效提升生成视频的质量与时序一致性。

Conclusion: 本文提出通过将视频扩散模型生成器的中间特征与预训练视觉编码器的特征对齐，提升视频生成质量。

Abstract: Video diffusion models have advanced rapidly in the recent years as a result
of series of architectural innovations (e.g., diffusion transformers) and use
of novel training objectives (e.g., flow matching). In contrast, less attention
has been paid to improving the feature representation power of such models. In
this work, we show that training video diffusion models can benefit from
aligning the intermediate features of the video generator with feature
representations of pre-trained vision encoders. We propose a new metric and
conduct an in-depth analysis of various vision encoders to evaluate their
discriminability and temporal consistency, thereby assessing their suitability
for video feature alignment. Based on the analysis, we present Align4Gen which
provides a novel multi-feature fusion and alignment method integrated into
video diffusion model training. We evaluate Align4Gen both for unconditional
and class-conditional video generation tasks and show that it results in
improved video generation as quantified by various metrics. Full video results
are available on our project page: https://align4gen.github.io/align4gen/

</details>


### [67] [InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation](https://arxiv.org/abs/2509.09555)
*Sirui Xu,Dongting Li,Yucheng Zhang,Xiyan Xu,Qi Long,Ziyin Wang,Yunzhi Lu,Shuchang Dong,Hezi Jiang,Akshat Gupta,Yu-Xiong Wang,Liang-Yan Gui*

Main category: cs.CV

TL;DR: InterAct提供了一个大规模、优化后的3D人-物交互数据集（30.70小时）与标准基准，通过统一优化与接触不变性扩增提高质量并推动HOI生成研究。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集规模小、标注不充分且存在物理与动作伪影，阻碍高质量3D人-物交互建模与生成。

Method: 整合并标准化来自多源的21.81小时数据，设计统一优化框架以修复接触穿透、漂浮与手部错误动作；基于接触不变性制造运动变体将数据扩展至30.70小时；定义六个基准任务并提出统一生成建模视角以评估模型性能。

Result: 发布了含详细文本注释的30.70小时高质量HOI数据集InterAct，并在六项任务上展示SOTA生成性能，数据集公开可获得。

Conclusion: InterAct通过整合多数据源、统一优化和接触不变性扩增等方法，显著提升了3D人-物交互数据质量与规模，为HOI生成与建模提供了重要基准资源。

Abstract: While large-scale human motion capture datasets have advanced human motion
generation, modeling and generating dynamic 3D human-object interactions (HOIs)
remain challenging due to dataset limitations. Existing datasets often lack
extensive, high-quality motion and annotation and exhibit artifacts such as
contact penetration, floating, and incorrect hand motions. To address these
issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset
and methodological advancements. First, we consolidate and standardize 21.81
hours of HOI data from diverse sources, enriching it with detailed textual
annotations. Second, we propose a unified optimization framework to enhance
data quality by reducing artifacts and correcting hand motions. Leveraging the
principle of contact invariance, we maintain human-object relationships while
introducing motion variations, expanding the dataset to 30.70 hours. Third, we
define six benchmarking tasks and develop a unified HOI generative modeling
perspective, achieving state-of-the-art performance. Extensive experiments
validate the utility of our dataset as a foundational resource for advancing 3D
human-object interaction generation. To support continued research in this
area, the dataset is publicly available at
https://github.com/wzyabcas/InterAct, and will be actively maintained.

</details>


### [68] [Invisible Attributes, Visible Biases: Exploring Demographic Shortcuts in MRI-based Alzheimer's Disease Classification](https://arxiv.org/abs/2509.09558)
*Akshit Achara,Esther Puyol Anton,Alexander Hammers,Andrew P. King*

Main category: cs.CV

TL;DR: 该研究在多个数据集与模型上实证了基于脑MRI的AD深度学习诊断中存在种族与性别的shortcut learning与偏差，并通过归因分析揭示特征重叠，强调需采取公平性干预以构建更可信的诊断系统。


<details>
  <summary>Details</summary>
Motivation: 担心深度学习模型在MRI诊断中通过与疾病标签无直接因果关系但与受保护属性相关的伪特征进行预测，从而对弱势群体（如不同种族与性别）产生性能偏差，需揭示并为更公平的诊断工具奠定基础。

Method: 作者使用多数据集与两个主流深度学习模型（ResNet与SwinTransformer），首先训练模型识别MRI中的种族与性别以检验分布差异；随后通过在人口学不平衡的训练集上训练AD分类模型评估性能下降以揭示shortcut learning与偏差；最后对不同脑区的特征归因进行定量与定性分析以比较受保护属性与AD分类的特征重叠。

Result: 实验表明：1）模型能从3D脑MRI识别出种族与性别，说明存在分布性差异；2）在种族或性别不平衡的数据集上训练的AD分类模型性能下降，表明shortcut learning与偏差存在；3）特征归因分析显示用于识别受保护属性的脑区与用于AD分类的脑区存在部分重叠，提示模型可能利用相同伪特征。

Conclusion: 本文证明在基于MRI的阿尔茨海默病（AD）深度学习诊断中存在种族和性别的shortcut learning与偏差，表明模型可能利用与标签无关但与受保护属性相关的伪特征进行预测。

Abstract: Magnetic resonance imaging (MRI) is the gold standard for brain imaging. Deep
learning (DL) algorithms have been proposed to aid in the diagnosis of diseases
such as Alzheimer's disease (AD) from MRI scans. However, DL algorithms can
suffer from shortcut learning, in which spurious features, not directly related
to the output label, are used for prediction. When these features are related
to protected attributes, they can lead to performance bias against
underrepresented protected groups, such as those defined by race and sex. In
this work, we explore the potential for shortcut learning and demographic bias
in DL based AD diagnosis from MRI. We first investigate if DL algorithms can
identify race or sex from 3D brain MRI scans to establish the presence or
otherwise of race and sex based distributional shifts. Next, we investigate
whether training set imbalance by race or sex can cause a drop in model
performance, indicating shortcut learning and bias. Finally, we conduct a
quantitative and qualitative analysis of feature attributions in different
brain regions for both the protected attribute and AD classification tasks.
Through these experiments, and using multiple datasets and DL models (ResNet
and SwinTransformer), we demonstrate the existence of both race and sex based
shortcut learning and bias in DL based AD classification. Our work lays the
foundation for fairer DL diagnostic tools in brain MRI. The code is provided at
https://github.com/acharaakshit/ShortMR

</details>


### [69] [PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection](https://arxiv.org/abs/2509.09572)
*Sijun Dong,Yuxuan Hu,LiBo Wang,Geng Chen,Xiaoliang Meng*

Main category: cs.CV

TL;DR: PeftCD将LoRA/Adapter等PEFT模块整合进SAM2/DINOv3为骨干的Siamese VFM，训练极少参数即可实现高精度、强泛化的遥感变化检测。


<details>
  <summary>Details</summary>
Motivation: 遥感变化检测面临伪变化频繁、标注样本稀缺和跨域泛化差等挑战，VFMs具有强大表征能力，结合PEFT可在资源受限情况下高效适配下游任务。

Method: 采用共享权重的Siamese编码器（基于VFM），在编码器中集成LoRA和Adapter模块，仅训练少量附加参数；对比并评估SAM2和DINOv3两种背骨；使用轻量级解码器保持对特征表示的关注。

Result: 在多个公开数据集上取得SOTA性能：SYSU-CD IoU 73.81%、WHUCD 92.05%、MSRSCD 64.07%、MLCD 76.89%、CDD 97.01%、S2Looking 52.25%、LEVIR-CD 85.62%，表现出边界精确和伪变化抑制能力强。

Conclusion: PeftCD通过在Vision Foundation Models上引入参数高效微调（PEFT），实现了在多时相多源遥感变化检测任务中对伪变化抑制、样本稀缺和跨域泛化问题的有效解决。

Abstract: To tackle the prevalence of pseudo changes, the scarcity of labeled samples,
and the difficulty of cross-domain generalization in multi-temporal and
multi-source remote sensing imagery, we propose PeftCD, a change detection
framework built upon Vision Foundation Models (VFMs) with Parameter-Efficient
Fine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siamese
encoder derived from a VFM, into which LoRA and Adapter modules are seamlessly
integrated. This design enables highly efficient task adaptation by training
only a minimal set of additional parameters. To fully unlock the potential of
VFMs, we investigate two leading backbones: the Segment Anything Model v2
(SAM2), renowned for its strong segmentation priors, and DINOv3, a
state-of-the-art self-supervised representation learner. The framework is
complemented by a deliberately lightweight decoder, ensuring the focus remains
on the powerful feature representations from the backbones. Extensive
experiments demonstrate that PeftCD achieves state-of-the-art performance
across multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD
(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) and
LEVIR-CD (85.62%), with notably precise boundary delineation and strong
suppression of pseudo-changes. In summary, PeftCD presents an optimal balance
of accuracy, efficiency, and generalization. It offers a powerful and scalable
paradigm for adapting large-scale VFMs to real-world remote sensing change
detection applications. The code and pretrained models will be released at
https://github.com/dyzy41/PeftCD.

</details>


### [70] [Visual Grounding from Event Cameras](https://arxiv.org/abs/2509.09584)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event：首个用于事件相机与自然语言联合定位的大规模基准，含5,567场景、13,458对象和30k+指代表达，且每条表达带外观/状态/与观察者关系/与周围关系四类属性，支持可解释和时序感知的多模态研究。


<details>
  <summary>Details</summary>
Motivation: 现有工作缺乏将高时间分辨率的事件相机数据与自然语言理解结合的数据基准，阻碍了时序感知与语言驱动的感知任务发展。

Method: 构建并标注了包含5,567个场景的真实驾驶事件相机数据集，针对13,458个对象收集并验证了超过30,000条指代表达，每条表达附带外观、状态、与观察者关系、与周围物体关系等四类结构化属性。

Result: 构建了大规模、属性化的Talk2Event基准，支持在动态环境中进行可解释和组合化的语言驱动目标定位，促进多模态和时间感知感知研究。

Conclusion: Talk2Event在任务和数据层面为事件相机与自然语言的联合研究提供了首个大规模基准，强调属性化表达以支持可解释与组合化的指代定位。

Abstract: Event cameras capture changes in brightness with microsecond precision and
remain reliable under motion blur and challenging illumination, offering clear
advantages for modeling highly dynamic scenes. Yet, their integration with
natural language understanding has received little attention, leaving a gap in
multimodal perception. To address this, we introduce Talk2Event, the first
large-scale benchmark for language-driven object grounding using event data.
Built on real-world driving scenarios, Talk2Event comprises 5,567 scenes,
13,458 annotated objects, and more than 30,000 carefully validated referring
expressions. Each expression is enriched with four structured attributes --
appearance, status, relation to the viewer, and relation to surrounding objects
-- that explicitly capture spatial, temporal, and relational cues. This
attribute-centric design supports interpretable and compositional grounding,
enabling analysis that moves beyond simple object recognition to contextual
reasoning in dynamic environments. We envision Talk2Event as a foundation for
advancing multimodal and temporally-aware perception, with applications
spanning robotics, human-AI interaction, and so on.

</details>


### [71] [Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis](https://arxiv.org/abs/2509.09595)
*Yikang Ding,Jiwen Liu,Wenyuan Zhang,Zekun Wang,Wentao Hu,Liyuan Cui,Mingming Lao,Yingchao Shao,Hui Liu,Xiaohan Li,Ming Chen,Xiaoqiang Liu,Yu-Shen Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: Kling-Avatar用MLLM生成蓝图再并行细化，实现在多模态指令下的高保真、长时、语义驱动头像视频合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将指令视为低级跟踪信号，未建模指令的交流意图，导致叙事连贯性与角色表现力不足。提出方法旨在将指令语义融入生成过程以提升可控性和表现力。

Method: 两阶段流水线：第一阶段由MLLM导演根据多模态指令生成蓝图视频，负责高层语义（动作、情绪）；第二阶段基于蓝图关键帧采用首末帧并行策略生成多子片段，保留细节并并行合成长时视频。

Result: 在375个多样化样本的基准测试及大量实验中，Kling-Avatar在唇同步精度、情绪与动态表现力、指令可控性、身份保持和跨域泛化能力上均优于现有方法，能生成最高1080p、48fps的长时生动视频。

Conclusion: Kling-Avatar通过引入多模态大语言模型作为导演并采用级联的全局-局部生成框架，有效解决了以往音频驱动头像生成忽视指令语义目的的问题，实现了更具叙事性和表情表现力的高质量长视频合成。

Abstract: Recent advances in audio-driven avatar video generation have significantly
enhanced audio-visual realism. However, existing methods treat instruction
conditioning merely as low-level tracking driven by acoustic or visual cues,
without modeling the communicative purpose conveyed by the instructions. This
limitation compromises their narrative coherence and character expressiveness.
To bridge this gap, we introduce Kling-Avatar, a novel cascaded framework that
unifies multimodal instruction understanding with photorealistic portrait
generation. Our approach adopts a two-stage pipeline. In the first stage, we
design a multimodal large language model (MLLM) director that produces a
blueprint video conditioned on diverse instruction signals, thereby governing
high-level semantics such as character motion and emotions. In the second
stage, guided by blueprint keyframes, we generate multiple sub-clips in
parallel using a first-last frame strategy. This global-to-local framework
preserves fine-grained details while faithfully encoding the high-level intent
behind multimodal instructions. Our parallel architecture also enables fast and
stable generation of long-duration videos, making it suitable for real-world
applications such as digital human livestreaming and vlogging. To
comprehensively evaluate our method, we construct a benchmark of 375 curated
samples covering diverse instructions and challenging scenarios. Extensive
experiments demonstrate that Kling-Avatar is capable of generating vivid,
fluent, long-duration videos at up to 1080p and 48 fps, achieving superior
performance in lip synchronization accuracy, emotion and dynamic
expressiveness, instruction controllability, identity preservation, and
cross-domain generalization. These results establish Kling-Avatar as a new
benchmark for semantically grounded, high-fidelity audio-driven avatar
synthesis.

</details>


### [72] [Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth](https://arxiv.org/abs/2509.09610)
*Daria Laslo,Efthymios Georgiou,Marius George Linguraru,Andreas Rauschecker,Sabine Muller,Catherine R. Jutzeler,Sarah Bruningk*

Main category: cs.CV

TL;DR: 将ODE肿瘤力学模型与梯度引导DDIM结合，通过机制先验引导图像生成，能在数据受限时合成解剖与生物学一致的未来MRI并产出生长概率图。


<details>
  <summary>Details</summary>
Motivation: 预测脑肿瘤时空进展对神经肿瘤学临床决策至关重要，但长期随访数据稀缺，单纯数据驱动方法难以保证生物学一致性；因此将机制模型作为先验结合生成模型以提高可解释性与生物学逼真性。

Method: 构建了一个常微分方程（ODE）形式的肿瘤生长力学模型，包含放疗影响并预测未来肿瘤负荷；将该预测作为条件，采用梯度引导的DDIM在图像空间生成未来MRI切片；在BraTS成人与儿科数据集上训练，并在60个轴向切片的体内DMG纵向病例上评估。

Result: 在空间相似性指标上生成的随访扫描逼真；提出了肿瘤生长概率图，能捕捉生长范围与方向性，95百分位Hausdorff距离显示方法能反映临床相关的扩展与方向性。

Conclusion: 该论文提出的混合机制学习框架有效结合了力学肿瘤生长模型与引导去噪扩散隐式模型（DDIM），能够在数据有限情形下生成符合解剖学与生物学先验的未来MRI，具有临床潜在价值。

Abstract: Predicting the spatio-temporal progression of brain tumors is essential for
guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic
learning framework that combines a mathematical tumor growth model with a
guided denoising diffusion implicit model (DDIM) to synthesize anatomically
feasible future MRIs from preceding scans. The mechanistic model, formulated as
a system of ordinary differential equations, captures temporal tumor dynamics
including radiotherapy effects and estimates future tumor burden. These
estimates condition a gradient-guided DDIM, enabling image synthesis that
aligns with both predicted growth and patient anatomy. We train our model on
the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices
of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our
framework generates realistic follow-up scans based on spatial similarity
metrics. It also introduces tumor growth probability maps, which capture both
clinically relevant extent and directionality of tumor growth as shown by 95th
percentile Hausdorff Distance. The method enables biologically informed image
generation in data-limited scenarios, offering generative-space-time
predictions that account for mechanistic priors.

</details>


### [73] [Measuring Epistemic Humility in Multimodal Large Language Models](https://arxiv.org/abs/2509.09658)
*Bingkui Tong,Jiaer Xia,Sifeng Shang,Kaiyang Zhou*

Main category: cs.CV

TL;DR: HumbleBench evaluates MLLMs on rejecting plausible but incorrect answers across object, relation, attribute hallucinations using scene graphs and GPT-4-Turbo-generated MCQs with 'None of the above'.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks miss evaluating models' ability to refuse plausible but wrong answers; need to measure epistemic humility for safety-critical applications.

Method: Use panoptic scene graph dataset to extract entities/relations, GPT-4-Turbo to generate multiple-choice Qs, manual filtering; include 'None of the above' option.

Result: Evaluated various SOTA MLLMs; HumbleBench reveals gaps in rejecting false options and offers a more realistic reliability measure; dataset and code released.

Conclusion: Paper introduces HumbleBench to test MLLMs' ability to reject incorrect options, emphasizing epistemic humility.

Abstract: Hallucinations in multimodal large language models (MLLMs) -- where the model
generates content inconsistent with the input image -- pose significant risks
in real-world applications, from misinformation in visual question answering to
unsafe errors in decision-making. Existing benchmarks primarily test
recognition accuracy, i.e., evaluating whether models can select the correct
answer among distractors. This overlooks an equally critical capability for
trustworthy AI: recognizing when none of the provided options are correct, a
behavior reflecting epistemic humility. We present HumbleBench, a new
hallucination benchmark designed to evaluate MLLMs' ability to reject plausible
but incorrect answers across three hallucination types: object, relation, and
attribute. Built from a panoptic scene graph dataset, we leverage fine-grained
scene graph annotations to extract ground-truth entities and relations, and
prompt GPT-4-Turbo to generate multiple-choice questions, followed by a
rigorous manual filtering process. Each question includes a "None of the above"
option, requiring models not only to recognize correct visual information but
also to identify when no provided answer is valid. We evaluate a variety of
state-of-the-art MLLMs -- including both general-purpose and specialized
reasoning models -- on HumbleBench and share valuable findings and insights
with the community. By incorporating explicit false-option rejection,
HumbleBench fills a key gap in current evaluation suites, providing a more
realistic measure of MLLM reliability in safety-critical settings. Our code and
dataset are released publicly and can be accessed at
https://github.com/maifoundations/HumbleBench.

</details>


### [74] [Can Understanding and Generation Truly Benefit Together -- or Just Coexist?](https://arxiv.org/abs/2509.09666)
*Zhiyuan Yan,Kaiqing Lin,Zongjian Li,Junyan Ye,Hui Han,Zhendong Wang,Hao Liu,Bin Lin,Hao Li,Xue Xu,Xinyan Xiao,Jingdong Wang,Haifeng Wang,Li Yuan*

Main category: cs.CV

TL;DR: 论文通过UAE和Unified-GRPO实现理解与生成的联合训练，使用重建保真度作为统一目标，编码器生成更细致描述，解码器学习从长上下文caption重建高保真图像，并提出Unified-Bench进行评估。


<details>
  <summary>Details</summary>
Motivation: 希望通过用重建一致性作为统一目标，将视觉理解（图像->文本）与生成（文本->图像）融合，利用两者互补性提升长上下文描述理解和生成保真度。

Method: 提出UAE框架：先用大规模长上下文图像描述预训练解码器，然后通过Unified-GRPO（一种基于强化学习的三阶段流程）进行联合训练。三阶段为：冷启动（语义重建损失初始化）、生成以增强理解（训练编码器生成能最大化重建质量的描述）、理解以促进生成（训练解码器从这些描述中重建图像）。

Result: 提出了Unified-Bench评估统一多模态模型程度。实验表明在RL过程中编码器会生成更加描述性的长上下文caption，解码器能够理解并高保真重建，带来显著互促效果。

Conclusion: 该论文提出了通过自编码器视角统一视觉理解与生成任务，通过重建保真度作为训练目标，促进编码器（I2T）与解码器（T2I）之间的信息双向流动，带来互惠提升。

Abstract: In this paper, we introduce an insightful paradigm through the Auto-Encoder
lens-understanding as the encoder (I2T) that compresses images into text, and
generation as the decoder (T2I) that reconstructs images from that text. Using
reconstruction fidelity as the unified training objective, we enforce the
coherent bidirectional information flow between the understanding and
generation processes, bringing mutual gains. To implement this, we propose UAE,
a novel framework for unified multimodal learning. We begin by pre-training the
decoder with large-scale long-context image captions to capture fine-grained
semantic and complex spatial relationships. We then propose Unified-GRPO via
reinforcement learning (RL), which covers three stages: (1) A cold-start phase
to gently initialize both encoder and decoder with a semantic reconstruction
loss; (2) Generation for Understanding, where the encoder is trained to
generate informative captions that maximize the decoder's reconstruction
quality, enhancing its visual understanding; (3) Understanding for Generation,
where the decoder is refined to reconstruct from these captions, forcing it to
leverage every detail and improving its long-context instruction following and
generation fidelity. For evaluation, we introduce Unified-Bench, the first
benchmark tailored to assess the degree of unification of the UMMs. A
surprising "aha moment" arises within the multimodal learning domain: as RL
progresses, the encoder autonomously produces more descriptive captions, while
the decoder simultaneously demonstrates a profound ability to understand these
intricate descriptions, resulting in reconstructions of striking fidelity.

</details>


### [75] [Geometric Neural Distance Fields for Learning Human Motion Priors](https://arxiv.org/abs/2509.09667)
*Zhengdi Yu,Simone Foti,Linguang Zhang,Amy Zhao,Cem Keskin,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.CV

TL;DR: NRMF通过在关节旋转及其高阶动力学流形上学习神经距离场的零水平集，提供了一种新颖的高阶人类运动先验，配合自适应投影与几何积分器，实现更稳定、物理合理的运动恢复与生成。


<details>
  <summary>Details</summary>
Motivation: 现有VAE或扩散模型在时序一致性与物理合理性方面不足，需一种能显式编码高阶动力学且尊重关节流形结构的运动先验以提升多任务泛化能力。

Method: 构造了三个对应姿态、速度、加速度的神经距离场（NDFs），并在这些零水平集合上定义运动先验；提出自适应步长的混合投影算法用于将任意轨迹投影到可行运动集，以及一个几何积分器用于测试时优化和生成阶段的轨迹滚动输出。

Result: 在AMASS数据集上训练后，NRMF在去噪、运动补全（in-betweening）、以及拟合部分2D/3D观测等任务上取得显著且一致的性能提升，并具有跨模态泛化能力。

Conclusion: NRMF提出了一种基于神经距离场的高阶运动先验，在关节旋转、角速度和角加速度的乘积流形上建模，能生成时序一致且物理合理的人体运动。

Abstract: We introduce Neural Riemannian Motion Fields (NRMF), a novel 3D generative
human motion prior that enables robust, temporally consistent, and physically
plausible 3D motion recovery. Unlike existing VAE or diffusion-based methods,
our higher-order motion prior explicitly models the human motion in the zero
level set of a collection of neural distance fields (NDFs) corresponding to
pose, transition (velocity), and acceleration dynamics. Our framework is
rigorous in the sense that our NDFs are constructed on the product space of
joint rotations, their angular velocities, and angular accelerations,
respecting the geometry of the underlying articulations. We further introduce:
(i) a novel adaptive-step hybrid algorithm for projecting onto the set of
plausible motions, and (ii) a novel geometric integrator to "roll out"
realistic motion trajectories during test-time-optimization and generation. Our
experiments show significant and consistent gains: trained on the AMASS
dataset, NRMF remarkably generalizes across multiple input modalities and to
diverse tasks ranging from denoising to motion in-betweening and fitting to
partial 2D / 3D observations.

</details>


### [76] [Locality in Image Diffusion Models Emerges from Data Statistics](https://arxiv.org/abs/2509.09672)
*Artem Lukoianov,Chenyang Yuan,Justin Solomon,Vincent Sitzmann*

Main category: cs.CV

TL;DR: 作者发现扩散模型中观察到的局部性来自自然图像的像素相关性，而非卷积网络的归纳偏置；并据此提出了更能匹配深度扩散模型得分的解析去噪器。


<details>
  <summary>Details</summary>
Motivation: 动机是解释为何带有封闭式最优解的扩散模型（即“最优去噪器”）无法重现深度扩散模型的行为，尤其聚焦在先前认为由卷积网络的平移等变性和局部性归纳偏置导致的差距。作者怀疑局部性可能来自数据本身而非模型结构，从而希望构建一个更符合深度模型得分的解析替代。

Method: 作者通过理论推导与实证分析，首先构造并训练参数化的线性最优去噪器以观察其局部性；然后分析像素相关性对局部性的影响，并证明在自然图像统计下局部性是必然出现的；最后基于这些洞见设计新的解析去噪器，并与之前的方法在匹配深度模型得分上的表现进行比较。

Result: 结果包括：1）实验证明参数化线性最优去噪器表现出与深度去噪器相似的局部性；2）理论证明像素相关性可以产生这种局部性；3）基于像素相关性的解析去噪器在匹配深度模型得分上优于先前专家设计的方法。

Conclusion: 本文结论为：扩散模型中“局部性”并非源自卷积网络的归纳偏置，而是自然图像数据集像素相关性产生的统计属性；线性最优去噪器也能展现与深度去噪器相似的局部性；基于此可构造出比先前专家构建模型更贴近深度扩散模型得分的解析去噪器。

Abstract: Among generative models, diffusion models are uniquely intriguing due to the
existence of a closed-form optimal minimizer of their training objective, often
referred to as the optimal denoiser. However, diffusion using this optimal
denoiser merely reproduces images in the training set and hence fails to
capture the behavior of deep diffusion models. Recent work has attempted to
characterize this gap between the optimal denoiser and deep diffusion models,
proposing analytical, training-free models that can generate images that
resemble those generated by a trained UNet. The best-performing method
hypothesizes that shift equivariance and locality inductive biases of
convolutional neural networks are the cause of the performance gap, hence
incorporating these assumptions into its analytical model. In this work, we
present evidence that the locality in deep diffusion models emerges as a
statistical property of the image dataset, not due to the inductive bias of
convolutional neural networks. Specifically, we demonstrate that an optimal
parametric linear denoiser exhibits similar locality properties to the deep
neural denoisers. We further show, both theoretically and experimentally, that
this locality arises directly from the pixel correlations present in natural
image datasets. Finally, we use these insights to craft an analytical denoiser
that better matches scores predicted by a deep diffusion model than the prior
expert-crafted alternative.

</details>


### [77] [SpatialVID: A Large-Scale Video Dataset with Spatial Annotations](https://arxiv.org/abs/2509.09676)
*Jiahao Wang,Yufeng Yuan,Rujie Zheng,Youtian Lin,Jian Gao,Lin-Zhuo Chen,Yajie Bao,Yi Zhang,Chang Zeng,Yanxi Zhou,Xiaoxiao Long,Hao Zhu,Zhaoxiang Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 作者构建了大规模野外视频数据集SpatialVID（21,000小时原始视频、2.7M clips、7,089小时动态内容），并提供丰富的空间和语义标注，旨在解决空间智能模型训练的数据瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有空间智能模型受限于高质量、大规模训练数据稀缺，尤其在真实世界动态场景与带真实相机运动的标注方面。为解决数据规模、丰富性与标注质量不足的问题，作者提出并构建SpatialVID数据集。

Method: 作者从21,000+小时原始视频出发，构建层次化过滤管线将其切分为270万个clip（共7,089小时动态内容），并通过注释管线为每个clip生成逐帧相机位姿、深度图、动态掩码、结构化字幕与运动指令等稠密3D与语义标注。

Result: 生成的SpatialVID包含大量多样化场景、相机运动与稠密注释，数据统计显示其在场景多样性、运动类型与标注丰富性方面优于现有数据集，从而能够直接促进模型的泛化与性能提升。

Conclusion: SpatialVID通过大规模、多样化、带稠密3D标注的野外视频数据集，显著提升了空间智能任务（如空间重建与世界探索）训练的数据基础，有助于提高模型的可扩展性与真实世界泛化能力。

Abstract: Significant progress has been made in spatial intelligence, spanning both
spatial reconstruction and world exploration. However, the scalability and
real-world fidelity of current models remain severely constrained by the
scarcity of large-scale, high-quality training data. While several datasets
provide camera pose information, they are typically limited in scale,
diversity, and annotation richness, particularly for real-world dynamic scenes
with ground-truth camera motion. To this end, we collect \textbf{SpatialVID}, a
dataset consists of a large corpus of in-the-wild videos with diverse scenes,
camera movements and dense 3D annotations such as per-frame camera poses,
depth, and motion instructions. Specifically, we collect more than 21,000 hours
of raw video, and process them into 2.7 million clips through a hierarchical
filtering pipeline, totaling 7,089 hours of dynamic content. A subsequent
annotation pipeline enriches these clips with detailed spatial and semantic
information, including camera poses, depth maps, dynamic masks, structured
captions, and serialized motion instructions. Analysis of SpatialVID's data
statistics reveals a richness and diversity that directly foster improved model
generalization and performance, establishing it as a key asset for the video
and 3D vision research community.

</details>


### [78] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 作者用15000 A100 GPU天构建了一个6M图+20M双语说明的推理导向T2I数据集，并提出七赛道的PRISM-Bench评测。评测19个模型揭示性能短板，数据与基准已开源。


<details>
  <summary>Details</summary>
Motivation: 目前开源T2I模型在处理复杂语义和推理需求时落后于闭源模型，部分原因是缺乏规模化、专注于推理能力的数据与评测工具。为此需提供训练数据和评测基准以推动研究进展。

Method: 作者使用FLUX模型大规模生成图像和双语描述，基于六大维度（想象力、实体、文字渲染、风格、情感、构图）设计样本，并引入生成推理链（GCoT）记录生成步骤。同时耗费大量算力（约15000 A100 GPU天）进行数据整理；评估方面建立PRISM-Bench，包含七个赛道并利用进阶视觉-语言模型配合精心设计的prompt进行对齐与美学评估。

Result: 发布FLUX-Reason-6M数据集（600万图像，2000万中英描述）与PRISM-Bench基准，并在基准上评测19个主流模型，发现显著性能缺口并指出具体弱点。将数据与代码开源以促进行业发展。

Conclusion: 该工作通过构建大规模推理导向的图像生成数据集和细粒度评估基准，旨在缩小开源与闭源T2I模型在复杂场景生成上的差距。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [79] [Koza and Koza-Hub for born-interoperable knowledge graph generation using KGX](https://arxiv.org/abs/2509.09096)
*Daniel R Korn,Patrick Golden,Aaron Odell,Katherina Cortes,Shilpa Sundar,Kevin Schaper,Sarah Gehrke,Corey Cox,Harry Caufield,Justin Reese,Evan Morris,Christopher J Mungall,Melissa Haendel*

Main category: cs.DB

TL;DR: 提出Koza/Koza-Hub：用原语操作+YAML配置把30个生物医学数据源自动转成KGX格式，强制模式合规，减轻重复劳动并促进标准化。


<details>
  <summary>Details</summary>
Motivation: 当前生物医学知识图谱构建耗费大量重复劳动，主要因数据源缺乏统一标准和无法直接生成“knowledge-graph ready”数据。为减少冗余并提高互操作性，提出基于KGX标准的统一转换工具与流程库。

Method: 作者基于KGX标准实现了一个Python工具包（Koza）和一个包含30个权威生物医学数据源的转换流程集合（Koza-Hub）。方法包括：将图谱摄取拆解为原语操作、使用YAML配置转换流程、在转换过程中强制数据模式（schema）验证以确保输出符合KGX。

Result: 产出了一套Python软件（Koza）和一个包含30个金标准数据源的转换流程（Koza-Hub），能够将原始生物医学数据流化为KGX格式并验证模式合规，理论上减少重复劳动并促进数据标准化与可复用。

Conclusion: 该工作提出了Koza与Koza-Hub，通过将知识图谱构建过程抽象为一组原语操作、用YAML配置并强制数据模式合规，以便将生物医学原始数据转换为KGX标准格式，从而减少重复劳动并提高可复用性。

Abstract: Knowledge graph construction has become an essential domain for the future of
biomedical research. But current approaches demand a high amount of redundant
labor. These redundancies are the result of the lack of data standards and
"knowledge-graph ready" data from sources. Using the KGX standard, we aim to
solve these issues. Herein we introduce Koza and the Koza-Hub, a Python
software package which streamlines ingesting raw biomedical information into
the KGX format, and an associated set of conversion processes for thirty gold
standard biomedical data sources. Our approach is to turn knowledge graph
ingests into a set of primitive operations, provide configuration through YAML
files, and enforce compliance with the chosen data schema.

</details>


### [80] [Let's Simply Count: Quantifying Distributional Similarity Between Activities in Event Data](https://arxiv.org/abs/2509.09440)
*Henrik Kirchmann,Stephan A. Fahrenkrog-Petersen,Xixi Lu,Matthias Weidlich*

Main category: cs.DB

TL;DR: 作者提出用简单的计数嵌入替代复杂的神经嵌入来捕获活动的分布相似性，构建了全面基准并在实验中证明了其在效果、效率和可解释性上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有用于过程挖掘的分布相似性方法多借用NLP中基于神经网络的表示学习（如word2vec、自编码器），尽管有效，但训练成本高且表示难以解释；因此作者主张回归简单、可解释且高效的计数基础方法。

Method: 提出并使用简单的计数式嵌入方法：直接从事件数据中统计活动共现/上下文计数，构造高维稀疏表示，可能辅以简单的降维或归一化步骤；同时构建全面的基准框架，评估嵌入的内在质量、下游应用效果与计算效率，实证比较计数嵌入与现有神经网络嵌入方法。

Result: 实验结果表明：计数式嵌入在多个衡量维度上表现优良——在内在相似性评估、若干下游任务（例如聚类、相似性检索或流程模型增强）中的效果与神经方法相当或更好，同时显著降低训练时间和资源消耗，并提供更直接可解释的特征。

Conclusion: 本文结论是：基于计数的嵌入（count-based embeddings）在建模活动的分布相似性方面，能够在可解释性、计算效率和下游任务性能上，与基于神经网络的方法（如word2vec和自编码器）竞争甚至优于之。

Abstract: To obtain insights from event data, advanced process mining methods assess
the similarity of activities to incorporate their semantic relations into the
analysis. Here, distributional similarity that captures similarity from
activity co-occurrences is commonly employed. However, existing work for
distributional similarity in process mining adopt neural network-based
approaches as developed for natural language processing, e.g., word2vec and
autoencoders. While these approaches have been shown to be effective, their
downsides are high computational costs and limited interpretability of the
learned representations.
  In this work, we argue for simplicity in the modeling of distributional
similarity of activities. We introduce count-based embeddings that avoid a
complex training process and offer a direct interpretable representation. To
underpin our call for simple embeddings, we contribute a comprehensive
benchmarking framework, which includes means to assess the intrinsic quality of
embeddings, their performance in downstream applications, and their
computational efficiency. In experiments that compare against the state of the
art, we demonstrate that count-based embeddings provide a highly effective and
efficient basis for distributional similarity between activities in event data.

</details>


### [81] [Database Views as Explanations for Relational Deep Learning](https://arxiv.org/abs/2509.09482)
*Agapi Rissaki,Ilias Fountalis,Wolfgang Gatterbauer,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 为关系数据库上的深度模型（尤其hetero-GNN）提出以视图定义为单位的全局解释框架，基于确定性建立解释性量化，结合启发式搜索与可学习掩码实现高效生成，在RelBench上证明了有效性。


<details>
  <summary>Details</summary>
Motivation: 现代基于关系数据库训练的深度模型（尤其基于异构图结构的模型）通常参数众多且结构复杂，难以用人类可理解的方式解释其决策因果与依赖来源。需要一种能指出数据库中哪些部分（列、外键、行组等）真正驱动模型预测的全局解释机制，同时能控制解释粒度与简洁性，便于数据库专家理解与验证。

Method: 引入基于确定性的解释框架：将解释形式化为视图定义并量化其对模型预测的确定性，同时通过约简或正则化控制简洁性。实现上：1) 定义不同粒度的视图语言（列级、外键边、元组集合等）；2) 开发启发式搜索算法以在数据库空间高效寻找高确定性且简洁的视图；3) 提出模型无关技术（如基于置换/遮挡策略的影响评估）和针对hetero-GNN的可学习掩码方法，利用模型内部结构与学习过程来识别重要输入子集。最终通过实证在RelBench多任务、多域上评估性能。

Result: 在RelBench基准上，提出的方法能生成具有高确定性且相对简洁的视图式解释，覆盖不同任务与领域。启发式算法与可学习掩码方法在效率上优于穷举基线，且生成的解释被证明有助于理解模型如何利用表结构与记录间的关系。实验展示了可控粒度解释（列、关系、子图）能有效反映模型依赖，同时在运行时间与可解释性质量间取得良好折中。

Conclusion: 本文提出了在关系数据库上为复杂深度学习模型（尤其是异构图神经网络）生成可解释性解释的新框架，以视图定义（view definitions）为解释形式，突出对模型预测贡献最大的数据库片段。通过引入并适应Nash等人的确定性(determinacy)概念，提出全局反事实/溯因式(abductive)解释，允许在确定性与简洁性之间调节，并通过选择不同的视图片段语言控制粒度（列、外键、元组组等）。针对hetero-GNN，设计了既有模型无关的启发式算法，又有利用可学习掩码(masking)的模型相关方法，以避免穷举数据库搜索。实验在RelBench数据集上显示了方法在解释质量与生成效率上的有效性。

Abstract: In recent years, there has been significant progress in the development of
deep learning models over relational databases, including architectures based
on heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graph
transformers. In effect, such architectures state how the database records and
links (e.g., foreign-key references) translate into a large, complex numerical
expression, involving numerous learnable parameters. This complexity makes it
hard to explain, in human-understandable terms, how a model uses the available
data to arrive at a given prediction. We present a novel framework for
explaining machine-learning models over relational databases, where
explanations are view definitions that highlight focused parts of the database
that mostly contribute to the model's prediction. We establish such global
abductive explanations by adapting the classic notion of determinacy by Nash,
Segoufin, and Vianu (2010). In addition to tuning the tradeoff between
determinacy and conciseness, the framework allows controlling the level of
granularity by adopting different fragments of view definitions, such as ones
highlighting whole columns, foreign keys between tables, relevant groups of
tuples, and so on. We investigate the realization of the framework in the case
of hetero-GNNs. We develop heuristic algorithms that avoid the exhaustive
search over the space of all databases. We propose techniques that are
model-agnostic, and others that are tailored to hetero-GNNs via the notion of
learnable masking. Our approach is evaluated through an extensive empirical
study on the RelBench collection, covering a variety of domains and different
record-level tasks. The results demonstrate the usefulness of the proposed
explanations, as well as the efficiency of their generation.

</details>
