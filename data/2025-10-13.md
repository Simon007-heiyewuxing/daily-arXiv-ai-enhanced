<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 91]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes](https://arxiv.org/abs/2510.08589)
*Nirmal Elamon,Rouzbeh Davoudi*

Main category: cs.CV

TL;DR: Fine-tuning multi-modal LLMs on limited data yields large gains for artificial text overlay detection, outperforming or matching CNNs and demonstrating cross-modal data efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper aims to compare traditional CNNs and multi-modal LLMs for object detection, especially artificial text overlay detection, and to show that fine-tuned LLMs can be data-efficient and effective.

Method: Comprehensive comparison including fine-tuned CNNs, zero-shot pre-trained LLMs, and fine-tuned LLMs; experiments on artificial text overlay detection; fine-tuning LLMs on <1,000 images; evaluation against CNN baselines; code release.

Result: Fine-tuned multi-modal LLMs, even with <1,000 images, can improve accuracy up to 36% and match/surpass CNNs that need much more data.

Conclusion: LLM-based multi-modal models are adaptable and data-efficient for specialized visual tasks; they can be fine-tuned with minimal supervision to match or exceed CNN baselines, offering practical guidance for low-resource visual applications.

Abstract: The field of object detection and understanding is rapidly evolving, driven
by advances in both traditional CNN-based models and emerging multi-modal large
language models (LLMs). While CNNs like ResNet and YOLO remain highly effective
for image-based tasks, recent transformer-based LLMs introduce new capabilities
such as dynamic context reasoning, language-guided prompts, and holistic scene
understanding. However, when used out-of-the-box, the full potential of LLMs
remains underexploited, often resulting in suboptimal performance on
specialized visual tasks. In this work, we conduct a comprehensive comparison
of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and
fine-tuned multi-modal LLMs on the challenging task of artificial text overlay
detection in images. A key contribution of our study is demonstrating that LLMs
can be effectively fine-tuned on very limited data (fewer than 1,000 images) to
achieve up to 36% accuracy improvement, matching or surpassing CNN-based
baselines that typically require orders of magnitude more data. By exploring
how language-guided models can be adapted for precise visual understanding with
minimal supervision, our work contributes to the broader effort of bridging
vision and language, offering novel insights into efficient cross-modal
learning strategies. These findings highlight the adaptability and data
efficiency of LLM-based approaches for real-world object detection tasks and
provide actionable guidance for applying multi-modal transformers in
low-resource visual environments. To support continued progress in this area,
we have made the code used to fine-tune the models available in our GitHub,
enabling future improvements and reuse in related applications.

</details>


### [2] [Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation](https://arxiv.org/abs/2510.08617)
*Saumya B*

Main category: cs.CV

TL;DR: 使用U-Net + focal loss 并测评水平翻转、旋转、缩放三种简单数据增强，达到与最新方法相当的90%精确率，公开代码以确保可复现性，作为基线研究。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分割中类别不平衡和模型泛化能力受限的问题，提供一个透明可复现的基线供后续研究比对。

Method: 在公开脑肿瘤 MRI 数据集上训练 U-Net，调节 focal loss 参数（如 gamma、alpha），并分别/组合应用水平翻转、旋转、缩放三种增强，报告精确率等指标。

Result: This paper evaluates U-Net with focal loss and simple augmentations for brain tumor MRI segmentation, providing reproducible code and results.

Conclusion: U-Net 结合 focal loss 在公开 MRI 数据集上能达到较高精度；基本的数据增强对性能有影响；公开代码建立了透明可复现基线，有助于后续研究聚焦于增强策略及损失函数设计。

Abstract: Brain tumor segmentation is crucial for diagnosis and treatment planning, yet
challenges such as class imbalance and limited model generalization continue to
hinder progress. This work presents a reproducible evaluation of U-Net
segmentation performance on brain tumor MRI using focal loss and basic data
augmentation strategies. Experiments were conducted on a publicly available MRI
dataset, focusing on focal loss parameter tuning and assessing the impact of
three data augmentation techniques: horizontal flip, rotation, and scaling. The
U-Net with focal loss achieved a precision of 90%, comparable to
state-of-the-art results. By making all code and results publicly available,
this study establishes a transparent, reproducible baseline to guide future
research on augmentation strategies and loss function design in brain tumor
segmentation.

</details>


### [3] [Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.08625)
*Hyeonggeun Han,Sehwan Kim,Hyungjun Joo,Sangwoo Hong,Jungwoo Lee*

Main category: cs.CV

TL;DR: 初始噪声决定何时从记忆吸引盆地中逃逸；通过集体或个体地调整初始噪声可提早逃逸，减少模型记忆化并维持对提示的对齐。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在生成过程中记忆并复制训练数据导致的隐私和版权问题，尤其是在分类器自由引导（CFG）应用时会将去噪轨迹引向记忆化输出的“吸引盆地（attraction basin）”。

Method: 基于对不同初始噪声样本导致逃逸时间差异的实证观察，提出两种策略：一是集体调整（可能对一组噪声样本进行优化或筛选），二是个体调整（对单个样本进行局部修改），以找到鼓励早期逃逸的初始噪声，并在生成过程中使用这些噪声。

Result: 提出两种通过调整初始噪声（集体或个体方式）来促进去噪轨迹更早脱离吸引盆地的策略，从而显著降低记忆化现象且保持图像与文本提示的一致性。

Conclusion: 通过选择更有利的初始噪声样本，可以促使去噪轨迹更早摆脱吸引盆地，使得在更早阶段应用CFG成为可能，从而在减少训练数据复制的同时保持生成图像与提示的对齐。

Abstract: Despite their impressive generative capabilities, text-to-image diffusion
models often memorize and replicate training data, prompting serious concerns
over privacy and copyright. Recent work has attributed this memorization to an
attraction basin-a region where applying classifier-free guidance (CFG) steers
the denoising trajectory toward memorized outputs-and has proposed deferring
CFG application until the denoising trajectory escapes this basin. However,
such delays often result in non-memorized images that are poorly aligned with
the input prompts, highlighting the need to promote earlier escape so that CFG
can be applied sooner in the denoising process. In this work, we show that the
initial noise sample plays a crucial role in determining when this escape
occurs. We empirically observe that different initial samples lead to varying
escape times. Building on this insight, we propose two mitigation strategies
that adjust the initial noise-either collectively or individually-to find and
utilize initial samples that encourage earlier basin escape. These approaches
significantly reduce memorization while preserving image-text alignment.

</details>


### [4] [The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images](https://arxiv.org/abs/2510.08628)
*Siiri Leppälampi,Sonja M. Hyrynsalmi,Erno Vanhala*

Main category: cs.CV

TL;DR: 本文研究了DALL-E 3和Ideogram在生成职业场景图像时的性别再现偏差，通过对750余张生成图像的主题分析，发现两者都会在不同程度上强化传统性别刻板印象，并讨论了年龄与情绪等因素的影响。研究最后提出了提升性别多样性与减少偏见的实践与研究建议。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注生成过程与图像质量，忽视了AI生成图像中的代表性偏见。鉴于AI工具在媒体与职业场景中的广泛应用，研究如何检测并缓解性别偏见具有重要意义。

Method: 向DALL-E 3与Ideogram分别输入关于不同职业的提示，生成超过750张图像并进行主题分析，评估性别呈现、年龄与情绪等维度的表现差异。

Result: 主题分析显示，两款工具均倾向于以传统性别角色呈现职业（例如护士偏女性、工程师偏男性），并在年龄与情绪表现上存在偏差，提示需要在生成实践与模型设计中采取干预以提升多样性与公平性。

Conclusion: 两款AI图像生成工具都会在职业图像中强化传统性别刻板印象，尽管程度不同；需要通过改进提示、模型训练与使用指南等手段来减少有害偏见，确保更广泛的代表性。

Abstract: Generative AI offers vast opportunities for creating visualisations, such as
graphics, videos, and images. However, recent studies around AI-generated
visualisations have primarily focused on the creation process and image
quality, overlooking representational biases. This study addresses this gap by
testing representation biases in AI-generated pictures in an occupational
setting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram,
compare. Additionally, the study discusses topics such as ageing and emotions
in AI-generated images. As AI image tools are becoming more widely used,
addressing and mitigating harmful gender biases becomes essential to ensure
diverse representation in media and professional settings. In this study, over
750 AI-generated images of occupations were prompted. The thematic analysis
results revealed that both DALL-E 3 and Ideogram reinforce traditional gender
stereotypes in AI-generated images, although to varying degrees. These findings
emphasise that AI visualisation tools risk reinforcing narrow representations.
In our discussion section, we propose suggestions for practitioners,
individuals and researchers to increase representation when generating images
with visible genders.

</details>


### [5] [Dynamic Mixture-of-Experts for Visual Autoregressive Model](https://arxiv.org/abs/2510.08629)
*Jort Vincenti,Metod Jazbec,Guoxuan Xia*

Main category: cs.CV

TL;DR: 在VAR中引入动态MoE路由和尺度感知阈值选择专家，可在不额外训练下节省约20%计算、加速11%并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 减少视觉自回归模型在分辨率增加时重复调用Transformer导致的计算冗余，提高推理效率并保持图像质量。

Method: 在Transformer中嵌入动态Mixture-of-Experts路由器，使用按token复杂度和分辨率自适应的阈值决定每层激活的专家数量，且阈值可随分辨率调整，无需重新训练模型。

Result: 通过在VAR中集成动态Mixture-of-Experts路由器并使用基于尺度的阈值策略，在不额外训练的情况下实现了计算与质量的折衷：减少20% FLOPs，推理速度提升11%，图像质量与稠密基线匹配。

Conclusion: 动态MoE路由配合尺度阈值在视觉自回归生成中能够显著减少冗余计算并保持质量，是一种有效的计算-质量折衷方案。

Abstract: Visual Autoregressive Models (VAR) offer efficient and high-quality image
generation but suffer from computational redundancy due to repeated Transformer
calls at increasing resolutions. We introduce a dynamic Mixture-of-Experts
router integrated into VAR. The new architecture allows to trade compute for
quality through scale-aware thresholding. This thresholding strategy balances
expert selection based on token complexity and resolution, without requiring
additional training. As a result, we achieve 20% fewer FLOPs, 11% faster
inference and match the image quality achieved by the dense baseline.

</details>


### [6] [Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs](https://arxiv.org/abs/2510.08631)
*Hanieh Shojaei Miandashti,Claus Brenner*

Main category: cs.CV

TL;DR: 提出一种无监督OOD检测方法，在深度网络特征空间对GMM参数进行层次贝叶斯建模，利用获得的模型不确定性（epistemic）区分OOD与模糊内部分布区域，无需辅助OOD数据或额外训练，在SemanticKITTI上显著优于基于预测熵的方法。


<details>
  <summary>Details</summary>
Motivation: 传统无监督OOD方法使用预测分布的熵，但该熵混合了模型不确定性和数据内在噪声（aleatoric），导致在模糊或难以分类的内部分布区域误报为OOD；因此需解耦两类不确定性，单独利用epistemic以更准确识别真实OOD实例。

Method: 在深度神经网络的特征空间对每个语义类建立高斯混合模型，并对GMM参数采取层次贝叶斯建模以获得参数后验（通过近似推断/变分推断或MCMC采样得到多组参数样本），利用这些参数样本计算基于模型参数不确定性的epistemic度量作为OOD评分，无需额外数据或再训练。

Result: 在SemanticKITTI数据集上，与基于预测熵的先前方法相比，AUROC提高约18%、AUPRC提高约22%、FPR95从76%降至40%（减少36个百分点），显示明显性能提升。

Conclusion: 该方法通过对特征空间GMM参数做层次贝叶斯推断，仅用模型不确定性即可有效检测点云中的OOD实例，在SemanticKITTI上分别提升AUROC 18%、AUPRC 22%、将FPR95从76%降至40%，表明其在避免将不确定（模糊）内部分布误判为OOD方面更鲁棒。

Abstract: In addition to accurate scene understanding through precise semantic
segmentation of LiDAR point clouds, detecting out-of-distribution (OOD)
objects, instances not encountered during training, is essential to prevent the
incorrect assignment of unknown objects to known classes. While supervised OOD
detection methods depend on auxiliary OOD datasets, unsupervised methods avoid
this requirement but typically rely on predictive entropy, the entropy of the
predictive distribution obtained by averaging over an ensemble or multiple
posterior weight samples. However, these methods often conflate epistemic
(model) and aleatoric (data) uncertainties, misclassifying ambiguous in
distribution regions as OOD. To address this issue, we present an unsupervised
OOD detection approach that employs epistemic uncertainty derived from
hierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in
the feature space of a deep neural network. Without requiring auxiliary data or
additional training stages, our approach outperforms existing uncertainty-based
methods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,
22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),
compared to the predictive entropy approach used in prior works.

</details>


### [7] [Hi-OSCAR: Hierarchical Open-set Classifier for Human Activity Recognition](https://arxiv.org/abs/2510.08635)
*Conor McCarthy,Loes Quirijnen,Jan Peter van Zandwijk,Zeno Geradts,Marcel Worring*

Main category: cs.CV

TL;DR: 提出层级开放集分类器 Hi-OSCAR，用于活动识别中同时进行已知类识别与未知类拒绝，并能将未知类定位到层级节点；并发布新公开数据集 NFI_FARED。


<details>
  <summary>Details</summary>
Motivation: 现实生活中的活动种类远超训练数据集，且类内外相似度不均，导致标准闭集分类器在遇到未知活动时不可靠；需要能拒绝未知活动并利用类别间层级关系提供更丰富的判定信息。

Method: 构建活动类别的层级结构，将分类器设计为层级开放集模型（Hi-OSCAR），结合拒绝机制判定未知样本，并将未知样本映射到最近的内部节点以提供更细粒度的信息。

Result: Hi-OSCAR 提出一种基于层级结构的开放集活动识别方法，能在保持已知活动高准确率的同时拒绝未知活动，并把未知活动定位到最近的内部节点。作者还发布了新数据集 NFI_FARED，包含 19 类活动的传感器数据并公开下载。

Conclusion: Hi-OSCAR 在保持已知活动分类性能的同时实现了有效的未知活动检测与层级定位，提升了开放集 HAR 的可解释性与实用性；新数据集 NFI_FARED 有助于推动该领域研究。

Abstract: Within Human Activity Recognition (HAR), there is an insurmountable gap
between the range of activities performed in life and those that can be
captured in an annotated sensor dataset used in training. Failure to properly
handle unseen activities seriously undermines any HAR classifier's reliability.
Additionally within HAR, not all classes are equally dissimilar, some
significantly overlap or encompass other sub-activities. Based on these
observations, we arrange activity classes into a structured hierarchy. From
there, we propose Hi-OSCAR: a Hierarchical Open-set Classifier for Activity
Recognition, that can identify known activities at state-of-the-art accuracy
while simultaneously rejecting unknown activities. This not only enables
open-set classification, but also allows for unknown classes to be localized to
the nearest internal node, providing insight beyond a binary "known/unknown"
classification. To facilitate this and future open-set HAR research, we
collected a new dataset: NFI_FARED. NFI_FARED contains data from multiple
subjects performing nineteen activities from a range of contexts, including
daily living, commuting, and rapid movements, which is fully public and
available for download.

</details>


### [8] [Detection of high-frequency oscillations using time-frequency analysis](https://arxiv.org/abs/2510.08637)
*Mostafa Mohammadpour,Mehdi Zekriyapanah Gashti,Yusif S. Gasimov*

Main category: cs.CV

TL;DR: 该文提出一种基于S变换时频表示与无监督聚类的HFO检测方法，能在80-500Hz内区分ripple与fast ripple，并在受控数据集与癫痫患者数据上取得高灵敏度(97.67%)与精确度(98.57%)，与手术结局相关性较强。


<details>
  <summary>Details</summary>
Motivation: 传统人工视觉判读耗时且主观，现有自动检测方法对伪影与棘波区分不足，亟需一种高灵敏、高精确并能在临床数据上验证的自动HFO检测工具以辅助手术规划。

Method: 对原始电信号进行S变换提取时频事件特征，随后使用无监督聚类对事件进行分类以区分HFOs、棘波、背景与伪影。方法在80-500Hz频带检测ripple与fast ripple并在受控与临床数据上验证性能指标。

Result: 在受控数据集上达到灵敏度97.67%、精确度98.57%、F-score97.78%；在患者数据中，切除与未切除通道HFOs率比为0.73，切除更多HFOs（尤其fast ripple）与术后无癫痫发作相关。

Conclusion: 研究验证了所提方法能准确检测HFOs，特别是fast ripple，与手术切除区域的HFOs减少与癫痫治愈相关，剩余HFOs与复发相关，支持HFOs为表征癫痫病灶的生物标志物。

Abstract: High-frequency oscillations (HFOs) are a new biomarker for identifying the
epileptogenic zone. Mapping HFO-generating regions can improve the precision of
resection sites in patients with refractory epilepsy. However, detecting HFOs
remains challenging, and their clinical features are not yet fully defined.
Visual identification of HFOs is time-consuming, labor-intensive, and
subjective. As a result, developing automated methods to detect HFOs is
critical for research and clinical use. In this study, we developed a novel
method for detecting HFOs in the ripple and fast ripple frequency bands (80-500
Hz). We validated it using both controlled datasets and data from epilepsy
patients. Our method employs an unsupervised clustering technique to categorize
events extracted from the time-frequency domain using the S-transform. The
proposed detector differentiates HFOs events from spikes, background activity,
and artifacts. Compared to existing detectors, our method achieved a
sensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the
controlled dataset. In epilepsy patients, our results showed a stronger
correlation with surgical outcomes, with a ratio of 0.73 between HFOs rates in
resected versus non-resected contacts. The study confirmed previous findings
that HFOs are promising biomarkers of epileptogenicity in epileptic patients.
Removing HFOs, especially fast ripple, leads to seizure freedom, while
remaining HFOs lead to seizure recurrence.

</details>


### [9] [Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry](https://arxiv.org/abs/2510.08638)
*Thomas Fel,Binxu Wang,Michael A. Lepori,Matthew Kowal,Andrew Lee,Randall Balestriero,Sonia Joseph,Ekdeep S. Lubana,Talia Konkle,Demba Ba,Martin Wattenberg*

Main category: cs.CV

TL;DR: 用SAE在DINOv2上学得3.2万个概念，揭示功能专用性和几何统计，提出并验证Minkowski表示假设，认为token是原型凸组合，超越纯线性稀疏视角。


<details>
  <summary>Details</summary>
Motivation: DINOv2广泛部署但其感知内容未知，需解释其表征结构与任务专用性，从而理解与可解释化Vision Transformer表示。

Method: 基于线性表示假设，使用SAE学习3.2万个字典单元；分析下游任务对概念的调用；研究字典几何统计；提出基于概念空间与多头注意力机制的Minkowski表示假设并检验其经验特征。

Result: 发现任务专用性（分类用“Elsewhere”概念、分割用边界探测器、深度估计用三类单目线索）；表示部分稠密而非严格稀疏，字典向更有凝聚性的结构演化且非正交，图像内tokens占据低维局部连通集；支持MRH：tokens为原型的凸组合，注意力产生凸混合并定义由原型边界的区域。

Conclusion: 提出MRH，认为视觉Transformer的token由凸组合的原型构成，超越线性稀疏表示。

Abstract: DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet
the nature of what it perceives remains unknown. As a working baseline, we
adopt the Linear Representation Hypothesis (LRH) and operationalize it using
SAEs, producing a 32,000-unit dictionary that serves as the interpretability
backbone of our study, which unfolds in three parts.
  In the first part, we analyze how different downstream tasks recruit concepts
from our learned dictionary, revealing functional specialization:
classification exploits "Elsewhere" concepts that fire everywhere except on
target objects, implementing learned negations; segmentation relies on boundary
detectors forming coherent subspaces; depth estimation draws on three distinct
monocular depth cues matching visual neuroscience principles.
  Following these functional results, we analyze the geometry and statistics of
the concepts learned by the SAE. We found that representations are partly dense
rather than strictly sparse. The dictionary evolves toward greater coherence
and departs from maximally orthogonal ideals (Grassmannian frames). Within an
image, tokens occupy a low dimensional, locally connected set persisting after
removing position. These signs suggest representations are organized beyond
linear sparsity alone.
  Synthesizing these observations, we propose a refined view: tokens are formed
by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown
among colors, fluffy among textures). This structure is grounded in Gardenfors'
conceptual spaces and in the model's mechanism as multi-head attention produces
sums of convex mixtures, defining regions bounded by archetypes. We introduce
the Minkowski Representation Hypothesis (MRH) and examine its empirical
signatures and implications for interpreting vision-transformer
representations.

</details>


### [10] [PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration](https://arxiv.org/abs/2510.08653)
*Zhe Dong,Yuzhe Sun,Haochen Jiang,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

TL;DR: PhyDAE将隐式降解特征显式化，通过RMP和FADD挖掘降解，结合物理专家和温控稀疏路由，实现高效且更准确的多任务遥感图像修复。


<details>
  <summary>Details</summary>
Motivation: 现有一体化修复方法过度依赖隐式特征，缺乏对成像物理的显式建模，导致对复杂异质降解的处理能力不足且效率较低。

Method: 采用两段级联结构：第一阶段通过残差流形投影器（RMP）和频率感知降解分解器（FADD）挖掘并显式化降解信息；第二阶段使用基于物理的专家模块及温度控制的稀疏激活策略，按降解类型路由并恢复图像。

Result: 在三个基准数据集（MD-RSID、MD-RRSHID、MDRS-Landsat）上，对雾、噪声、模糊、低光四项任务均显著优于现有方法，同时大幅减少参数量与计算复杂度，实现性能与效率的最佳平衡。

Conclusion: 该文提出了一种物理引导的多专家自适应遥感图像修复框架PhyDAE，通过将隐式降解特征显式化为决策信号，实现对多种异质降解（霾、噪声、模糊、低光）的精确识别与差异化处理。

Abstract: Remote sensing images inevitably suffer from various degradation factors
during acquisition, including atmospheric interference, sensor limitations, and
imaging conditions. These complex and heterogeneous degradations pose severe
challenges to image quality and downstream interpretation tasks. Addressing
limitations of existing all-in-one restoration methods that overly rely on
implicit feature representations and lack explicit modeling of degradation
physics, this paper proposes Physics-Guided Degradation-Adaptive Experts
(PhyDAE). The method employs a two-stage cascaded architecture transforming
degradation information from implicit features into explicit decision signals,
enabling precise identification and differentiated processing of multiple
heterogeneous degradations including haze, noise, blur, and low-light
conditions. The model incorporates progressive degradation mining and
exploitation mechanisms, where the Residual Manifold Projector (RMP) and
Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze
degradation characteristics from manifold geometry and frequency perspectives.
Physics-aware expert modules and temperature-controlled sparse activation
strategies are introduced to enhance computational efficiency while ensuring
imaging physics consistency. Extensive experiments on three benchmark datasets
(MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves
superior performance across all four restoration tasks, comprehensively
outperforming state-of-the-art methods. Notably, PhyDAE substantially improves
restoration quality while achieving significant reductions in parameter count
and computational complexity, resulting in remarkable efficiency gains compared
to mainstream approaches and achieving optimal balance between performance and
efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.

</details>


### [11] [Hulu-Med: A Transparent Generalist Model towards Holistic Medical Vision-Language Understanding](https://arxiv.org/abs/2510.08668)
*Songtao Jiang,Yuan Wang,Sibo Song,Tianxiang Hu,Chenyi Zhou,Bin Pu,Yan Zhang,Zhibo Yang,Yang Feng,Joey Tianyi Zhou,Jin Hao,Zijian Chen,Ruijia Wu,Tao Tang,Junhui Lv,Hongxia Xu,Hongwei Wang,Jun Xiao,Bin Feng,Fudong Zhu,Kenli Li,Weidi Xie,Jimeng Sun,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: Hulu-Med是一款开源、透明的医学视觉-语言大模型，通过统一patch视觉编码器和LLM解码器、医学感知的token压缩、在16.7M样本上逐步训练，实现从2D到3D及视频的多模态理解，并在30个基准上表现领先。


<details>
  <summary>Details</summary>
Motivation: Integrate diverse clinical data modalities (text, 2D/3D images, video) into a transparent medical vision-language model to improve diagnostic efficiency and reduce oversight. Address challenges in medical VLM development: opaque pipelines, data scarcity, and architectural inflexibility.

Method: 使用统一的patch-based视觉编码器和LLM解码器结构，设计医学感知的token压缩策略以降低序列长度并节省计算，采用逐步训练策略在16.7M样本上扩展模型从2D到3D及视频，训练成本控制在4k–40k GPU小时，根据参数规模训练不同版本（7B–32B）。

Result: Hulu-Med: a transparent medical VLM with unified patch-based vision encoder and LLM decoder, trained on 16.7M samples to handle 2D, 3D, and video. Medical-aware token reduction enables efficient training (4k–40k GPU hours for 7B–32B models). Achieves state-of-the-art across 30 benchmarks, surpassing open-source and competing with proprietary models in VQA, report generation, complex reasoning, multilingual and rare disease tasks. Full pipeline open-sourced.

Conclusion: 高性能医疗VLM可以通过透明、可复制的开放流程实现，Hulu-Med为临床AI提供了可获取的基础工具，推动医学多模态理解与应用。

Abstract: Real-world clinical decision-making grapples with integrating information
from diverse data modalities, including medical text, 2D/3D images, and video,
leading to inefficiencies and potential diagnostic oversights. While generalist
vision-language models (VLMs) offer promise, their medical development faces
challenges of opaque pipelines, data scarcity, and architectural inflexibility.
Here we present Hulu-Med, a transparent medical VLM that unifies understanding
across all these modalities. Built upon a unified patch-based vision encoder
and an LLM decoder, Hulu-Med was progressively trained on 16.7 million (M)
samples to scale from 2D to 3D and video comprehension. The medical-aware token
reduction enables efficient training, requiring only 4,000 to 40,000 GPU hours
for 7B to 32B parameter variants. Extensive evaluation across 30 benchmarks
exhibits state-of-the-art performance, surpassing leading open-source models
and competing with proprietary systems in tasks spanning visual
question-answering, medical report generation, and complex reasoning in
multilingual and rare disease scenarios. By open-sourcing our complete
pipeline, we establish that high-performance medical VLM can be achieved
transparently, providing a foundational tool for accessible and impactful
clinical AI. Code is released on
\href{https://github.com/ZJUI-AI4H/Hulu-Med}{https://github.com/ZJUI-AI4H/Hulu-Med}.

</details>


### [12] [Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation](https://arxiv.org/abs/2510.08673)
*Kang Liao,Size Wu,Zhonghua Wu,Linyi Jin,Chao Wang,Yikai Wang,Fei Wang,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: 该论文提出Puffin，一种统一的相机中心多模态模型，结合语言回归与扩散生成以实现任意视角的场景理解与生成；引入“将相机视为语言”的范式，并基于4M视觉-语言-相机三元组数据集训练。


<details>
  <summary>Details</summary>
Motivation: 当前空间智能领域对相机中心的理解（如视角推理）与生成（跨视图图像合成）通常分离，缺乏统一模型能同时执行二者，故提出统一的相机中心多模态模型以扩展相机维度的空间感知。

Method: 模型通过融合语言回归与基于扩散的生成，使用全局相机参数与像素级相机图（camera maps），并以“相机作为语言”的范式将摄影术语与视觉线索对齐；在4M三元组数据上训练，并进行指令调优。

Result: 在多项相机中心生成与理解基准上表现优于专门模型；展示了在空间想象、世界探索与摄影指导任务中的泛化能力。

Conclusion: Puffin在相机中心的生成与理解任务上优于专用模型，并能通过指令调整推广到空间想象、世界探索和摄影指导等跨视图任务。作者将开源代码、模型和数据集流水线。

Abstract: Camera-centric understanding and generation are two cornerstones of spatial
intelligence, yet they are typically studied in isolation. We present Puffin, a
unified camera-centric multimodal model that extends spatial awareness along
the camera dimension. Puffin integrates language regression and diffusion-based
generation to interpret and create scenes from arbitrary viewpoints. To bridge
the modality gap between cameras and vision-language, we introduce a novel
paradigm that treats camera as language, enabling thinking with camera. This
guides the model to align spatially grounded visual cues with photographic
terminology while reasoning across geometric context. Puffin is trained on
Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets.
We incorporate both global camera parameters and pixel-wise camera maps,
yielding flexible and reliable spatial generation. Experiments demonstrate
Puffin superior performance over specialized models for camera-centric
generation and understanding. With instruction tuning, Puffin generalizes to
diverse cross-view tasks such as spatial imagination, world exploration, and
photography guidance. We will release the code, models, dataset pipeline, and
benchmark to advance multimodal spatial intelligence research.

</details>


### [13] [Structured Output Regularization: a framework for few-shot transfer learning](https://arxiv.org/abs/2510.08728)
*Nicolas Ewen,Jairo Diaz-Rodriguez,Kelly Ramsay*

Main category: cs.CV

TL;DR: 提出了一种称为结构化输出正则化（SOR）的迁移学习方法：冻结网络内部结构（如卷积滤波器），对未冻结参数施加group lasso与L1正则，旨在在小样本医疗影像分类上减少过拟合并保持适应性。方法可应用于不同网络组件，在DenseNet121与EfficientNetB4上在三个few-shot医疗分类任务中取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 传统迁移学习通过冻结部分权重并添加任务层在计算上高效，但限制了模型适应域特征的能力，并在样本极少时仍可能导致过拟合。为此作者提出在保持原有结构的前提下，通过结构化正则化微调少量参数以达到更好的泛化与适应性。

Method: 核心思想是冻结内部网络结构（例如卷积核的结构或网络块），只对少量参数进行微调，并对这些参数同时施加group lasso（促进组级稀疏）和L1（促进元素级稀疏）正则化，从而限制过拟合同时保留对目标域的适应能力。实现上可将SOR应用于卷积滤波器或网络块，结合少量额外参数进行训练。

Result: 在三个few-shot医疗影像分类任务上，基于DenseNet121和EfficientNetB4的模型使用SOR获得了具有竞争力的性能，对比已建立的基线表现良好。

Conclusion: SOR通过在冻结内部结构的同时对少量可训练参数使用group lasso和L1惩罚，实现了在小样本医疗影像分类任务上的有效迁移，参数开销小且易于推广到不同网络组件，性能与基线方法相当或更好。

Abstract: Traditional transfer learning typically reuses large pre-trained networks by
freezing some of their weights and adding task-specific layers. While this
approach is computationally efficient, it limits the model's ability to adapt
to domain-specific features and can still lead to overfitting with very limited
data. To address these limitations, we propose Structured Output Regularization
(SOR), a simple yet effective framework that freezes the internal network
structures (e.g., convolutional filters) while using a combination of group
lasso and $L_1$ penalties. This framework tailors the model to specific data
with minimal additional parameters and is easily applicable to various network
components, such as convolutional filters or various blocks in neural networks
enabling broad applicability for transfer learning tasks. We evaluate SOR on
three few shot medical imaging classification tasks and we achieve competitive
results using DenseNet121, and EfficientNetB4 bases compared to established
benchmarks.

</details>


### [14] [BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities](https://arxiv.org/abs/2510.08759)
*Yu Qi,Haibo Zhao,Ziyu Guo,Siyuan Ma,Ziyan Chen,Yaokun Han,Renrui Zhang,Zitiantao Lin,Shiji Xin,Yijian Huang,Kai Cheng,Peiheng Wang,Jiazheng Liu,Jiayi Zhang,Yizhe Zhu,Wenqing Wang,Yiran Qin,Xupeng Zhu,Haojie Huang,Lawson L. S. Wong*

Main category: cs.CV

TL;DR: 提出BEAR：一个细粒度具身能力基准（4,469条目，14领域，6类），评估显示20个MLLM普遍不足；提出BEAR-Agent，通过整合预训练视觉模型提升MLLM的感知与规划，在BEAR上提升显著（GPT-5绝对+9.12%）。


<details>
  <summary>Details</summary>
Motivation: 现有评测偏向特定领域（如规划或空间理解），缺乏对MLLM具身能力的全面细粒度评估；同时需要方法来提升MLLM在现实物理世界感知、理解与交互的能力。

Method: 构建BEAR基准，包含4,469条图像-视频-文本混合条目，覆盖14个领域和6类任务（从指向、轨迹理解、空间推理到高层规划）。对20个代表性MLLM进行广泛评估，发现普遍局限。提出BEAR-Agent：一个可对话的多模态代理，集成预训练视觉模型以增强感知、三维理解与规划能力，并在BEAR上评测性能提升。

Result: BEAR基准构建完成并公开，评估20个MLLM揭示普遍短板。BEAR-Agent在BEAR上实现了显著性能提升（GPT-5上绝对提升9.12%，相对提升17.5%），并能推动模拟环境中具身任务表现的提升。

Conclusion: 该论文提出了BEAR基准与BEAR-Agent，旨在细粒度评估与提升多模态大语言模型（MLLM）的具身能力。结论是现有MLLM在所有具身能力领域均存在明显不足，而通过引入预训练视觉模型并构建可交互代理（BEAR-Agent），可显著提升MLLM在BEAR基准上的表现（在GPT-5上绝对提升9.12%，相对提升17.5%），并对模拟环境任务有正向迁移效果。

Abstract: Embodied capabilities refer to a suite of fundamental abilities for an agent
to perceive, comprehend, and interact with the physical world. While multimodal
large language models (MLLMs) show promise as embodied agents, a thorough and
systematic evaluation of their embodied capabilities remains underexplored, as
existing benchmarks primarily focus on specific domains such as planning or
spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive
and fine-grained benchmark that evaluates MLLMs on atomic embodied
capabilities. BEAR comprises 4,469 interleaved image-video-text entries across
14 domains in 6 categories, including tasks from low-level pointing, trajectory
understanding, spatial reasoning, to high-level planning. Extensive evaluation
results of 20 representative MLLMs reveal their persistent limitations across
all domains of embodied capabilities. To tackle the shortfall, we propose
BEAR-Agent, a multimodal conversable agent that integrates pretrained vision
models to strengthen MLLM perception, 3D understanding, and planning
capabilities. It substantially enhances MLLM performance across diverse
embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative
improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that
improving MLLM embodied capabilities can benefit embodied tasks in simulated
environments. Project website: https://bear-official66.github.io/

</details>


### [15] [SAFER-AiD: Saccade-Assisted Foveal-peripheral vision Enhanced Reconstruction for Adversarial Defense](https://arxiv.org/abs/2510.08761)
*Jiayang Liu,Daniel Tso,Yiming Bu,Qinru Qiu*

Main category: cs.CV

TL;DR: 提出一种借鉴人类注视机制的前处理防御：强化学习驱动的眼跳采样多视点视图并重建图像以滤除对抗扰动，提升鲁棒性且无需重训练分类器。


<details>
  <summary>Details</summary>
Motivation: 观察到人类视觉系统对对抗扰动具有先天鲁棒性，研究者假设注意力引导的非均匀采样和预测编码是关键机制，因而尝试将这些生物机制引入深度模型防御以降低计算成本并提高通用性。

Method: 框架结合三种生物机制：中央—周边处理（foveal-peripheral）、眼跳（saccadic）策略和皮层填补（cortical filling-in）。使用强化学习策略生成一系列注视点，采集对应的中央—周边视图（glimpses），将这些glimpses通过重建模块融合成一幅图像，随后送入已训练分类器。该方法为前处理步骤，无需改变下游模型参数。

Result: 在ImageNet上实验表明，该方法在多种分类器和攻击类型下均提升了鲁棒性，同时相比于其他生物学或非生物学防御方法显著降低了训练开销；无需对下游模型再训练便可无缝集成。

Conclusion: 该论文提出了一种基于生物视觉机制的防御框架，通过注意力引导的非均匀稀疏采样和预测编码，利用强化学习驱动的眼跳选择多视点注视，重建图像以抵御对抗样本，且无需对下游分类器进行再训练。

Abstract: Adversarial attacks significantly challenge the safe deployment of deep
learning models, particularly in real-world applications. Traditional defenses
often rely on computationally intensive optimization (e.g., adversarial
training or data augmentation) to improve robustness, whereas the human visual
system achieves inherent robustness to adversarial perturbations through
evolved biological mechanisms. We hypothesize that attention guided
non-homogeneous sparse sampling and predictive coding plays a key role in this
robustness. To test this hypothesis, we propose a novel defense framework
incorporating three key biological mechanisms: foveal-peripheral processing,
saccadic eye movements, and cortical filling-in. Our approach employs
reinforcement learning-guided saccades to selectively capture multiple
foveal-peripheral glimpses, which are integrated into a reconstructed image
before classification. This biologically inspired preprocessing effectively
mitigates adversarial noise, preserves semantic integrity, and notably requires
no retraining or fine-tuning of downstream classifiers, enabling seamless
integration with existing systems. Experiments on the ImageNet dataset
demonstrate that our method improves system robustness across diverse
classifiers and attack types, while significantly reducing training overhead
compared to both biologically and non-biologically inspired defense techniques.

</details>


### [16] [Detecting spills using thermal imaging, pretrained deep learning models, and a robotic platform](https://arxiv.org/abs/2510.08770)
*Gregory Yeghiyan,Jurius Azar,Devson Butani,Chan-Jin Chung*

Main category: cs.CV

TL;DR: 论文利用预训练模型对RGB与热成像进行迁移学习构建实时溢出检测系统，热成像在准确率、速度和模型体积上优于RGB，VGG19在热数据上表现最佳，系统可在消费级GPU上低延迟部署。


<details>
  <summary>Details</summary>
Motivation: 提高在多种环境和光照条件下对溢出检测的实时性、准确性和部署可行性，探索热成像相对于可见光在安全关键场景下的优势，以支持机器人和检测系统的实际部署。

Method: 使用4,000张平衡的二分类数据集（溢出 vs 非溢出），分别基于预训练模型（如VGG19、NasNetMobile等）对RGB和热图像进行迁移学习训练与评估，比较准确率、推理速度、模型大小，并在RTX 4080的消费级硬件和真实机器人平台上进行了实时推理测试。

Result: 通过实验，热成像模型在不同光照条件下表现更快更鲁棒，轻量级模型在热数据上可达最高100%准确率，最短推理时间约44 ms，模型小于350 MB，在真实机器人测试中基于热成像的VGG19表现最佳。

Conclusion: 该论文提出了基于预训练深度学习模型结合可见光（RGB）和热成像的实时溢出检测系统，结论是热成像在速度、准确率和模型大小方面优于RGB，轻量级模型（如VGG19和NasNetMobile）在热成像上可实现最高可达100%的分类准确率，并且在消费级硬件上可实现低延迟推理，适用于安全关键场景。

Abstract: This paper presents a real-time spill detection system that utilizes
pretrained deep learning models with RGB and thermal imaging to classify spill
vs. no-spill scenarios across varied environments. Using a balanced binary
dataset (4,000 images), our experiments demonstrate the advantages of thermal
imaging in inference speed, accuracy, and model size. We achieve up to 100%
accuracy using lightweight models like VGG19 and NasNetMobile, with thermal
models performing faster and more robustly across different lighting
conditions. Our system runs on consumer-grade hardware (RTX 4080) and achieves
inference times as low as 44 ms with model sizes under 350 MB, highlighting its
deployability in safety-critical contexts. Results from experiments with a real
robot and test datasets indicate that a VGG19 model trained on thermal imaging
performs best.

</details>


### [17] [LinearSR: Unlocking Linear Attention for Stable and Efficient Image Super-Resolution](https://arxiv.org/abs/2510.08771)
*Xiaohui Li,Shaobin Zhuang,Shuo Cao,Yang Yang,Yuandong Pu,Qi Qin,Siqi Luo,Bin Fu,Yihao Liu*

Main category: cs.CV

TL;DR: LinearSR enables efficient photorealistic image super-resolution by making Linear Attention practical through ESGF for stability, SNR-based MoE to manage perception-distortion trade-off, and TAG guidance; achieves SOTA perceptual quality and speed.


<details>
  <summary>Details</summary>
Motivation: Address computational bottleneck of self-attention's O(N^2) in generative image super-resolution by leveraging Linear Attention's O(N) efficiency while solving previous challenges preventing its photorealistic SR adoption.

Method: Introduce ESGF (knee point-based early-stopping fine-tuning) to prevent divergence, design SNR-based MoE to address perception-distortion trade-off, and develop TAG guidance based on precision-over-volume; implement Linear Attention backbone and optimize inference for 1-NFE diffusion forward pass and competitive multi-step inference.

Result: LinearSR: a framework that overcomes training instability using knee point-based Early-Stopping Guided Fine-tuning (ESGF), uses SNR-based Mixture of Experts to balance perception-distortion, and introduces TAG guidance from a precision-over-volume principle; achieves SOTA perceptual quality with high efficiency and 1-NFE diffusion forward pass.

Conclusion: LinearSR demonstrates that Linear Attention can be robustly applied to photorealistic SR, providing an efficient, high-quality generative framework and a foundation for future research.

Abstract: Generative models for Image Super-Resolution (SR) are increasingly powerful,
yet their reliance on self-attention's quadratic complexity (O(N^2)) creates a
major computational bottleneck. Linear Attention offers an O(N) solution, but
its promise for photorealistic SR has remained largely untapped, historically
hindered by a cascade of interrelated and previously unsolved challenges. This
paper introduces LinearSR, a holistic framework that, for the first time,
systematically overcomes these critical hurdles. Specifically, we resolve a
fundamental, training instability that causes catastrophic model divergence
using our novel "knee point"-based Early-Stopping Guided Fine-tuning (ESGF)
strategy. Furthermore, we mitigate the classic perception-distortion trade-off
with a dedicated SNR-based Mixture of Experts (MoE) architecture. Finally, we
establish an effective and lightweight guidance paradigm, TAG, derived from our
"precision-over-volume" principle. Our resulting LinearSR model simultaneously
delivers state-of-the-art perceptual quality with exceptional efficiency. Its
core diffusion forward pass (1-NFE) achieves SOTA-level speed, while its
overall multi-step inference time remains highly competitive. This work
provides the first robust methodology for applying Linear Attention in the
photorealistic SR domain, establishing a foundational paradigm for future
research in efficient generative super-resolution.

</details>


### [18] [Re-Identifying Kākā with AI-Automated Video Key Frame Extraction](https://arxiv.org/abs/2510.08775)
*Paula Maddigan,Andrew Lensen,Rachael C. Shaw*

Main category: cs.CV

TL;DR: 提出了一个结合目标检测、光流模糊过滤、DINOv2编码与聚类的无监督关键帧提取流水线，能从喂食器视频中自动选出用于kākā个体重识别的高质量帧，提供非侵入性监测替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统鸟类个体识别方法如绑环侵入且耗时，随着计算机视觉进展，研发非侵入、高效的个体识别方法对野生动物监测与保护具有重要意义。

Method: 使用YOLO和Grounding DINO进行目标检测，利用光流检测模糊帧，采用DINOv2提取图像编码，然后通过聚类选择代表性关键帧。整体为无监督流程，适用于视频来源的自动化帧抽取。

Result: 所提方法在kākā重识别任务中取得高准确率，证明关键帧选择策略能产生对重识别有利的图像集合。该方法为更复杂环境下的扩展研究打下基础。

Conclusion: 本文提出的无监督关键帧提取流水线能够从喂食器视频中选取高质量帧用于kākā个体重识别，实验表明所选帧在重识别任务上表现良好，为非侵入式种群监测提供可行替代方案。

Abstract: Accurate recognition and re-identification of individual animals is essential
for successful wildlife population monitoring. Traditional methods, such as leg
banding of birds, are time consuming and invasive. Recent progress in
artificial intelligence, particularly computer vision, offers encouraging
solutions for smart conservation and efficient automation. This study presents
a unique pipeline for extracting high-quality key frames from videos of
k\={a}k\={a} (Nestor meridionalis), a threatened forest-dwelling parrot in New
Zealand. Key frame extraction is well-studied in person re-identification,
however, its application to wildlife is limited. Using video recordings at a
custom-built feeder, we extract key frames and evaluate the re-identification
performance of our pipeline. Our unsupervised methodology combines object
detection using YOLO and Grounding DINO, optical flow blur detection, image
encoding with DINOv2, and clustering methods to identify representative key
frames. The results indicate that our proposed key frame selection methods
yield image collections which achieve high accuracy in k\={a}k\={a}
re-identification, providing a foundation for future research using media
collected in more diverse and challenging environments. Through the use of
artificial intelligence and computer vision, our non-invasive and efficient
approach provides a valuable alternative to traditional physical tagging
methods for recognising k\={a}k\={a} individuals and therefore improving the
monitoring of populations. This research contributes to developing fresh
approaches in wildlife monitoring, with applications in ecology and
conservation biology.

</details>


### [19] [Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization](https://arxiv.org/abs/2510.08789)
*Shuo Xing,Soumik Dey,Mingyang Wu,Ashirbad Mishra,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: Q-Router用VLM做实时路由，把多个专家模型按多层路由动态集成，提升VQA在不同视频类型与任务上的泛化、可解释性与扩展性，并在多项基准中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有直接监督的VQA模型在跨内容、跨任务泛化性差、可解释性不足和扩展性受限。提出基于专家集成与VLM路由的代理式框架以解决这些问题。

Method: 设计多层级模型路由器：使用VLM实时解析视频语义并选择/集成最适合的专家模型，最重的层级包含时空伪影定位模块以增强可解释性。根据计算预算动态选择路由策略。

Result: 在多项基准测试上，Q-Router匹配或超越了最先进的VQA模型，在泛化性和可解释性方面有明显提升，在Q-Bench-Video上表现优异并能定位时空伪影，可作为视频生成后训练的奖励函数。

Conclusion: Q-Router通过多层路由系统将多种专家模型和视觉语言模型（VLM）作为动态路由器结合，显著提升了VQA的泛化能力、可解释性和可扩展性。

Abstract: Video quality assessment (VQA) is a fundamental computer vision task that
aims to predict the perceptual quality of a given video in alignment with human
judgments. Existing performant VQA models trained with direct score supervision
suffer from (1) poor generalization across diverse content and tasks, ranging
from user-generated content (UGC), short-form videos, to AI-generated content
(AIGC), (2) limited interpretability, and (3) lack of extensibility to novel
use cases or content types. We propose Q-Router, an agentic framework for
universal VQA with a multi-tier model routing system. Q-Router integrates a
diverse set of expert models and employs vision--language models (VLMs) as
real-time routers that dynamically reason and then ensemble the most
appropriate experts conditioned on the input video semantics. We build a
multi-tiered routing system based on the computing budget, with the heaviest
tier involving a specific spatiotemporal artifacts localization for
interpretability. This agentic design enables Q-Router to combine the
complementary strengths of specialized experts, achieving both flexibility and
robustness in delivering consistent performance across heterogeneous video
sources and tasks. Extensive experiments demonstrate that Q-Router matches or
surpasses state-of-the-art VQA models on a variety of benchmarks, while
substantially improving generalization and interpretability. Moreover, Q-Router
excels on the quality-based question answering benchmark, Q-Bench-Video,
highlighting its promise as a foundation for next-generation VQA systems.
Finally, we show that Q-Router capably localizes spatiotemporal artifacts,
showing potential as a reward function for post-training video generation
models.

</details>


### [20] [Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2510.08791)
*Yuanhao Zou,Zhaozheng Yin*

Main category: cs.CV

TL;DR: 本文提出一个用于医学视觉问答（Med-VQA）的统一框架：多层次、多模态、多视角和多阶段的对齐策略（包含对比学习和最优传输）、结合软标签的困难负样本挖掘，以及引入答案词表先验的门控交叉注意模块。实验在多个Med-VQA数据集上超越先前SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前Med-VQA中模态对齐缺乏统一方案、困难负样本问题未充分研究且知识融合常带入无关信息，需设计统一对齐、负样本挖掘和更精确的知识融合模块。

Method: 方法包含三部分：1) 统一的异构模态对齐机制，跨层次/模态/视角/阶段，使用对比学习和最优传输来减小表示差异；2) 困难负样本挖掘，使用软标签来进行多模态对齐并增强难负样本区分；3) 门控交叉注意模块，把答案词表作为先验引入，门控选择相关信息完成知识融合。

Result: 在RAD-VQA、SLAKE、PathVQA和VQA-2019等数据集上，提出的方法优于之前的最先进方法，表明对齐策略、困难负样本挖掘与门控知识融合的有效性。

Conclusion: 所提框架通过统一模态对齐、软标签的困难负样本挖掘和基于答案词表的门控交叉注意模块，有效提高了Med-VQA性能，并在RAD-VQA、SLAKE、PathVQA和VQA-2019上取得了SOTA结果。

Abstract: Medical Visual Question Answering (Med-VQA) is a challenging task that
requires a deep understanding of both medical images and textual questions.
Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP)
have shown strong performance on the Med-VQA task, there is still no unified
solution for modality alignment, and the issue of hard negatives remains
under-explored. Additionally, commonly used knowledge fusion techniques for
Med-VQA may introduce irrelevant information. In this work, we propose a
framework to address these challenges through three key contributions: (1) a
unified solution for heterogeneous modality alignments across multiple levels,
modalities, views, and stages, leveraging methods like contrastive learning and
optimal transport theory; (2) a hard negative mining method that employs soft
labels for multi-modality alignments and enforces the hard negative pair
discrimination; and (3) a Gated Cross-Attention Module for Med-VQA that
integrates the answer vocabulary as prior knowledge and selects relevant
information from it. Our framework outperforms the previous state-of-the-art on
widely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019.

</details>


### [21] [SkipSR: Faster Super Resolution with Token Skipping](https://arxiv.org/abs/2510.08799)
*Rohan Choudhury,Shanchuan Lin,Jianyi Wang,Hao Chen,Qi Zhao,Feng Cheng,Lu Jiang,Kris Kitani,Laszlo A. Jeni*

Main category: cs.CV

TL;DR: 提出SkipSR，通过在低分辨率输入上识别低细节区域并跳过这些区域的超分辨率计算，只对需要细化的区域执行SR，从而显著加速视频超分辨率，兼顾感知质量。


<details>
  <summary>Details</summary>
Motivation: 观察到视频中许多区域本身细节较低，对细化帮助不大，但现有方法对所有像素一视同仁，造成不必要的计算开销。

Method: 在低分辨率输入上直接识别低细节区域（无需对所有像素统一处理），仅对高细节区域执行扩散基或一步扩散模型的超分辨率计算，跳过低细节区域的计算以节省时间和资源。

Result: 在标准SR基准上，对720p视频达到最多60%更快的端到端延迟，且在视觉上无明显质量损失；支持标准和一步扩散的SR模型。

Conclusion: SkipSR在保持感知质量的同时，通过跳过低细节区域的计算，在720p视频上相比先前模型实现了高达60%的端到端延迟加速，适用于标准和一步扩散SR模型。

Abstract: Diffusion-based super-resolution (SR) is a key component in video generation
and video restoration, but is slow and expensive, limiting scalability to
higher resolutions and longer videos. Our key insight is that many regions in
video are inherently low-detail and gain little from refinement, yet current
methods process all pixels uniformly. To take advantage of this, we propose
SkipSR, a simple framework for accelerating video SR by identifying low-detail
regions directly from low-resolution input, then skipping computation on them
entirely, only super-resolving the areas that require refinement. This simple
yet effective strategy preserves perceptual quality in both standard and
one-step diffusion SR models while significantly reducing computation. In
standard SR benchmarks, our method achieves up to 60% faster end-to-end latency
than prior models on 720p videos with no perceptible loss in quality. Video
demos are available at https://rccchoudhury.github.io/skipsr/

</details>


### [22] [D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition](https://arxiv.org/abs/2510.08818)
*Yiyang Huang,Yizhou Wang,Yun Fu*

Main category: cs.CV

TL;DR: 提出了D-CoDe，一个无需训练的适配框架，通过动态压缩（选帧与空间token聚合）和问题分解（将问题拆为子问题）来缓解从图像预训练VLM到视频领域迁移时的感知瓶颈与token过载问题，在多数据集和长视频任务上显著提升视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 图像预训练的VLM在扩展到视频任务时面临两个主要挑战：一是感知瓶颈（视频帧多、时序长导致模型难以有效感知全部信息）；二是token过载（输入token数量大，超出模型处理能力）。需要一种无需训练且高效的方法来压缩冗余、聚焦关键信息并引导模型分步理解视频内容。

Method: D-CoDe包含两大模块：1) 动态压缩：自适应选择代表性帧并进行内容感知的空间token聚合，以减少冗余并保留关键信息；2) 问题分解：将原始查询重写为多个子问题，引导模型分别关注视频的不同方面，从而缓解token过载并提高理解全面性。该框架为训练免费，与现有图像预训练VLM结合使用。

Result: 在多项视频理解基准上，D-CoDe显著提升性能；尤其在长视频基准上表现强劲，证明其在处理复杂与长时序视频语言任务中的有效性。代码已开源。

Conclusion: D-CoDe在无需额外训练的条件下，能够有效缓解视频理解中的感知瓶颈与token过载问题，从而提升基于图像预训练的VLM在多项视频-语言基准及长视频任务上的表现，展现出处理复杂视频语言任务的潜力。

Abstract: Video large language models (Vid-LLMs), which excel in diverse video-language
tasks, can be effectively constructed by adapting image-pretrained
vision-language models (VLMs). However, this adaptation remains challenging, as
it requires processing dense and temporally extended visual inputs that exceed
the capacity of image-based models. This paper identifies the perception
bottleneck and token overload as key challenges in extending image-based VLMs
to the video domain. To address these issues, we propose D-CoDe, a
training-free adaptation framework that incorporates dynamic compression and
question decomposition. Specifically, dynamic compression alleviates the
perception bottleneck through adaptive selection of representative frames and
content-aware aggregation of spatial tokens, thereby reducing redundancy while
preserving informative content. In parallel, question decomposition mitigates
token overload by reformulating the original query into sub-questions, guiding
the model to focus on distinct aspects of the video and enabling more
comprehensive understanding. Experiments demonstrate that D-CoDe effectively
improves video understanding across various benchmarks. Furthermore, strong
performance on the challenging long-video benchmark highlights the potential of
D-CoDe in handling complex video-language tasks. Code is available at
https://github.com/hukcc/D-CoDe.

</details>


### [23] [FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation](https://arxiv.org/abs/2510.08849)
*Hongrui Wu,Zhicheng Gao,Jin Cao,Kelu Yao,Wen Shen,Zhihua Wei*

Main category: cs.CV

TL;DR: 提出将2D CLIP嵌入作为教师监督，通过标签一致性筛选高质量视角嵌入并蒸馏到3D实例嵌入，实现在ScanNet200上SOTA并大幅加速（约6x-152x）。


<details>
  <summary>Details</summary>
Motivation: 解决现有将3D实例映射到2D并调用VLM带来的遮挡噪声、计算/内存开销大和推理慢等问题，旨在实现更鲁棒且高效的开放词汇3D实例分割。

Method: 1) 教师：从多视角2D图像生成每个3D实例的CLIP嵌入，并通过可见性和视角多样性增强；2) 学生：在点云上直接生成3D实例嵌入；3) 标签引导蒸馏：仅对标签一致的2D嵌入进行蒸馏，指导学生学习开放词汇语义。

Result: FOLK提出了一种通过标签引导知识蒸馏实现快速开放词汇3D实例分割的方法，核心在于设计一个高质量2D教师模型生成CLIP实例嵌入，并将其知识蒸馏到直接在点云上运行的3D学生模型，从而避免2D投影的遮挡噪声并显著加速推理。

Conclusion: 通过标签引导的蒸馏策略，3D学生模型能够在不依赖2D投影和VLM推理的情况下直接对点云实例进行开放词汇分类，实现性能与速度的双提升。

Abstract: Open-vocabulary 3D instance segmentation seeks to segment and classify
instances beyond the annotated label space. Existing methods typically map 3D
instances to 2D RGB-D images, and then employ vision-language models (VLMs) for
classification. However, such a mapping strategy usually introduces noise from
2D occlusions and incurs substantial computational and memory costs during
inference, slowing down the inference speed. To address the above problems, we
propose a Fast Open-vocabulary 3D instance segmentation method via Label-guided
Knowledge distillation (FOLK). Our core idea is to design a teacher model that
extracts high-quality instance embeddings and distills its open-vocabulary
knowledge into a 3D student model. In this way, during inference, the distilled
3D model can directly classify instances from the 3D point cloud, avoiding
noise caused by occlusions and significantly accelerating the inference
process. Specifically, we first design a teacher model to generate a 2D CLIP
embedding for each 3D instance, incorporating both visibility and viewpoint
diversity, which serves as the learning target for distillation. We then
develop a 3D student model that directly produces a 3D embedding for each 3D
instance. During training, we propose a label-guided distillation algorithm to
distill open-vocabulary knowledge from label-consistent 2D embeddings into the
student model. FOLK conducted experiments on the ScanNet200 and Replica
datasets, achieving state-of-the-art performance on the ScanNet200 dataset with
an AP50 score of 35.7, while running approximately 6.0x to 152.2x faster than
previous methods. All codes will be released after the paper is accepted.

</details>


### [24] [Modeling Time-Lapse Trajectories to Characterize Cranberry Growth](https://arxiv.org/abs/2510.08901)
*Ronan John,Anis Chihoub,Ryan Meegan,Gina Sidelli,Jeffery Neyhart,Peter Oudemans,Kristin Dana*

Main category: cs.CV

TL;DR: 该论文提出一种无监督微调Vision Transformer(ViT)的方法，用以建立蔓越莓果实生长的可解释时间序列模型。通过时间回归和类别预测两个前任务构建二维时间轨迹，用于生长预测和品种区分，并提供包含8个品种、52次观测的蔓越莓时序数据集。


<details>
  <summary>Details</summary>
Motivation: 人工手工标注和手工监测蔓越莓生长昂贵耗时，且深度学习特征难以解释；因此希望通过自监督方法避免标注并得到可解释的时间序列模型。

Method: 基于预训练的ViT，采用自监督双前任务：时间回归（预测拍摄时间点）和类预测（预测时间段或类别），通过微调使图像映射到二维潜在空间并沿时间形成轨迹；使用这些轨迹进行生长预测与品种区分。

Result: 在包含8个品种、52次观测的新数据集上，方法成功生成可解释的时间轨迹，能预测生长并区分品种，且提供了代码与数据以便推广。

Conclusion: 所提方法无需人工标注即可学习蔓越莓果实外观随时间演变的潜在空间，生成可解释的2D时间轨迹，能用于生长预测和品种差异识别，同时附带了新构建的时序数据集，具有可推广性。

Abstract: Change monitoring is an essential task for cranberry farming as it provides
both breeders and growers with the ability to analyze growth, predict yield,
and make treatment decisions. However, this task is often done manually,
requiring significant time on the part of a cranberry grower or breeder. Deep
learning based change monitoring holds promise, despite the caveat of
hard-to-interpret high dimensional features and hand-annotations for
fine-tuning. To address this gap, we introduce a method for modeling crop
growth based on fine-tuning vision transformers (ViTs) using a self-supervised
approach that avoids tedious image annotations. We use a two-fold pretext task
(time regression and class prediction) to learn a latent space for the
time-lapse evolution of plant and fruit appearance. The resulting 2D temporal
tracks provide an interpretable time-series model of crop growth that can be
used to: 1) predict growth over time and 2) distinguish temporal differences of
cranberry varieties. We also provide a novel time-lapse dataset of cranberry
fruit featuring eight distinct varieties, observed 52 times over the growing
season (span of around four months), annotated with information about fungicide
application, yield, and rot. Our approach is general and can be applied to
other crops and applications (code and dataset can be found at https://github.
com/ronan-39/tlt/).

</details>


### [25] [PHyCLIP: $\ell_1$-Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning](https://arxiv.org/abs/2510.08919)
*Daiki Yoshikawa,Takashi Matsubara*

Main category: cs.CV

TL;DR: 提出PHyCLIP：在多个双曲因子构成的笛卡尔乘积空间上使用L1-Product度量，同时表示层级（在因子内）与组合性（由L1合成），在多项任务上优于单一空间方法。


<details>
  <summary>Details</summary>
Motivation: Address tension between representing hierarchical relations and compositionality in vision-language embeddings; hyperbolic spaces capture hierarchy but may fail at compositionality, so propose a new product-space design.

Method: 构建由多个双曲空间因子组成的笛卡尔乘积空间，采用L1-Product度量，使得每个因子表征概念族的层级结构，而整体通过L1聚合实现跨族的组合性，训练并在多项下游任务上验证。

Result: PHyCLIP: uses L1-Product metric on Cartesian product of hyperbolic factors; captures intra-family hierarchies in individual hyperbolic factors and cross-family composition via L1-product metric; shows improved performance on zero-shot classification, retrieval, hierarchical classification, compositional tasks; yields more interpretable embeddings.

Conclusion: PHyCLIP有效弥合了层级与组合性表示的矛盾，结合多因子双曲空间与L1-Product度量，提升性能并增加语义结构可解释性。

Abstract: Vision-language models have achieved remarkable success in multi-modal
representation learning from large-scale pairs of visual scenes and linguistic
descriptions. However, they still struggle to simultaneously express two
distinct types of semantic structures: the hierarchy within a concept family
(e.g., dog $\preceq$ mammal $\preceq$ animal) and the compositionality across
different concept families (e.g., "a dog in a car" $\preceq$ dog, car). Recent
works have addressed this challenge by employing hyperbolic space, which
efficiently captures tree-like hierarchy, yet its suitability for representing
compositionality remains unclear. To resolve this dilemma, we propose PHyCLIP,
which employs an $\ell_1$-Product metric on a Cartesian product of Hyperbolic
factors. With our design, intra-family hierarchies emerge within individual
hyperbolic factors, and cross-family composition is captured by the
$\ell_1$-product metric, analogous to a Boolean algebra. Experiments on
zero-shot classification, retrieval, hierarchical classification, and
compositional understanding tasks demonstrate that PHyCLIP outperforms existing
single-space approaches and offers more interpretable structures in the
embedding space.

</details>


### [26] [SegTrans: Transferable Adversarial Examples for Segmentation Models](https://arxiv.org/abs/2510.08922)
*Yufei Song,Ziqi Zhou,Qi Lu,Hangtao Zhang,Yifan Hu,Lulu Xue,Shengshan Hu,Minghui Li,Leo Yu Zhang*

Main category: cs.CV

TL;DR: SegTrans通过局部语义重映射生成增强样本以替代原输入进行扰动优化，显著提升分割模型对抗样本的跨模型迁移性且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有针对分割模型的迁移攻击受复杂上下文依赖和替代模型与目标模型间特征分布差异影响，导致可迁移性差，需设计能扩展样本语义多样性并降低模型依赖性的攻击方法。

Method: 将输入划分为多个局部区域，保留局部语义信息并进行重映射以生成多样化的增强样本，然后用这些样本替换原始输入进行扰动优化。与使用全局语义的方法不同，仅保留局部语义以避免上下文依赖差异。

Result: 在PASCAL VOC和Cityscapes数据集、四种分割模型及三种主干网络上，SegTrans平均将迁移攻击成功率提高8.55%，并将计算效率提升超过100%。

Conclusion: SegTrans通过局部语义重映射产生增强样本并替换原始输入用于扰动优化，从而显著提升了分割模型间对抗样本的可迁移性，且未增加额外计算开销。

Abstract: Segmentation models exhibit significant vulnerability to adversarial examples
in white-box settings, but existing adversarial attack methods often show poor
transferability across different segmentation models. While some researchers
have explored transfer-based adversarial attack (i.e., transfer attack) methods
for segmentation models, the complex contextual dependencies within these
models and the feature distribution gaps between surrogate and target models
result in unsatisfactory transfer success rates. To address these issues, we
propose SegTrans, a novel transfer attack framework that divides the input
sample into multiple local regions and remaps their semantic information to
generate diverse enhanced samples. These enhanced samples replace the original
ones for perturbation optimization, thereby improving the transferability of
adversarial examples across different segmentation models. Unlike existing
methods, SegTrans only retains local semantic information from the original
input, rather than using global semantic information to optimize perturbations.
Extensive experiments on two benchmark datasets, PASCAL VOC and Cityscapes,
four different segmentation models, and three backbone networks show that
SegTrans significantly improves adversarial transfer success rates without
introducing additional computational overhead. Compared to the current
state-of-the-art methods, SegTrans achieves an average increase of 8.55% in
transfer attack success rate and improves computational efficiency by more than
100%.

</details>


### [27] [Defense against Unauthorized Distillation in Image Restoration via Feature Space Perturbation](https://arxiv.org/abs/2510.08925)
*Han Hu,Zhuoran Zheng,Chen Lyu*

Main category: cs.CV

TL;DR: 针对图像修复的知识蒸馏威胁，ASVP通过在特征空间放大主奇异值注入结构化高频扰动，显著降低学生模型质量，同时保持教师输出，提供实际可行的保护手段。


<details>
  <summary>Details</summary>
Motivation: 现有基于输出概率扰动的防御方法难以直接扩展到图像修复任务，因为修复属于生成性任务，输出连续且高维，对空间连贯性和细节敏感，简单扰动通常不足以阻止蒸馏。

Method: 在运行时对教师模型中间特征图做SVD分解，选择前k个奇异值并放大，注入结构化的高频扰动以破坏学生模型学习到的映射。

Result: 在五类图像修复任务（超分辨、低光增强、海洋/水下增强、去雾、去雨）上，ASVP使学生模型PSNR最多下降约4 dB，SSIM下降60-75%，且对教师性能影响可忽略。与先前方法相比，ASVP更稳健且防御强度更高。

Conclusion: 提出的方法ASVP能够在不显著影响教师模型输出质量的情况下，有效破坏基于教师模型输出的知识蒸馏，从而保护图像修复模型的知识产权。

Abstract: Knowledge distillation (KD) attacks pose a significant threat to deep model
intellectual property by enabling adversaries to train student networks using a
teacher model's outputs. While recent defenses in image classification have
successfully disrupted KD by perturbing output probabilities, extending these
methods to image restoration is difficult. Unlike classification, restoration
is a generative task with continuous, high-dimensional outputs that depend on
spatial coherence and fine details. Minor perturbations are often insufficient,
as students can still learn the underlying mapping.To address this, we propose
Adaptive Singular Value Perturbation (ASVP), a runtime defense tailored for
image restoration models. ASVP operates on internal feature maps of the teacher
using singular value decomposition (SVD). It amplifies the topk singular values
to inject structured, high-frequency perturbations, disrupting the alignment
needed for distillation. This hinders student learning while preserving the
teacher's output quality.We evaluate ASVP across five image restoration tasks:
super-resolution, low-light enhancement, underwater enhancement, dehazing, and
deraining. Experiments show ASVP reduces student PSNR by up to 4 dB and SSIM by
60-75%, with negligible impact on the teacher's performance. Compared to prior
methods, ASVP offers a stronger and more consistent defense.Our approach
provides a practical solution to protect open-source restoration models from
unauthorized knowledge distillation.

</details>


### [28] [RO-Bench: Large-scale robustness evaluation of MLLMs with text-driven counterfactual videos](https://arxiv.org/abs/2510.08936)
*Zixi Yang,Jiapeng Li,Muxi Diao,Yinuo Jing,Kongming Liang*

Main category: cs.CV

TL;DR: Created Ro-Bench of counterfactual edited videos (Style/Object/Background/compositions); evaluated 8 MLLMs showing major robustness drops; fine-tuning with counterfactuals significantly improves robustness


<details>
  <summary>Details</summary>
Motivation: Assess robustness of MLLMs to dynamic OOD counterfactual video edits and provide a benchmark

Method: Empirical evaluation and fine-tuning with counterfactuals

Result: Current MLLMs degrade substantially on Ro-Bench; fine-tuning with counterfactual data improves performance by 21.73% on Ro-Bench and 12.78% on MVBench

Conclusion: Counterfactual training data effectively enhances video understanding robustness of MLLMs; Ro-Bench is useful for future evaluation

Abstract: Recently, Multi-modal Large Language Models (MLLMs) have demonstrated
significant performance across various video understanding tasks. However,
their robustness, particularly when faced with manipulated video content,
remains largely unexplored. In this paper, we introduce Ro-Bench, the first
benchmark for evaluating MLLMs on dynamic out-of-distribution (OOD)
counterfactual video test sets. Ro-Bench incorporates high-quality, diverse and
temporally relevant video data, by editing Style, Object, Background and their
compositions. We evaluated eight recent video MLLMs and found that current
models exhibit substantial performance degradation on Ro-Bench when exposed to
counterfactual video content. Furthermore, we demonstrate that fine-tuning
MLLMs with counterfactual data enhances robustness, achieving a 21.73%
performance increase on Ro-Bench and a 12.78% improvement across 20 tasks in
the MVBench dataset. These findings underscore the effectiveness of
counterfactual data in enhancing the video understanding ability of MLLMs. The
code and data will be released shortly.

</details>


### [29] [Denoised Diffusion for Object-Focused Image Augmentation](https://arxiv.org/abs/2510.08955)
*Nisha Pillai,Aditi Virupakshaiah,Harrison W. Smith,Amanda J. Ashworth,Prasanna Gowda,Phillip R. Owens,Adam R. Rivers,Bindu Nanduri,Mahalingam Ramkumar*

Main category: cs.CV

TL;DR: 针对农场无人机动物监测数据稀缺问题，作者提出了将分割后的动物通过变换与扩散模型合成到多场景中的目标级数据增强方法，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决无人机动物监测在数据稀缺及场景特定问题（小目标、遮挡、部分可见）下的检测性能不足。通过生成与农场条件匹配的合成数据来提升模型鲁棒性。

Method: 动物目标分割 → 基于几何/外观变换的传统增强 → 基于扩散模型的合成图像生成 → 将合成动物嵌入多样背景 → 用增强数据训练检测模型并评估性能。

Result: 提出一种面向动物的目标级数据增强框架：先从背景中分割动物，再通过几何/外观变换和基于扩散的合成生成多样且真实的场景，实验表明增强数据在动物检测任务上优于基线模型。

Conclusion: 面向动物的对象级数据增强能在数据受限情形下生成领域相关样本，显著提升检测性能，促进实时动物健康监测的实用化。

Abstract: Modern agricultural operations increasingly rely on integrated monitoring
systems that combine multiple data sources for farm optimization. Aerial
drone-based animal health monitoring serves as a key component but faces
limited data availability, compounded by scene-specific issues such as small,
occluded, or partially visible animals. Transfer learning approaches often fail
to address this limitation due to the unavailability of large datasets that
reflect specific farm conditions, including variations in animal breeds,
environments, and behaviors. Therefore, there is a need for developing a
problem-specific, animal-focused data augmentation strategy tailored to these
unique challenges. To address this gap, we propose an object-focused data
augmentation framework designed explicitly for animal health monitoring in
constrained data settings. Our approach segments animals from backgrounds and
augments them through transformations and diffusion-based synthesis to create
realistic, diverse scenes that enhance animal detection and monitoring
performance. Our initial experiments demonstrate that our augmented dataset
yields superior performance compared to our baseline models on the animal
detection task. By generating domain-specific data, our method empowers
real-time animal health monitoring solutions even in data-scarce scenarios,
bridging the gap between limited data and practical applicability.

</details>


### [30] [Unleashing Perception-Time Scaling to Multimodal Reasoning Models](https://arxiv.org/abs/2510.08964)
*Yifan Li,Zhenghao Chen,Ziheng Wu,Kun Zhou,Ruipu Luo,Can Zhang,Zhentao He,Yufei Zhan,Wayne Xin Zhao,Minghui Qiu*

Main category: cs.CV

TL;DR: 提出了DisTANCE视觉估计基准，发现现有大视觉-语言模型在感知精度上表现欠佳且推理时放大效果有限；提出Perception-Time Scaling(PTS)通过分解感知问题并生成更多感知相关token，结合强化学习显著提升感知精度（DisTANCE高精度从8.0%提升到64.7%），并对域外任务和推理能力也有增益。


<details>
  <summary>Details</summary>
Motivation: 观察到推理时放大（inference-time scaling）能提升推理能力，但对视觉感知的影响有限。目标在于探究并提升LVLM的视觉估计精度，突破现有"一次性快速感知"的限制。

Method: 构建DisTANCE基准测评视觉估计任务；提出PTS范式，通过分解复杂感知问题为中间子问题、生成更多感知相关tokens并与强化学习（带可验证奖励）结合进行训练/调优；在合成数据上训练PTS并评估与对比推理时放大方法的效果。

Result: 在DisTANCE基准上，PTS将高精度表现从8.0%提升到64.7%；PTS带来更多感知相关tokens并增强模型对图像tokens的注意力；合成PTS数据与数学推理数据结合还能提升推理与真实感知基准表现。

Conclusion: PTS能将感知过程从一次性输出转为多步骤、token丰富的处理，显著提高LVLM的视觉估计精度并带来跨域与推理性能提升；当前LVLM的快速感知范式限制了其精度，PTS是有效解决方案。

Abstract: Recent advances in inference-time scaling, particularly those leveraging
reinforcement learning with verifiable rewards, have substantially enhanced the
reasoning capabilities of Large Vision-Language Models (LVLMs). Inspired by
this success, similar strategies have been applied to multimodal reasoning, yet
their impact on visual perception remains unclear. To investigate this gap, we
introduce DisTANCE, a perception-centric benchmark for visual estimation tasks.
Evaluation results show that LVLMs exhibit limited estimation precision, and
inference-time scaling offers only marginal gains. We attribute this to the
fast perception paradigm of current LVLMs, where visual understanding is
treated as a one-shot output without modeling the underlying perceptual
process. To address this, we propose Perception-Time Scaling (PTS), a novel
paradigm that encourages token-rich perception and decomposes complex
perception problems into intermediate tractable sub-problems, thereby enabling
perception to align with and benefit from inference-time scaling. Combined with
reinforcement learning techniques, PTS significantly improves perception
accuracy, raising high-precision performance on DisTANCE from 8.0% to 64.7%,
and generalizes well to out-of-domain tasks. Surprisingly, even though PTS data
are purely synthetic, combining them with math reasoning data yields consistent
gains in both reasoning and real-world perception benchmarks. Further analysis
reveals that PTS introduces more perception-related tokens and increases the
model's attention to image tokens. Our code and data will be publicly released.

</details>


### [31] [mmJoints: Expanding Joint Representations Beyond (x,y,z) in mmWave-Based 3D Pose Estimation](https://arxiv.org/abs/2510.08970)
*Zhenyu Wang,Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

TL;DR: 提出mmJoints，通过估计关节感测概率与位置可靠性来增强黑盒mmWave姿态估计器，降低描述符误差(<4.2%)，提升关节定位精度(最多12.5%)和活动识别性能(最多16%)。


<details>
  <summary>Details</summary>
Motivation: mmWave信号稀疏且反射弱，导致模型过度依赖统计先验而非传感器数据，影响下游任务，作者希望显式表征这种先验依赖并利用其改善可解释性和任务性能。

Method: 在预训练黑盒3D姿态估计器输出之上，训练一个模块估计每个关节被感测的概率及位置可靠性两个描述符，并将其与原始关节输出联合输入下游任务；通过115k帧、13种设置评估描述符误差、位置精度和活动识别提升。

Result: mmJoints提出一种在mmWave 3D姿态估计输出上增添关节描述符的框架，显式建模关节被传感到的概率和预测位置的可靠性，从而提高可解释性和下游任务性能。

Conclusion: mmJoints没有消除先验偏差，而是显式刻画并补偿其影响，通过额外描述符改善了可解释性和下游任务结果，在大量数据和多种设置下证明了有效性。

Abstract: In mmWave-based pose estimation, sparse signals and weak reflections often
cause models to infer body joints from statistical priors rather than sensor
data. While prior knowledge helps in learning meaningful representations,
over-reliance on it degrades performance in downstream tasks like gesture and
activity recognition. In this paper, we introduce mmJoints, a framework that
augments a pre-trained, black-box mmWave-based 3D pose estimator's output with
additional joint descriptors. Rather than mitigating bias, mmJoints makes it
explicit by estimating the likelihood of a joint being sensed and the
reliability of its predicted location. These descriptors enhance
interpretability and improve downstream task accuracy. Through extensive
evaluations using over 115,000 signal frames across 13 pose estimation
settings, we show that mmJoints estimates descriptors with an error rate below
4.2%. mmJoints also improves joint position accuracy by up to 12.5% and boosts
activity recognition by up to 16% over state-of-the-art methods.

</details>


### [32] [Hierarchical Scheduling for Multi-Vector Image Retrieval](https://arxiv.org/abs/2510.08976)
*Maoliang Li,Ke Li,Yaoyang Liu,Jiayu Chen,Zihao Zheng,Yinjun Wu,Xiang Chen*

Main category: cs.CV

TL;DR: HiMIR提出分层多粒度检索并通过一致性与稀疏性剪枝冗余，实现更高效准确的图像检索，兼具自动参数配置


<details>
  <summary>Details</summary>
Motivation: 解决MVR检索对齐不足与冗余问题，提高检索精度与效率

Method: 分析方法与创新点

Result: 在多层次粒度下提升对齐并通过相似性一致性与稀疏性减少冗余，实验显示精度提升且计算量减少最高3.5倍

Conclusion: HiMIR在精度与效率上均优于现有MVR方法，适配多个数据集

Abstract: To effectively leverage user-specific data, retrieval augmented generation
(RAG) is employed in multimodal large language model (MLLM) applications.
However, conventional retrieval approaches often suffer from limited retrieval
accuracy. Recent advances in multi-vector retrieval (MVR) improve accuracy by
decomposing queries and matching against segmented images. They still suffer
from sub-optimal accuracy and efficiency, overlooking alignment between the
query and varying image objects and redundant fine-grained image segments. In
this work, we present an efficient scheduling framework for image retrieval -
HiMIR. First, we introduce a novel hierarchical paradigm, employing multiple
intermediate granularities for varying image objects to enhance alignment.
Second, we minimize redundancy in retrieval by leveraging cross-hierarchy
similarity consistency and hierarchy sparsity to minimize unnecessary matching
computation. Furthermore, we configure parameters for each dataset
automatically for practicality across diverse scenarios. Our empirical study
shows that, HiMIR not only achieves substantial accuracy improvements but also
reduces computation by up to 3.5 times over the existing MVR system.

</details>


### [33] [HandEval: Taking the First Step Towards Hand Quality Evaluation in Generated Images](https://arxiv.org/abs/2510.08978)
*Zichuan Wang,Bo Peng,Songlin Yang,Zhenchen Tang,Jing Dong*

Main category: cs.CV

TL;DR: 提出HandPair数据集与HandEval模型，专注于生成图像手部质量评估，能更好对齐人类评价并提升生成优化与AIGC检测效能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在整体视觉质量提升的同时，仍难以准确生成复杂局部区域（尤其是手部），而针对手部质量的评估尚被忽视，限制了生成质量优化和AIGC检测等下游任务。

Method: 构建由48k高质量/低质量手部图像对组成的HandPair数据集用于无人工注释监督；设计HandEval模型，结合多模态大模型的视觉理解能力与手部关键点先验；并构建人工标注的测试集用于评估。

Result: HandEval在手部质量评估上比现有SOTA方法更符合人工判断；将HandEval嵌入图像生成和AIGC检测管线后，显著提升了生成手部的真实感与检测准确率。

Conclusion: 本文提出了首个针对生成图像中手部区域的质量评估任务与模型，验证显示在评估一致性和下游应用上优于现有方法。

Abstract: Although recent text-to-image (T2I) models have significantly improved the
overall visual quality of generated images, they still struggle in the
generation of accurate details in complex local regions, especially human
hands. Generated hands often exhibit structural distortions and unrealistic
textures, which can be very noticeable even when the rest of the body is
well-generated. However, the quality assessment of hand regions remains largely
neglected, limiting downstream task performance like human-centric generation
quality optimization and AIGC detection. To address this, we propose the first
quality assessment task targeting generated hand regions and showcase its
abundant downstream applications. We first introduce the HandPair dataset for
training hand quality assessment models. It consists of 48k images formed by
high- and low-quality hand pairs, enabling low-cost, efficient supervision
without manual annotation. Based on it, we develop HandEval, a carefully
designed hand-specific quality assessment model. It leverages the powerful
visual understanding capability of Multimodal Large Language Model (MLLM) and
incorporates prior knowledge of hand keypoints, gaining strong perception of
hand quality. We further construct a human-annotated test set with hand images
from various state-of-the-art (SOTA) T2I models to validate its quality
evaluation capability. Results show that HandEval aligns better with human
judgments than existing SOTA methods. Furthermore, we integrate HandEval into
image generation and AIGC detection pipelines, prominently enhancing generated
hand realism and detection accuracy, respectively, confirming its universal
effectiveness in downstream applications. Code and dataset will be available.

</details>


### [34] [Uncolorable Examples: Preventing Unauthorized AI Colorization via Perception-Aware Chroma-Restrictive Perturbation](https://arxiv.org/abs/2510.08979)
*Yuki Nii,Futa Waseda,Ching-Chun Chang,Isao Echizen*

Main category: cs.CV

TL;DR: 提出PAChroma，通过感知感知和色度限制扰动，生成不可上色示例，满足有效性、不可察觉、可迁移、鲁棒性四项要求，能对抗多种自动上色模型并耐受压缩等后处理。


<details>
  <summary>Details</summary>
Motivation: 应对AI自动上色可能带来的版权侵害风险，如未经授权上色并销售单色漫画/电影，提出防御性工具保护视觉内容所有者权益。

Method: 在灰度图像上优化小幅色度扰动，使用Laplacian滤波约束保持感知质量，并在优化过程中加入多样化输入变换（如缩放、压缩、噪声）以提升对不同上色模型和后处理的适应性。

Result: PAChroma在添加微小扰动后能显著降低自动上色模型的输出质量，同时保持肉眼不可察觉的外观变化。

Conclusion: PAChroma能够在保持视觉质量的前提下，有效破坏AI自动上色结果，具有一定迁移性和对常见后处理的鲁棒性，是保护单色作品免被非法上色的可行起点。

Abstract: AI-based colorization has shown remarkable capability in generating realistic
color images from grayscale inputs. However, it poses risks of copyright
infringement -- for example, the unauthorized colorization and resale of
monochrome manga and films. Despite these concerns, no effective method
currently exists to prevent such misuse. To address this, we introduce the
first defensive paradigm, Uncolorable Examples, which embed imperceptible
perturbations into grayscale images to invalidate unauthorized colorization. To
ensure real-world applicability, we establish four criteria: effectiveness,
imperceptibility, transferability, and robustness. Our method, Perception-Aware
Chroma-Restrictive Perturbation (PAChroma), generates Uncolorable Examples that
meet these four criteria by optimizing imperceptible perturbations with a
Laplacian filter to preserve perceptual quality, and applying diverse input
transformations during optimization to enhance transferability across models
and robustness against common post-processing (e.g., compression). Experiments
on ImageNet and Danbooru datasets demonstrate that PAChroma effectively
degrades colorization quality while maintaining the visual appearance. This
work marks the first step toward protecting visual content from illegitimate AI
colorization, paving the way for copyright-aware defenses in generative media.

</details>


### [35] [Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation](https://arxiv.org/abs/2510.08994)
*Yao Teng,Fuyun Wang,Xian Liu,Zhekai Chen,Han Shi,Yu Wang,Zhenguo Li,Weiyang Liu,Difan Zou,Xihui Liu*

Main category: cs.CV

TL;DR: SJD2将去噪融入Jacobi并行迭代，微调自回归模型以从噪声嵌入预测下一个干净令牌，通过并行验证多令牌和迭代细化，在保持图像质量下显著减少推理前向次数。


<details>
  <summary>Details</summary>
Motivation: 自回归文本到图像模型推理慢（逐令牌解码需数千次前向），希望通过并行化生成过程减少前向次数而不损失图像质量。

Method: 引入‘下一个干净令牌’预测范式：将已训练的自回归模型接受噪声扰动的令牌嵌入，通过低成本微调学习去噪；在嵌入空间用高斯噪声初始化序列，执行并行的预测-验证-细化循环，使用概率判据并行接受多个令牌，未接受的令牌沿去噪轨迹细化并进入下一次迭代。

Result: 实验表明SJD2在减少模型前向次数的同时保持生成图像的视觉质量，实现了生成加速。

Conclusion: 提出了SJD2，一种结合去噪过程与Jacobi迭代以实现自回归模型并行生成的方法，通过噪声扰动嵌入并微调预测“下一个干净令牌”，在推理时用高斯噪声初始化并迭代预测与并行验证多令牌，显著减少前向次数同时保持图像质量。

Abstract: As a new paradigm of visual content generation, autoregressive text-to-image
models suffer from slow inference due to their sequential token-by-token
decoding process, often requiring thousands of model forward passes to generate
a single image. To address this inefficiency, we propose Speculative
Jacobi-Denoising Decoding (SJD2), a framework that incorporates the denoising
process into Jacobi iterations to enable parallel token generation in
autoregressive models. Our method introduces a next-clean-token prediction
paradigm that enables the pre-trained autoregressive models to accept
noise-perturbed token embeddings and predict the next clean tokens through
low-cost fine-tuning. This denoising paradigm guides the model towards more
stable Jacobi trajectories. During inference, our method initializes token
sequences with Gaussian noise and performs iterative
next-clean-token-prediction in the embedding space. We employ a probabilistic
criterion to verify and accept multiple tokens in parallel, and refine the
unaccepted tokens for the next iteration with the denoising trajectory.
Experiments show that our method can accelerate generation by reducing model
forward passes while maintaining the visual quality of generated images.

</details>


### [36] [On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2510.09008)
*Hoigi Seo,Dong Un Kang,Hyunjin Cho,Joohoon Lee,Se Young Chun*

Main category: cs.CV

TL;DR: 通过对抗扰动识别不确定视觉token并在视觉编码器中掩码，能有效减轻LVLM的对象幻觉


<details>
  <summary>Details</summary>
Motivation: 解决LVLMs中的对象幻觉问题，识别并抑制导致幻觉的不确定视觉tokens

Method: 使用代理对抗扰动估计早期层视觉tokens的不确定性，然后在视觉编码器中间层的自注意力中屏蔽这些高不确定性tokens以减少它们对后续表示的影响

Result: 提出通过对抗扰动高效识别不确定视觉tokens，并在VE中间层掩码这些tokens以抑制其自注意力影响，从而显著减少对象幻觉，且可与其他方法协同

Conclusion: 仅修改视觉编码器，通过屏蔽不确定视觉tokens能降低幻觉出现率，并可与现有方法联合使用

Abstract: Large vision-language models (LVLMs), which integrate a vision encoder (VE)
with a large language model, have achieved remarkable success across various
tasks. However, there are still crucial challenges in LVLMs such as object
hallucination, generating descriptions of objects that are not in the input
image. Here, we argue that uncertain visual tokens within the VE is a key
factor that contributes to object hallucination. Our statistical analysis found
that there are positive correlations between visual tokens with high epistemic
uncertainty and the occurrence of hallucinations. Furthermore, we show
theoretically and empirically that visual tokens in early VE layers that
exhibit large representation deviations under small adversarial perturbations
indicate high epistemic uncertainty. Based on these findings, we propose a
simple yet effective strategy to mitigate object hallucination by modifying the
VE only. Our method comprises a proxy method with adversarial perturbations for
identifying uncertain visual tokens efficiently and a method to mask these
uncertain visual tokens during the self-attention process in the middle layers
of the VE, suppressing their influence on visual encoding and thus alleviating
hallucinations. Extensive experiments show that our method significantly
reduces object hallucinations in LVLMs and can synergistically work with other
prior arts.

</details>


### [37] [Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy](https://arxiv.org/abs/2510.09012)
*Xiaoxiao Ma,Feng Zhao,Pengyang Ling,Haibo Qiu,Zhixiang Wei,Hu Yu,Jie Huang,Zhixiong Zeng,Lin Ma*

Main category: cs.CV

TL;DR: 提出基于空间熵的自回归图像生成解码策略：动态温度控制与熵感知投机式接受规则，在不增加计算的情况下提升质量并将推理成本降至约85%。


<details>
  <summary>Details</summary>
Motivation: Revisit sampling issues in autoregressive image generation, identifying differences from text tokens (lower information density, non-uniform spatial distribution) and propose entropy-informed decoding to improve quality and speed.

Method: 1) 空间熵引导的动态温度控制；2) 熵感知的投机式解码接受规则，协同实现高质量近无损生成并加速采样。

Result: Dynamic temperature control guided by spatial entropy and entropy-aware acceptance rules in speculative decoding, achieving better quality and ~85% inference cost of conventional methods; extensive experiments show effectiveness and generalizability.

Conclusion: 基于空间熵的解码策略能有效提升自回归图像模型的生成质量与采样速度，且对多种模型与基准具有良好泛化性。

Abstract: In this work, we first revisit the sampling issues in current autoregressive
(AR) image generation models and identify that image tokens, unlike text
tokens, exhibit lower information density and non-uniform spatial distribution.
Accordingly, we present an entropy-informed decoding strategy that facilitates
higher autoregressive generation quality with faster synthesis speed.
Specifically, the proposed method introduces two main innovations: 1) dynamic
temperature control guided by spatial entropy of token distributions, enhancing
the balance between content diversity, alignment accuracy, and structural
coherence in both mask-based and scale-wise models, without extra computational
overhead, and 2) entropy-aware acceptance rules in speculative decoding,
achieving near-lossless generation at about 85\% of the inference cost of
conventional acceleration methods. Extensive experiments across multiple
benchmarks using diverse AR image generation models demonstrate the
effectiveness and generalizability of our approach in enhancing both generation
quality and sampling speed.

</details>


### [38] [Exploring Single Domain Generalization of LiDAR-based Semantic Segmentation under Imperfect Labels](https://arxiv.org/abs/2510.09035)
*Weitong Kong,Zichao Zeng,Di Wen,Jiale Wei,Kunyu Peng,June Moh Goo,Jan Boehm,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文定义了含噪标签的LiDAR跨域语义分割任务（DGLSS-NL），提出双视图DuNe框架通过特征一致性与置信度过滤损失有效抑制噪声并实现显著的跨域鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶感知中关键，但标注存在噪声且跨域泛化（不同环境/传感器/天气）需求高，现有噪声标签学习多针对图像，难以直接应用于稀疏不规则的点云，故需研究在噪声标签下的跨域泛化问题。

Method: DuNe为双视图框架，包含强分支与弱分支，利用特征级一致性约束和基于置信度过滤的交叉熵损失来抑制噪声标签影响；并将三种图像噪声学习方法适配为基准进行比较。

Result: 在10%对称噪声下，DuNe在SemanticKITTI、nuScenes、SemanticPOSS分别达56.86%、42.28%、52.58% mIoU；总体算术平均49.57%、调和平均48.50%，优于基准方法。

Conclusion: 提出DGLSS-NL新任务并建立基准，发现现有噪声标签方法在LiDAR上表现差，提出DuNe方法能在含噪标签的跨域LiDAR语义分割上取得领先结果。

Abstract: Accurate perception is critical for vehicle safety, with LiDAR as a key
enabler in autonomous driving. To ensure robust performance across
environments, sensor types, and weather conditions without costly
re-annotation, domain generalization in LiDAR-based 3D semantic segmentation is
essential. However, LiDAR annotations are often noisy due to sensor
imperfections, occlusions, and human errors. Such noise degrades segmentation
accuracy and is further amplified under domain shifts, threatening system
reliability. While noisy-label learning is well-studied in images, its
extension to 3D LiDAR segmentation under domain generalization remains largely
unexplored, as the sparse and irregular structure of point clouds limits direct
use of 2D methods. To address this gap, we introduce the novel task Domain
Generalization for LiDAR Semantic Segmentation under Noisy Labels (DGLSS-NL)
and establish the first benchmark by adapting three representative noisy-label
learning strategies from image classification to 3D segmentation. However, we
find that existing noisy-label learning approaches adapt poorly to LiDAR data.
We therefore propose DuNe, a dual-view framework with strong and weak branches
that enforce feature-level consistency and apply cross-entropy loss based on
confidence-aware filtering of predictions. Our approach shows state-of-the-art
performance by achieving 56.86% mIoU on SemanticKITTI, 42.28% on nuScenes, and
52.58% on SemanticPOSS under 10% symmetric label noise, with an overall
Arithmetic Mean (AM) of 49.57% and Harmonic Mean (HM) of 48.50%, thereby
demonstrating robust domain generalization in DGLSS-NL tasks. The code is
available on our project page.

</details>


### [39] [Lesion-Aware Post-Training of Latent Diffusion Models for Synthesizing Diffusion MRI from CT Perfusion](https://arxiv.org/abs/2510.09056)
*Junhyeok Lee,Hyunwoong Kim,Hyungjin Chung,Heeseong Eom,Joon Jang,Chul-Ho Sohn,Kyu Sung Choi*

Main category: cs.CV

TL;DR: 提出一种对预训练LDM进行后训练的框架，加入病灶感知的像素空间目标以提高医学影像翻译中病灶重建与图像质量，在脑卒中CT->MRI任务上优于现有方法


<details>
  <summary>Details</summary>
Motivation: LDMs are efficient but may lose critical pixel-level detail (lesions) in medical images; need to improve lesion reconstruction for clinical reliability

Method: Post-training lesion-aware objectives for LDMs

Result: Framework improves overall image quality and lesion delineation when synthesizing DWI and ADC from CTP in 817 stroke patients, outperforming existing models

Conclusion: 后训练策略能在不需从头训练的情况下提升LDM在医学图像翻译中对病灶的重建精度和整体图像质量，可推广到其他医学影像任务

Abstract: Image-to-Image translation models can help mitigate various challenges
inherent to medical image acquisition. Latent diffusion models (LDMs) leverage
efficient learning in compressed latent space and constitute the core of
state-of-the-art generative image models. However, this efficiency comes with a
trade-off, potentially compromising crucial pixel-level detail essential for
high-fidelity medical images. This limitation becomes particularly critical
when generating clinically significant structures, such as lesions, which often
occupy only a small portion of the image. Failure to accurately reconstruct
these regions can severely impact diagnostic reliability and clinical
decision-making. To overcome this limitation, we propose a novel post-training
framework for LDMs in medical image-to-image translation by incorporating
lesion-aware medical pixel space objectives. This approach is essential, as it
not only enhances overall image quality but also improves the precision of
lesion delineation. We evaluate our framework on brain CT-to-MRI translation in
acute ischemic stroke patients, where early and accurate diagnosis is critical
for optimal treatment selection and improved patient outcomes. While diffusion
MRI is the gold standard for stroke diagnosis, its clinical utility is often
constrained by high costs and low accessibility. Using a dataset of 817
patients, we demonstrate that our framework improves overall image quality and
enhances lesion delineation when synthesizing DWI and ADC images from CT
perfusion scans, outperforming existing image-to-image translation models.
Furthermore, our post-training strategy is easily adaptable to pre-trained LDMs
and exhibits substantial potential for broader applications across diverse
medical image translation tasks.

</details>


### [40] [Visual Anomaly Detection for Reliable Robotic Implantation of Flexible Microelectrode Array](https://arxiv.org/abs/2510.09071)
*Yitong Chen,Xinyao Xu,Ping Zhu,Xinyong Han,Fangbo Qin,Shan Yu*

Main category: cs.CV

TL;DR: 本文提出基于显微相机的图像异常检测框架，用于机器人柔性微电极(FME)植入过程的四个关键检查点，利用已有目标定位提取对齐ROI，输入预训练ViT并通过渐进粒度贴片特征采样与通道选择改善灵敏度-容忍度权衡与信噪比，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: FME探针结构柔性且与脑组织交互复杂，植入时易发生形变或损伤，需实时监控以保证可靠性与安全性；现有检测不足以兼顾不同位置对敏感性与容忍度的需求，故提出基于视觉的异常检测方法并优化特征采样与通道选择。

Method: 从显微相机图像利用已有的目标定位提取对齐ROI，输入预训练ViT以提取特征；采用渐进粒度patch采样（在不同位置使用不同粒度以平衡敏感性与容忍度）；从ViT特征中选择高信噪比的部分通道作为场景专用描述子；在四个检查点（微针、FME探针、挂钩结果、植入点）分别应用该统一框架进行异常检测。

Result: 在作者机器人植入系统采集的图像数据集上验证，所提方法在各检查点均表现良好，能有效检测异常并提高检测鲁棒性（论文报告的具体数值未在摘要给出）。

Conclusion: 提出的方法能在四个检查点实现鲁棒的图像异常检测，通过渐进粒度采样和高信噪通道选择提升特征表达，有助于提高FME植入过程的可靠性与安全性。

Abstract: Flexible microelectrode (FME) implantation into brain cortex is challenging
due to the deformable fiber-like structure of FME probe and the interaction
with critical bio-tissue. To ensure reliability and safety, the implantation
process should be monitored carefully. This paper develops an image-based
anomaly detection framework based on the microscopic cameras of the robotic FME
implantation system. The unified framework is utilized at four checkpoints to
check the micro-needle, FME probe, hooking result, and implantation point,
respectively. Exploiting the existing object localization results, the aligned
regions of interest (ROIs) are extracted from raw image and input to a
pretrained vision transformer (ViT). Considering the task specifications, we
propose a progressive granularity patch feature sampling method to address the
sensitivity-tolerance trade-off issue at different locations. Moreover, we
select a part of feature channels with higher signal-to-noise ratios from the
raw general ViT features, to provide better descriptors for each specific
scene. The effectiveness of the proposed methods is validated with the image
datasets collected from our implantation system.

</details>


### [41] [MambaH-Fit: Rethinking Hyper-surface Fitting-based Point Cloud Normal Estimation via State Space Modelling](https://arxiv.org/abs/2510.09088)
*Weijia Wang,Yuanzhi Su,Pei-Gen Ye,Yuan-Gen Wang,Xuequan Lu*

Main category: cs.CV

TL;DR: MambaH-Fit uses hierarchical attention fusion and patch-wise SSMs to model local hyper-surfaces for improved point cloud normal estimation, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing normal estimation methods lack modelling of fine-grained geometric structures; SSMs like Mamba show promise but prior uses focus on global shape, not local details.

Method: Introduce Attention-driven Hierarchical Feature Fusion to merge multi-scale patch features, then apply Patch-wise State Space Model to model patches as implicit hyper-surfaces via state dynamics for normal prediction; train and evaluate on benchmarks with ablations.

Result: Proposed MambaH-Fit with AHFF for multi-scale patch feature fusion and PSSM modelling patches as implicit hyper-surfaces via state dynamics; achieves better accuracy, robustness, flexibility on benchmarks; ablations validate components.

Conclusion: AHFF and PSSM effectively capture fine-grained local geometry for normal estimation, leading to superior performance over existing methods; ablations confirm component contributions.

Abstract: We present MambaH-Fit, a state space modelling framework tailored for
hyper-surface fitting-based point cloud normal estimation. Existing normal
estimation methods often fall short in modelling fine-grained geometric
structures, thereby limiting the accuracy of the predicted normals. Recently,
state space models (SSMs), particularly Mamba, have demonstrated strong
modelling capability by capturing long-range dependencies with linear
complexity and inspired adaptations to point cloud processing. However,
existing Mamba-based approaches primarily focus on understanding global shape
structures, leaving the modelling of local, fine-grained geometric details
largely under-explored. To address the issues above, we first introduce an
Attention-driven Hierarchical Feature Fusion (AHFF) scheme to adaptively fuse
multi-scale point cloud patch features, significantly enhancing geometric
context learning in local point cloud neighbourhoods. Building upon this, we
further propose Patch-wise State Space Model (PSSM) that models point cloud
patches as implicit hyper-surfaces via state dynamics, enabling effective
fine-grained geometric understanding for normal prediction. Extensive
experiments on benchmark datasets show that our method outperforms existing
ones in terms of accuracy, robustness, and flexibility. Ablation studies
further validate the contribution of the proposed components.

</details>


### [42] [GL-DT: Multi-UAV Detection and Tracking with Global-Local Integration](https://arxiv.org/abs/2510.09092)
*Juanqin Liu,Leonardo Plotegher,Eloy Roura,Shaoming He*

Main category: cs.CV

TL;DR: 提出GL-DT（STFF + 全局-局部检测）与JPTrack跟踪器，专为无人机小目标场景设计，显著提升检测召回与轨迹连续性且接近实时。


<details>
  <summary>Details</summary>
Motivation: 无人机视角下背景复杂、目标尺度小、遮挡与交互频繁，导致传统检测与跟踪方法在召回率、精度和轨迹连续性上表现不佳，因此需要兼顾小目标检测和轨迹稳定性的专用方法。

Method: 方法包括：1）Spatio-Temporal Feature Fusion (STFF)模块用于融合运动与外观特征以增强时序一致性；2）全局-局部协同检测策略改善小尺度目标的可检出性；3）基于上述检测，提出JPTrack跟踪算法以减少ID切换和轨迹碎片化。

Result: 实验显示，所提方法在连续性与稳定性指标（如ID Switch、Fragmentation、MOTA/IDF1等）上有显著改进，同时保持接近实时的推理速度，验证了方法在无人机检测与跟踪任务中的有效性。

Conclusion: 该文提出的GL-DT框架针对无人机场景下小目标检测与多目标跟踪问题，通过联合建模时空特征与全局-局部协同检测，有效提升了检测召回和跟踪连续性，并在实时性上保持优势。

Abstract: The extensive application of unmanned aerial vehicles (UAVs) in military
reconnaissance, environmental monitoring, and related domains has created an
urgent need for accurate and efficient multi-object tracking (MOT)
technologies, which are also essential for UAV situational awareness. However,
complex backgrounds, small-scale targets, and frequent occlusions and
interactions continue to challenge existing methods in terms of detection
accuracy and trajectory continuity. To address these issues, this paper
proposes the Global-Local Detection and Tracking (GL-DT) framework. It employs
a Spatio-Temporal Feature Fusion (STFF) module to jointly model motion and
appearance features, combined with a global-local collaborative detection
strategy, effectively enhancing small-target detection. Building upon this, the
JPTrack tracking algorithm is introduced to mitigate common issues such as ID
switches and trajectory fragmentation. Experimental results demonstrate that
the proposed approach significantly improves the continuity and stability of
MOT while maintaining real-time performance, providing strong support for the
advancement of UAV detection and tracking technologies.

</details>


### [43] [Dense2MoE: Restructuring Diffusion Transformer to MoE for Efficient Text-to-Image Generation](https://arxiv.org/abs/2510.09094)
*Youwei Zheng,Yuxi Ren,Xin Xia,Xuefeng Xiao,Xiaohua Xie*

Main category: cs.CV

TL;DR: 通过Dense2MoE把DiT的FFN替换为MoE并引入MoB与多步蒸馏，实现约60%激活参数减少且保持或超越原有性能，提供了一种高效的文本到图像生成新范式。


<details>
  <summary>Details</summary>
Motivation: 现有压缩方法（如剪枝）虽然能减小模型，但由于容量下降导致性能严重退化；因此希望通过结构化稀疏化在不损失模型容量的情况下减少激活参数，从而降低推理开销。

Method: 将DiT中的FFN替换为MoE层，提出Mixture of Blocks (MoB)以选择性激活DiT块；设计多步蒸馏流程，包括基于泰勒度量的专家初始化、带负载均衡的知识蒸馏和用于MoB优化的组特征损失。

Result: 在将大型扩散Transformer（如FLUX.1）转换为MoE结构后，激活参数减少约60%，性能与原始模型持平，并优于基于剪枝的方法。

Conclusion: 本文提出将密集的Diffusion Transformer(DiT)通过结构化稀疏化转换为Mixture of Experts (MoE)，在保持模型容量的同时大幅减少推理时激活参数，从而实现高效的文本到图像生成。

Abstract: Diffusion Transformer (DiT) has demonstrated remarkable performance in
text-to-image generation; however, its large parameter size results in
substantial inference overhead. Existing parameter compression methods
primarily focus on pruning, but aggressive pruning often leads to severe
performance degradation due to reduced model capacity. To address this
limitation, we pioneer the transformation of a dense DiT into a Mixture of
Experts (MoE) for structured sparsification, reducing the number of activated
parameters while preserving model capacity. Specifically, we replace the
Feed-Forward Networks (FFNs) in DiT Blocks with MoE layers, reducing the number
of activated parameters in the FFNs by 62.5\%. Furthermore, we propose the
Mixture of Blocks (MoB) to selectively activate DiT blocks, thereby further
enhancing sparsity. To ensure an effective dense-to-MoE conversion, we design a
multi-step distillation pipeline, incorporating Taylor metric-based expert
initialization, knowledge distillation with load balancing, and group feature
loss for MoB optimization. We transform large diffusion transformers (e.g.,
FLUX.1 [dev]) into an MoE structure, reducing activated parameters by 60\%
while maintaining original performance and surpassing pruning-based approaches
in extensive experiments. Overall, Dense2MoE establishes a new paradigm for
efficient text-to-image generation.

</details>


### [44] [A Novel Multi-branch ConvNeXt Architecture for Identifying Subtle Pathological Features in CT Scans](https://arxiv.org/abs/2510.09107)
*Irash Perera,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 提出了一种多分支ConvNeXt用于CT图像COVID-19诊断，三路池化（GAP/GMP/注意力加权池化）+两阶段迁移学习，2,609切片上验证集AUC=0.9937，Acc=0.9757，F1=0.9825


<details>
  <summary>Details</summary>
Motivation: 识别医学影像中细微病理特征，需要更强特征提取与注意力机制，传统单支架网络难以同时捕捉全局与局部表征，故提出多分支设计以兼顾多尺度信息

Method: 基于ConvNeXt骨干，三并行特征提取支路（GAP、GMP、注意力加权池化），两阶段训练策略（冻结预训练骨干仅训分类头，随后微调整个网络），数据预处理和增强，融合三路特征进行分类

Result: Multi-branch ConvNeXt with GAP, GMP, Attention-weighted Pooling; two-phase transfer learning; dataset 2,609 CT slices; metrics ROC-AUC 0.9937, acc 0.9757, F1 0.9825

Conclusion: 多分支现代化架构与严格数据处理可显著提升医学图像分类性能，在所用数据集上超过既有方法，表明方法具有很高的诊断潜力但需进一步外部验证

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis, especially for identifying subtle pathological features.
This paper introduces a novel multi-branch ConvNeXt architecture designed
specifically for the nuanced challenges of medical image analysis. While
applied here to the specific problem of COVID-19 diagnosis, the methodology
offers a generalizable framework for classifying a wide range of pathologies
from CT scans. The proposed model incorporates a rigorous end-to-end pipeline,
from meticulous data preprocessing and augmentation to a disciplined two-phase
training strategy that leverages transfer learning effectively. The
architecture uniquely integrates features extracted from three parallel
branches: Global Average Pooling, Global Max Pooling, and a new
Attention-weighted Pooling mechanism. The model was trained and validated on a
combined dataset of 2,609 CT slices derived from two distinct datasets.
Experimental results demonstrate a superior performance on the validation set,
achieving a final ROC-AUC of 0.9937, a validation accuracy of 0.9757, and an
F1-score of 0.9825 for COVID-19 cases, outperforming all previously reported
models on this dataset. These findings indicate that a modern, multi-branch
architecture, coupled with careful data handling, can achieve performance
comparable to or exceeding contemporary state-of-the-art models, thereby
proving the efficacy of advanced deep learning techniques for robust medical
diagnostics.

</details>


### [45] [SOS: Synthetic Object Segments Improve Detection, Segmentation, and Grounding](https://arxiv.org/abs/2510.09110)
*Weikai Huang,Jieyu Zhang,Taoyang Jia,Chenhao Zheng,Ziqi Gao,Jae Sung Park,Ranjay Krishna*

Main category: cs.CV

TL;DR: SOS pastes high-quality synthetic object segments into images with layout priors and relighting to produce diverse accurate labels; 100k synthetic images beat much larger real datasets and boost performance especially in low-data and targeted scenarios


<details>
  <summary>Details</summary>
Motivation: reduce cost, bias, and scalability limits of real annotated datasets by using synthetic data that is flexible, accurate, and compositionally diverse

Method: object-centric composition with structured layout priors and generative relighting

Result: models trained on 100k SOS synthetic images outperform larger real datasets (GRIT 20M, V3Det 200k) e.g., +10.9 AP on LVIS detection and +8.4 N_Acc on gRefCOCO grounding; improves low-data and closed-vocab generalization; augmenting LVIS/COCO yields strong gains including +3.83 AP_rare on LVIS and +6.59 AP with 1% COCO

Conclusion: Simple, scalable object-centric synthetic data generation can surpass large real datasets for visual grouping tasks, offering controllable dataset construction that improves detection, grounding, and segmentation, particularly under limited real data and for targeted challenges

Abstract: Visual grouping -- operationalized via instance segmentation, visual
grounding, and object detection -- underpins applications from robotic
perception to photo editing. Large annotated datasets are costly, biased in
coverage, and hard to scale. Synthetic data are promising but often lack
flexibility, accuracy, and compositional diversity.
  We present SOS, a simple and scalable data synthesis pipeline based on an
object-centric composition strategy. It pastes high-quality synthetic object
segments into new images using structured layout priors and generative
relighting, producing accurate and diverse masks, boxes, and referring
expressions. Models trained on 100000 synthetic images from SOS outperform
those trained on larger real-image datasets such as GRIT (20M) and V3Det (200K)
on detection and grounding tasks, achieving +10.9 AP on LVIS detection and +8.4
$N_{\text{Acc}}$ on gRefCOCO grounding. SOS enables controllable dataset
construction and improves generalization in both low-data and closed-vocabulary
settings. Augmenting LVIS and COCO with synthetic object segments yields strong
performance across real-data scales and even larger gains under extremely
limited real data (for example, +3.83 $AP_{\text{rare}}$ on LVIS instance
segmentation and +6.59 AP with a 1 percent COCO setup). This controllability
also supports targeted data generation for challenging intra-class referring in
visual grounding.

</details>


### [46] [MSDM: Generating Task-Specific Pathology Images with a Multimodal Conditioned Diffusion Model for Cell and Nuclei Segmentation](https://arxiv.org/abs/2510.09121)
*Dominik Winter,Mai Bui,Monica Azqueta Gavaldon,Nicolas Triltsch,Marco Rosati,Nicolas Brieu*

Main category: cs.CV

TL;DR: MSDM用多模态条件控制扩散生成像素级图像-掩码对，能有针对性生成罕见形态样本并改善分割模型的鲁棒性与泛化性。


<details>
  <summary>Details</summary>
Motivation: 临床病理学中细胞/细胞核分割受限于标注数据稀缺，尤其是罕见或非典型形态。人工标注耗时高、成本大，故引入合成数据以经济高效地补齐数据分布空缺。

Method: MSDM通过将形态学信息（水平/垂直映射）、RGB颜色特征及经BERT编码的检测信息作为条件输入，利用多头交叉注意力将多模态信息融合进扩散生成过程，产出具有精细可控形态的图像-掩码对。

Result: 定量评估显示，在相同生物学条件下合成图像与真实图像在嵌入空间上具有较低的Wasserstein距离，表明分布逼近良好；将合成样本（如柱状细胞）加入训练显著提升了分割精度。

Conclusion: 该工作提出了一种多模态语义扩散模型（MSDM），用于合成像素精确的细胞/细胞核图像-掩码对，有效缓解标注稀缺问题并能针对罕见形态定向生成样本，从而提升分割模型在困难形态上的表现。

Abstract: Scarcity of annotated data, particularly for rare or atypical morphologies,
present significant challenges for cell and nuclei segmentation in
computational pathology. While manual annotation is labor-intensive and costly,
synthetic data offers a cost-effective alternative. We introduce a Multimodal
Semantic Diffusion Model (MSDM) for generating realistic pixel-precise
image-mask pairs for cell and nuclei segmentation. By conditioning the
generative process with cellular/nuclear morphologies (using horizontal and
vertical maps), RGB color characteristics, and BERT-encoded assay/indication
metadata, MSDM generates datasests with desired morphological properties. These
heterogeneous modalities are integrated via multi-head cross-attention,
enabling fine-grained control over the generated images. Quantitative analysis
demonstrates that synthetic images closely match real data, with low
Wasserstein distances between embeddings of generated and real images under
matching biological conditions. The incorporation of these synthetic samples,
exemplified by columnar cells, significantly improves segmentation model
accuracy on columnar cells. This strategy systematically enriches data sets,
directly targeting model deficiencies. We highlight the effectiveness of
multimodal diffusion-based augmentation for advancing the robustness and
generalizability of cell and nuclei segmentation models. Thereby, we pave the
way for broader application of generative models in computational pathology.

</details>


### [47] [Polar Separable Transform for Efficient Orthogonal Rotation-Invariant Image Representation](https://arxiv.org/abs/2510.09125)
*Satya P. Singh,Rashmi Chaudhry,Anand Srivastava,Jagath C. Rajapakse*

Main category: cs.CV

TL;DR: PSepT通过径向DCT与角向傅里叶基的张量积实现极坐标下的可分离正交变换，大幅提升效率和稳定性，使高阶矩分析实用化。


<details>
  <summary>Details</summary>
Motivation: 解决传统Zernike及伪Zernike矩在高阶时计算复杂度高、数值不稳定且不可分离的问题，以支持高阶矩的可行分析。

Method: 通过构造径向DCT基与角向傅里叶谐波的张量积核实现核的完全因式分解，从而实现径向与角向的独立处理，复杂度从多项式级减少到近线性。

Result: 在理论上将复杂度降低到O(N^2 log N)、内存降至O(N^2)、条件数缩减到O(√N)；实验显示数值稳定性、计算效率和分类性能优于传统方法，并保持精确重建。

Conclusion: PSepT提出了在极坐标下实现完全可分离的正交变换的创新方法，显著降低了计算复杂度和条件数增长，保持正交性、完备性和旋转协变性，并在高阶矩分析中表现稳健。

Abstract: Orthogonal moment-based image representations are fundamental in computer
vision, but classical methods suffer from high computational complexity and
numerical instability at large orders. Zernike and pseudo-Zernike moments, for
instance, require coupled radial-angular processing that precludes efficient
factorization, resulting in $\mathcal{O}(n^3N^2)$ to $\mathcal{O}(n^6N^2)$
complexity and $\mathcal{O}(N^4)$ condition number scaling for the $n$th-order
moments on an $N\times N$ image. We introduce \textbf{PSepT} (Polar Separable
Transform), a separable orthogonal transform that overcomes the
non-separability barrier in polar coordinates. PSepT achieves complete kernel
factorization via tensor-product construction of Discrete Cosine Transform
(DCT) radial bases and Fourier harmonic angular bases, enabling independent
radial and angular processing. This separable design reduces computational
complexity to $\mathcal{O}(N^2 \log N)$, memory requirements to
$\mathcal{O}(N^2)$, and condition number scaling to $\mathcal{O}(\sqrt{N})$,
representing exponential improvements over polynomial approaches. PSepT
exhibits orthogonality, completeness, energy conservation, and
rotation-covariance properties. Experimental results demonstrate better
numerical stability, computational efficiency, and competitive classification
performance on structured datasets, while preserving exact reconstruction. The
separable framework enables high-order moment analysis previously infeasible
with classical methods, opening new possibilities for robust image analysis
applications.

</details>


### [48] [Training Feature Attribution for Vision Models](https://arxiv.org/abs/2510.09135)
*Aziz Bacha,Thomas George*

Main category: cs.CV

TL;DR: 提出训练特征归因，将测试预测映射到训练图像的具体区域，提供细粒度、测试相关的解释，能发现导致错误和伪相关的训练片段。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法只关注输入特征或训练样本的单一维度，无法联合追踪测试时预测与训练数据中具体区域的关系。作者认为应同时研究这两种透视以获得更细粒度的解释。

Method: 在视觉任务上实现训练特征归因，将影响力从训练样本层面细分到图像区域，结合实验展示其在发现有害样本和伪相关方面的有效性。

Result: 提出并验证了“训练特征归因”（training feature attribution），该方法将测试预测关联到具体训练图像的特定区域。在视觉数据集上，能识别导致错误分类的有害训练样本和暴露模型依赖的伪相关（如补丁捷径），这些是传统归因方法难以发现的。

Conclusion: 训练特征归因为理解深度模型的训练来源与预测行为提供了更细致的工具，能帮助发现有害样本和数据捷径，从而改进模型调试与信任。

Abstract: Deep neural networks are often considered opaque systems, prompting the need
for explainability methods to improve trust and accountability. Existing
approaches typically attribute test-time predictions either to input features
(e.g., pixels in an image) or to influential training examples. We argue that
both perspectives should be studied jointly. This work explores *training
feature attribution*, which links test predictions to specific regions of
specific training images and thereby provides new insights into the inner
workings of deep models. Our experiments on vision datasets show that training
feature attribution yields fine-grained, test-specific explanations: it
identifies harmful examples that drive misclassifications and reveals spurious
correlations, such as patch-based shortcuts, that conventional attribution
methods fail to expose.

</details>


### [49] [Online Topological Localization for Navigation Assistance in Bronchoscopy](https://arxiv.org/abs/2510.09144)
*Clara Tomasini,Luis Riazuelo,Ana C. Murillo*

Main category: cs.CV

TL;DR: 提出一种只用幻影数据训练、无需患者CT的图像驱动支气管镜拓扑定位方法，能在真实数据上优于现有方法，为手术导航提供低成本可行的替代方案。


<details>
  <summary>Details</summary>
Motivation: 常规导航依赖患者CT建模加传感器或图像配准，成本高、流程复杂且需额外训练；而在许多手术场景中只需拓扑级定位即可辅助医生导航，因此提出一种无CT、仅基于图像且可用仿真/幻影数据训练的方法以降低成本并提升可用性。

Method: 构建一个仅用幻影数据训练的图像驱动定位流水线，通过与通用气道拓扑模型对齐实现拓扑级别的定位，而非精确的度量定位，省去额外传感器、CT扫描或复杂配准；系统在训练阶段利用幻影图像及对应拓扑标签，并在推理时仅以内窥镜视频帧进行分类/匹配以确定当前分支/位点。

Result: 在公开/自建的真实数据测试序列上，该方法的表现超过了现有方法，尤其在真实内窥镜视频上展现出更好的定位准确性和鲁棒性，证明了仅用幻影数据训练仍能实现良好泛化。

Conclusion: 本文提出一种基于图像的支气管镜拓扑定位方法，可在无需患者CT扫描的情况下为手术提供导航支持，且仅以幻影（phantom）数据训练，具有较好泛化性并在真实数据上优于现有方法。

Abstract: Video bronchoscopy is a fundamental procedure in respiratory medicine, where
medical experts navigate through the bronchial tree of a patient to diagnose or
operate the patient. Surgeons need to determine the position of the scope as
they go through the airway until they reach the area of interest. This task is
very challenging for practitioners due to the complex bronchial tree structure
and varying doctor experience and training. Navigation assistance to locate the
bronchoscope during the procedure can improve its outcome. Currently used
techniques for navigational guidance commonly rely on previous CT scans of the
patient to obtain a 3D model of the airway, followed by tracking of the scope
with additional sensors or image registration. These methods obtain accurate
locations but imply additional setup, scans and training. Accurate metric
localization is not always required, and a topological localization with regard
to a generic airway model can often suffice to assist the surgeon with
navigation. We present an image-based bronchoscopy topological localization
pipeline to provide navigation assistance during the procedure, with no need of
patient CT scan. Our approach is trained only on phantom data, eliminating the
high cost of real data labeling, and presents good generalization capabilities.
The results obtained surpass existing methods, particularly on real data test
sequences.

</details>


### [50] [Instance-Level Generation for Representation Learning](https://arxiv.org/abs/2510.09171)
*Yankun Wu,Zakaria Laskar,Giorgos Kordopatis-Zilos,Noa Garcia,Giorgos Tolias*

Main category: cs.CV

TL;DR: 该工作提出了一个仅用目标域名称即可自动合成大规模多域实例数据的方法，并用于微调基础视觉模型，从而在多项ILR基准上显著提升实例检索性能，免去了真实图像采集与标注。


<details>
  <summary>Details</summary>
Motivation: 实例级识别需要细粒度且大量标注数据，但构建大规模注释数据集昂贵且难以覆盖多域场景。作者希望通过合成数据突破数据稀缺与跨域泛化问题，使ILR技术更易应用于现实场景。

Method: 基于输入的目标域名称，自动生成不同背景、光照和拍摄条件下的多域对象实例合成图像，形成大规模训练集；随后用这些合成数据对预训练的基础视觉模型进行微调以增强实例检索能力。方法没有依赖任何真实图像，强调跨域多样性与条件变化。

Result: 在七个跨域ILR基准上进行评估，微调后的模型在检索性能上有显著提升，证明了纯合成数据在提高实例级识别效果方面的有效性与泛化能力。

Conclusion: 该论文提出了一种无需真实图像的合成数据生成方法，专为实例级识别(ILR)构建大规模多域多条件训练集，通过微调基础视觉模型显著提升了多个ILR基准的检索性能，展示了数据合成在ILR领域替代人工采集的潜力。

Abstract: Instance-level recognition (ILR) focuses on identifying individual objects
rather than broad categories, offering the highest granularity in image
classification. However, this fine-grained nature makes creating large-scale
annotated datasets challenging, limiting ILR's real-world applicability across
domains. To overcome this, we introduce a novel approach that synthetically
generates diverse object instances from multiple domains under varied
conditions and backgrounds, forming a large-scale training set. Unlike prior
work on automatic data synthesis, our method is the first to address
ILR-specific challenges without relying on any real images. Fine-tuning
foundation vision models on the generated data significantly improves retrieval
performance across seven ILR benchmarks spanning multiple domains. Our approach
offers a new, efficient, and effective alternative to extensive data collection
and curation, introducing a new ILR paradigm where the only input is the names
of the target domains, unlocking a wide range of real-world applications.

</details>


### [51] [TARO: Toward Semantically Rich Open-World Object Detection](https://arxiv.org/abs/2510.09173)
*Yuchen Zhang,Yao Lu,Johannes Betz*

Main category: cs.CV

TL;DR: 该论文提出TARO，一种在开放集目标检测中对未知物体进行粗分类的框架：通过稀疏化的objectness head、层级引导的重新标注和层级分类模块，将未知物分配到语义父类以提高安全决策。实验表明在未知分类、已知/未知混淆和检测性能上具有优势。


<details>
  <summary>Details</summary>
Motivation: 传统开放集检测只将未知标为单一类，难以在实际（如自动驾驶）中提供安全决策所需的更细语义信息。将未知细分为语义父类能帮助区分需紧急停车的未知动物与可绕行的未知碎片等情形。

Method: 提出由三部分组成的体系：1) 使用sparsemax的head对objectness进行稀疏化建模以更好区分未知；2) 层级引导的relabel模块利用语义层级为训练样本提供辅助监督，将一些label为未知的区域映射到粗类别；3) 分类模块学习类别之间的层级关系以输出未知的粗类别分配。整体在训练中结合这些组件进行联合优化。

Result: 在实验中，TARO能将最多29.9%的未知对象分类到有意义的粗类，显著降低未知与已知类别的混淆，同时在未知召回和已知mAP上达到有竞争力的结果。

Conclusion: TARO能将未知目标细分为语义父类（如动物、碎片等），显著降低未知与已知类别间的混淆，在未知召回和已知mAP上表现具有竞争力，为开放世界检测在安全关键场景中的应用提供更实用的未知处理方案。

Abstract: Modern object detectors are largely confined to a "closed-world" assumption,
limiting them to a predefined set of classes and posing risks when encountering
novel objects in real-world scenarios. While open-set detection methods aim to
address this by identifying such instances as 'Unknown', this is often
insufficient. Rather than treating all unknowns as a single class, assigning
them more descriptive subcategories can enhance decision-making in
safety-critical contexts. For example, identifying an object as an 'Unknown
Animal' (requiring an urgent stop) versus 'Unknown Debris' (requiring a safe
lane change) is far more useful than just 'Unknown' in autonomous driving. To
bridge this gap, we introduce TARO, a novel detection framework that not only
identifies unknown objects but also classifies them into coarse parent
categories within a semantic hierarchy. TARO employs a unique architecture with
a sparsemax-based head for modeling objectness, a hierarchy-guided relabeling
component that provides auxiliary supervision, and a classification module that
learns hierarchical relationships. Experiments show TARO can categorize up to
29.9% of unknowns into meaningful coarse classes, significantly reduce
confusion between unknown and known classes, and achieve competitive
performance in both unknown recall and known mAP. Code will be made available.

</details>


### [52] [Online Video Depth Anything: Temporally-Consistent Depth Prediction with Low Memory Consumption](https://arxiv.org/abs/2510.09182)
*Johann-Friedrich Feiden,Tim Küchler,Denis Zavadski,Bogdan Savchynskyy,Carsten Rother*

Main category: cs.CV

TL;DR: oVDA modifies VDA with latent caching and training-time frame masking to enable efficient online monocular video depth estimation, achieving superior accuracy and low VRAM usage for real-time edge deployment.


<details>
  <summary>Details</summary>
Motivation: Existing VDA achieves strong long-sequence depth estimation but requires batch processing, preventing online/real-time use; need for low-VRAM, deployable method for edge devices.

Method: Introduce caching of latent features during inference and apply masking of frames during training (inspired by LLM techniques) to enable online operation. Optimize for VRAM efficiency and real-time throughput on GPU and edge devices.

Result: oVDA outperforms all competing online video depth methods in accuracy and VRAM usage, achieving 42 FPS on NVIDIA A100 and 20 FPS on NVIDIA Jetson; code and deployment scripts to be released.

Conclusion: oVDA successfully adapts VDA for online depth estimation, enabling real-time, low-VRAM monocular video depth inference suitable for edge deployment.

Abstract: Depth estimation from monocular video has become a key component of many
real-world computer vision systems. Recently, Video Depth Anything (VDA) has
demonstrated strong performance on long video sequences. However, it relies on
batch-processing which prohibits its use in an online setting. In this work, we
overcome this limitation and introduce online VDA (oVDA). The key innovation is
to employ techniques from Large Language Models (LLMs), namely, caching latent
features during inference and masking frames at training. Our oVDA method
outperforms all competing online video depth estimation methods in both
accuracy and VRAM usage. Low VRAM usage is particularly important for
deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an
NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release
both, code and compilation scripts, making oVDA easy to deploy on low-power
hardware.

</details>


### [53] [Modern Deep Learning Approaches for Cricket Shot Classification: A Comprehensive Baseline Study](https://arxiv.org/abs/2510.09187)
*Sungwoo Kang*

Main category: cs.CV

TL;DR: 作者系统重现并比较七种深度学习方法，发现文献结果被高估，重现结果远低于原报告；采用EfficientNet-B0+GRU达92.25%准确率，提供可复现基线。


<details>
  <summary>Details</summary>
Motivation: 解决体育视频分析中缺乏标准化评估与可复现实现的问题，建立统一基线并验证现代架构在板球击球分类任务上的有效性。

Method: 实现并统一评估了CNN-LSTM、注意力模型、Vision Transformer、迁移学习和EfficientNet-GRU等方法，使用PyTorch Lightning构建可复现训练流水线，统一数据预处理与评价指标。

Result: Comparative baseline study implementing 7 deep learning methods across 4 paradigms for cricket shot classification; found large gap between reported literature accuracies and re-implemented results; EfficientNet-B0 + GRU achieves SOTA 92.25%; reproducible PyTorch Lightning pipeline.

Conclusion: 重现实验揭示了体育视频分类中文献结果与实际实现之间的显著差距；现代网络与规范化评估能显著提升性能并提供可复现性。

Abstract: Cricket shot classification from video sequences remains a challenging
problem in sports video analysis, requiring effective modeling of both spatial
and temporal features. This paper presents the first comprehensive baseline
study comparing seven different deep learning approaches across four distinct
research paradigms for cricket shot classification. We implement and
systematically evaluate traditional CNN-LSTM architectures, attention-based
models, vision transformers, transfer learning approaches, and modern
EfficientNet-GRU combinations on a unified benchmark. A critical finding of our
study is the significant performance gap between claims in academic literature
and practical implementation results. While previous papers reported accuracies
of 96\% (Balaji LRCN), 99.2\% (IJERCSE), and 93\% (Sensors), our standardized
re-implementations achieve 46.0\%, 55.6\%, and 57.7\% respectively. Our modern
SOTA approach, combining EfficientNet-B0 with a GRU-based temporal model,
achieves 92.25\% accuracy, demonstrating that substantial improvements are
possible with modern architectures and systematic optimization. All
implementations follow modern MLOps practices with PyTorch Lightning, providing
a reproducible research platform that exposes the critical importance of
standardized evaluation protocols in sports video analysis research.

</details>


### [54] [Towards Safer and Understandable Driver Intention Prediction](https://arxiv.org/abs/2510.09200)
*Mukilan Karuppasamy,Shankar Gangisetty,Shyam Nandan Rai,Carlo Masone,C V Jawahar*

Main category: cs.CV

TL;DR: 提出可解释的驾驶意图预测任务，发布DAAD-X数据集，提出VCBM框架并证明Transformer更具可解释性，提供代码与模型。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶与人类交互增多，解释性对决策至关重要，现有深度学习系统难以提供可理解的因果推理，故提出提前预测并解释驾驶动作以提升安全性。

Method: 构建包含司机视线与车载视角的多模态视频数据集并标注层级文本解释；提出Video Concept Bottleneck Model，使用概念瓶颈在时空上生成解释性预测；比较Transformer与CNN基线并用多标签t-SNE进行可视化分析。

Result: 该论文提出了解释性驾驶动作预测的新任务，构建了多模态视频数据集DAAD-X，并提出了Video Concept Bottleneck Model（VCBM）以生成时空一致的可解释性解释，实验显示Transformer模型在可解释性上优于传统CNN，并引入多标签t-SNE可视化展示解释之间的解耦与因果相关性。

Conclusion: VCBM在DAAD-X上能生成固有的时空一致解释，Transformers在解释性上优于CNN，且多标签t-SNE有助于展示解释的解耦与因果关系。

Abstract: Autonomous driving (AD) systems are becoming increasingly capable of handling
complex tasks, mainly due to recent advances in deep learning and AI. As
interactions between autonomous systems and humans increase, the
interpretability of decision-making processes in driving systems becomes
increasingly crucial for ensuring safe driving operations. Successful
human-machine interaction requires understanding the underlying representations
of the environment and the driving task, which remains a significant challenge
in deep learning-based systems. To address this, we introduce the task of
interpretability in maneuver prediction before they occur for driver safety,
i.e., driver intent prediction (DIP), which plays a critical role in AD
systems. To foster research in interpretable DIP, we curate the eXplainable
Driving Action Anticipation Dataset (DAAD-X), a new multimodal, ego-centric
video dataset to provide hierarchical, high-level textual explanations as
causal reasoning for the driver's decisions. These explanations are derived
from both the driver's eye-gaze and the ego-vehicle's perspective. Next, we
propose Video Concept Bottleneck Model (VCBM), a framework that generates
spatio-temporally coherent explanations inherently, without relying on post-hoc
techniques. Finally, through extensive evaluations of the proposed VCBM on the
DAAD-X dataset, we demonstrate that transformer-based models exhibit greater
interpretability than conventional CNN-based models. Additionally, we introduce
a multilabel t-SNE visualization technique to illustrate the disentanglement
and causal correlation among multiple explanations. Our data, code and models
are available at: https://mukil07.github.io/VCBM.github.io/

</details>


### [55] [Cattle-CLIP: A Multimodal Framework for Cattle Behaviour Recognition](https://arxiv.org/abs/2510.09203)
*Huimin Liu,Jing Gao,Daria Baran,AxelX Montout,Neill W Campbell,Andrew W Dowsey*

Main category: cs.CV

TL;DR: 提出Cattle-CLIP，一种用于牛行为识别的多模态深度学习框架，通过在CLIP基础上加入时间整合模块并结合专门的数据增强与文本提示，提升视频行为识别性能；并发布包含6类室内行为的CattleBehaviours6数据集（1905段视频）进行验证。


<details>
  <summary>Details</summary>
Motivation: 弥补现有基于视觉的牛行为识别在数据稀缺与域差距方面的不足，通过利用图文对齐的语义信息提升视频行为识别的准确性与泛化能力。

Method: 基于预训练的图文模型CLIP，加入时间整合模块以处理视频帧序列；采用针对农场监控与牛类视觉特性的专门数据增强策略与定制文本提示，分别在全监督与少样本设置下训练与评估。

Result: 在新构建的CattleBehaviours6数据集（1905段，200头牛）上，全监督准确率96.1%，少样本场景也表现良好，尤其喂食、饮水、站立反刍行为召回率接近100%。

Conclusion: Cattle-CLIP在全监督环境下对六类行为达成96.1%准确率，对喂食、饮水与站立反刍召回率接近100%，在少样本场景下也表现出稳健的泛化能力，证明多模态学习对畜牧行为分析具有潜力。

Abstract: Cattle behaviour is a crucial indicator of an individual animal health,
productivity and overall well-being. Video-based monitoring, combined with deep
learning techniques, has become a mainstream approach in animal biometrics, and
it can offer high accuracy in some behaviour recognition tasks. We present
Cattle-CLIP, a multimodal deep learning framework for cattle behaviour
recognition, using semantic cues to improve the performance of video-based
visual feature recognition. It is adapted from the large-scale image-language
model CLIP by adding a temporal integration module. To address the domain gap
between web data used for the pre-trained model and real-world cattle
surveillance footage, we introduce tailored data augmentation strategies and
specialised text prompts. Cattle-CLIP is evaluated under both fully-supervised
and few-shot learning scenarios, with a particular focus on data-scarce
behaviour recognition - an important yet under-explored goal in livestock
monitoring. To evaluate the proposed method, we release the CattleBehaviours6
dataset, which comprises six types of indoor behaviours: feeding, drinking,
standing-self-grooming, standing-ruminating, lying-self-grooming and
lying-ruminating. The dataset consists of 1905 clips collected from our John
Oldacre Centre dairy farm research platform housing 200 Holstein-Friesian cows.
Experiments show that Cattle-CLIP achieves 96.1% overall accuracy across six
behaviours in a supervised setting, with nearly 100% recall for feeding,
drinking and standing-ruminating behaviours, and demonstrates robust
generalisation with limited data in few-shot scenarios, highlighting the
potential of multimodal learning in agricultural and animal behaviour analysis.

</details>


### [56] [3D Reconstruction from Transient Measurements with Time-Resolved Transformer](https://arxiv.org/abs/2510.09205)
*Yue Li,Shida Sun,Yu Hong,Feihu Xu,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 提出针对时空瞬态测量的TRT变换器，通过多尺度时空自注意力与交叉注意力融合局部与全局特征，得到用于LOS与NLOS重建的两种模型，实验显示性能优越并附带数据集与代码。


<details>
  <summary>Details</summary>
Motivation: 提升光子效率条件下基于瞬态测量的3D重建性能，解决传感器量子效率低和噪声高导致的长距离或复杂场景重建困难。

Method: 设计多尺度时空自注意力编码器（局部与全局相关性）和时空交叉注意力解码器（在token空间整合特征）；根据不同任务调整输出分支得到TRT-LOS与TRT-NLOS；进行合成与真实数据实验并提供数据集与代码。

Result: 提出通用Time-Resolved Transformer (TRT)架构，包含专为时空瞬态数据设计的自注意力和交叉注意力；基于TRT构建TRT-LOS与TRT-NLOS两种任务专用模型，在合成与真实数据上显著优于现有方法，并发布大规模合成LOS数据集及真实NLOS测量数据。

Conclusion: TRT通过专门设计的时空注意力机制有效提高了在高噪声、低光子预算下的3D重建性能，具备良好泛化性并推动了LOS/NLOS成像的数据资源建设。

Abstract: Transient measurements, captured by the timeresolved systems, are widely
employed in photon-efficient reconstruction tasks, including line-of-sight
(LOS) and non-line-of-sight (NLOS) imaging. However, challenges persist in
their 3D reconstruction due to the low quantum efficiency of sensors and the
high noise levels, particularly for long-range or complex scenes. To boost the
3D reconstruction performance in photon-efficient imaging, we propose a generic
Time-Resolved Transformer (TRT) architecture. Different from existing
transformers designed for high-dimensional data, TRT has two elaborate
attention designs tailored for the spatio-temporal transient measurements.
Specifically, the spatio-temporal self-attention encoders explore both local
and global correlations within transient data by splitting or downsampling
input features into different scales. Then, the spatio-temporal cross attention
decoders integrate the local and global features in the token space, resulting
in deep features with high representation capabilities. Building on TRT, we
develop two task-specific embodiments: TRT-LOS for LOS imaging and TRT-NLOS for
NLOS imaging. Extensive experiments demonstrate that both embodiments
significantly outperform existing methods on synthetic data and real-world data
captured by different imaging systems. In addition, we contribute a
large-scale, high-resolution synthetic LOS dataset with various noise levels
and capture a set of real-world NLOS measurements using a custom-built imaging
system, enhancing the data diversity in this field. Code and datasets are
available at https://github.com/Depth2World/TRT.

</details>


### [57] [Stable Video Infinity: Infinite-Length Video Generation with Error Recycling](https://arxiv.org/abs/2510.09212)
*Wuyang Li,Wentao Pan,Po-Chien Luan,Yang Gao,Alexandre Alahi*

Main category: cs.CV

TL;DR: Introduce SVI with error-recycling fine-tuning to bridge train-test gap by injecting and replaying model errors to teach self-correction, enabling infinite, coherent conditional video generation


<details>
  <summary>Details</summary>
Motivation: Handle training/test discrepancy in autoregressive video generation to avoid drift and homogeneous outputs

Method: Error-Recycling Fine-Tuning: inject DiT self-generated errors via closed-loop recycling; one-step bidirectional integration for approximate prediction; bank errors in replay memory across timesteps; train DiT to correct its own errors; compatible with audio/skeleton/text conditions

Result: Generates infinite-length videos with high temporal consistency, plausible transitions, controllable storylines; scales from seconds to infinite durations with no extra inference cost; state-of-the-art on three benchmarks (consistent, creative, conditional)

Conclusion: SVI effectively mitigates autoregressive drift by training DiT on its own errors, improving long-duration video generation quality and versatility.

Abstract: We propose Stable Video Infinity (SVI) that is able to generate
infinite-length videos with high temporal consistency, plausible scene
transitions, and controllable streaming storylines. While existing long-video
methods attempt to mitigate accumulated errors via handcrafted anti-drifting
(e.g., modified noise scheduler, frame anchoring), they remain limited to
single-prompt extrapolation, producing homogeneous scenes with repetitive
motions. We identify that the fundamental challenge extends beyond error
accumulation to a critical discrepancy between the training assumption (seeing
clean data) and the test-time autoregressive reality (conditioning on
self-generated, error-prone outputs). To bridge this hypothesis gap, SVI
incorporates Error-Recycling Fine-Tuning, a new type of efficient training that
recycles the Diffusion Transformer (DiT)'s self-generated errors into
supervisory prompts, thereby encouraging DiT to actively identify and correct
its own errors. This is achieved by injecting, collecting, and banking errors
through closed-loop recycling, autoregressively learning from error-injected
feedback. Specifically, we (i) inject historical errors made by DiT to
intervene on clean inputs, simulating error-accumulated trajectories in flow
matching; (ii) efficiently approximate predictions with one-step bidirectional
integration and calculate errors with residuals; (iii) dynamically bank errors
into replay memory across discretized timesteps, which are resampled for new
input. SVI is able to scale videos from seconds to infinite durations with no
additional inference cost, while remaining compatible with diverse conditions
(e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks,
including consistent, creative, and conditional settings, thoroughly verifying
its versatility and state-of-the-art role.

</details>


### [58] [Tag-Enriched Multi-Attention with Large Language Models for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2510.09224)
*Wangyu Wu,Xuhang Chen,Zhenhong Chen,Jing-En Jiang,Kim-Fung Tsang,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: 用LLM生成语义标签并通过多注意力机制融合多模态特征，改善跨域顺序推荐效果，在四个电商数据集上显著超越基线。


<details>
  <summary>Details</summary>
Motivation: 解决跨域顺序推荐中捕捉域内与跨域行为模式的挑战，通过语义标签增强来提升物品表示和建模用户偏好。

Method: 用LLM基于域感知提示从标题与描述生成描述性标签，将标签嵌入与物品ID、文本、视觉特征拼接/融合，设计Tag-Enriched Multi-Attention模块联合建模跨域和域内序列行为，训练并在四个电商数据集上评估性能。

Result: 提出TEMA-LLM框架，使用LLM生成域感知语义标签并与ID、文本、视觉特征融合，通过Tag-Enriched Multi-Attention联合建模跨域及域内偏好，在四个大规模电商数据集上优于SOTA。

Conclusion: LLM赋能的语义标签与多注意力融合能有效捕捉复杂跨域兴趣，提升消费者端推荐系统的性能与可解释性，适用于大规模电商场景。

Abstract: Cross-Domain Sequential Recommendation (CDSR) plays a crucial role in modern
consumer electronics and e-commerce platforms, where users interact with
diverse services such as books, movies, and online retail products. These
systems must accurately capture both domain-specific and cross-domain
behavioral patterns to provide personalized and seamless consumer experiences.
To address this challenge, we propose \textbf{TEMA-LLM} (\textit{Tag-Enriched
Multi-Attention with Large Language Models}), a practical and effective
framework that integrates \textit{Large Language Models (LLMs)} for semantic
tag generation and enrichment. Specifically, TEMA-LLM employs LLMs to assign
domain-aware prompts and generate descriptive tags from item titles and
descriptions. The resulting tag embeddings are fused with item identifiers as
well as textual and visual features to construct enhanced item representations.
A \textit{Tag-Enriched Multi-Attention} mechanism is then introduced to jointly
model user preferences within and across domains, enabling the system to
capture complex and evolving consumer interests. Extensive experiments on four
large-scale e-commerce datasets demonstrate that TEMA-LLM consistently
outperforms state-of-the-art baselines, underscoring the benefits of LLM-based
semantic tagging and multi-attention integration for consumer-facing
recommendation systems. The proposed approach highlights the potential of LLMs
to advance intelligent, user-centric services in the field of consumer
electronics.

</details>


### [59] [Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation](https://arxiv.org/abs/2510.09228)
*Vijay M. Galshetwar,Praful Hambarde,Prashant W. Patil,Akshay Dudhane,Sachin Chaudhary,Santosh Kumar Vipparathi,Subrahmanyam Murala*

Main category: cs.CV

TL;DR: 本文综述了用于缓解雾、雨、雪等天气造成的图像/视频降质的传统与现代方法，涵盖模型分类、任务分类、数据集、评估与未来研究方向，强调在智能交通系统中的应用与挑战。


<details>
  <summary>Details</summary>
Motivation: 提升智能交通系统在恶劣天气下的视觉感知能力，减轻雾、雨、雪等天气导致的图像与视频质量下降，从而保障自动驾驶、交通监控与安防等关键应用的鲁棒性与安全性。

Method: 通过文献梳理与分类，比较传统先验方法与各类数据驱动模型（CNN、Transformer、扩散模型、VLMs），并按单任务/多任务/全能框架划分；总结昼夜特殊问题、数据集、评测指标；提出未来研究建议与开放实现资源链接。

Result: 对图像/视频去雾、去雨、去雪等多种天气退化修复方法进行了全面综述，涵盖了基于先验的传统方法与基于数据驱动的现代方法（CNN、Transformer、扩散模型与视-语模型），并按照单任务、多任务/多天气与全能框架进行分类；讨论了昼夜恢复问题、基准数据集与评估协议；指出当前研究局限并提出未来方向，如混合/复合退化恢复、实时部署与智能体式AI框架。

Conclusion: 尽管目前研究在单一天气或单一任务上已取得显著进展，但在处理复合天气、保持实时性、夜间场景与跨域泛化方面仍存在显著挑战；未来研究应关注多模态、轻量化、在线与智能体驱动的恢复系统以满足ITS实际需求。

Abstract: Adverse weather conditions such as haze, rain, and snow significantly degrade
the quality of images and videos, posing serious challenges to intelligent
transportation systems (ITS) that rely on visual input. These degradations
affect critical applications including autonomous driving, traffic monitoring,
and surveillance. This survey presents a comprehensive review of image and
video restoration techniques developed to mitigate weather-induced visual
impairments. We categorize existing approaches into traditional prior-based
methods and modern data-driven models, including CNNs, transformers, diffusion
models, and emerging vision-language models (VLMs). Restoration strategies are
further classified based on their scope: single-task models,
multi-task/multi-weather systems, and all-in-one frameworks capable of handling
diverse degradations. In addition, we discuss day and night time restoration
challenges, benchmark datasets, and evaluation protocols. The survey concludes
with an in-depth discussion on limitations in current research and outlines
future directions such as mixed/compound-degradation restoration, real-time
deployment, and agentic AI frameworks. This work aims to serve as a valuable
reference for advancing weather-resilient vision systems in smart
transportation environments. Lastly, to stay current with rapid advancements in
this field, we will maintain regular updates of the latest relevant papers and
their open-source implementations at
https://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration

</details>


### [60] [Diagnosing Shoulder Disorders Using Multimodal Large Language Models and Consumer-Grade Cameras](https://arxiv.org/abs/2510.09230)
*Jindong Hong,Wencheng Zhang,Shiqin Qiao,Jianhai Chen,Jianing Qiu,Chuanyang Zheng,Qian Xu,Yun Ji,Qianyue Wen,Weiwei Sun,Hao Li,Huizhen Li,Huichao Wang,Kai Wu,Meng Li,Yijun He,Lingjie Luo,Jiankai Sun*

Main category: cs.CV

TL;DR: 论文用消费级视频与两阶段多模态大模型构建HMVDx框架，提出Usability Index，诊断准确率较直接视频法提升79.6%，展示了低成本MLLM在医疗初筛的潜力。


<details>
  <summary>Details</summary>
Motivation: 在医疗资源匮乏地区，早期准确诊断肩部疾病困难，需低成本、易扩展的辅助诊断方法；利用消费级视频和多模态大模型降低成本并提高可及性。

Method: 提出Hybrid Motion Video Diagnosis框架(HMVDx)，使用消费级设备采集视频，分别使用两个多模态大语言模型完成动作识别和疾病诊断；引入Usability Index按医疗决策流程（动作识别、运动诊断、最终诊断）评估模型效果。

Result: HMVDx在肩关节损伤诊断上的准确率相比直接视频诊断提升了79.6%；提出了Usability Index用于更全面评估MLLM在医疗诊断路径中的有效性。

Conclusion: 该论文提出了一种基于低成本视频和多模态大模型的肩部疾病辅助诊断框架HMVDx，并通过拆分动作理解和疾病诊断两个任务、引入可用性指数评估诊断流程，显著提升了诊断准确率。

Abstract: Shoulder disorders, such as frozen shoulder (a.k.a., adhesive capsulitis),
are common conditions affecting the health of people worldwide, and have a high
incidence rate among the elderly and workers engaged in repetitive shoulder
tasks. In regions with scarce medical resources, achieving early and accurate
diagnosis poses significant challenges, and there is an urgent need for
low-cost and easily scalable auxiliary diagnostic solutions. This research
introduces videos captured by consumer-grade devices as the basis for
diagnosis, reducing the cost for users. We focus on the innovative application
of Multimodal Large Language Models (MLLMs) in the preliminary diagnosis of
shoulder disorders and propose a Hybrid Motion Video Diagnosis framework
(HMVDx). This framework divides the two tasks of action understanding and
disease diagnosis, which are respectively completed by two MLLMs. In addition
to traditional evaluation indicators, this work proposes a novel metric called
Usability Index by the logical process of medical decision-making (action
recognition, movement diagnosis, and final diagnosis). This index evaluates the
effectiveness of MLLMs in the medical field from the perspective of the entire
medical diagnostic pathway, revealing the potential value of low-cost MLLMs in
medical applications for medical practitioners. In experimental comparisons,
the accuracy of HMVDx in diagnosing shoulder joint injuries has increased by
79.6\% compared with direct video diagnosis, a significant technical
contribution to future research on the application of MLLMs for video
understanding in the medical field.

</details>


### [61] [Zero-shot image privacy classification with Vision-Language Models](https://arxiv.org/abs/2510.09253)
*Alina Elena Baia,Alessio Xompero,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 建立零样本基准后发现，前三大开源视觉-语言模型在准确率上落后于小型专门模型，但在对抗扰动和图像扰动下更稳健；VLM推理慢且参数多，性价比低。


<details>
  <summary>Details</summary>
Motivation: 当前文献倾向用通用VLM替代专用隐私模型，可能掩盖专用模型的上限；需要系统评估以明确是否应采用VLM。

Method: 建立零样本基准；选取隐私基准上排名前三的开源VLMs；使用任务对齐提示(prompt engineering)；与视觉模型和多模态专门方法对比，评估准确率、效率(参数量、推理速度)和鲁棒性(扰动下性能)。

Result: VLMs在隐私图像分类任务上表现不如专门模型，但更鲁棒

Conclusion: 当前开源VLMs在资源消耗大且推理慢的代价下，未能超越为隐私预测专门设计的小型模型，因此在准确率优先的场景应首选专门模型；若重视鲁棒性或零样本能力，可考虑VLMs。

Abstract: While specialized learning-based models have historically dominated image
privacy prediction, the current literature increasingly favours adopting large
Vision-Language Models (VLMs) designed for generic tasks. This trend risks
overlooking the performance ceiling set by purpose-built models due to a lack
of systematic evaluation. To address this problem, we establish a zero-shot
benchmark for image privacy classification, enabling a fair comparison. We
evaluate the top-3 open-source VLMs, according to a privacy benchmark, using
task-aligned prompts and we contrast their performance, efficiency, and
robustness against established vision-only and multi-modal methods.
Counter-intuitively, our results show that VLMs, despite their
resource-intensive nature in terms of high parameter count and slower
inference, currently lag behind specialized, smaller models in privacy
prediction accuracy. We also find that VLMs exhibit higher robustness to image
perturbations.

</details>


### [62] [Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy](https://arxiv.org/abs/2510.09256)
*Patrick Wienholt,Sophie Caselitz,Robert Siepmann,Philipp Bruners,Keno Bressem,Christiane Kuhl,Jakob Nikolas Kather,Sven Nebelung,Daniel Truhn*

Main category: cs.CV

TL;DR: 用DSE量化语义不一致并过滤高熵问题，可显著降低幻觉并提升放射影像VQA中黑箱VLM的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 评估是否用离散语义熵(DSE)拒绝可能产生幻觉的问题，能否提高黑箱视觉-语言模型在放射影像VQA中的准确性。

Method: 对两个公开数据集（VQA-Med 2019与一个含206病例的诊断放射学数据集）进行回顾性评估。对每个问题用GPT-4o和GPT-4.1各生成15次回答（temperature=1.0），基线用低温（0.1）。通过双向蕴含判定将等义回答聚类，从聚类频率计算DSE；排除DSE阈值>0.6或>0.3的问题并使用自助法和Bonferroni校正计算统计显著性与置信区间。

Result: 在706个图像-问题对上，基线准确率GPT-4o为51.7%、GPT-4.1为54.8%。排除DSE>0.3的问题后，GPT-4o准确率提升到76.3%（保留334/706），GPT-4.1提升到63.8%（保留499/706），均p<.001，多数结果在Bonferroni校正后仍显著。

Conclusion: DSE可作为可靠的幻觉检测与过滤策略，在临床VLM应用中显著提高诊断回答准确率。

Abstract: To determine whether using discrete semantic entropy (DSE) to reject
questions likely to generate hallucinations can improve the accuracy of
black-box vision-language models (VLMs) in radiologic image based visual
question answering (VQA). This retrospective study evaluated DSE using two
publicly available, de-identified datasets: (i) the VQA-Med 2019 benchmark (500
images with clinical questions and short-text answers) and (ii) a diagnostic
radiology dataset (206 cases: 60 computed tomography scans, 60 magnetic
resonance images, 60 radiographs, 26 angiograms) with corresponding
ground-truth diagnoses. GPT-4o and GPT-4.1 answered each question 15 times
using a temperature of 1.0. Baseline accuracy was determined using
low-temperature answers (temperature 0.1). Meaning-equivalent responses were
grouped using bidirectional entailment checks, and DSE was computed from the
relative frequencies of the resulting semantic clusters. Accuracy was
recalculated after excluding questions with DSE > 0.6 or > 0.3. p-values and
95% confidence intervals were obtained using bootstrap resampling and a
Bonferroni-corrected threshold of p < .004 for statistical significance. Across
706 image-question pairs, baseline accuracy was 51.7% for GPT-4o and 54.8% for
GPT-4.1. After filtering out high-entropy questions (DSE > 0.3), accuracy on
the remaining questions was 76.3% (retained questions: 334/706) for GPT-4o and
63.8% (retained questions: 499/706) for GPT-4.1 (both p < .001). Accuracy gains
were observed across both datasets and largely remained statistically
significant after Bonferroni correction. DSE enables reliable hallucination
detection in black-box VLMs by quantifying semantic inconsistency. This method
significantly improves diagnostic answer accuracy and offers a filtering
strategy for clinical VLM applications.

</details>


### [63] [MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding](https://arxiv.org/abs/2510.09274)
*Ming Dai,Sen Yang,Boqiang Duan,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 提出联合优化TSG与RefVOS的方法：用[FIND] token进行关键时刻定位，设计Moment-Centric Sampling稠密采样重要时刻并稀疏采样其他帧，和Bidirectional Anchor-updated Propagation提高掩码传播稳定性。


<details>
  <summary>Details</summary>
Motivation: RefVOS needs temporal reasoning and fine-grained visual comprehension; existing sampling strategies rely on heuristics or external keyframe models which have drawbacks; unify TSG and RefVOS to improve sampling and grounding without external timestamps.

Method: 训练时引入[FIND] token通过时间token相似度匹配定位关键时刻；推理时用MCS在关键信息处稠密采样并在非必要处稀疏采样；使用BAP以最相关时刻作为起点并在采样点动态更新以减轻误差累积。

Result: Introduce TSG paradigm with [FIND] token for key moment identification via temporal token similarity; Moment-Centric Sampling (MCS) for dense/sparse sampling; Bidirectional Anchor-updated Propagation (BAP) for stable tracking; code to be released.

Conclusion: 联合TSG和RefVOS能自然实现关键时刻定位并提升分割质量；MCS和BAP分别解决了采样效率与跟踪累积误差问题。

Abstract: Referring Video Object Segmentation (RefVOS) seeks to segment target objects
in videos guided by natural language descriptions, demanding both temporal
reasoning and fine-grained visual comprehension. Existing sampling strategies
for LLM-based approaches typically rely on either handcrafted heuristics or
external keyframe models. The former often overlooks essential temporal cues,
while the latter increases system complexity. To address this, we propose a
unified framework that jointly optimizes Temporal Sentence Grounding (TSG) and
RefVOS, naturally incorporating key moment grounding capability. During
training, we introduce a novel TSG paradigm that employs a dedicated
\texttt{[FIND]} token for key moment identification through temporal token
similarity matching, thereby avoiding the need for external timestamp
encodings. For inference, we design a Moment-Centric Sampling (MCS) strategy
that densely samples informative moments while sparsely sampling non-essential
frames, preserving both motion details and global context. To further enhance
tracking stability, we develop Bidirectional Anchor-updated Propagation (BAP),
which leverages the most relevant moment as start point for high-quality mask
initialization and dynamically updates at sampled points to mitigate
accumulated errors. Code and model will be available at:
https://github.com/Dmmm1997/MomentSeg

</details>


### [64] [Spotlight on Token Perception for Multimodal Reinforcement Learning](https://arxiv.org/abs/2510.09285)
*Siyuan Huang,Xiaoye Qu,Yafu Li,Yun Luo,Zefeng He,Daizong Liu,Yu Cheng*

Main category: cs.CV

TL;DR: 提出通过token-level视觉依赖性重加权和选择性更新的VPPO算法，显著提升LVLM在多模态推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态RLVR忽视视觉感知在优化过程中的作用；作者认为从token层面量化视觉依赖性可以更精细地指导强化学习信号，从而提升LVLM的多模态推理表现。

Method: 通过测量生成序列中每个token的视觉依赖性（token perception），对回合整体视觉依赖性进行重加权优势函数，并仅对被判定为高视觉依赖的token施加策略梯度更新。

Result: VPPO提出了一种基于token感知的策略优化方法，通过衡量每个生成token对视觉信息的依赖性来改进RLVR中的学习信号。方法包括对整个回合的视觉依赖性加权以及仅对感知关键token进行策略更新。实验在8个多模态推理与感知基准上，并在7B和32B模型尺度上验证，结果显示超过现有开源RL调优模型，显著提升多模态推理能力。

Conclusion: 从token感知角度优化RLVR能有效加强模型的视觉推理能力；VPPO在多个基准和模型规模上均表现出稳健的改进，验证了该视角和方法的有效性。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the
reasoning capabilities of Large Vision-Language Models (LVLMs), most existing
methods in multimodal reasoning neglect the critical role of visual perception
within the RLVR optimization process. In this paper, we undertake a pioneering
exploration of multimodal RLVR through the novel perspective of token
perception, which measures the visual dependency of each generated token. With
a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key
insights: first, token perception in a rollout trajectory is sparsely
distributed, where only a small fraction of tokens have high visual dependency
for visually-grounded reasoning; second, different trajectories exhibit
significant divergence in their overall visual dependency. Based on these
observations, we propose Visually-Perceptive Policy Optimization (VPPO), a
novel policy gradient algorithm that explicitly leverages token perception to
refine the learning signal. Specifically, VPPO achieves this through a dual
mechanism: it reweights a trajectory's advantage by its overall visual
dependency, and focuses policy updates exclusively on perceptually pivotal
tokens. On a comprehensive suite of eight perception and reasoning benchmarks,
VPPO demonstrates substantial gains over leading open-source RL-tuned models,
with its effectiveness consistently validated across 7B and 32B model scales.
Our findings not only establish a new token-level perceptual perspective for
analyzing multimodal RLVR but also present a novel and effective optimization
strategy to significantly enhance the multimodal reasoning capabilities of
LVLMs.

</details>


### [65] [Foraging with the Eyes: Dynamics in Human Visual Gaze and Deep Predictive Modeling](https://arxiv.org/abs/2510.09299)
*Tejaswi V. Panchagnula*

Main category: cs.CV

TL;DR: Human gaze during free viewing exhibits Levy walk dynamics similar to animal foraging; CNN can predict fixation maps from image structure.


<details>
  <summary>Details</summary>
Motivation: Study whether human visual gaze follows Levy walk patterns like animal foraging and explore implications for attention modeling and vision interfaces.

Method: Large-scale eye-tracking experiment and statistical analysis of gaze trajectories; trained CNN to predict fixation heatmaps from images.

Result: Large-scale eye-tracking data (40 participants, 50 images, >4M gaze points) show human gaze trajectories follow Levy walks; a CNN trained on images predicts fixation heatmaps accurately.

Conclusion: Human visual exploration obeys statistical laws analogous to natural foraging; gaze behavior components are learnable from images, enabling generative/predictive gaze models.

Abstract: Animals often forage via Levy walks stochastic trajectories with heavy tailed
step lengths optimized for sparse resource environments. We show that human
visual gaze follows similar dynamics when scanning images. While traditional
models emphasize image based saliency, the underlying spatiotemporal statistics
of eye movements remain underexplored. Understanding these dynamics has broad
applications in attention modeling and vision-based interfaces. In this study,
we conducted a large scale human subject experiment involving 40 participants
viewing 50 diverse images under unconstrained conditions, recording over 4
million gaze points using a high speed eye tracker. Analysis of these data
shows that the gaze trajectory of the human eye also follows a Levy walk akin
to animal foraging. This suggests that the human eye forages for visual
information in an optimally efficient manner. Further, we trained a
convolutional neural network (CNN) to predict fixation heatmaps from image
input alone. The model accurately reproduced salient fixation regions across
novel images, demonstrating that key components of gaze behavior are learnable
from visual structure alone. Our findings present new evidence that human
visual exploration obeys statistical laws analogous to natural foraging and
open avenues for modeling gaze through generative and predictive frameworks.

</details>


### [66] [CapGeo: A Caption-Assisted Approach to Geometric Reasoning](https://arxiv.org/abs/2510.09302)
*Yuying Li,Siyi Qian,Hao Liang,Leqi Zheng,Ruichuan An,Yongzhen Guo,Wentao Zhang*

Main category: cs.CV

TL;DR: 通过将几何图形转换为结构化caption并用关键点指标评估，CapGeo显著提升了MLLM几何推理能力，同时提供了用于筛选高质量几何caption模型的基准。


<details>
  <summary>Details</summary>
Motivation: 发现当前MLLM在纯文本数学推理（如IMO题）表现很好，但在几何问题上失败，原因在于几何图像理解而非推理本身；因此把图像转换为简洁准确的文本描述能提高下游几何推理。

Method: 设计caption-assisted reasoning框架：先将几何图生成结构化caption（包含关键点、关系等），再将caption与原图/题目一起输入MLLM进行推理。同时构建CapGeo-Bench数据集，并提出关键点为基础的评价指标。

Result: 在caption辅助下，多模型性能显著提升：Qwen2.5-VL-72B从8.6%提升到59.0%，Claude-Opus-4从44.8%提升到73.0%。CapGeo-Bench包含4641个图-注对，并提出关键点评价指标，该指标与下游表现高度相关。

Conclusion: 作者提出CapGeo，通过将几何图转为文本描述（caption）弥合视觉与文本模态差距，从而提升几何推理性能。

Abstract: Geometric reasoning remains a core challenge for Multimodal Large Language
Models (MLLMs). Even the most advanced closed-source systems, such as GPT-O3
and Gemini-2.5-Pro, still struggle to solve geometry problems reliably, despite
exhibiting strong textual reasoning abilities on tasks like the International
Mathematical Olympiad (IMO). This gap suggests that the bottleneck lies in
understanding geometric diagrams rather than reasoning itself. Since geometric
figures can often be faithfully described in concise textual form, converting
visual content into captions offers a promising direction. Motivated by this
insight, we introduce CapGeo, a caption-assisted reasoning framework that
bridges visual and textual modalities. Experiments show substantial
improvements when models are equipped with captions: Qwen2.5-VL-72B improves
from 8.6% (vision-only) to 59.0%, while Claude-Opus-4 rises from 44.8% to
73.0%. To systematically evaluate and identify high-quality geometric
captioning models, we further propose CapGeo-Bench, a dataset of 4,641 curated
figure-caption pairs. Crucially, CapGeo-Bench incorporates a keypoint-based
evaluation metric that correlates strongly with downstream CapGeo performance,
enabling reliable assessment of geometric captioning ability. Together, our
framework and benchmark highlight a new pathway toward advancing geometric
reasoning in MLLMs.

</details>


### [67] [RadioFlow: Efficient Radio Map Construction Framework with Flow Matching](https://arxiv.org/abs/2510.09314)
*Haozhe Jia,Wenshuo Chen,Xiucheng Wang,Nan Cheng,Hongbo Zhang,Kuimou Yu,Songning Lai,Nanjian Jia,Bowen Tian,Hongru Xiao,Yutao Yue*

Main category: cs.CV

TL;DR: RadioFlow uses flow-matching to learn continuous noise-to-data transport enabling single-step sampling, achieving similar or better accuracy with much smaller models and faster inference than diffusion baselines


<details>
  <summary>Details</summary>
Motivation: diffusion models have large size, slow iterative denoising, high latency; need accurate real-time radio map generation for next-gen wireless

Method: flow-matching generative framework (RadioFlow) using continuous transport trajectories

Result: state-of-the-art RM generation with up to 8x fewer parameters and over 4x faster inference compared to RadioDiff

Conclusion: RadioFlow enables scalable, energy-efficient, real-time electromagnetic digital twins for 6G by replacing diffusion with flow-matching single-step generative modeling

Abstract: Accurate and real-time radio map (RM) generation is crucial for
next-generation wireless systems, yet diffusion-based approaches often suffer
from large model sizes, slow iterative denoising, and high inference latency,
which hinder practical deployment. To overcome these limitations, we propose
\textbf{RadioFlow}, a novel flow-matching-based generative framework that
achieves high-fidelity RM generation through single-step efficient sampling.
Unlike conventional diffusion models, RadioFlow learns continuous transport
trajectories between noise and data, enabling both training and inference to be
significantly accelerated while preserving reconstruction accuracy.
Comprehensive experiments demonstrate that RadioFlow achieves state-of-the-art
performance with \textbf{up to 8$\times$ fewer parameters} and \textbf{over
4$\times$ faster inference} compared to the leading diffusion-based baseline
(RadioDiff). This advancement provides a promising pathway toward scalable,
energy-efficient, and real-time electromagnetic digital twins for future 6G
networks. We release the code at
\href{https://github.com/Hxxxz0/RadioFlow}{GitHub}.

</details>


### [68] [Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation](https://arxiv.org/abs/2510.09320)
*Wenyao Zhang,Hongsi Liu,Bohan Li,Jiawei He,Zekun Qi,Yunnan Wang,Shengyang Zhao,Xinqiang Yu,Wenjun Zeng,Xin Jin*

Main category: cs.CV

TL;DR: Hybrid-depth用CLIP（全局语义）与DINO（局部空间）在语言引导下进行多粒度对比与像素对齐，通过粗到细流程作为可插拔深度编码器，显著提升自监督单目深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前自监督单目深度估计受限于语义-空间知识提取不足，导致性能瓶颈。

Method: 两阶段：1) 在文本提示下用CLIP和DINO抽取并聚合多粒度特征，设计近远补丁对比代理任务促使深度感知对齐；2) 基于粗糙特征结合相机位姿与像素级语言对齐细化深度预测，作为插拔式深度编码器融入现有自监督MDE流水线。

Result: 提出Hybrid-depth框架，融合CLIP和DINO的多粒度视觉先验，通过粗到细的进阶学习和语言引导的对比任务提升深度估计，作为可插拔编码器增强Monodepth2/ManyDepth等，自KITTI上优于SOTA并改善BEV感知。

Conclusion: 融合语言引导的多粒度视觉先验能缓解特征粒度不匹配，显著提升自监督MDE效果，并促进下游BEV任务表现。

Abstract: Current self-supervised monocular depth estimation (MDE) approaches encounter
performance limitations due to insufficient semantic-spatial knowledge
extraction. To address this challenge, we propose Hybrid-depth, a novel
framework that systematically integrates foundation models (e.g., CLIP and
DINO) to extract visual priors and acquire sufficient contextual information
for MDE. Our approach introduces a coarse-to-fine progressive learning
framework: 1) Firstly, we aggregate multi-grained features from CLIP (global
semantics) and DINO (local spatial details) under contrastive language
guidance. A proxy task comparing close-distant image patches is designed to
enforce depth-aware feature alignment using text prompts; 2) Next, building on
the coarse features, we integrate camera pose information and pixel-wise
language alignment to refine depth predictions. This module seamlessly
integrates with existing self-supervised MDE pipelines (e.g., Monodepth2,
ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth
estimation. By aggregating CLIP's semantic context and DINO's spatial details
through language guidance, our method effectively addresses feature granularity
mismatches. Extensive experiments on the KITTI benchmark demonstrate that our
method significantly outperforms SOTA methods across all metrics, which also
indeed benefits downstream tasks like BEV perception. Code is available at
https://github.com/Zhangwenyao1/Hybrid-depth.

</details>


### [69] [Instance-Aware Robust Consistency Regularization for Semi-Supervised Nuclei Instance Segmentation](https://arxiv.org/abs/2510.09329)
*Zenan Lin,Wei Li,Jintao Chen,Zihao Wu,Wenxiong Kang,Changxin Gao,Liansheng Wang,Jin-Gang Yu*

Main category: cs.CV

TL;DR: 提出一种结合实例级一致性与形态学先验的半监督细胞核实例分割方法，有效减少伪标签噪声并提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像中标注稀缺导致的细胞核实例分割困难，提高半监督方法的实例级一致性正则化，利用结构先验减少伪标签噪声。

Method: 基于双网络(教师-学生)框架，引入匹配驱动与先验驱动的实例级一致性正则，利用形态学先验评估并过滤伪标签，增强高质量预测，丢弃低质量伪标签。

Result: 提出IRCR-Net，包括匹配驱动实例一致性(MIAC)和先验驱动实例一致性(PIAC)，结合形态学先验评估并筛选伪标签，提升半监督实例分割性能，在多个公开数据集上优于现有方法，部分场景超越全监督。

Conclusion: IRCR-Net通过MIAC和PIAC机制及先验引导的伪标签筛选，增强了半监督训练的鲁棒性和精度，对密集重叠细胞核场景尤为有效。

Abstract: Nuclei instance segmentation in pathological images is crucial for downstream
tasks such as tumor microenvironment analysis. However, the high cost and
scarcity of annotated data limit the applicability of fully supervised methods,
while existing semi-supervised methods fail to adequately regularize
consistency at the instance level, lack leverage of the inherent prior
knowledge of pathological structures, and are prone to introducing noisy
pseudo-labels during training. In this paper, we propose an Instance-Aware
Robust Consistency Regularization Network (IRCR-Net) for accurate
instance-level nuclei segmentation. Specifically, we introduce the
Matching-Driven Instance-Aware Consistency (MIAC) and Prior-Driven
Instance-Aware Consistency (PIAC) mechanisms to refine the nuclei instance
segmentation result of the teacher and student subnetwork, particularly for
densely distributed and overlapping nuclei. We incorporate morphological prior
knowledge of nuclei in pathological images and utilize these priors to assess
the quality of pseudo-labels generated from unlabeled data. Low-quality
pseudo-labels are discarded, while high-quality predictions are enhanced to
reduce pseudo-label noise and benefit the network's robust training.
Experimental results demonstrate that the proposed method significantly
enhances semi-supervised nuclei instance segmentation performance across
multiple public datasets compared to existing approaches, even surpassing fully
supervised methods in some scenarios.

</details>


### [70] [Enhancing Infrared Vision: Progressive Prompt Fusion Network and Benchmark](https://arxiv.org/abs/2510.09343)
*Jinyuan Liu,Zihang Chen,Zhu Liu,Zhiying Jiang,Long Ma,Xin Fan,Risheng Liu*

Main category: cs.CV

TL;DR: 提出基于成像机制的PPFN与SPT，针对热红外图像的单一与复合退化提供自适应增强，在新建多场景基准上取得显著性能提升（+8.76%）。


<details>
  <summary>Details</summary>
Motivation: 现有方法多只针对单一退化（噪声、对比度、模糊等），难以应对耦合退化；且RGB通用增强方法在热成像上效果欠佳，需专门考虑成像机制。

Method: 提出Progressive Prompt Fusion Network (PPFN)，基于热成像过程构建退化类型的prompt对，并对应融合以调制特征；引入Selective Progressive Training (SPT)逐步优化模型处理复合退化的能力；建立高质量多场景红外基准并进行大量实验验证。

Result: 在提出的数据集与基准上，方法在复杂退化场景实现了显著提升，报告了约8.76%的性能提升，并在视觉效果和保结构细节方面表现良好。

Conclusion: 本文提出了针对热红外图像增强的进步方法，能在单一及耦合退化条件下提供自适应引导与逐步细化训练，显著提升复杂退化场景的表现。

Abstract: We engage in the relatively underexplored task named thermal infrared image
enhancement. Existing infrared image enhancement methods primarily focus on
tackling individual degradations, such as noise, contrast, and blurring, making
it difficult to handle coupled degradations. Meanwhile, all-in-one enhancement
methods, commonly applied to RGB sensors, often demonstrate limited
effectiveness due to the significant differences in imaging models. In sight of
this, we first revisit the imaging mechanism and introduce a Progressive Prompt
Fusion Network (PPFN). Specifically, the PPFN initially establishes prompt
pairs based on the thermal imaging process. For each type of degradation, we
fuse the corresponding prompt pairs to modulate the model's features, providing
adaptive guidance that enables the model to better address specific
degradations under single or multiple conditions. In addition, a Selective
Progressive Training (SPT) mechanism is introduced to gradually refine the
model's handling of composite cases to align the enhancement process, which not
only allows the model to remove camera noise and retain key structural details,
but also enhancing the overall contrast of the thermal image. Furthermore, we
introduce the most high-quality, multi-scenarios infrared benchmark covering a
wide range of scenarios. Extensive experiments substantiate that our approach
not only delivers promising visual results under specific degradation but also
significantly improves performance on complex degradation scenes, achieving a
notable 8.76\% improvement. Code is available at
https://github.com/Zihang-Chen/HM-TIR.

</details>


### [71] [Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models](https://arxiv.org/abs/2510.09358)
*Qihang Ma,Shengyu Li,Jie Tang,Dingkang Yang,Shaodong Chen,Yingyi Zhang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: Use zero-shot, supervised fine-tuning, Fine-tune-CoT and a dynamic Chain-of-Thought (CoT) injection strategy to improve VLM performance on MMKP; showed effectiveness on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal keyphrase prediction methods struggle with missing/unseen scenarios and benchmarks overestimate performance due to train-test overlap; leveraging VLMs with reasoning-focused finetuning can address these issues.

Method: Evaluate VLMs with zero-shot and supervised fine-tuning; generate high-quality CoT reasoning data via a teacher model to finetune smaller models (Fine-tune-CoT); introduce dynamic CoT training that adaptively injects CoT examples during training for flexible inference use.

Result: The paper proposes using vision-language models (VLMs) for multi-modal keyphrase prediction (MMKP) and introduces training strategies to improve reasoning and avoid overthinking.

Conclusion: Dynamic CoT and Fine-tune-CoT enhance VLM reasoning in MMKP and mitigate overthinking, leading to better performance across evaluated datasets.

Abstract: Multi-modal keyphrase prediction (MMKP) aims to advance beyond text-only
methods by incorporating multiple modalities of input information to produce a
set of conclusive phrases. Traditional multi-modal approaches have been proven
to have significant limitations in handling the challenging absence and unseen
scenarios. Additionally, we identify shortcomings in existing benchmarks that
overestimate model capability due to significant overlap in training tests. In
this work, we propose leveraging vision-language models (VLMs) for the MMKP
task. Firstly, we use two widely-used strategies, e.g., zero-shot and
supervised fine-tuning (SFT) to assess the lower bound performance of VLMs.
Next, to improve the complex reasoning capabilities of VLMs, we adopt
Fine-tune-CoT, which leverages high-quality CoT reasoning data generated by a
teacher model to finetune smaller models. Finally, to address the
"overthinking" phenomenon, we propose a dynamic CoT strategy which adaptively
injects CoT data during training, allowing the model to flexibly leverage its
reasoning capabilities during the inference stage. We evaluate the proposed
strategies on various datasets and the experimental results demonstrate the
effectiveness of the proposed approaches. The code is available at
https://github.com/bytedance/DynamicCoT.

</details>


### [72] [BLINK-Twice: You see, but do you observe? A Reasoning Benchmark on Visual Perception](https://arxiv.org/abs/2510.09361)
*Junyan Ye,Dongzhi Jiang,Jun He,Baichuan Zhou,Zilong Huang,Zhiyuan Yan,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: BLINK-Twice is a vision-grounded reasoning benchmark with challenging perceptual tasks, adversarial image pairs, and annotated reasoning chains; current MLLMs perform poorly, but repeated observation and active interaction improve results


<details>
  <summary>Details</summary>
Motivation: Current MLLM reasoning benchmarks focus on language; need vision-centric tasks requiring reasoning purely from images

Method: Introduce BLINK-Twice benchmark and evaluation methodology

Result: Evaluated 20 MLLMs; found models struggle, chain-of-thought/self-criticism yield unstable/redundant reasoning; repeated viewing and active visual interaction help

Conclusion: A new paradigm for vision reasoning is needed emphasizing repeated/interactive observation and grounded reasoning chains rather than language-only strategies

Abstract: Recently, Multimodal Large Language Models (MLLMs) have made rapid progress,
particularly in enhancing their reasoning capabilities. However, existing
reasoning benchmarks still primarily assess language-based reasoning, often
treating visual input as replaceable context. To address this gap, we introduce
BLINK-Twice, a vision-centric reasoning benchmark grounded in challenging
perceptual tasks. Instead of relying on external knowledge, our tasks require
models to reason from visual content alone, shifting the focus from
language-based to image-grounded reasoning. Compared to prior perception
benchmarks, it moves beyond shallow perception ("see") and requires
fine-grained observation and analytical reasoning ("observe"). BLINK-Twice
integrates three core components: seven types of visual challenges for testing
visual reasoning, natural adversarial image pairs that enforce reliance on
visual content, and annotated reasoning chains for fine-grained evaluation of
the reasoning process rather than final answers alone. We evaluate 20 leading
MLLMs, including 12 foundation models and 8 reasoning-enhanced models.
BLINK-Twice poses a significant challenge to current models. While existing
reasoning strategies in the language space-such as chain-of-thought or
self-criticism can improve performance, they often result in unstable and
redundant reasoning. We observe that repeated image observation improves
performance across models, and active visual interaction, as demonstrated by
models like o3, highlights the need for a new paradigm for vision reasoning.
The dataset is publicly available at https://github.com/PicoTrex/BLINK-Twice

</details>


### [73] [Visibility-Aware Densification for 3D Gaussian Splatting in Dynamic Urban Scenes](https://arxiv.org/abs/2510.09364)
*Yikang Zhang,Rui Fan*

Main category: cs.CV

TL;DR: VAD-GS enhances 3D Gaussian splatting for urban scenes by detecting unreliable regions, selecting diverse views, and reconstructing missing geometry with patch-matching MVS to create new Gaussians, improving reconstruction quality over prior methods.


<details>
  <summary>Details</summary>
Motivation: Address failures of 3D Gaussian splatting when initial point cloud is partial or non-overlapping in urban scenes, causing distortions and artifacts due to incorrect gradient propagation and inability of existing densification to reconstruct missing structures.

Method: VAD-GS

Result: VAD-GS detects unreliable geometry via voxel-based visibility, selects supporting views with diversity-aware selection, reconstructs missing structures using patch-matching MVS, and generates new Gaussian primitives guided by geometric priors; shows superior performance on Waymo and nuScenes for static and dynamic objects.

Conclusion: VAD-GS effectively recovers missing geometry and improves 3DGS robustness in challenging urban environments, validated on large-scale autonomous driving datasets.

Abstract: 3D Gaussian splatting (3DGS) has demonstrated impressive performance in
synthesizing high-fidelity novel views. Nonetheless, its effectiveness
critically depends on the quality of the initialized point cloud. Specifically,
achieving uniform and complete point coverage over the underlying scene
structure requires overlapping observation frustums, an assumption that is
often violated in unbounded, dynamic urban environments. Training Gaussian
models with partially initialized point clouds often leads to distortions and
artifacts, as camera rays may fail to intersect valid surfaces, resulting in
incorrect gradient propagation to Gaussian primitives associated with occluded
or invisible geometry. Additionally, existing densification strategies simply
clone and split Gaussian primitives from existing ones, incapable of
reconstructing missing structures. To address these limitations, we propose
VAD-GS, a 3DGS framework tailored for geometry recovery in challenging urban
scenes. Our method identifies unreliable geometry structures via voxel-based
visibility reasoning, selects informative supporting views through
diversity-aware view selection, and recovers missing structures via patch
matching-based multi-view stereo reconstruction. This design enables the
generation of new Gaussian primitives guided by reliable geometric priors, even
in regions lacking initial points. Extensive experiments on the Waymo and
nuScenes datasets demonstrate that VAD-GS outperforms state-of-the-art 3DGS
approaches and significantly improves the quality of reconstructed geometry for
both static and dynamic objects. Source code will be released upon publication.

</details>


### [74] [Minkowski-MambaNet: A Point Cloud Framework with Selective State Space Models for Forest Biomass Quantification](https://arxiv.org/abs/2510.09367)
*Jinxiang Tu,Dayong Ren,Fei Shi,Zhenhong Jia,Yahong Ren,Jiwei Qin,Fang He*

Main category: cs.CV

TL;DR: New deep model integrates SSM into Minkowski sparse convolutions to capture long-range context in airborne LiDAR, enabling direct, DTM-free, and robust AGB/volume estimation with superior performance.


<details>
  <summary>Details</summary>
Motivation: Existing LiDAR-based methods struggle to model long-range dependencies needed to distinguish trees and often rely on preprocessing like DTM; aim to directly estimate volume and AGB from raw point clouds by capturing global context.

Method: Combine Minkowski 3D sparse convolutional network with Mamba Selective State Space Model (SSM) modules and skip connections; train end-to-end on raw airborne LiDAR voxelized inputs to directly regress woody volume and AGB.

Result: On Danish National Forest Inventory LiDAR, Minkowski-MambaNet outperforms state-of-the-art, providing more accurate and robust estimates, not needing DTM and reducing boundary artifacts.

Conclusion: The paper introduces Minkowski-MambaNet, which effectively integrates SSM into Minkowski networks to model long-range dependencies in LiDAR point clouds, yielding improved forest woody volume and AGB estimation without requiring DTM and showing robustness to boundary artifacts.

Abstract: Accurate forest biomass quantification is vital for carbon cycle monitoring.
While airborne LiDAR excels at capturing 3D forest structure, directly
estimating woody volume and Aboveground Biomass (AGB) from point clouds is
challenging due to difficulties in modeling long-range dependencies needed to
distinguish trees.We propose Minkowski-MambaNet, a novel deep learning
framework that directly estimates volume and AGB from raw LiDAR. Its key
innovation is integrating the Mamba model's Selective State Space Model (SSM)
into a Minkowski network, enabling effective encoding of global context and
long-range dependencies for improved tree differentiation. Skip connections are
incorporated to enhance features and accelerate convergence.Evaluated on Danish
National Forest Inventory LiDAR data, Minkowski-MambaNet significantly
outperforms state-of-the-art methods, providing more accurate and robust
estimates. Crucially, it requires no Digital Terrain Model (DTM) and is robust
to boundary artifacts. This work offers a powerful tool for large-scale forest
biomass analysis, advancing LiDAR-based forest inventories.

</details>


### [75] [Utilizing dynamic sparsity on pretrained DETR](https://arxiv.org/abs/2510.09380)
*Reza Sedghi,Anand Subramoney,David Kappel*

Main category: cs.CV

TL;DR: 提出两种无需重训练的DETR MLP稀疏化方法：基于静态指示器的启发式SIBS和小型动态门控MGS，其中MGS在不损失精度的情况下实现85–95%激活稀疏度，大幅降低计算量。


<details>
  <summary>Details</summary>
Motivation: Transformer中MLP层存在大量可利用的稀疏性，尤其在DETR等视觉检测模型中，直接利用这些稀疏性可在不重训练或少量训练下显著降低推理成本，便于部署。

Method: SIBS：统计固定激活模式，基于阈值或指示器静态屏蔽神经元；MGS：在预训练模型上添加并训练一个小线性门控层，按输入预测每个神经元是否激活，动态稀疏化MLP输出，推理时跳过被关掉的神经元计算。

Result: Paper proposes exploiting sparsity in MLP layers of DETR for efficient inference via two post-hoc methods: Static Indicator-Based Sparsification (SIBS) and Micro-Gated Sparsification (MGS). SIBS uses fixed activation patterns to predict inactive neurons; MGS adds a small learned gating linear layer to predict input-dependent sparsity, achieving 85-95% activation sparsity and reducing computation while maintaining/improving COCO performance.

Conclusion: MGS能在已有预训练DETR上通过轻量门控预测动态稀疏性，显著减少推理计算且保持或提升目标检测性能；SIBS简单但受限于输入依赖性。

Abstract: Efficient inference with transformer-based models remains a challenge,
especially in vision tasks like object detection. We analyze the inherent
sparsity in the MLP layers of DETR and introduce two methods to exploit it
without retraining. First, we propose Static Indicator-Based Sparsification
(SIBS), a heuristic method that predicts neuron inactivity based on fixed
activation patterns. While simple, SIBS offers limited gains due to the
input-dependent nature of sparsity. To address this, we introduce Micro-Gated
Sparsification (MGS), a lightweight gating mechanism trained on top of a
pretrained DETR. MGS predicts dynamic sparsity using a small linear layer and
achieves up to 85 to 95% activation sparsity. Experiments on the COCO dataset
show that MGS maintains or even improves performance while significantly
reducing computation. Our method offers a practical, input-adaptive approach to
sparsification, enabling efficient deployment of pretrained vision transformers
without full model retraining.

</details>


### [76] [Mono4DEditor: Text-Driven 4D Scene Editing from Monocular Video via Point-Level Localization of Language-Embedded Gaussians](https://arxiv.org/abs/2510.09438)
*Jin-Chuan Shi,Chengye Su,Jiajun Wang,Ariel Shamir,Miao Wang*

Main category: cs.CV

TL;DR: 提出Mono4DEditor：将量化CLIP特征与3D高斯体结合、两阶段定位和扩散视频编辑相结合，实现对单目重建的4D场景进行精确、局部且保持一致性的文本驱动编辑。


<details>
  <summary>Details</summary>
Motivation: 目标是解决单目视频重建的4D场景在基于文本编辑时如何实现语义精确、局部化编辑且不破坏未编辑内容的完整性这一难题。

Method: 方法包括：1）用量化CLIP特征增强3D高斯体形成语言嵌入的动态表示，支持任意空间区域语义查询；2）两阶段点级定位：先基于CLIP相似度选出候选高斯体，再精确细化其空间范围；3）在局部区域采用扩散视频编辑模型进行目标编辑，结合光流与涂鸦引导以保证空间与时间一致性。

Result: 实验表明Mono4DEditor在多种场景和对象类型上能实现高质量的文本驱动局部编辑，较 prior 方法在灵活性和视觉保真度上均有提升，并能较好保留未编辑区域的外观与几何结构。

Conclusion: Mono4DEditor提出了一个基于文本驱动的4D场景编辑框架，通过将3D高斯体与量化的CLIP特征结合，形成了可查询语言语义的动态表示，并使用两阶段点级定位策略与扩散视频编辑模型实现局部精确编辑，同时保持未编辑区域的一致性。

Abstract: Editing 4D scenes reconstructed from monocular videos based on text prompts
is a valuable yet challenging task with broad applications in content creation
and virtual environments. The key difficulty lies in achieving semantically
precise edits in localized regions of complex, dynamic scenes, while preserving
the integrity of unedited content. To address this, we introduce Mono4DEditor,
a novel framework for flexible and accurate text-driven 4D scene editing. Our
method augments 3D Gaussians with quantized CLIP features to form a
language-embedded dynamic representation, enabling efficient semantic querying
of arbitrary spatial regions. We further propose a two-stage point-level
localization strategy that first selects candidate Gaussians via CLIP
similarity and then refines their spatial extent to improve accuracy. Finally,
targeted edits are performed on localized regions using a diffusion-based video
editing model, with flow and scribble guidance ensuring spatial fidelity and
temporal coherence. Extensive experiments demonstrate that Mono4DEditor enables
high-quality, text-driven edits across diverse scenes and object types, while
preserving the appearance and geometry of unedited areas and surpassing prior
approaches in both flexibility and visual fidelity.

</details>


### [77] [Dynamic Weight-based Temporal Aggregation for Low-light Video Enhancement](https://arxiv.org/abs/2510.09450)
*Ruirui Lin,Guoxi Huang,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: 提出两阶段DWTA-Net：第一阶段多帧对齐恢复局部一致性，第二阶段基于光流的动态权重循环聚合精修，辅以纹理自适应损失，实现优越的低光视频增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有学习型方法在真实低光场景中对重噪声处理不力，主要原因是未能有效利用时间信息（短期与长期）进行对齐与融合。

Method: 两阶段框架：阶段一使用Visual State-Space块进行多帧对齐，恢复亮度、颜色和局部结构；阶段二引入基于光流的动态权重时序聚合的循环精修模块，平衡静态与动态区域；并设计纹理自适应损失以在保细节与去噪平滑之间权衡。

Result: 在真实低光视频数据集上，DWTA-Net在主观视觉质量和去噪、细节保留方面优于最新方法，能有效抑制噪声和伪影，同时保持细节。

Conclusion: DWTA-Net通过联合利用短期和长期时间信息，有效抑制低光视频中的噪声与伪影，提升亮度、颜色与结构恢复，最终在真实低光视频上超过现有方法。

Abstract: Low-light video enhancement (LLVE) is challenging due to noise, low contrast,
and color degradations. Learning-based approaches offer fast inference but
still struggle with heavy noise in real low-light scenes, primarily due to
limitations in effectively leveraging temporal information. In this paper, we
address this issue with DWTA-Net, a novel two-stage framework that jointly
exploits short- and long-term temporal cues. Stage I employs Visual State-Space
blocks for multi-frame alignment, recovering brightness, color, and structure
with local consistency. Stage II introduces a recurrent refinement module with
dynamic weight-based temporal aggregation guided by optical flow, adaptively
balancing static and dynamic regions. A texture-adaptive loss further preserves
fine details while promoting smoothness in flat areas. Experiments on
real-world low-light videos show that DWTA-Net effectively suppresses noise and
artifacts, delivering superior visual quality compared with state-of-the-art
methods.

</details>


### [78] [SilvaScenes: Tree Segmentation and Species Classification from Under-Canopy Images in Natural Forests](https://arxiv.org/abs/2510.09458)
*David-Alexandre Duclos,William Guimont-Martin,Gabriel Jeanson,Arthur Larochelle-Tremblay,Théo Defosse,Frédéric Moore,Philippe Nolet,François Pomerleau,Philippe Giguère*

Main category: cs.CV

TL;DR: 提出面向林下图像的树木实例分割数据集SilvaScenes（1476棵树，24个物种），并基准测试现代实例分割算法，结果显示分割较容易（mAP 67.65%）但物种分类仍具挑战（mAP 35.69%）


<details>
  <summary>Details</summary>
Motivation: Current datasets insufficient for under-canopy tree species instance segmentation; need diverse, expert-annotated dataset across bioclimatic domains for forestry robotics perception

Method: benchmark and dataset paper

Result: SilvaScenes dataset with 1476 trees, 24 species, five bioclimatic domains; benchmarking shows segmentation mAP 67.65%, species classification mAP 35.69%

Conclusion: SilvaScenes填补了林下树木实例分割数据空白，促进林业机器人感知研究；未来需改进物种分类性能及扩大数据集覆盖范围

Abstract: Interest in robotics for forest management is growing, but perception in
complex, natural environments remains a significant hurdle. Conditions such as
heavy occlusion, variable lighting, and dense vegetation pose challenges to
automated systems, which are essential for precision forestry, biodiversity
monitoring, and the automation of forestry equipment. These tasks rely on
advanced perceptual capabilities, such as detection and fine-grained species
classification of individual trees. Yet, existing datasets are inadequate to
develop such perception systems, as they often focus on urban settings or a
limited number of species. To address this, we present SilvaScenes, a new
dataset for instance segmentation of tree species from under-canopy images.
Collected across five bioclimatic domains in Quebec, Canada, SilvaScenes
features 1476 trees from 24 species with annotations from forestry experts. We
demonstrate the relevance and challenging nature of our dataset by benchmarking
modern deep learning approaches for instance segmentation. Our results show
that, while tree segmentation is easy, with a top mean average precision (mAP)
of 67.65%, species classification remains a significant challenge with an mAP
of only 35.69%. Our dataset and source code will be available at
https://github.com/norlab-ulaval/SilvaScenes.

</details>


### [79] [D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models](https://arxiv.org/abs/2510.09473)
*Jisu Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: 针对测试时提示调优导致的VLM校准退化，提出对文本特征做维度熵最大化以均衡特征维度，降低主导维度影响，从而改善校准性能和部署可靠性。


<details>
  <summary>Details</summary>
Motivation: 发现对比VLM在文本与图像模态之间存在因单一主导特征维度引起的模态差距，该主导维度对预测结果影响过大且敏感，导致在测试时提示调优下校准性能下降；因此希望通过抑制或均衡特征维度贡献来提升模型在实际部署下的可靠性。

Method: 提出了维度熵最大化（dimensional entropy maximization），作为正则项在测试时提示调优阶段对文本特征分布施加约束，促使各维度分布更接近均匀，弱化主导维度的预测敏感性；并结合对比VLM的特征分析，验证该方法对校准误差的改善效果。

Result: 实验表明，加入维度熵最大化后，测试时提示调优中的校准误差显著降低，模型对主导维度的依赖减弱，从而在无标签目标域的即时适应场景下提高了VLM的可靠性与稳定性。

Conclusion: 本文识别并缓解了对比视觉-语言模型（VLM）在测试时提示调优中因模态间“主导特征维度”导致的模态差距问题，通过对文本特征维度的分布进行熵最大化，使特征更均匀，从而降低这些主导维度的影响，提高校准性能与可靠性。

Abstract: Test-time adaptation paradigm provides flexibility towards domain shifts by
performing immediate adaptation on unlabeled target data from the source model.
Vision-Language Models (VLMs) leverage their generalization capabilities for
diverse downstream tasks, and test-time prompt tuning has emerged as a
prominent solution for adapting VLMs. In this work, we explore contrastive VLMs
and identify the modality gap caused by a single dominant feature dimension
across modalities. We observe that the dominant dimensions in both text and
image modalities exhibit high predictive sensitivity, and that constraining
their influence can improve calibration error. Building on this insight, we
propose dimensional entropy maximization that regularizes the distribution of
textual features toward uniformity to mitigate the dependency of dominant
dimensions. Our method alleviates the degradation of calibration performance in
test-time prompt tuning, offering a simple yet effective solution to enhance
the reliability of VLMs in real-world deployment scenarios.

</details>


### [80] [Few-shot multi-token DreamBooth with LoRa for style-consistent character generation](https://arxiv.org/abs/2510.09475)
*Ruben Pascual,Mikel Sesma-Sara,Aranzazu Jurio,Daniel Paternain,Mikel Galar*

Main category: cs.CV

TL;DR: 在少样本条件下，使用多token分配+LoRA微调改造DreamBooth，通过聚类和随机token机制，可以生成风格一致且多样化的角色。


<details>
  <summary>Details</summary>
Motivation: 解决在少量人工设计参考角色下，如何自动生成无限多但仍保留共同艺术风格的新角色，以扩展动画和游戏中的创意可能性。

Method: 基于DreamBooth的微调框架，使用聚类将参考图像分配到多个token（表示个体角色和集体风格），采用LoRA进行参数高效微调，移除类特定正则化数据集，并在生成阶段插入随机token和随机嵌入以扩展生成多样性。

Result: 提出一种基于DreamBooth的多token策略和LoRA微调的方法，用于在少量参考人物的情况下生成大量保持艺术风格一致的新角色。通过聚类为单个角色及其共同风格分配不同token，去除类别特定的正则化集，并在生成时引入随机token和嵌入，从而实现无限角色生成。

Conclusion: 该方法在五个小型数据集上对比基线并通过人类评估，证明能够生成高质量、多样且保持参考角色美学特征的新角色，显示出在动画和游戏创作中扩展角色设计的潜力。

Abstract: The audiovisual industry is undergoing a profound transformation as it is
integrating AI developments not only to automate routine tasks but also to
inspire new forms of art. This paper addresses the problem of producing a
virtually unlimited number of novel characters that preserve the artistic style
and shared visual traits of a small set of human-designed reference characters,
thus broadening creative possibilities in animation, gaming, and related
domains. Our solution builds upon DreamBooth, a well-established fine-tuning
technique for text-to-image diffusion models, and adapts it to tackle two core
challenges: capturing intricate character details beyond textual prompts and
the few-shot nature of the training data. To achieve this, we propose a
multi-token strategy, using clustering to assign separate tokens to individual
characters and their collective style, combined with LoRA-based
parameter-efficient fine-tuning. By removing the class-specific regularization
set and introducing random tokens and embeddings during generation, our
approach allows for unlimited character creation while preserving the learned
style. We evaluate our method on five small specialized datasets, comparing it
to relevant baselines using both quantitative metrics and a human evaluation
study. Our results demonstrate that our approach produces high-quality, diverse
characters while preserving the distinctive aesthetic features of the reference
characters, with human evaluation further reinforcing its effectiveness and
highlighting the potential of our method.

</details>


### [81] [A methodology for clinically driven interactive segmentation evaluation](https://arxiv.org/abs/2510.09499)
*Parhom Esmaeili,Virginia Fernandez,Pedro Borges,Eli Gibson,Sebastien Ourselin,M. Jorge Cardoso*

Main category: cs.CV

TL;DR: 提出临床导向的评估方法和标准化框架，系统评测交互分割方法，发现信息保留、可变放缩、训练/验证提示一致性、2D/3D上下文及域差异影响性能。


<details>
  <summary>Details</summary>
Motivation: Interactive segmentation needs clinically realistic evaluation to ensure fair comparison and reflect real-world performance; current inconsistent evaluation misleads.

Method: 构建评估任务与指标的临床化定义，搭建软件框架构建标准化评估管线，跨多样化任务评估多种方法并分析关键因素的影响。

Result: Standardised, clinically grounded evaluation methodology and software framework; empirical findings about factors affecting performance across tasks.

Conclusion: 需统一评估协议并注意交互信息处理、可变放缩与训练-验证一致性；对于复杂目标优先采用3D或专门训练的模型。

Abstract: Interactive segmentation is a promising strategy for building robust,
generalisable algorithms for volumetric medical image segmentation. However,
inconsistent and clinically unrealistic evaluation hinders fair comparison and
misrepresents real-world performance. We propose a clinically grounded
methodology for defining evaluation tasks and metrics, and built a software
framework for constructing standardised evaluation pipelines. We evaluate
state-of-the-art algorithms across heterogeneous and complex tasks and observe
that (i) minimising information loss when processing user interactions is
critical for model robustness, (ii) adaptive-zooming mechanisms boost
robustness and speed convergence, (iii) performance drops if validation
prompting behaviour/budgets differ from training, (iv) 2D methods perform well
with slab-like images and coarse targets, but 3D context helps with large or
irregularly shaped targets, (v) performance of non-medical-domain models (e.g.
SAM2) degrades with poor contrast and complex shapes.

</details>


### [82] [PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs](https://arxiv.org/abs/2510.09507)
*Zixin Zhang,Kanghao Chen,Xingwang Lin,Lutao Jiang,Xu Zheng,Yuanhuiyi Lyu,Litao Guo,Yinchuan Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出PhysToolBench评估MLLM对物理工具的理解，包含识别、理解、创造三大任务，评测32款模型，发现显著不足并提供分析与建议。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型（MLLMs）对物理工具的理解能力，弥补现有评估在具体工具理解上的空白。

Method: 构建视觉问答数据集并全面评测多类MLLMs

Result: 构建了PhysToolBench——一个包含1000+图文对、分为识别、理解、创造三层难度的VQA基准，对32种MLLMs进行评测，发现普遍存在工具理解不足，并给出分析和初步改进方案。

Conclusion: 当前MLLMs在物理工具理解上存在明显缺陷；PhysToolBench可作为标准基准推动相关改进，后续可通过针对性数据和训练提升模型能力。

Abstract: The ability to use, understand, and create tools is a hallmark of human
intelligence, enabling sophisticated interaction with the physical world. For
any general-purpose intelligent agent to achieve true versatility, it must also
master these fundamental skills. While modern Multimodal Large Language Models
(MLLMs) leverage their extensive common knowledge for high-level planning in
embodied AI and in downstream Vision-Language-Action (VLA) models, the extent
of their true understanding of physical tools remains unquantified. To bridge
this gap, we present PhysToolBench, the first benchmark dedicated to evaluating
the comprehension of physical tools by MLLMs. Our benchmark is structured as a
Visual Question Answering (VQA) dataset comprising over 1,000 image-text pairs.
It assesses capabilities across three distinct difficulty levels: (1) Tool
Recognition: Requiring the recognition of a tool's primary function. (2) Tool
Understanding: Testing the ability to grasp the underlying principles of a
tool's operation. (3) Tool Creation: Challenging the model to fashion a new
tool from surrounding objects when conventional options are unavailable. Our
comprehensive evaluation of 32 MLLMs-spanning proprietary, open-source,
specialized embodied, and backbones in VLAs-reveals a significant deficiency in
tool understanding. Furthermore, we provide an in-depth analysis and propose
preliminary solutions. Code and dataset are publicly available.

</details>


### [83] [Diagonal Artifacts in Samsung Images: PRNU Challenges and Solutions](https://arxiv.org/abs/2510.09509)
*David Vázquez-Padín,Fernando Pérez-González,Alejandro Martín-Del-Río*

Main category: cs.CV

TL;DR: 某些三星手机的图像处理在JPEG输出中引入相同的对角伪影，造成PRNU指纹冲突；使用raw图像或PRO模式可规避，但对中端无raw支持机型或无raw证据的取证仍存在挑战，同时这些伪影可作为辅助法医线索。


<details>
  <summary>Details</summary>
Motivation: 近期PRNU鉴定在智能手机图像取证中的广泛应用，但发现某些三星型号出现指纹碰撞和误判，需识别伪影来源并评估其对鉴定可靠性的影响及潜在的法医利用价值。

Method: 通过对多款三星Galaxy S系列和部分A系列手机拍摄的大量图像分析，识别出一致的对角性纹理伪影，比较不同机型指纹之间的相关性，并在支持PRO模式/raw捕获的设备上对比raw与JPEG图像的PRNU匹配效果；还对HDR和人像合成“散景”区域的伪影影响进行实验性验证。

Result: 发现Galaxy S系列（若干型号）及部分A系列存在共通的对角伪影，导致指纹相关性异常上升；raw图像鉴定不受影响；提出可利用这些伪影改善HDR误检率和定位合成散景区域的方法。

Conclusion: 存在对角人工伪影会导致不同设备间的PRNU指纹冲突，使得基于PRNU的设备鉴定面临误判风险，但在支持原始（raw）捕获的设备上仍可通过绕过图像处理管线恢复可靠鉴定。

Abstract: We investigate diagonal artifacts present in images captured by several
Samsung smartphones and their impact on PRNU-based camera source verification.
We first show that certain Galaxy S series models share a common pattern
causing fingerprint collisions, with a similar issue also found in some Galaxy
A models. Next, we demonstrate that reliable PRNU verification remains feasible
for devices supporting PRO mode with raw capture, since raw images bypass the
processing pipeline that introduces artifacts. This option, however, is not
available for the mid-range A series models or in forensic cases without access
to raw images. Finally, we outline potential forensic applications of the
diagonal artifacts, such as reducing misdetections in HDR images and localizing
regions affected by synthetic bokeh in portrait-mode images.

</details>


### [84] [PRNet: Original Information Is All You Have](https://arxiv.org/abs/2510.09531)
*PeiHuang Zheng,Yunlong Zhao,Zheng Cui,Yang Li*

Main category: cs.CV

TL;DR: PRNet通过保留并高效利用浅层空间特征，结合PRN和ESSamp模块，改善了无人机影像小目标检测的准确性与实时性。


<details>
  <summary>Details</summary>
Motivation: 小目标在航空影像中因像素有限导致特征萎缩，浅层空间细节与语义信息难以对齐，现有FPN后处理重建的细节偏离原图难以有效融合。

Method: 提出PRNet，包含Progressive Refinement Neck (PRN)和Enhanced SliceSamp (ESSamp)。PRN通过重用骨干特征与迭代细化实现空间-语义对齐；ESSamp通过优化重排与卷积在下采样时保留浅层信息。

Result: 在VisDrone、AI-TOD和UAVDT数据集上，PRNet在可比算力下优于现有方法，取得更好的准确率-效率权衡。

Conclusion: PRNet通过优先保留浅层原始空间特征并进行逐步对齐与增强，有效提升了航空影像中小目标的检测表现。

Abstract: Small object detection in aerial images suffers from severe information
degradation during feature extraction due to limited pixel representations,
where shallow spatial details fail to align effectively with semantic
information, leading to frequent misses and false positives. Existing FPN-based
methods attempt to mitigate these losses through post-processing enhancements,
but the reconstructed details often deviate from the original image
information, impeding their fusion with semantic content. To address this
limitation, we propose PRNet, a real-time detection framework that prioritizes
the preservation and efficient utilization of primitive shallow spatial
features to enhance small object representations. PRNet achieves this via two
modules:the Progressive Refinement Neck (PRN) for spatial-semantic alignment
through backbone reuse and iterative refinement, and the Enhanced SliceSamp
(ESSamp) for preserving shallow information during downsampling via optimized
rearrangement and convolution. Extensive experiments on the VisDrone, AI-TOD,
and UAVDT datasets demonstrate that PRNet outperforms state-of-the-art methods
under comparable computational constraints, achieving superior
accuracy-efficiency trade-offs.

</details>


### [85] [FLOWING: Implicit Neural Flows for Structure-Preserving Morphing](https://arxiv.org/abs/2510.09537)
*Arthur Bizzi,Matias Grynberg,Vitor Matias,Daniel Perazzo,João Paulo Lima,Luiz Velho,Nuno Gonçalves,João Pereira,Guilherme Schardong,Tiago Novello*

Main category: cs.CV

TL;DR: 将形变建模为结构化向量流，通过网络架构直接编码流的属性，获得稳定、准确且保持结构的形变方法（2D/3D），表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 标准MLP作为隐式神经表征用于形变时，通常依赖昂贵且不稳定的正则化来获得连贯和准确的形变。作者希望通过直接建模向量流，使得形变在理论上具备连续性、可逆性与时间一致性，从而提升训练稳定性和形变质量。

Method: 将形变表示为时间向量场，通过网络预测满足连续性和可逆性的流场，利用基于流的神学约束替代昂贵的正则化，结合插值/融合策略实现平滑形变；在架构层面编码结构化流特性以保证稳定性与时间一致性。

Result: FLOWING提出通过构造微分向量场(flow)来实现时变形变和融合，内建连续性、可逆性和时间一致性，避免了标准MLP需昂贵正则化的问题；在2D/3D形变任务上比现有方法收敛更快、质量更高。

Conclusion: FLOWING通过构造差分向量流并在网络中编码流特性，实现了稳定且保结构的形变，适用于图像和3D形状，在质量和收敛速度上优于传统MLP方法。

Abstract: Morphing is a long-standing problem in vision and computer graphics,
requiring a time-dependent warping for feature alignment and a blending for
smooth interpolation. Recently, multilayer perceptrons (MLPs) have been
explored as implicit neural representations (INRs) for modeling such
deformations, due to their meshlessness and differentiability; however,
extracting coherent and accurate morphings from standard MLPs typically relies
on costly regularizations, which often lead to unstable training and prevent
effective feature alignment. To overcome these limitations, we propose FLOWING
(FLOW morphING), a framework that recasts warping as the construction of a
differential vector flow, naturally ensuring continuity, invertibility, and
temporal coherence by encoding structural flow properties directly into the
network architectures. This flow-centric approach yields principled and stable
transformations, enabling accurate and structure-preserving morphing of both 2D
images and 3D shapes. Extensive experiments across a range of applications -
including face and image morphing, as well as Gaussian Splatting morphing -
show that FLOWING achieves state-of-the-art morphing quality with faster
convergence. Code and pretrained models are available at
http://schardong.github.io/flowing.

</details>


### [86] [TC-LoRA: Temporally Modulated Conditional LoRA for Adaptive Diffusion Control](https://arxiv.org/abs/2510.09561)
*Minkyoung Cho,Ruben Ohana,Christian Jacobsen,Adityan Jothi,Min-Hung Chen,Z. Morley Mao,Ethem Can*

Main category: cs.CV

TL;DR: TC-LoRA dynamically conditions diffusion model weights via a hypernetwork that outputs time- and condition-dependent LoRA adapters, outperforming static activation-based control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations of current controllable diffusion models that use static, activation-based conditioning which cannot adapt across the multi-stage denoising process.

Method: Use a hypernetwork to generate LoRA adapters on-the-fly conditioned on time and user inputs; apply these adapters to freeze backbone weights at each diffusion step to provide temporally modulated guidance.

Result: They propose TC-LoRA, which uses a hypernetwork to generate temporally modulated LoRA adapters that modify backbone weights at each diffusion step, improving fidelity and spatial adherence across domains.

Conclusion: Dynamic, parametric weight conditioning via TC-LoRA enables more effective, stage-aware control than static activation-based methods, improving generation quality and adherence to conditions.

Abstract: Current controllable diffusion models typically rely on fixed architectures
that modify intermediate activations to inject guidance conditioned on a new
modality. This approach uses a static conditioning strategy for a dynamic,
multi-stage denoising process, limiting the model's ability to adapt its
response as the generation evolves from coarse structure to fine detail. We
introduce TC-LoRA (Temporally Modulated Conditional LoRA), a new paradigm that
enables dynamic, context-aware control by conditioning the model's weights
directly. Our framework uses a hypernetwork to generate LoRA adapters
on-the-fly, tailoring weight modifications for the frozen backbone at each
diffusion step based on time and the user's condition. This mechanism enables
the model to learn and execute an explicit, adaptive strategy for applying
conditional guidance throughout the entire generation process. Through
experiments on various data domains, we demonstrate that this dynamic,
parametric control significantly enhances generative fidelity and adherence to
spatial conditions compared to static, activation-based methods. TC-LoRA
establishes an alternative approach in which the model's conditioning strategy
is modified through a deeper functional adaptation of its weights, allowing
control to align with the dynamic demands of the task and generative stage.

</details>


### [87] [FSP-DETR: Few-Shot Prototypical Parasitic Ova Detection](https://arxiv.org/abs/2510.09583)
*Shubham Trehan,Udhav Ramachandran,Akash Rao,Ruth Scimeca,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: FSP-DETR通过原型构建与联合损失，在单一模型下实现少样本检测、开放集识别和跨任务泛化，并在新卵类基准及其他生物医学任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学目标检测中标注数据稀缺及新类别出现频繁的问题，支持少样本检测、开放集识别和无需重训练的跨任务泛化。

Method: 使用class-agnostic DETR骨干，基于支持图像构建类别原型；对支持与查询图像生成增强视图并通过轻量Transformer解码器学习嵌入；设计原型匹配损失、对齐-分离损失与KL散度正则化联合训练；在推理阶段利用原型相似性进行分类/拒绝/开放集判断，支持跨任务直接适配。

Result: 提出FSP-DETR：基于class-agnostic DETR的统一检测框架，构建支持图像原型、用增强视图和轻量级Transformer解码器学习嵌入空间；联合优化原型匹配损失、对齐分离损失和KL正则化；在推理时支持未见类识别、背景拒绝和跨任务适配；在新的20类寄生虫卵基准及血细胞、疟疾任务上显著优于先前方法，尤其在低样本和开放集场景。

Conclusion: FSP-DETR是一个在稀疏监督下具备强泛化与开放集能力的统一生物医学目标检测框架，能在推理时灵活处理未见类与背景，不需重训练并取得明显性能提升。

Abstract: Object detection in biomedical settings is fundamentally constrained by the
scarcity of labeled data and the frequent emergence of novel or rare
categories. We present FSP-DETR, a unified detection framework that enables
robust few-shot detection, open-set recognition, and generalization to unseen
biomedical tasks within a single model. Built upon a class-agnostic DETR
backbone, our approach constructs class prototypes from original support images
and learns an embedding space using augmented views and a lightweight
transformer decoder. Training jointly optimizes a prototype matching loss, an
alignment-based separation loss, and a KL divergence regularization to improve
discriminative feature learning and calibration under scarce supervision.
Unlike prior work that tackles these tasks in isolation, FSP-DETR enables
inference-time flexibility to support unseen class recognition, background
rejection, and cross-task adaptation without retraining. We also introduce a
new ova species detection benchmark with 20 parasite classes and establish
standardized evaluation protocols. Extensive experiments across ova, blood
cell, and malaria detection tasks demonstrate that FSP-DETR significantly
outperforms prior few-shot and prototype-based detectors, especially in
low-shot and open-set scenarios.

</details>


### [88] [Vision Language Models: A Survey of 26K Papers](https://arxiv.org/abs/2510.09586)
*Fengming Lin*

Main category: cs.CV

TL;DR: 对2023-2025年三大会议论文进行词表驱动的自动标注与分析，揭示多模态VLM、生成模型和3D/视频研究的主要趋势，并公开资源以便审计与扩展；受限于词表召回与仅依赖摘要。


<details>
  <summary>Details</summary>
Motivation: 为了解和量化近年来计算机视觉与机器学习研究主题的演变，提供透明、可复现的度量工具，帮助研究者与政策制定者把握领域趋势。

Method: 对论文标题和摘要进行规范化、短语保护，并使用人工设计的词表进行匹配，标注最多35个主题标签，挖掘任务、架构、训练范式、目标、数据集和共现模态等细粒度线索；并进行纵向与跨会议比较。

Result: 发现三大宏观变化：多模态视觉-语言-LLM工作快速上升；生成方法稳步扩展，扩散研究聚焦可控性、蒸馏与加速；3D与视频研究持续活跃且向以人为中心/代理感知转移。VLM内部观察到参数高效适配方法、轻量视觉-语言桥接、从自训编码器转为指令微调及微调强骨干，损失函数从对比下降到交叉熵/排序与蒸馏。CVPR偏3D，ICLR VLM占比最高；效率与鲁棒性等可靠性主题跨领域蔓延。

Conclusion: 该论文提供了一种透明、可复现的研究趋势测量方法，覆盖2023-2025年CVPR、ICLR、NeurIPS共26,104篇论文，揭示了多模态VLMs崛起、生成方法扩展、3D与视频研究韧性三大宏观转变，并公开了词表和方法。

Abstract: We present a transparent, reproducible measurement of research trends across
26,104 accepted papers from CVPR, ICLR, and NeurIPS spanning 2023-2025. Titles
and abstracts are normalized, phrase-protected, and matched against a
hand-crafted lexicon to assign up to 35 topical labels and mine fine-grained
cues about tasks, architectures, training regimes, objectives, datasets, and
co-mentioned modalities. The analysis quantifies three macro shifts: (1) a
sharp rise of multimodal vision-language-LLM work, which increasingly reframes
classic perception as instruction following and multi-step reasoning; (2)
steady expansion of generative methods, with diffusion research consolidating
around controllability, distillation, and speed; and (3) resilient 3D and video
activity, with composition moving from NeRFs to Gaussian splatting and a
growing emphasis on human- and agent-centric understanding. Within VLMs,
parameter-efficient adaptation like prompting/adapters/LoRA and lightweight
vision-language bridges dominate; training practice shifts from building
encoders from scratch to instruction tuning and finetuning strong backbones;
contrastive objectives recede relative to cross-entropy/ranking and
distillation. Cross-venue comparisons show CVPR has a stronger 3D footprint and
ICLR the highest VLM share, while reliability themes such as efficiency or
robustness diffuse across areas. We release the lexicon and methodology to
enable auditing and extension. Limitations include lexicon recall and
abstract-only scope, but the longitudinal signals are consistent across venues
and years.

</details>


### [89] [SpaceVista: All-Scale Visual Spatial Reasoning from mm to km](https://arxiv.org/abs/2510.09606)
*Peiwen Sun,Shiqiang Lang,Dongming Wu,Yi Ding,Kaituo Feng,Huadai Liu,Zhen Ye,Rui Liu,Yun-Hui Liu,Jianan Wang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出SpaceVista-1M数据集与SpaceVista-7B模型，目标是提升多尺度空间推理能力，解决室内3D扫描依赖和场景过拟合问题。通过自动化任务专家管线构建约1M问答对覆盖5个尺度与19类任务，并构建手工注释基准SpaceVista-Bench；提出以尺度为锚的专家和逐步奖励机制实现尺度感知建模。实验显示模型在5个基准上具有竞争力和良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 动机在于克服当前空间推理研究的两大瓶颈：过度依赖室内3D扫描与人工标注，导致数据覆盖面窄；以及缺乏有效的全尺度场景建模，模型易对单一场景过拟合，难以在机器人和自动驾驶等多场景应用中泛化。

Method: 方法包括：1) 专家驱动自动化数据生成管线，按任务类型与尺度生成38K视频场景、约1M问答对；2) 构建高质量手工注释基准，通过记录、检索、拼装视频数据；3) 设计SpaceVista-7B模型，支持密集输入（非仅语义），采用尺度作为锚点，使用多尺度专家和渐进奖励训练以避免知识冲突；4) 在多项基准上进行广泛评估。

Result: 作者发布了SpaceVista-1M数据集、SpaceVista-7B模型和SpaceVista-Bench基准；实验表明在5个基准上模型取得竞争性表现并具备跨尺度、跨场景的较好泛化能力。

Conclusion: 作者构建了大规模多尺度视频问答数据集SpaceVista-1M并提出SpaceVista-7B模型，通过尺度锚定的专家系统与渐进训练策略，显著提升了全尺度空间推理的泛化能力；同时发布了人工注释基准SpaceVista-Bench以便公平评估。

Abstract: With the current surge in spatial reasoning explorations, researchers have
made significant progress in understanding indoor scenes, but still struggle
with diverse applications such as robotics and autonomous driving. This paper
aims to advance all-scale spatial reasoning across diverse scenarios by
tackling two key challenges: 1) the heavy reliance on indoor 3D scans and
labor-intensive manual annotations for dataset curation; 2) the absence of
effective all-scale scene modeling, which often leads to overfitting to
individual scenes. In this paper, we introduce a holistic solution that
integrates a structured spatial reasoning knowledge system, scale-aware
modeling, and a progressive training paradigm, as the first attempt to broaden
the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using
a task-specific, specialist-driven automated pipeline, we curate over 38K video
scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising
approximately 1M spatial QA pairs spanning 19 diverse task types. While
specialist models can inject useful domain knowledge, they are not reliable for
evaluation. We then build an all-scale benchmark with precise annotations by
manually recording, retrieving, and assembling video-based data. However, naive
training with SpaceVista-1M often yields suboptimal results due to the
potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a
spatial reasoning model that accepts dense inputs beyond semantics and uses
scale as an anchor for scale-aware experts and progressive rewards. Finally,
extensive evaluations across 5 benchmarks, including our SpaceVista-Bench,
demonstrate competitive performance, showcasing strong generalization across
all scales and scenarios. Our dataset, model, and benchmark will be released on
https://peiwensun2000.github.io/mm2km .

</details>


### [90] [VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation](https://arxiv.org/abs/2510.09607)
*Shaoqi Dong,Chaoyou Fu,Haihan Gao,Yi-Fan Zhang,Chi Yan,Chu Wu,Xiaoyu Liu,Yunhang Shen,Jing Huo,Deqiang Jiang,Haoyu Cao,Yang Gao,Xing Sun,Ran He,Caifeng Shan*

Main category: cs.CV

TL;DR: 通过对齐与选择性微调，将小型动作模型的动作解码能力蒸馏到预训练VLM：仅增添action token和state encoder，训练成本低而性能高，在仿真与真实机器人上均显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 训练完整的大型VLA模型代价高昂，希望利用已训练的小型动作模型的知识，使强感知能力的VLM能高效获得动作执行能力，减少预训练开销并提升泛化。

Method: 在原VLM结构上仅增加一个action token和一个state encoder；采用两阶段训练：1) 轻量级对齐，将VLM隐藏态映射到小型动作模型的动作空间，复用其动作解码器；2) 有选择地微调语言模型、状态编码器和动作模块以融合多模态输入并生成精确动作。

Result: 在LIBERO上取得97.3%成功率（提升11.8%），在LIBERO-LONG上93.5%（提升24.5%）；真实机器人五项任务成功率82.0%（提升17%），显示出显著性能和效率优势。

Conclusion: 本文提出通过蒸馏将小型动作模型的执行能力迁移到预训练视觉-语言模型，从而高效构建具动作生成能力的VLA模型；实验证明在基准数据集和真实机器人上均显著提升性能并降低训练成本。

Abstract: Vision-Language Action (VLA) models significantly advance robotic
manipulation by leveraging the strong perception capabilities of pretrained
vision-language models (VLMs). By integrating action modules into these
pretrained models, VLA methods exhibit improved generalization. However,
training them from scratch is costly. In this work, we propose a simple yet
effective distillation-based framework that equips VLMs with action-execution
capability by transferring knowledge from pretrained small action models. Our
architecture retains the original VLM structure, adding only an action token
and a state encoder to incorporate physical inputs. To distill action
knowledge, we adopt a two-stage training strategy. First, we perform
lightweight alignment by mapping VLM hidden states into the action space of the
small action model, enabling effective reuse of its pretrained action decoder
and avoiding expensive pretraining. Second, we selectively fine-tune the
language model, state encoder, and action modules, enabling the system to
integrate multimodal inputs with precise action generation. Specifically, the
action token provides the VLM with a direct handle for predicting future
actions, while the state encoder allows the model to incorporate robot dynamics
not captured by vision alone. This design yields substantial efficiency gains
over training large VLA models from scratch. Compared with previous
state-of-the-art methods, our method achieves 97.3% average success rate on
LIBERO (11.8% improvement) and 93.5% on LIBERO-LONG (24.5% improvement). In
real-world experiments across five manipulation tasks, our method consistently
outperforms the teacher model, achieving 82.0% success rate (17% improvement),
which demonstrate that action distillation effectively enables VLMs to generate
precise actions while substantially reducing training costs.

</details>


### [91] [StreamingVLM: Real-Time Understanding for Infinite Video Streams](https://arxiv.org/abs/2510.09608)
*Ruyi Xu,Guangxuan Xiao,Yukang Chen,Liuning He,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: StreamingVLM用重用注意力状态的KV缓存与短重叠块的SFT训练实现流式推理对齐，能在单卡实时处理超长视频并提升VQA表现。


<details>
  <summary>Details</summary>
Motivation: 解决对无限视频流进行理解时计算复杂度二次增长与滑动窗口方法导致的连贯性破坏或高延迟问题，设计一个既高效又稳定的流式视觉语言模型。

Method: 在训练阶段对短且重叠的视频块使用全注意力的监督微调，使模型学习到流式推理时的注意力模式；在推理阶段维护一个紧凑的KV缓存（重用注意力汇点的状态）、短视觉窗口和长文本窗口，以避免对全视频重复计算并保持低延迟与稳定性。

Result: StreamingVLM提出一种针对无限视频流的实时、稳定理解框架，通过在推理时维护紧凑的KV缓存、短视觉窗口和长文本窗口，并在训练时用短重叠视频块的全注意力SFT策略来对齐训练与流式推理。该方法在新构建的Inf-Streams-Eval基准上表现优异，并提升了通用VQA能力。

Conclusion: StreamingVLM通过训练-推理对齐和紧凑缓存策略，实现了对无限视频流的实时稳定理解，兼顾效率与性能，在新基准上超过GPT-4O mini并提升通用VQA性能。

Abstract: Vision-language models (VLMs) could power real-time assistants and autonomous
agents, but they face a critical challenge: understanding near-infinite video
streams without escalating latency and memory usage. Processing entire videos
with full attention leads to quadratic computational costs and poor performance
on long videos. Meanwhile, simple sliding window methods are also flawed, as
they either break coherence or suffer from high latency due to redundant
recomputation. In this paper, we introduce StreamingVLM, a model designed for
real-time, stable understanding of infinite visual input. Our approach is a
unified framework that aligns training with streaming inference. During
inference, we maintain a compact KV cache by reusing states of attention sinks,
a short window of recent vision tokens, and a long window of recent text
tokens. This streaming ability is instilled via a simple supervised fine-tuning
(SFT) strategy that applies full attention on short, overlapped video chunks,
which effectively mimics the inference-time attention pattern without training
on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a
new benchmark with videos averaging over two hours that requires dense,
per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM
achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time
performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy
also enhances general VQA abilities without any VQA-specific fine-tuning,
improving performance on LongVideoBench by +4.30 and OVOBench Realtime by
+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [92] [Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly](https://arxiv.org/abs/2510.08863)
*Deep Bodra,Sushil Khairnar*

Main category: cs.DB

TL;DR: 基于YCSB的对比测试表明：Redis适合延迟敏感的读负载，Aerospike在写密集和高并发场景更稳健且内存效率高，Dragonfly提供了折中选项；部署选择应基于负载类型与资源权衡。


<details>
  <summary>Details</summary>
Motivation: 随着分布式应用与云计算普及，对可扩展、高性能的键值存储需求增加，选择合适的NoSQL系统需要系统性的性能基准对比来揭示不同系统在典型负载下的表现与权衡。

Method: 基于YCSB框架，设计读重、写重、均衡三类负载，客户端并发从1扩展到32，测量并比较每秒吞吐（ops/s）、延迟分位数（P50/P95/P99）和内存使用情况；在真实配置下运行多次取均值以保证结果稳定性。

Result: 实验显示：
- 读重负载：Redis在低并发和中高并发下保持最低延迟和最高吞吐；Dragonfly在中等并发也接近Redis，Aerospike延迟较高但吞吐随并发增长稳定。
- 写重负载：Aerospike在高并发写入时吞吐最高且延迟稳定；Redis在高并发写入下延迟上升明显；Dragonfly表现介于二者之间。
- 均衡负载与内存：Aerospike在内存使用与吞吐比上最优，Redis内存压力大但响应快，Dragonfly内存与性能折中。
整体上，各系统在不同场景有各自优势，选择应基于具体负载和运维资源约束。

Conclusion: 在不同工作负载和并发客户端数下，三者在吞吐、延迟和内存占用上表现出明显差异；Redis在低延迟和高吞吐的读重场景表现优异，Aerospike在写密集型和大并发下稳定且内存效率高，Dragonfly在某些场景下能兼顾延迟与吞吐但资源消耗或可变。

Abstract: The rise of distributed applications and cloud computing has created a demand
for scalable, high-performance key-value storage systems. This paper presents a
performance evaluation of three prominent NoSQL key-value stores: Redis,
Aerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)
framework. We conducted extensive experiments across three distinct workload
patterns (read-heavy, write-heavy), and balanced while systematically varying
client concurrency from 1 to 32 clients. Our evaluation methodology captures
both latency, throughput, and memory characteristics under realistic
operational conditions, providing insights into the performance trade-offs and
scalability behaviour of each system

</details>


### [93] [HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance](https://arxiv.org/abs/2510.08896)
*Suming Qiu,Jing Li,Zhicheng Zhou,Junjie Huang,Linyuan Qiu,Zhijie Sun*

Main category: cs.DB

TL;DR: HES-SQL通过骨架评分、延迟感知奖励和思维模式自蒸馏的混合训练，在提高Text-to-SQL准确率的同时实现显著延迟下降，提出了一种兼顾正确性与效率的生成优化范式。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL模型在追求语义正确性时往往忽视执行效率，导致生成的SQL在实际DBMS上运行缓慢或资源消耗大；同时多模式（有/无推理）混合推理可能导致能力退化。作者旨在同时提升准确性与执行效率，并保持思维模式切换的稳定性。

Method: 提出混合训练框架：1) 骨架完整性评分用于对齐生成SQL与理想结构；2) 查询延迟感知奖励用于鼓励计算高效的SQL；3) 思维模式完成的自我蒸馏以维护推理能力。训练包括SFT与基于组相对策略优化(GRPO)的RL阶段，结合执行时的反馈作为奖励信号。

Result: 在MySQL 8.0与SQLite 3.42的受控单用户环境下，HES-SQL在BIRD与KaggleDBQA基准上分别达成79.14%与54.9%执行准确率，且相比监督基线在查询延迟上获得11%~20%的效率提升。

Conclusion: HES-SQL通过将思维模式融合的监督微调与基于执行信息的策略优化相结合，在提升Text-to-SQL的准确性同时显著降低查询延迟，证明了在生成任务中平衡语义正确性与计算效率的可行性。

Abstract: We present HES-SQL, a novel hybrid training framework that advances
Text-to-SQL generation through the integration of thinking-mode-fused
supervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).
Our approach introduces three key innovations: (1) a skeleton-completeness
scoring mechanism that enhances preference alignment between generated queries
and optimal SQL structures; (2) a query-latency-aware reward system that
incentivizes the generation of computationally efficient SQL queries; (3) a
self-distillation process for thinking-mode completion that prevents
degradation of the model's reasoning capabilities. This framework enables
hybrid thinking models to switch between reasoning and non-reasoning modes
while improving SQL query accuracy and execution efficiency.
  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under
controlled single-user conditions, demonstrates that HES-SQL achieves
competitive performance with execution accuracies of 79.14\% and 54.9\% on the
BIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the
end-to-end execution time of generated queries on the DBMS, averaged over
multiple runs to mitigate variance. Efficiency gains range from 11\% to 20\%
relative to supervised baselines. Our results establish a new paradigm for
Text-to-SQL systems that effectively balances semantic accuracy with
computational efficiency through execution-informed reinforcement learning
(RL). The proposed methodology has significant implications for developing
robust natural language interfaces to databases and can be extended to broader
structured generation tasks requiring both correctness and efficiency
optimization.

</details>
