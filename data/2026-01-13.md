<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 134]
- [cs.DB](#cs.DB) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders](https://arxiv.org/abs/2601.06067)
*Chimdi Walter Ndubuisi,Toni Kazic*

Main category: cs.CV

TL;DR: HyperTopo-Adapters在冻结编码器上用超曲-欧-球乘积流形嵌入与可微拓扑先验，显著提升叶片病变分割的边界与拓扑一致性，保持分类指标，同时提供开源可复现框架和细致消融。


<details>
  <summary>Details</summary>
Motivation: 标准像素级损失在欧氏潜空间中弱化了对拓扑变化（合并、分裂、假洞）的惩罚，而这些拓扑差异对生物学机制具有重要意义，因此需要引入结构感知的几何与拓扑先验以提升分割的拓扑保持能力。

Method: 在冻结的视觉编码器之上添加参数高效的头，将特征嵌入到超球面+欧式+球面乘积流形；引入两类拓扑先验：用于评估/选择的持久同调距离和用于训练的可微近似（软欧拉特征匹配+全变分正则化）；并加入超曲率对比学习暖启动、拓扑先验暖启动、按样本结构度量评估与min-PD within top-K Dice的检查点选择规则。

Result: 在Kaggle叶片病变数据集（N=2,940）上，实验显示边界与拓扑指标一致提升（例如减少Betti_1洞错误9%），同时Dice/IoU保持竞争力。并通过受控消融实验分析了曲率学习、潜维、对比温度、近似项设置等影响。提供可复现的训练/评估套件以便后续研究。

Conclusion: 该工作提出了HyperTopo-Adapters，通过在冻结的视觉编码器上训练轻量的多流形嵌入头（超曲率+欧式+球面）并结合拓扑先验，能够在叶片病变分割任务中改善边界与拓扑一致性，同时保持Dice/IoU竞争力。

Abstract: Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.

</details>


### [2] [OptFormer: Optical Flow-Guided Attention and Phase Space Reconstruction for SST Forecasting](https://arxiv.org/abs/2601.06078)
*Yin Wang,Chunlin Gong,Zhuozhen Xu,Lehan Zhang,Xiang Wu*

Main category: cs.CV

TL;DR: OptFormer uses phase-space reconstruction + optical-flow-guided attention to focus on dynamic regions, achieving better SST forecasting on NOAA datasets under 1:1 train-predict.


<details>
  <summary>Details</summary>
Motivation: SST prediction is hard due to nonlinear spatiotemporal dynamics and long horizons; leveraging phase-space reconstruction and explicit motion guidance (optical flow) can help models attend to dynamic regions and model long-term dependencies.

Method: Encoder-decoder framework with phase-space reconstruction of input series and a motion-aware attention module that weights attention by optical flow-derived inter-frame motion cues; trained under 1:1 training-to-prediction regime.

Result: Proposes OptFormer, encoder-decoder integrating phase-space reconstruction and motion-aware attention using optical flow; improves SST prediction long-term

Conclusion: OptFormer effectively captures nonlinear spatiotemporal SST dynamics and long-range dependencies, outperforming baselines in accuracy and robustness across scales.

Abstract: Sea Surface Temperature (SST) prediction plays a vital role in climate modeling and disaster forecasting. However, it remains challenging due to its nonlinear spatiotemporal dynamics and extended prediction horizons. To address this, we propose OptFormer, a novel encoder-decoder model that integrates phase-space reconstruction with a motion-aware attention mechanism guided by optical flow. Unlike conventional attention, our approach leverages inter-frame motion cues to highlight relative changes in the spatial field, allowing the model to focus on dynamic regions and capture long-range temporal dependencies more effectively. Experiments on NOAA SST datasets across multiple spatial scales demonstrate that OptFormer achieves superior performance under a 1:1 training-to-prediction setting, significantly outperforming existing baselines in accuracy and robustness.

</details>


### [3] [Semantic Event Graphs for Long-Form Video Question Answering](https://arxiv.org/abs/2601.06097)
*Aradhya Dixit,Tianxi Liang*

Main category: cs.CV

TL;DR: SEG用符号化时间事件图作为记忆层，使长视频问答在保持推理能力的同时显著节省token与成本。


<details>
  <summary>Details</summary>
Motivation: 长时段视频问答对现代视觉语言模型挑战大：直接处理小时级帧序列超出token与计算预算，且简单下采样或密集嵌入会丢失时间信息。需要一种高效保留时间关系的记忆层。

Method: 管线先用YOLOv11检测与跟踪对象，将邻近模式转为START/END人-物事件，构建时序场景图(TSG)。推理时通过查询感知的剪枝模块识别锚实体与相关事件，返回小规模子图并将其文字化后交给Gemini 2.5 Flash生成答案。

Result: 在五个YouTube视频和120个长时问答上，SEG以仅3.47k tokens/查询达到65.0%准确率，接近全日志基线的62.5%（40.39k tokens），token使用减少91.4%。短上下文基线（最后30s）准确率仅2.5%。

Conclusion: 本文提出了Semantic Event Graphs (SEG)，用紧凑的符号化时间事件图替代长视频的原始帧，作为视频与大语言模型间的轻量接口。

Abstract: Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.

</details>


### [4] [COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control](https://arxiv.org/abs/2601.06122)
*Canming Xia,Peixi Peng,Guang Tan,Zhan Su,Haoran Xu,Zhenxian Liu,Luntong Li*

Main category: cs.CV

TL;DR: COVR通过用RL交互数据微调VLM并反向用增强VLM指导策略，结合动态样本筛选和回报自适应损失，实现了VLM与RL的协同提升，显著提高了视觉控制任务的性能与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有工作多侧重将VLM的知识蒸馏到RL，忽视了RL交互数据对提升VLM的潜力。利用RL生成的数据回馈微调VLM，可使VLM的语义能力更契合具体任务，从而反向促进策略学习。

Method: COVR包括：1) 使用RL生成的数据对VLM进行微调，使其语义推理更贴合目标任务；2) 用增强后的VLM作为动作先验指导策略学习；同时为提高微调效率，引入探索驱动动态筛选（基于探索程度自适应保存有价值样本）和基于回报的自适应损失权重（通过RL回报量化采样动作不一致性以稳定训练），并采用渐进式微调以节省资源。

Result: 在多种挑战性的视觉控制任务上，COVR显示出优越性能，证明了双向协同优化策略和所提出两个模块及渐进式微调的有效性。

Conclusion: 本文提出的COVR框架通过双向优化视觉语言模型（VLM）与强化学习（RL）策略，实现了两者的协同增强，从而提升视觉控制任务的样本效率与语义推理能力。

Abstract: Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.

</details>


### [5] [Low-Back Pain Physical Rehabilitation by Movement Analysis in Clinical Trial](https://arxiv.org/abs/2601.06138)
*Sao Mai Nguyen*

Main category: cs.CV

TL;DR: Keraal：首个面向低背痛临床康复的标注数据集，支持动作评估、错误识别及时空定位，并对主流动作分析算法给出基准。


<details>
  <summary>Details</summary>
Motivation: 当前智能辅导系统在康复场景下缺乏真实临床患者数据，导致评估与推广受限；因此需要一个包含真实患者康复动作且带有细粒度标注的数据集来推动算法研究与系统开发。

Method: 采集临床病人在康复计划中执行低背痛康复动作的视频/动作数据，构建数据集并对四类任务（动作评估、错误识别、空间定位、时间定位）进行标注和基准实验，使用当前最先进的人体运动分析算法进行性能比较。

Result: 构建并发布了Keraal数据集，展示了在动作评估、错误识别、空间和时间定位四项任务上的基准结果，说明现有方法在临床康复数据上仍有提升空间。

Conclusion: 该论文提出了一个面向低背痛康复的临床动作数据集（Keraal），并在现有人体动作分析算法上进行了基准测试，旨在支持智能辅导系统（ITS）的开发与评估。

Abstract: To allow the development and assessment of physical rehabilitation by an intelligent tutoring system, we propose a medical dataset of clinical patients carrying out low back-pain rehabilitation exercises and benchmark on state of the art human movement analysis algorithms. This dataset is valuable because it includes rehabilitation motions in a clinical setting with patients in their rehabilitation program. This paper introduces the Keraal dataset, a clinically collected dataset to enable intelligent tutoring systems (ITS) for rehabilitation. It addresses four challenges in exercise monitoring: motion assessment, error recognition, spatial localization, temporal localization

</details>


### [6] [Forget-It-All: Multi-Concept Machine Unlearning via Concept-Aware Neuron Masking](https://arxiv.org/abs/2601.06163)
*Kaiyuan Deng,Bo Hui,Gen Li,Jie Ji,Minghai Qin,Geng Yuan,Xiaolong Ma*

Main category: cs.CV

TL;DR: FIA是一种训练免费、基于模型稀疏性和概念敏感神经元掩码融合的多概念遗忘框架，能有效移除多个目标概念并保留生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型需要有选择地删除多概念（如版权或敏感内容），但多数单概念方法难以扩展到多概念设置，导致效果、质量和稳定性问题。

Method: 提出Contrastive Concept Saliency衡量参数对目标概念的贡献，结合时空信息识别概念敏感神经元，构建并融合掩码去除概念特定神经元，同时保留概念不可知神经元。该方法训练免费，超参数少。

Result: 在三个不同的遗忘任务上，FIA在提高遗忘有效性、保持语义保真度和图像质量方面均表现良好。

Conclusion: FIA通过利用模型稀疏性和概念敏感神经元选择，实现了无需训练的多概念遗忘方法，在多项任务中提高了遗忘效果并保持生成质量。

Abstract: The widespread adoption of text-to-image (T2I) diffusion models has raised concerns about their potential to generate copyrighted, inappropriate, or sensitive imagery learned from massive training corpora. As a practical solution, machine unlearning aims to selectively erase unwanted concepts from a pre-trained model without retraining from scratch. While most existing methods are effective for single-concept unlearning, they often struggle in real-world scenarios that require removing multiple concepts, since extending them to this setting is both non-trivial and problematic, causing significant challenges in unlearning effectiveness, generation quality, and sensitivity to hyperparameters and datasets. In this paper, we take a unique perspective on multi-concept unlearning by leveraging model sparsity and propose the Forget It All (FIA) framework. FIA first introduces Contrastive Concept Saliency to quantify each weight connection's contribution to a target concept. It then identifies Concept-Sensitive Neurons by combining temporal and spatial information, ensuring that only neurons consistently responsive to the target concept are selected. Finally, FIA constructs masks from the identified neurons and fuses them into a unified multi-concept mask, where Concept-Agnostic Neurons that broadly support general content generation are preserved while concept-specific neurons are pruned to remove the targets. FIA is training-free and requires only minimal hyperparameter tuning for new tasks, thereby promoting a plug-and-play paradigm. Extensive experiments across three distinct unlearning tasks demonstrate that FIA achieves more reliable multi-concept unlearning, improving forgetting effectiveness while maintaining semantic fidelity and image quality.

</details>


### [7] [What Users Leave Unsaid: Under-Specified Queries Limit Vision-Language Models](https://arxiv.org/abs/2601.06165)
*Dasol Choi,Guijin Son,Hanwool Lee,Minhyuk Kim,Hyunwoo Ko,Teabin Lim,Ahn Eungyeol,Jungwhan Kim,Seunghyeok Hong,Youngsook Song*

Main category: cs.CV

TL;DR: 该论文构建了一个包含653个真实韩语视觉提问及显式重写的基准（HAERAE-Vision），显示当前VLM在非显式用户查询上表现较差；将查询显式化显著提高准确性，且检索不足以弥补信息缺失，提示评估与现实使用场景存在差距。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言基准与真实用户查询分布不符，真实问题常非正式且信息不足，导致模型在实际应用中性能下降，需构建更真实的评测集并研究显式化与检索的作用。

Method: 从韩语在线社区筛选86K候选问题，通过人工筛选和重写保留653个真实、低表达性问题，并为每条问题生成一个显式重写，形成1306条查询；评估39个VLM在原始、显式化及基于检索的设置下的表现。

Result: HAERAE-Vision 是针对韩语网络社区中真实视觉问题构建的新基准，涵盖653个原始、非正式且信息不足的问题，并为每个问题提供一个显式重写，总计1306个查询变体。评估了39个视觉语言模型（VLM），包括最先进模型（GPT-5、Gemini 2.5 Pro），在原始查询上的表现均低于50%。将查询显式化可带来8到22点的提升，小模型受益最大；即使结合网络检索，未显式化的查询仍不如未检索但显式化的查询，说明检索无法弥补用户未表达的信息。该研究表明，VLM困难很大程度来源于自然查询的信息不足，而非纯粹模型能力局限，展示了基准与实际部署间的显著差距。

Conclusion: 自然用户查询的缺乏明确性是导致VLM在真实场景中表现不佳的重要因素；改进模型或基准时应更多关注查询显式化、用户意图推断与更真实的数据分布。

Abstract: Current vision-language benchmarks predominantly feature well-structured questions with clear, explicit prompts. However, real user queries are often informal and underspecified. Users naturally leave much unsaid, relying on images to convey context. We introduce HAERAE-Vision, a benchmark of 653 real-world visual questions from Korean online communities (0.76% survival from 86K candidates), each paired with an explicit rewrite, yielding 1,306 query variants in total. Evaluating 39 VLMs, we find that even state-of-the-art models (GPT-5, Gemini 2.5 Pro) achieve under 50% on the original queries. Crucially, query explicitation alone yields 8 to 22 point improvements, with smaller models benefiting most. We further show that even with web search, under-specified queries underperform explicit queries without search, revealing that current retrieval cannot compensate for what users leave unsaid. Our findings demonstrate that a substantial portion of VLM difficulty stem from natural query under-specification instead of model capability, highlighting a critical gap between benchmark evaluation and real-world deployment.

</details>


### [8] [B-FIRE: Binning-Free Diffusion Implicit Neural Representation for Hyper-Accelerated Motion-Resolved MRI](https://arxiv.org/abs/2601.06166)
*Di Xu,Hengjie Liu,Yang Yang,Mary Feng,Jin Ning,Xin Miao,Jessica E. Scholey,Alexandra E. Hotca-cho,William C. Chen,Michael Ohliger,Martina Descovich,Huiming Dong,Wensha Yang,Ke Sheng*

Main category: cs.CV

TL;DR: B-FIRE通过CNN-INR与扩散优化，无需呼吸分箱即可从超加速非笛卡尔k空间恢复瞬时3D腹部MR，表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有4DMRI通常呈现呼吸相平均的伪影，导致瞬时动态信息模糊或失真；因此需要一种能从极度欠采样的非笛卡尔k空间恢复瞬时动态体数据的新范式。

Method: 采用CNN-INR（卷积神经网络嵌入隐式神经表示）编码-解码器作为基础，通过扩散模型优化，结合图像域保真和频域感知约束的综合损失函数。训练使用基于运动分箱的图像对作为参考，而推理直接在无分箱的欠采样数据上进行。实验在T1加权StarVIBE肝脏MRI数据集上，比较对象包括直接NuFFT、GRASP-CS和未展开的CNN方法。评估指标涵盖重建保真度、运动轨迹一致性和推理延迟。

Result: 在RV8到RV1超加速配置下，B-FIRE在重建质量和运动轨迹一致性上优于NuFFT、GRASP-CS和未展开CNN，且推理延迟满足临床实时性要求（论文宣称）。

Conclusion: B-FIRE提出了一种无分箱(diffusion implicit neural representation)的框架，可在超加速非笛卡尔k空间下恢复瞬时3D腹部解剖结构，克服传统4DMRI平均呼吸相的模糊问题。

Abstract: Accelerated dynamic volumetric magnetic resonance imaging (4DMRI) is essential for applications relying on motion resolution. Existing 4DMRI produces acceptable artifacts of averaged breathing phases, which can blur and misrepresent instantaneous dynamic information. Recovery of such information requires a new paradigm to reconstruct extremely undersampled non-Cartesian k-space data. We propose B-FIRE, a binning-free diffusion implicit neural representation framework for hyper-accelerated MR reconstruction capable of reflecting instantaneous 3D abdominal anatomy. B-FIRE employs a CNN-INR encoder-decoder backbone optimized using diffusion with a comprehensive loss that enforces image-domain fidelity and frequency-aware constraints. Motion binned image pairs were used as training references, while inference was performed on binning-free undersampled data. Experiments were conducted on a T1-weighted StarVIBE liver MRI cohort, with accelerations ranging from 8 spokes per frame (RV8) to RV1. B-FIRE was compared against direct NuFFT, GRASP-CS, and an unrolled CNN method. Reconstruction fidelity, motion trajectory consistency, and inference latency were evaluated.

</details>


### [9] [Analyzing the Structure of Handwritten Digits: A Comparative Study of PCA, Factor Analysis, and UMAP](https://arxiv.org/abs/2601.06168)
*Jyotiraditya Gupta*

Main category: cs.CV

TL;DR: PCA揭示全局方差方向并支持重构；FA给出可解释的笔画因子；UMAP发现非线性风格流形，三法共同表明MNIST手写数字位于结构化低维流形。


<details>
  <summary>Details</summary>
Motivation: 理解高维像素空间中手写数字的潜在组织结构，不仅关注分类性能，而是分析内在维数、共享变异与非线性几何。

Method: 采用PCA、FA和UMAP三种降维方法：PCA用于捕捉全局最大方差与重构能力；FA用于分解出可解释的笔画类潜变量；UMAP用于发现非线性流形与风格平滑过渡。

Result: PCA能用少数主成分高保真重构；FA识别出对应笔画、环路与对称性的可解释因子；UMAP展示了不同数字类之间的平滑风格流形。三者结合证实了低维结构及各自揭示的互补信息。

Conclusion: 手写数字在MNIST上表现为结构化的低维流形，不同降维方法揭示互补的特征。

Abstract: Handwritten digit images lie in a high-dimensional pixel space but exhibit strong geometric and statistical structure. This paper investigates the latent organization of handwritten digits in the MNIST dataset using three complementary dimensionality reduction techniques: Principal Component Analysis (PCA), Factor Analysis (FA), and Uniform Manifold Approximation and Projection (UMAP). Rather than focusing on classification accuracy, we study how each method characterizes intrinsic dimensionality, shared variation, and nonlinear geometry. PCA reveals dominant global variance directions and enables high-fidelity reconstructions using a small number of components. FA decomposes digits into interpretable latent handwriting primitives corresponding to strokes, loops, and symmetry. UMAP uncovers nonlinear manifolds that reflect smooth stylistic transitions between digit classes. Together, these results demonstrate that handwritten digits occupy a structured low-dimensional manifold and that different statistical frameworks expose complementary aspects of this structure.

</details>


### [10] [Think Bright, Diffuse Nice: Enhancing T2I-ICL via Inductive-Bias Hint Instruction and Query Contrastive Decoding](https://arxiv.org/abs/2601.06169)
*Zhiyong Ma,Zhenpeng Li,Yuanjie Shi,Zhengping Li,Jiahao Chen,Qingyuan Chuai*

Main category: cs.CV

TL;DR: TBDN是一个训练-free的双闭环框架（HI+QCD），分别通过提示注入和对比解码抑制合规失败与先验幻觉，显著提升T2I-ICL性能并具备良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 解决Text-to-Image In-Context Learning在定制图像合成中遇到的合规失败和先验主导的幻觉两大瓶颈，这两者互为因果形成恶性循环，且现有方法需专门训练，灵活性和部署成本受限。

Method: 方法无需额外训练：1) Hint Instruction(HI)：设计轻量任务提示（如示例格式、关键强调词）为模型提供映射规则的归纳偏置以提升提示遵从性；2) Query Contrastive Decoding(QCD)：在解码阶段计算完整输入与去除查询后的分布差异，利用对比调整生成概率以抑制模型基于先验的错误填充；两者在闭环中互为补充共同优化生成质量。

Result: 提出TBDN，一种无需训练的框架，包含Hint Instruction(HI)和Query Contrastive Decoding(QCD)两种闭环机制：HI通过轻量提示工程注入任务感知归纳偏置以减少合规失败；QCD通过对比完整输入与省略查询的解码分布来调整语言模型的解码，抑制先验主导的幻觉。TBDN在CoBSAT和Text-to-Image Fast Mini-ImageNet上达到最先进性能，并在不同模型骨干、提示设计和超参数下表现稳健，在Dreambench++上对概念保持和提示遵从也表现良好。

Conclusion: 通过同时解决合规失败与先验幻觉两个相互强化的瓶颈，TBDN提供了一个简洁高效且易部署的解决方案，推动了可靠的定制文本到图像的上下文学习。

Abstract: Text-to-Image In-Context Learning (T2I-ICL) enables customized image synthesis via interleaved text-image examples but faces two mutually reinforcing bottlenecks, compliance failure and prior-dominated hallucination, that form a vicious cycle degrading generation quality. Existing methods rely on tailored training, which limits flexibility and raises deployment costs. To address these challenges effectively, we propose TBDN, a training-free framework integrating two complementary closed-loop mechanisms: Hint Instruction (HI) and Query Contrastive Decoding (QCD). HI injects task-aware inductive bias via lightweight prompt engineering to anchor models on contextual mapping rules, thereby mitigating compliance failure. QCD adjusts the decoding distributions of language models by contrasting full-input and query-omitted distributions, suppressing prior-dominated hallucination. TBDN achieves State-of-the-Art performance on CoBSAT and Text-to-Image Fast Mini-ImageNet, with robust generalization across model backbones, prompt designs, and hyperparameters. It also maintains promising performance in concept preservation and prompt following on Dreambench++. By breaking the two bottlenecks, TBDN establishes a simple yet effective framework for efficient and reliable T2I-ICL.

</details>


### [11] [TIR-Flow: Active Video Search and Reasoning with Frozen VLMs](https://arxiv.org/abs/2601.06176)
*Hongbo Jin,Siyi Xie,Jiayu Ding,Kuanwei Lin,Ge Li*

Main category: cs.CV

TL;DR: TIR-Flow无需额外训练，通过任务分解、主动视觉取证与工作区记忆三模块，使冻结Video-LLMs具备主动搜索和推理能力，从而在多项视频推理基准上取得明显提升。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在推理上受限，传统通过大量CoT合成数据与SFT/RL进行优化，但主要改善输出分布而非激发模型主动探索动态视觉场景的能力。

Method: 框架由三部分组成：HDD（将复杂查询分解为可验证子任务）、HAP（主动引导视觉注意以获取高分辨率证据）、EBA（持续维护工作区以累积并更新线索）三模块协同工作，通过主动查询与证据验证闭环进行推理。

Result: 在七个基准上进行评测，平均提升5.9%，在Egoschema数据集上提升达10.5%，证明无参数更新情况下激活系统2式主动感知能显著提升长时序视频推理性能。

Conclusion: TIR-Flow通过引入主动搜索与推理机制，在不更新模型参数或额外数据的前提下，显著提升了冻结视频-语言模型的长时序视频推理能力。

Abstract: While Large Video-Language Models (Video-LLMs) have achieved remarkable progress in perception, their reasoning capabilities remain a bottleneck. Existing solutions typically resort to a heavy "data engineering" paradigm-synthesizing large-scale Chain-of-Thought (CoT) datasets followed by Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). This pipeline primarily optimizes probability sampling efficiency and aligns output distributions, but fails to activate the intrinsic intelligence required for dynamic visual exploration. In this work, we propose TIR-Flow, a novel framework that shifts the paradigm from passive processing to active video searching and reasoning without additional data or parameter updating. Concretely, our framework operates through three synergistic modules: HDD decomposes complex queries into a set of verifiable sub-tasks; HAP actively directs visual attention to gather high-resolution evidence for hypothesis validation; EBA maintains a persistent workspace to accumulate and update the discovered clues for logical reasoning. Extensive experiments on seven benchmarks demonstrate that TIR-Flow significantly outperforms recent strong baselines, delivering an average performance boost of 5.9%, with gains reaching 10.5% on Egoschema. Our analysis confirms that empowering frozen VLMs with System-2-like active perception is a scalable path toward solving long-horizon video reasoning.

</details>


### [12] [A Unified Attention U-Net Framework for Cross-Modality Tumor Segmentation in MRI and CT](https://arxiv.org/abs/2601.06187)
*Nishan Rai,Pushpa R. Dahal*

Main category: cs.CV

TL;DR: 提出将MRI（BraTS）与CT（LIDC-IDRI）联合训练于同一Attention U-Net的跨模态分割方法，使用统一预处理、注意力门跳跃连接和模态感知Focal Tversky损失，证明单一模型在两域均能取得有竞争力的Dice、IoU和AUC，建立了跨模态肿瘤分割的基线。


<details>
  <summary>Details</summary>
Motivation: 探究在不借助模态特定编码器或域适配的情况下，单一网络能否在不同影像模态（MRI与CT）和不同解剖部位的肿瘤分割任务上泛化，从而简化模型设计并提高可重复性。

Method: 采用模态统一的预处理流程，对输入进行归一化和尺寸统一；网络为Attention U-Net，包含注意力门的跳跃连接；训练时在混合的数据集上联合训练，并采用模态感知的Focal Tversky损失以应对不同模态的类别不平衡与难例采样。

Result: The unified Attention U-Net trained on both MRI and CT achieves competitive segmentation performance across modalities, showing feasibility of a single-model approach.

Conclusion: 单一的Attention U-Net在经适当预处理和损失设计下，可在MRI与CT两类影像上同时进行肿瘤分割并取得可比的结果，为跨模态分割提供了简单且可重复的基线。

Abstract: This study presents a unified Attention U-Net architecture trained jointly on MRI (BraTS 2021) and CT (LIDC-IDRI) datasets to investigate the generalizability of a single model across diverse imaging modalities and anatomical sites. Our proposed pipeline incorporates modality-harmonized preprocessing, attention-gated skip connections, and a modality-aware Focal Tversky loss function. To the best of our knowledge, this study is among the first to evaluate a single Attention U-Net trained simultaneously on separate MRI (BraTS) and CT (LIDC-IDRI) tumor datasets, without relying on modality-specific encoders or domain adaptation. The unified model demonstrates competitive performance in terms of Dice coefficient, IoU, and AUC on both domains, thereby establishing a robust and reproducible baseline for future research in cross-modality tumor segmentation.

</details>


### [13] [How Does India Cook Biryani?](https://arxiv.org/abs/2601.06198)
*Shubham Goel,Farzana S,C V Rishi,Aditya Arun,C V Jawahar*

Main category: cs.CV

TL;DR: 首次提出区域化Biryani烹饪视频数据集和基于VLM的细粒度流程对齐与比较框架，配套多层次QA基准与实验，促进对文化烹饪视频的计算分析。


<details>
  <summary>Details</summary>
Motivation: 印度Biryani在配料、烹饪顺序与呈现方式上地区差异显著，且在线视频资源丰富，现有视频理解方法难以捕捉此类细粒度、多模态与文化语境下的差异，因此需要新的数据集和方法来系统性研究烹饪流程的区域变体与可解释比较。

Method: 构建了包含12个区域风格、120个高质量YouTube视频的数据集；采用多阶段框架：用VLMs进行细粒度视频流程分割、将视频片段与音频转录与规范食谱文本对齐，进而构建视频间比较管线以自动发现并解释流程差异。使用多模型协同与人机交互验证以保证高精度，并在零样本与微调设置下评估多种SOTA模型。

Result: 发布了包含120段视频的区域化数据集、自动化的流程比较与解释管线、以及覆盖多推理层次的QA基准；并提供了基准测试结果和人机验证流程，所有数据与代码已公开发布，形成新的多模态推理测试床。

Conclusion: 本文贡献了首个大规模、分区域的手工筛选的印度Biryani烹饪视频数据集，并提出了基于视觉-语言模型的多阶段框架用于细粒度流程单元分割、跨模态对齐及区域间流程差异自动识别与解释。同时构建了多层次QA基准并进行了模型评测，推动了面向文化遗产烹饪视频的结构化多模态推理研究。

Abstract: Biryani, one of India's most celebrated dishes, exhibits remarkable regional diversity in its preparation, ingredients, and presentation. With the growing availability of online cooking videos, there is unprecedented potential to study such culinary variations using computational tools systematically. However, existing video understanding methods fail to capture the fine-grained, multimodal, and culturally grounded differences in procedural cooking videos. This work presents the first large-scale, curated dataset of biryani preparation videos, comprising 120 high-quality YouTube recordings across 12 distinct regional styles. We propose a multi-stage framework leveraging recent advances in vision-language models (VLMs) to segment videos into fine-grained procedural units and align them with audio transcripts and canonical recipe text. Building on these aligned representations, we introduce a video comparison pipeline that automatically identifies and explains procedural differences between regional variants. We construct a comprehensive question-answer (QA) benchmark spanning multiple reasoning levels to evaluate procedural understanding in VLMs. Our approach employs multiple VLMs in complementary roles, incorporates human-in-the-loop verification for high-precision tasks, and benchmarks several state-of-the-art models under zero-shot and fine-tuned settings. The resulting dataset, comparison methodology, and QA benchmark provide a new testbed for evaluating VLMs on structured, multimodal reasoning tasks and open new directions for computational analysis of cultural heritage through cooking videos. We release all data, code, and the project website at https://farzanashaju.github.io/how-does-india-cook-biryani/.

</details>


### [14] [QwenStyle: Content-Preserving Style Transfer with Qwen-Image-Edit](https://arxiv.org/abs/2601.06202)
*Shiwen Zhang,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出基于Qwen-Image-Edit和课程式持续学习的QwenStyle，用数据过滤+合成三元组训练，实现高质量内容保留风格迁移，V1在风格、内容和美学三项指标上领先。


<details>
  <summary>Details</summary>
Motivation: 动机是DiTs内部内容与风格特征纠缠，导致在给定内容与风格参考时难以同时实现精确内容保留与高质量风格迁移；因此希望利用Qwen-Image-Edit的已有能力并通过新的训练范式解决该挑战。

Method: 方法包括（1）采集并过滤高质量的特定风格数据，并合成大量类别的野外风格图像三元组；（2）基于Qwen-Image-Edit微调得到QwenStyle，利用其强内容保持与风格定制能力；（3）提出课程式持续学习框架（Curriculum Continual Learning），按难度和噪声水平逐步训练，结合干净与噪声样本以避免遗忘并提升泛化。

Result: QwenStyle V1在三项核心指标（风格相似度、内容一致性、美学质量）上达到SOTA，且在未见风格上的表现良好，说明课程式持续学习能在混合质量数据上训练出既保内容又能迁移风格的模型。

Conclusion: 本文提出了基于Qwen-Image-Edit训练的首个面向内容保留的风格迁移模型QwenStyle，通过课程式持续学习在混合干净与噪声三元组上训练，使模型在保持精确内容一致性的同时具备对未见风格的泛化能力。实验显示QwenStyle V1在风格相似度、内容一致性和美学质量三项核心指标上达到或超过现有最佳水平。

Abstract: Content-Preserving Style transfer, given content and style references, remains challenging for Diffusion Transformers (DiTs) due to its internal entangled content and style features. In this technical report, we propose the first content-preserving style transfer model trained on Qwen-Image-Edit, which activates Qwen-Image-Edit's strong content preservation and style customization capability. We collected and filtered high quality data of limited specific styles and synthesized triplets with thousands categories of style images in-the-wild. We introduce the Curriculum Continual Learning framework to train QwenStyle with such mixture of clean and noisy triplets, which enables QwenStyle to generalize to unseen styles without degradation of the precise content preservation capability. Our QwenStyle V1 achieves state-of-the-art performance in three core metrics: style similarity, content consistency, and aesthetic quality.

</details>


### [15] [Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification](https://arxiv.org/abs/2601.06204)
*Tayyab Rehman,Giovanni De Gasperis,Aly Shmahell*

Main category: cs.CV

TL;DR: 级联多智能体方法通过早退策略和按需推理，在显著降低延迟与能耗的同时，维持高质量的重建指标和可解释的异常归因，适合可扩展视觉监控部署。


<details>
  <summary>Details</summary>
Motivation: 动机是解决实时视觉监控中延迟、语义可解释性与计算成本之间的矛盾，结合重建模型、目标检测器和视觉-语言系统的优势以实现可扩展且能效高的异常检测。

Method: 方法包括：1）早期模块基于重建误差进行筛选以捕获低级异常；2）目标检测模块进行对象级评估；3）对语义模糊事件按需调用高层视觉-语言推理代理；4）引入自适应升级阈值和发布-订阅通信机制实现异步协调与异构部署。

Result: 在大规模监控数据集上的实验表明，与直接使用视觉-语言推理相比，系统延迟降低约三倍，同时保持高感知保真度（PSNR=38.3 dB, SSIM=0.965）并实现一致的语义标注。

Conclusion: 该论文提出了一个级联多智能体框架，通过在不同层级采用重建筛选、目标级评估和选择性语义推理，实现了低延迟与可解释语义检测的折中。

Abstract: Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.

</details>


### [16] [When Imbalance Comes Twice: Active Learning under Simulated Class Imbalance and Label Shift in Binary Semantic Segmentation](https://arxiv.org/abs/2601.06209)
*Julien Combes,Alexandre Derville,Jean-François Coeurjolly*

Main category: cs.CV

TL;DR: Active learning (entropy/core-set) works well with severe class imbalance common in machine vision, but label shift due to limited storage degrades performance.


<details>
  <summary>Details</summary>
Motivation: In machine vision many images lack defects and storage limits cause label shift; need to understand effects on active learning.

Method: Simulation study on two open-source datasets, artificially controlling class imbalance and label shift; compare random, entropy-based, and core-set sampling.

Result: Entropy and core-set selections effective for highly imbalanced datasets; strong label shift causes measurable loss of efficiency.

Conclusion: Active learning remains useful under class imbalance; entropy and core-set perform well; label shift reduces efficiency.

Abstract: The aim of Active Learning is to select the most informative samples from an unlabelled set of data. This is useful in cases where the amount of data is large and labelling is expensive, such as in machine vision or medical imaging. Two particularities of machine vision are first, that most of the images produced are free of defects, and second, that the amount of images produced is so big that we cannot store all acquired images. This results, on the one hand, in a strong class imbalance in defect distribution and, on the other hand, in a potential label shift caused by limited storage. To understand how these two forms of imbalance affect active learning algorithms, we propose a simulation study based on two open-source datasets. We artificially create datasets for which we control the levels of class imbalance and label shift. Three standard active learning selection strategies are compared: random sampling, entropy-based selection, and core-set selection. We demonstrate that active learning strategies, and in particular the entropy-based and core-set selections, remain interesting and efficient even for highly imbalanced datasets. We also illustrate and measure the loss of efficiency that occurs in the situation a strong label shift.

</details>


### [17] [Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur](https://arxiv.org/abs/2601.06212)
*Yani Meziani*

Main category: cs.CV

TL;DR: Akasha 2将物理启发的Hamiltonian机制与多模态潜空间预测模型结合，构建低延迟、高效且时空一致的视觉时序生成框架；实验证据强但若干关键细节与可重复性问题尚需补充。


<details>
  <summary>Details</summary>
Motivation: 旨在通过将物理学中的守恒律和辛结构引入深度时空建模，提升视频预测与视觉合成的时空一致性、效率与长期稳定性，同时满足移动端低延迟需求。

Method: 提出将H-SSD与VL-JEPA整合，基于Mamba-3 SSM并引入SMoE-HE和辛积分以在潜变量中强制守恒律；视觉合成方面采用HFM与持久化3D Gaussian Splatting以实现低延迟移动端渲染；整体架构包含全息记忆模块用于跨时空信息融合。

Result: 在视频预测（FVD:287）、视觉合成速度（比扩散模型快4倍）和推理速度（比transformer快3-18倍）方面取得显著性能提升，并声称在长时段预测中保持能量守恒。

Conclusion: Akasha 2有效结合了物理启发的归纳偏置与多模态表征，显著提升了时空一致性与推理效率，但论文中的某些细节（如训练数据、消融研究、泛化能力和能量守恒的严格证据）需进一步澄清以验证可重复性。

Abstract: We present Akasha 2, a state-of-the-art multimodal architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (<50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over transformer baselines while maintaining energy conservation over extended horizons.

</details>


### [18] [Two-step Authentication: Multi-biometric System Using Voice and Facial Recognition](https://arxiv.org/abs/2601.06218)
*Kuan Wei Chen,Ting Yi Lin,Wen Ren Yang,Aryan Kesarwani,Riya Singh*

Main category: cs.CV

TL;DR: 本文提出并实现了一个基于摄像头与麦克风的低成本两步认证系统：人脸先验筛选＋针对性说话人验证，分别在小规模人脸数据与LibriSpeech上取得95.1%与98.9%准确率（EER 3.456%），并开源代码。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的常见设备上实现一种低成本、计算高效且稳健的多生物特征两步认证方案。

Method: 使用MTCNN进行人脸定位；在924张（5人）增强数据上训练剪枝后的VGG-16分类器实现人脸识别；说话人验证使用在LibriSpeech(train-other-360)上训练的CNN模型。

Result: 人脸识别在该小规模数据集上达到95.1%准确率；说话人验证在test-clean上达到98.9%准确率且EER为3.456%。同时开源了代码与模型。

Conclusion: 提出了一种仅用常见设备摄像头与麦克风的两步认证系统：先进行人脸识别缩小候选，再对匹配身份做说话人验证，从而降低计算开销并提高鲁棒性。

Abstract: We present a cost-effective two-step authentication system that integrates face identification and speaker verification using only a camera and microphone available on common devices. The pipeline first performs face recognition to identify a candidate user from a small enrolled group, then performs voice recognition only against the matched identity to reduce computation and improve robustness. For face recognition, a pruned VGG-16 based classifier is trained on an augmented dataset of 924 images from five subjects, with faces localized by MTCNN; it achieves 95.1% accuracy. For voice recognition, a CNN speaker-verification model trained on LibriSpeech (train-other-360) attains 98.9% accuracy and 3.456% EER on test-clean. Source code and trained models are available at https://github.com/NCUE-EE-AIAL/Two-step-Authentication-Multi-biometric-System.

</details>


### [19] [SAPL: Semantic-Agnostic Prompt Learning in CLIP for Weakly Supervised Image Manipulation Localization](https://arxiv.org/abs/2601.06222)
*Xinghao Wang,Changtao Miao,Dianmo Sheng,Tao Gong,Qi Chu,Nenghai Yu,Quanchen Zou,Deyue Zhang,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: SAPL adds edge-aware textual prompts and hierarchical edge contrastive learning to make CLIP highlight manipulation boundaries, improving weakly supervised localization without dense labels.


<details>
  <summary>Details</summary>
Motivation: Pixel-level labels are costly and existing weakly supervised methods ignore local edge cues; manipulated boundaries show larger feature variations and are key for precise localization.

Method: 1) ECPL: use edge-enhanced image features and attention to generate learnable textual prompts that embed semantic-agnostic edge cues. 2) HECL: extract edge patches (genuine vs manipulated) and apply contrastive learning to separate them in feature space. Combine prompts in CLIP and predict masks from similarity maps.

Result: Proposes SAPL to improve manipulated-image localization by learning non-semantic, edge-focused prompts in CLIP.

Conclusion: SAPL effectively shifts CLIP's focus from semantics to manipulation edges via ECPL and HECL, yielding state-of-the-art localization on benchmarks.

Abstract: Malicious image manipulation threatens public safety and requires efficient localization methods. Existing approaches depend on costly pixel-level annotations which make training expensive. Existing weakly supervised methods rely only on image-level binary labels and focus on global classification, often overlooking local edge cues that are critical for precise localization. We observe that feature variations at manipulated boundaries are substantially larger than in interior regions. To address this gap, we propose Semantic-Agnostic Prompt Learning (SAPL) in CLIP, which learns text prompts that intentionally encode non-semantic, boundary-centric cues so that CLIPs multimodal similarity highlights manipulation edges rather than high-level object semantics. SAPL combines two complementary modules Edge-aware Contextual Prompt Learning (ECPL) and Hierarchical Edge Contrastive Learning (HECL) to exploit edge information in both textual and visual spaces. The proposed ECPL leverages edge-enhanced image features to generate learnable textual prompts via an attention mechanism, embedding semantic-irrelevant information into text features, to guide CLIP focusing on manipulation edges. The proposed HECL extract genuine and manipulated edge patches, and utilize contrastive learning to boost the discrimination between genuine edge patches and manipulated edge patches. Finally, we predict the manipulated regions from the similarity map after processing. Extensive experiments on multiple public benchmarks demonstrate that SAPL significantly outperforms existing approaches, achieving state-of-the-art localization performance.

</details>


### [20] [Ground What You See: Hallucination-Resistant MLLMs via Caption Feedback, Diversity-Aware Sampling, and Conflict Regularization](https://arxiv.org/abs/2601.06224)
*Miao Pan,Wangjie Gan,Jintao Chen,Wenqi Zhang,Bing Sun,Jianwei Yin,Xuhong Zhang*

Main category: cs.CV

TL;DR: 为解决RL训练下MLLM的幻觉问题，论文提出三模块方案：高质量定位与caption预设、基于奖励分布的多样化采样、以及基于InfoNCE的NTK相似性调节，实验证明能显著减少幻觉并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 在RL优化过程中，MLLM出现幻觉问题严重阻碍模型的可用性，尤其是当视觉描述不准确或训练样本相互干扰时，RL会放大这些问题。为此需要从前置视觉描述、训练样本采样策略及训练动态稳定性三方面入手进行系统性改进。

Method: 设计并实现三模块框架：1) 在推理前引入定位(planning)和captioning阶段，用基于质量的caption奖励强化初始描述的准确性；2) 根据样本奖励分布的均值和方差对样本进行分类，优先训练高方差样本以增加探索多样性；3) 计算样本对的NTK相似度，分组后对相似对应用InfoNCE损失以推动过相似样本间的距离并拉近不相似样本，调节梯度交互。

Result: 实验表明该框架在多个设置下显著降低幻觉率并提升推理准确率（具体数值未在摘要提供），验证了各模块对问题的有效缓解作用。

Conclusion: 本论文通过系统性分析和实证验证，认为在RL训练下MLLM的幻觉主要来源于连锁式视觉推理的误导、探索多样性不足导致的过度自信误答、以及训练样本间的NTK相似性引发的冲突。提出的三模块框架（定位与高质量caption预设、基于奖励分布的样本优先采样、以及基于InfoNCE的NTK相似性调节）能够协同减少幻觉并提升推理准确性。

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse tasks, their practical deployment is severely hindered by hallucination issues, which become particularly acute during Reinforcement Learning (RL) optimization. This paper systematically analyzes the root causes of hallucinations in MLLMs under RL training, identifying three critical factors: (1) an over-reliance on chained visual reasoning, where inaccurate initial descriptions or redundant information anchor subsequent inferences to incorrect premises; (2) insufficient exploration diversity during policy optimization, leading the model to generate overly confident but erroneous outputs; and (3) destructive conflicts between training samples, where Neural Tangent Kernel (NTK) similarity causes false associations and unstable parameter updates. To address these challenges, we propose a comprehensive framework comprising three core modules. First, we enhance visual localization by introducing dedicated planning and captioning stages before the reasoning phase, employing a quality-based caption reward to ensure accurate initial anchoring. Second, to improve exploration, we categorize samples based on the mean and variance of their reward distributions, prioritizing samples with high variance to focus the model on diverse and informative data. Finally, to mitigate sample interference, we regulate NTK similarity by grouping sample pairs and applying an InfoNCE loss to push overly similar pairs apart and pull dissimilar ones closer, thereby guiding gradient interactions toward a balanced range. Experimental results demonstrate that our proposed method significantly reduces hallucination rates and effectively enhances the inference accuracy of MLLMs.

</details>


### [21] [Synthetic FMCW Radar Range Azimuth Maps Augmentation with Generative Diffusion Model](https://arxiv.org/abs/2601.06228)
*Zhaoze Wang,Changxu Zhang,Tai Fei,Christopher Grimm,Yi Jin,Claas Tebruegge,Ernst Warsitz,Markus Gardill*

Main category: cs.CV

TL;DR: 通过条件扩散模型（结合Confidence Maps、几何感知和时间一致性）合成高质量、多样且物理合理的汽车雷达Range-Azimuth图，显著提升重建质量和检测任务泛化性能。


<details>
  <summary>Details</summary>
Motivation: 真实标注的汽车雷达数据稀缺且多样性不足，限制深度学习感知性能，需要合成真实且多样的雷达训练数据。

Method: 使用生成扩散模型（diffusion model）生成多类别雷达数据，采用Confidence Maps作为条件（每通道表示一个语义类并以高斯分布编码目标位置），并引入几何感知条件（Geometry Aware Conditioning）与时间一致性正则化（Temporal Consistency Regularization）。

Result: 在ROD2021数据集上，信号重建PSNR提升3.6 dB；使用真实与合成数据训练使mAP较传统基于图像处理的数据增强提高4.15%。

Conclusion: 本文提出的条件生成扩散模型在合成汽车雷达Range-Azimuth Maps方面有效，生成的数据在信号重建质量和下游检测任务上均优于基线方法。

Abstract: The scarcity and low diversity of well-annotated automotive radar datasets often limit the performance of deep-learning-based environmental perception. To overcome these challenges, we propose a conditional generative framework for synthesizing realistic Frequency-Modulated Continuous-Wave radar Range-Azimuth Maps. Our approach leverages a generative diffusion model to generate radar data for multiple object categories, including pedestrians, cars, and cyclists. Specifically, conditioning is achieved via Confidence Maps, where each channel represents a semantic class and encodes Gaussian-distributed annotations at target locations. To address radar-specific characteristics, we incorporate Geometry Aware Conditioning and Temporal Consistency Regularization into the generative process. Experiments on the ROD2021 dataset demonstrate that signal reconstruction quality improves by \SI{3.6}{dB} in Peak Signal-to-Noise Ratio over baseline methods, while training with a combination of real and synthetic datasets improves overall mean Average Precision by 4.15% compared with conventional image-processing-based augmentation. These results indicate that our generative framework not only produces physically plausible and diverse radar spectrum but also substantially improves model generalization in downstream tasks.

</details>


### [22] [A survey of facial recognition techniques](https://arxiv.org/abs/2601.06239)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 综述对比了主流面部识别方法与数据库评估，指出光照、年龄、姿态、遮挡、表情是主要挑战，建议采用混合与多模态方法提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着多媒体内容激增，面部识别成为重要研究领域，但人脸的复杂性和多变性（光照、年龄、姿态、遮挡、表情）使得识别仍具挑战，故需综述现有方法并评估其在不同数据库上的表现。

Method: 通过文献回顾对比多种面部检测与识别方法（如HMM、PCA、ECM、SVM、Gabor、ANN、Eigenfaces、ICA、3D变形模型），并在多个公开数据库（JAFEE、FEI、Yale、LFW、AT&T/ORL、AR）上进行实验评估。

Result: 论文总结了各类方法的优缺点：统计学习（PCA/ICA/Eigenfaces）在受控环境表现良好但对光照/姿态敏感；SVM/ANN等在特征学习上更鲁棒；Gabor波、3D模型和混合方法能更好处理局部特征、光照及姿态问题。数据库实验展示了不同方法在不同数据集上的性能差异，但具体数值和实验细节文中给出。

Conclusion: 这篇综述认为面部识别仍受光照、年龄、姿态、部分遮挡和表情变化等挑战影响，现有方法各有优劣并需结合多种技术和数据库验证以提升鲁棒性。

Abstract: As multimedia content is quickly growing, the field of facial recognition has become one of the major research fields, particularly in the recent years. The most problematic area to researchers in image processing and computer vision is the human face which is a complex object with myriads of distinctive features that can be used to identify the face. The survey of this survey is particularly focused on most challenging facial characteristics, including differences in the light, ageing, variation in poses, partial occlusion, and facial expression and presents methodological solutions. The factors, therefore, are inevitable in the creation of effective facial recognition mechanisms used on facial images. This paper reviews the most sophisticated methods of facial detection which are Hidden Markov Models, Principal Component Analysis (PCA), Elastic Cluster Plot Matching, Support Vector Machine (SVM), Gabor Waves, Artificial Neural Networks (ANN), Eigenfaces, Independent Component Analysis (ICA), and 3D Morphable Model. Alongside the works mentioned above, we have also analyzed the images of a number of facial databases, namely JAFEE, FEI, Yale, LFW, AT&T (then called ORL), and AR (created by Martinez and Benavente), to analyze the results. However, this survey is aimed at giving a thorough literature review of face recognition, and its applications, and some experimental results are provided at the end after a detailed discussion.

</details>


### [23] [EyeTheia: A Lightweight and Accessible Eye-Tracking Toolbox](https://arxiv.org/abs/2601.06279)
*Stevenson Pather,Niels Martignène,Arnaud Bugnet,Fouad Boutaleb,Fabien D'Hondt,Deise Santana Maia*

Main category: cs.CV

TL;DR: EyeTheia 是一个可在浏览器中运行、基于普通摄像头的开放注视估计管道，结合 MediaPipe 与轻量 CNN，并通过用户微调提升精度，表现接近商用方案，适合大规模可重复的实验与临床研究。


<details>
  <summary>Details</summary>
Motivation: 提供一种透明、低成本、可扩展且易在浏览器中部署的注视追踪解决方案，填补现有商业或闭源工具在可重复性、可扩展性和开放性方面的不足。

Method: 结合 MediaPipe 提取的人脸/眼部关键点与受 iTracker 启发的卷积神经网络架构，提出两种策略：基于移动数据的预训练模型微调以及从零开始使用桌面数据训练；支持轻量的用户特定微调以降低误差，并在任务中与 SeeSo SDK 比较。

Result: 在 MPIIFaceGaze 验证集上，预训练迁移和从头训练在校准前表现相当；用户特定的轻量微调持续降低预测误差。在 Dot-Probe 实验中，与 SeeSo SDK 在左右注视分配上高度一致，但时间上波动更大。

Conclusion: EyeTheia 是一个轻量、开源的基于摄像头的注视估计管道，适用于浏览器实验平台和实际认知/临床研究，能用普通笔记本摄像头实现实时注视追踪并提供可选用户微调。

Abstract: We introduce EyeTheia, a lightweight and open deep learning pipeline for webcam-based gaze estimation, designed for browser-based experimental platforms and real-world cognitive and clinical research. EyeTheia enables real-time gaze tracking using only a standard laptop webcam, combining MediaPipe-based landmark extraction with a convolutional neural network inspired by iTracker and optional user-specific fine-tuning. We investigate two complementary strategies: adapting a model pretrained on mobile data and training the same architecture from scratch on a desktop-oriented dataset. Validation results on MPIIFaceGaze show comparable performance between both approaches prior to calibration, while lightweight user-specific fine-tuning consistently reduces gaze prediction error. We further evaluate EyeTheia in a realistic Dot-Probe task and compare it to the commercial webcam-based tracker SeeSo SDK. Results indicate strong agreement in left-right gaze allocation during stimulus presentation, despite higher temporal variability. Overall, EyeTheia provides a transparent and extensible solution for low-cost gaze tracking, suitable for scalable and reproducible experimental and clinical studies. The code, trained models, and experimental materials are publicly available.

</details>


### [24] [NAS-GS: Noise-Aware Sonar Gaussian Splatting](https://arxiv.org/abs/2601.06285)
*Shida Xu,Jingqi Jiang,Jonatan Scharff Willners,Sen Wang*

Main category: cs.CV

TL;DR: NAS-GS通过双向splatting和GMM噪声建模，有效处理声纳影像特有噪声与无深度信息问题，提升渲染速度与重建精度，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 声纳影像具有复杂噪声和缺乏深度信息，传统基于点云或体积渲染的方法难以在真实海洋场景中兼顾速度与质量，需提出专门针对声纳成像物理与噪声特性的重建与合成方法。

Method: 提出Two-Ways Splatting以分别模拟强度累计和透射率计算的双向过程，从而优化渲染流程；引入基于高斯混合模型的噪声模型，描述侧瓣、散斑与多路径等复杂噪声，并将其融入训练与渲染过程以约束三维高斯学习。

Result: 在模拟和真实大规模离岸声纳数据集上均达到或超过现有方法的性能，在新视角合成和三维重建任务上表现优异，验证了Two-Ways Splatting和GMM噪声模型的有效性。

Conclusion: 本文提出的NAS-GS框架针对声纳影像的噪声特性与无高程信息问题，能在保持渲染质量的同时显著提升渲染速度，并通过GMM噪声建模抑制三维高斯对噪声的过拟合，提高重建精度。

Abstract: Underwater sonar imaging plays a crucial role in various applications, including autonomous navigation in murky water, marine archaeology, and environmental monitoring. However, the unique characteristics of sonar images, such as complex noise patterns and the lack of elevation information, pose significant challenges for 3D reconstruction and novel view synthesis. In this paper, we present NAS-GS, a novel Noise-Aware Sonar Gaussian Splatting framework specifically designed to address these challenges. Our approach introduces a Two-Ways Splatting technique that accurately models the dual directions for intensity accumulation and transmittance calculation inherent in sonar imaging, significantly improving rendering speed without sacrificing quality. Moreover, we propose a Gaussian Mixture Model (GMM) based noise model that captures complex sonar noise patterns, including side-lobes, speckle, and multi-path noise. This model enhances the realism of synthesized images while preventing 3D Gaussian overfitting to noise, thereby improving reconstruction accuracy. We demonstrate state-of-the-art performance on both simulated and real-world large-scale offshore sonar scenarios, achieving superior results in novel view synthesis and 3D reconstruction.

</details>


### [25] [Perception Test 2025: Challenge Summary and a Unified VQA Extension](https://arxiv.org/abs/2601.06287)
*Joseph Heyward,Nikhil Pathasarathy,Tyler Zhu,Aravindh Mahendran,João Carreira,Dima Damen,Andrew Zisserman,Viorica Pătrăucean*

Main category: cs.CV

TL;DR: Perception Test 2025通过任务合并与将传统感知任务重构为多项选择视频QA，强调了现有多模态模型在统一接口下的局限性，推动通用视频感知研究。


<details>
  <summary>Details</summary>
Motivation: 评估和推动多模态视频感知模型向通用化与统一接口方向发展，减少工程化的任务特定流水线，揭示模型在跨任务泛化与一致性处理上的不足。

Method: 组织五个合并后的竞赛赛道（统一视频QA、统一物体与点跟踪、统一动作与声音定位、接地视频QA与长时视频QA），并引入将传统感知任务重写为多项选择视频问答的子集，要求参赛者采用统一模型而非任务专用流水线；另外提供分析与可解释性赛道。

Result: 展示了任务统一后模型性能普遍下降或不稳定，表明现有SOTA在统一处理多种感知任务时能力有限；同时通过新子任务（多选题形式）验证了视频-语言模型在原本非QA任务上的应用潜力，但仍需改进。

Conclusion: Perception Test 2025通过统一任务接口强化了对多模态视频理解模型的基准测试，结果显示当前最先进模型在处理多样化感知任务的一致性和通用性方面仍存在显著挑战。

Abstract: The Third Perception Test challenge was organised as a full-day workshop alongside the IEEE/CVF International Conference on Computer Vision (ICCV) 2025. Its primary goal is to benchmark state-of-the-art video models and measure the progress in multimodal perception. This year, the workshop featured 2 guest tracks as well: KiVA (an image understanding challenge) and Physic-IQ (a video generation challenge). In this report, we summarise the results from the main Perception Test challenge, detailing both the existing tasks as well as novel additions to the benchmark. In this iteration, we placed an emphasis on task unification, as this poses a more challenging test for current SOTA multimodal models. The challenge included five consolidated tracks: unified video QA, unified object and point tracking, unified action and sound localisation, grounded video QA, and hour-long video QA, alongside an analysis and interpretability track that is still open for submissions. Notably, the unified video QA track introduced a novel subset that reformulates traditional perception tasks (such as point tracking and temporal action localisation) as multiple-choice video QA questions that video-language models can natively tackle. The unified object and point tracking merged the original object tracking and point tracking tasks, whereas the unified action and sound localisation merged the original temporal action localisation and temporal sound localisation tracks. Accordingly, we required competitors to use unified approaches rather than engineered pipelines with task-specific models. By proposing such a unified challenge, Perception Test 2025 highlights the significant difficulties existing models face when tackling diverse perception tasks through unified interfaces.

</details>


### [26] [VideoWeave: A Data-Centric Approach for Efficient Video Understanding](https://arxiv.org/abs/2601.06309)
*Zane Durante,Silky Singh,Arpandeep Khatua,Shobhit Agarwal,Reuben Tan,Yong Jae Lee,Jianfeng Gao,Ehsan Adeli,Li Fei-Fei*

Main category: cs.CV

TL;DR: VideoWeave通过拼接带字幕的短视频构造合成长上下文样本，在相同计算预算下比常规模型微调提高视频问答准确率，强调数据重组是可扩展的高效训练策略。


<details>
  <summary>Details</summary>
Motivation: 长视频标注稀缺且处理长帧序列计算昂贵，目标是通过数据重组来扩展时间多样性，从而在计算受限情况下提升模型对长上下文理解的能力。

Method: 在现有带文本短视频数据集中，按照不同策略（随机拼接或按视觉相似性聚类拼接，以及对字幕进行扩充）构造合成长视频—文本对，用这些样本在固定计算预算下训练模型，比较与常规微调的效果。

Result: 采用VideoWeave的数据构成策略训练的模型在视频问答任务上比传统视频微调方法取得更高准确率，表明仅通过重组训练数据即可获得显著收益。

Conclusion: VideoWeave通过合成长时序训练样本（将已有短视频拼接并配文）在不改动模型或优化目标下提高了视频-语言模型的数据利用效率，实验证明在相同计算预算下能提升视频问答性能。

Abstract: Training video-language models is often prohibitively expensive due to the high cost of processing long frame sequences and the limited availability of annotated long videos. We present VideoWeave, a simple yet effective approach to improve data efficiency by constructing synthetic long-context training samples that splice together short, captioned videos from existing datasets. Rather than modifying model architectures or optimization objectives, VideoWeave reorganizes available video-text pairs to expand temporal diversity within fixed compute. We systematically study how different data composition strategies like random versus visually clustered splicing and caption enrichment affect downstream performance on downstream video question answering. Under identical compute constraints, models trained with VideoWeave achieve higher accuracy than conventional video finetuning. Our results highlight that reorganizing training data, rather than altering architectures, may offer a simple and scalable path for training video-language models. We link our code for all experiments here.

</details>


### [27] [Object-WIPER : Training-Free Object and Associated Effect Removal in Videos](https://arxiv.org/abs/2601.06391)
*Saksham Singh Kushwaha,Sayan Nag,Yapeng Tian,Kuldeep Kulkarni*

Main category: cs.CV

TL;DR: Object-WIPER 是一个基于预训练 DiT 的无训练视频对象及视觉效果移除框架，通过token级反演与重初始化并保留背景token，实现语义一致且时间连贯的修补，且在新基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不经训练的情况下同时实现语义一致性与时间连贯的动态对象移除；因此作者希望借助强大的预训练DiT模型，在训练-free设置下完成高质量的视频对象去除和修补。

Method: 利用用户提供的对象掩码和查询token，通过视觉-文本交叉注意力和视觉自注意力定位相关视觉token，生成中间效果掩码并融合为最终前景token掩码；将视频通过DiT反演得到结构化噪声，对掩码区域用高斯噪声重初始化，保留背景token的反演值，在去噪过程中拷贝背景token以保持背景一致性。

Result: 在DAVIS和新构建的WIPER-Bench上，Object-WIPER 在作者提出的新评估指标上优于训练型和其它训练-free基线，能够实现干净的对象移除与时间稳定的重建。作者将公开基准、源码及预训练模型。

Conclusion: Object-WIPER 能在无需额外训练的情况下，利用预训练的文本到视频扩散Transformer（DiT）去除视频中的动态对象及其关联视觉效果，并以语义一致且时间上连贯的内容进行修补。该方法在反演视频后对前景token重初始化，并在去噪阶段保留背景token以维持场景一致性。

Abstract: In this paper, we introduce Object-WIPER, a training-free framework for removing dynamic objects and their associated visual effects from videos, and inpainting them with semantically consistent and temporally coherent content. Our approach leverages a pre-trained text-to-video diffusion transformer (DiT). Given an input video, a user-provided object mask, and query tokens describing the target object and its effects, we localize relevant visual tokens via visual-text cross-attention and visual self-attention. This produces an intermediate effect mask that we fuse with the user mask to obtain a final foreground token mask to replace. We first invert the video through the DiT to obtain structured noise, then reinitialize the masked tokens with Gaussian noise while preserving background tokens. During denoising, we copy values for the background tokens saved during inversion to maintain scene fidelity. To address the lack of suitable evaluation, we introduce a new object removal metric that rewards temporal consistency among foreground tokens across consecutive frames, coherence between foreground and background tokens within each frame, and dissimilarity between the input and output foreground tokens. Experiments on DAVIS and a newly curated real-world associated effect benchmark (WIPER-Bench) show that Object-WIPER surpasses both training-based and training-free baselines in terms of the metric, achieving clean removal and temporally stable reconstruction without any retraining. Our new benchmark, source code, and pre-trained models will be publicly available.

</details>


### [28] [Context Matters: Peer-Aware Student Behavioral Engagement Measurement via VLM Action Parsing and LLM Sequence Classification](https://arxiv.org/abs/2601.06394)
*Ahmed Abdelkawy,Ahmed Elsayed,Asem Ali,Aly Farag,Thomas Tretter,Michael McIntyre*

Main category: cs.CV

TL;DR: 提出一套少样本微调视觉-语言模型+时间窗口生成动作序列+大语言模型分类的三阶段框架，用少量标注和课堂上下文有效判断学生课堂参与度。


<details>
  <summary>Details</summary>
Motivation: 动机是现有参与度预测方法需要大量带标注数据且常忽略课堂背景（同伴行为），而隐私限制使得获取大量共享数据困难，因此提出一种在少样本和考虑课堂上下文情况下的可行方案。

Method: 方法包括：1) 使用少样本适配技术微调视觉-语言模型以识别学生动作类别；2) 将每个2分钟视频划分为不重叠时间片段，利用微调模型对每片段进行动作预测，构建动作序列；3) 使用大语言模型输入动作序列与课堂上下文（如同学行为）对学生进行二分类（参与/不参与）。

Result: 实验结果表明该方法在识别学生参与度方面有效，但摘要未给出详细的定量指标、基线比较或数据集细节。

Conclusion: 该论文提出了一个三阶段的视频驱动学生参与度测量框架，通过少样本微调视-觉-语言模型进行动作识别、滑动时间窗口生成动作序列，并借助大语言模型结合课堂上下文对行为序列进行参与度分类，从而在数据受限和隐私受限场景下有效识别学生参与度。

Abstract: Understanding student behavior in the classroom is essential to improve both pedagogical quality and student engagement. Existing methods for predicting student engagement typically require substantial annotated data to model the diversity of student behaviors, yet privacy concerns often restrict researchers to their own proprietary datasets. Moreover, the classroom context, represented in peers' actions, is ignored. To address the aforementioned limitation, we propose a novel three-stage framework for video-based student engagement measurement. First, we explore the few-shot adaptation of the vision-language model for student action recognition, which is fine-tuned to distinguish among action categories with a few training samples. Second, to handle continuous and unpredictable student actions, we utilize the sliding temporal window technique to divide each student's 2-minute-long video into non-overlapping segments. Each segment is assigned an action category via the fine-tuned VLM model, generating a sequence of action predictions. Finally, we leverage the large language model to classify this entire sequence of actions, together with the classroom context, as belonging to an engaged or disengaged student. The experimental results demonstrate the effectiveness of the proposed approach in identifying student engagement.

</details>


### [29] [GlobalPaint: Spatiotemporal Coherent Video Outpainting with Global Feature Guidance](https://arxiv.org/abs/2601.06413)
*Yueming Pan,Ruoyu Feng,Jianmin Bao,Chong Luo,Nanning Zheng*

Main category: cs.CV

TL;DR: GlobalPaint 用关键帧先行+插值完成中间帧的分层策略，结合3D窗口时空注意力与全局OpenCLIP特征指导，提升视频外扩的时空一致性与视觉质量。


<details>
  <summary>Details</summary>
Motivation: 视频外扩不仅需保证每帧空间合理性，还需长距离的时间一致性，尤其在摄像机或物体运动时，外扩区域会随时间显现，现有方法在时序一致性和误差累积上存在不足。

Method: 采用分层流水线：先对关键帧进行外扩，再用插值模型在已完成边界条件下生成中间帧；在模型上在预训练图像修复骨干上增加（i）含3D窗口注意力的增强时空模块用于更强的时空交互；（ii）全局特征引导，通过专门提取器将全部帧观测区域的OpenCLIP特征蒸馏为紧凑的全局tokens，作为条件输入。

Result: 在基准数据集上，GlobalPaint 在重构质量指标和运动自然性上均优于先前方法；消融显示3D窗口注意力与全局特征指导均有显著贡献。

Conclusion: GlobalPaint 在视频外扩任务上通过扩展图像修复的扩散模型并引入时空交互模块与全局特征指导，实现了更好的重建质量与运动一致性。

Abstract: Video outpainting extends a video beyond its original boundaries by synthesizing missing border content. Compared with image outpainting, it requires not only per-frame spatial plausibility but also long-range temporal coherence, especially when outpainted content becomes visible across time under camera or object motion. We propose GlobalPaint, a diffusion-based framework for spatiotemporal coherent video outpainting. Our approach adopts a hierarchical pipeline that first outpaints key frames and then completes intermediate frames via an interpolation model conditioned on the completed boundaries, reducing error accumulation in sequential processing. At the model level, we augment a pretrained image inpainting backbone with (i) an Enhanced Spatial-Temporal module featuring 3D windowed attention for stronger spatiotemporal interaction, and (ii) global feature guidance that distills OpenCLIP features from observed regions across all frames into compact global tokens using a dedicated extractor. Comprehensive evaluations on benchmark datasets demonstrate improved reconstruction quality and more natural motion compared to prior methods. Our demo page is https://yuemingpan.github.io/GlobalPaint/

</details>


### [30] [WHU-PCPR: A cross-platform heterogeneous point cloud dataset for place recognition in complex urban scenes](https://arxiv.org/abs/2601.06442)
*Xianghong Zou,Jianping Li,Yandi Yang,Weitong Wu,Yuan Wang,Qiegen Liu,Zhen Dong*

Main category: cs.CV

TL;DR: WHU-PCPR是一个跨平台异构点云地点识别数据集，包含车载与头盔式LiDAR采集、复杂长期变化场景与大规模轨迹，为PCPR方法的评估与发展提供了更现实的基准。


<details>
  <summary>Details</summary>
Motivation: 现有PCPR研究受限于数据集的单一性：场景、平台与传感器缺乏多样性，导致方法在跨平台与长期变化场景中泛化能力差。需要一个跨平台、长期、多场景的大规模数据集以推动研究。

Method: 通过使用多种设备采集点云数据（高精度车载MLS系统与低成本便携头盔式PLS系统，包含机械和固态LiDAR），覆盖城市与校园道路场景，跨越60个月、82.3公里轨迹并包含约30公里未重复路线，构建数据集；基于此数据集对若干代表性PCPR方法进行了大规模评估和深入分析。

Result: 发布WHU-PCPR数据集与基准代码，并通过实验展示现有方法在该数据集上的性能与局限，指出关键挑战（如跨平台点云分布差异、长期变化鲁棒性、低成本传感器噪声）并提出未来研究方向。

Conclusion: 本论文提出了WHU-PCPR，一个跨平台异构点云数据集，弥补了现有PCPR数据集在场景、平台、传感器多样性上的不足，为点云基于地点识别的研究提供了更具挑战性和现实意义的基准。

Abstract: Point Cloud-based Place Recognition (PCPR) demonstrates considerable potential in applications such as autonomous driving, robot localization and navigation, and map update. In practical applications, point clouds used for place recognition are often acquired from different platforms and LiDARs across varying scene. However, existing PCPR datasets lack diversity in scenes, platforms, and sensors, which limits the effective development of related research. To address this gap, we establish WHU-PCPR, a cross-platform heterogeneous point cloud dataset designed for place recognition. The dataset differentiates itself from existing datasets through its distinctive characteristics: 1) cross-platform heterogeneous point clouds: collected from survey-grade vehicle-mounted Mobile Laser Scanning (MLS) systems and low-cost Portable helmet-mounted Laser Scanning (PLS) systems, each equipped with distinct mechanical and solid-state LiDAR sensors. 2) Complex localization scenes: encompassing real-time and long-term changes in both urban and campus road scenes. 3) Large-scale spatial coverage: featuring 82.3 km of trajectory over a 60-month period and an unrepeated route of approximately 30 km. Based on WHU-PCPR, we conduct extensive evaluation and in-depth analysis of several representative PCPR methods, and provide a concise discussion of key challenges and future research directions. The dataset and benchmark code are available at https://github.com/zouxianghong/WHU-PCPR.

</details>


### [31] [How to Build Robust, Scalable Models for GSV-Based Indicators in Neighborhood Research](https://arxiv.org/abs/2601.06443)
*Xiaoya Tang,Xiaohe Yue,Heran Mane,Dapeng Li,Quynh Nguyen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 对比基础模型与自监督适配在GSV街景影像上的表现，发现自监督微调在有限标注情况下能显著提升下游任务效果，且选择合适的预训练模型与合理的训练规模可在计算受限条件下实现高效迁移。


<details>
  <summary>Details</summary>
Motivation: 旨在解决视觉模型从通用视觉域（如ImageNet）迁移到街景影像中泛化性差的问题，并为社会健康研究者在面对有限标注与计算资源时如何选择与适配基础模型提供可操作建议。

Method: 作者通过比较不同基础模型（如ImageNet预训练模型与更大、多源预训练的模型）、有监督微调与无监督（自监督）适配方法，结合不同训练规模与计算预算设置，在GSV数据集上进行系统实证评估；分析包括定量指标（如下游任务准确率/回归性能）和可视化特征表示的变化。

Result: 实验结果表明：1）无监督适配（自监督训练）通常能在下游任务中带来稳健提升，尤其在标签稀缺场景；2）模型选择影响显著，规模更大或预训练多样性更高的模型迁移性更好；3）训练规模与计算预算存在边际收益递减，适度的无监督训练即可获取大部分收益；4）可视化分析揭示了适配后特征对目标域语义更敏感。

Conclusion: 该论文表明在将大规模视觉基础模型应用于街景影像（如GSV）时，通过无监督适配可以显著提升下游任务性能，但效益受模型选择、训练规模和计算资源限制影响。实验显示，针对目标域进行自监督微调在有限标签设置下尤为有效，同时适当选择预训练模型（如更通用或在多样化数据上训练的模型）能提高迁移性。

Abstract: A substantial body of health research demonstrates a strong link between neighborhood environments and health outcomes. Recently, there has been increasing interest in leveraging advances in computer vision to enable large-scale, systematic characterization of neighborhood built environments. However, the generalizability of vision models across fundamentally different domains remains uncertain, for example, transferring knowledge from ImageNet to the distinct visual characteristics of Google Street View (GSV) imagery. In applied fields such as social health research, several critical questions arise: which models are most appropriate, whether to adopt unsupervised training strategies, what training scale is feasible under computational constraints, and how much such strategies benefit downstream performance. These decisions are often costly and require specialized expertise.
  In this paper, we answer these questions through empirical analysis and provide practical insights into how to select and adapt foundation models for datasets with limited size and labels, while leveraging larger, unlabeled datasets through unsupervised training. Our study includes comprehensive quantitative and visual analyses comparing model performance before and after unsupervised adaptation.

</details>


### [32] [Tone Matters: The Impact of Linguistic Tone on Hallucination in VLMs](https://arxiv.org/abs/2601.06460)
*Weihao Hong,Zhiyuan Jiang,Bingyu Shen,Xinlei Guan,Yangyi Feng,Meng Xu,Boyang Li*

Main category: cs.CV

TL;DR: 通过Ghost-100合成缺失场景与5级提示强度测试，作者发现提示压力对VLM幻觉的影响非单调，语义敌意更易被安全对齐抑制，而结构性强制仍能诱发模型特异性的幻觉失败。


<details>
  <summary>Details</summary>
Motivation: 当前对VLM幻觉的研究多关注对象存在/缺失判定，尚未充分刻画提示措辞和结构性约束如何系统性诱发幻觉；因此需可控合成数据与分级提示以分析提示压力影响。

Method: 构建了Ghost-100合成场景数据集，故意移除关键视觉细节以模拟缺失情形；提出5级提示强度框架（从中性询问到带毒性要求和严格格式约束）；在三个开源大模型（MiniCPM-V 2.6-8B、Qwen2-VL-7B、Qwen3-VL-8B）上以该框架和数据集系统评估缺失触发的幻觉率。

Result: 发现三款模型在提示强度提升下并非单调增加幻觉率：在不同阈值处高强度提示反而降低幻觉（但并非在最大强制下都保持下降）；说明模型对语义敌意的对齐较好但对格式/服从类压力存在短板，且不同模型表现各异。

Conclusion: 本文揭示了提示强度和格式压力会以复杂非单调的方式影响视觉-语言模型（VLM）的幻觉行为，表明现有安全对齐在应对语义敌意方面较为有效，但对结构性强制（格式/遵从性压力）检测不足。

Abstract: Vision-Language Models (VLMs) are increasingly used in safety-critical applications that require reliable visual grounding. However, these models often hallucinate details that are not present in the image to satisfy user prompts. While recent datasets and benchmarks have been introduced to evaluate systematic hallucinations in VLMs, many hallucination behaviors remain insufficiently characterized. In particular, prior work primarily focuses on object presence or absence, leaving it unclear how prompt phrasing and structural constraints can systematically induce hallucinations. In this paper, we investigate how different forms of prompt pressure influence hallucination behavior. We introduce Ghost-100, a procedurally generated dataset of synthetic scenes in which key visual details are deliberately removed, enabling controlled analysis of absence-based hallucinations. Using a structured 5-Level Prompt Intensity Framework, we vary prompts from neutral queries to toxic demands and rigid formatting constraints. We evaluate three representative open-weight VLMs: MiniCPM-V 2.6-8B, Qwen2-VL-7B, and Qwen3-VL-8B. Across all three models, hallucination rates do not increase monotonically with prompt intensity. All models exhibit reductions at higher intensity levels at different thresholds, though not all show sustained reduction under maximum coercion. These results suggest that current safety alignment is more effective at detecting semantic hostility than structural coercion, revealing model-specific limitations in handling compliance pressure. Our dataset is available at: https://github.com/bli1/tone-matters

</details>


### [33] [On the Adversarial Robustness of 3D Large Vision-Language Models](https://arxiv.org/abs/2601.06464)
*Chao Liu,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 研究首次系统探讨基于点云的3D VLM对抗鲁棒性，设计了Vision/Caption两类攻击，发现3D VLM在非目标攻击上脆弱，但对精确控制输出的目标攻击更坚韧，强调需要改进安全防护。


<details>
  <summary>Details</summary>
Motivation: 评估将3D视觉引入VLMs后是否会像2D VLMs那样降低模型对抗鲁棒性，尤其在安全敏感应用中需要了解潜在风险。

Method: 提出两类攻击：Vision Attack（扰动3D编码器与投影器输出的视觉token特征）与Caption Attack（直接操纵输出token序列），每类包含untargeted与targeted变体，以评估视觉-语言对齐鲁棒性与端到端系统脆弱性。

Result: 实验表明：在untargeted攻击下，3D VLMs较易产生错误或混乱输出；在targeted攻击下，比2D VLMs更难以被强制生成特定有害文本。

Conclusion: 3D VLMs对抗性脆弱性显著，在非目标攻击下易被扰动，但对目标攻击（强制产生特定有害输出）比2D VLM更鲁棒。需加强针对3D VLM的防护，特别是在安全关键场景。

Abstract: 3D Vision-Language Models (VLMs), such as PointLLM and GPT4Point, have shown strong reasoning and generalization abilities in 3D understanding tasks. However, their adversarial robustness remains largely unexplored. Prior work in 2D VLMs has shown that the integration of visual inputs significantly increases vulnerability to adversarial attacks, making these models easier to manipulate into generating toxic or misleading outputs. In this paper, we investigate whether incorporating 3D vision similarly compromises the robustness of 3D VLMs. To this end, we present the first systematic study of adversarial robustness in point-based 3D VLMs. We propose two complementary attack strategies: \textit{Vision Attack}, which perturbs the visual token features produced by the 3D encoder and projector to assess the robustness of vision-language alignment; and \textit{Caption Attack}, which directly manipulates output token sequences to evaluate end-to-end system robustness. Each attack includes both untargeted and targeted variants to measure general vulnerability and susceptibility to controlled manipulation. Our experiments reveal that 3D VLMs exhibit significant adversarial vulnerabilities under untargeted attacks, while demonstrating greater resilience against targeted attacks aimed at forcing specific harmful outputs, compared to their 2D counterparts. These findings highlight the importance of improving the adversarial robustness of 3D VLMs, especially as they are deployed in safety-critical applications.

</details>


### [34] [SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning](https://arxiv.org/abs/2601.06474)
*Chenxu Dang,Jie Wang,Guang Li,Zhiwen Hou,Zihan You,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: 提出SparseOccVLA：用稀疏占据查询连接视觉-语言模型与语义占据，结合LLM推理与Anchor-Diffusion规划，实现场景理解、占据预测与轨迹规划的统一，取得多项基准提升。


<details>
  <summary>Details</summary>
Motivation: Existing VLMs and semantic occupancy methods are complementary but hard to integrate due to token explosion and density; need a compact bridge to combine high-level language reasoning with fine-grained spatial occupancy.

Method: Designs a Sparse Occupancy Encoder to produce sparse occupancy queries that align into language space for LLM reasoning; introduces an LLM-guided Anchor-Diffusion Planner with decoupled anchor scoring and denoising and cross-model trajectory-condition fusion.

Result: 7% relative improvement in CIDEr on OmniDrive-nuScenes, +0.5 mIoU on Occ3D-nuScenes, and state-of-the-art open-loop planning on nuScenes.

Conclusion: SparseOccVLA successfully bridges VLMs and semantic occupancy by introducing sparse occupancy queries as a compact, informative interface, enabling unified scene understanding, occupancy forecasting, and trajectory planning with improved metrics.

Abstract: In autonomous driving, Vision Language Models (VLMs) excel at high-level reasoning , whereas semantic occupancy provides fine-grained details. Despite significant progress in individual fields, there is still no method that can effectively integrate both paradigms. Conventional VLMs struggle with token explosion and limited spatiotemporal reasoning, while semantic occupancy provides a unified, explicit spatial representation but is too dense to integrate efficiently with VLMs. To address these challenges and bridge the gap between VLMs and occupancy, we propose SparseOccVLA, a novel vision-language-action model that unifies scene understanding, occupancy forecasting, and trajectory planning powered by sparse occupancy queries. Starting with a lightweight Sparse Occupancy Encoder, SparseOccVLA generates compact yet highly informative sparse occupancy queries that serve as the single bridge between vision and language. These queries are aligned into the language space and reasoned by the LLM for unified scene understanding and future occupancy forecasting. Furthermore, we introduce an LLM-guided Anchor-Diffusion Planner featuring decoupled anchor scoring and denoising, as well as cross-model trajectory-condition fusion. SparseOccVLA achieves a 7% relative improvement in CIDEr over the state-of-the-art on OmniDrive-nuScenes, a 0.5 increase in mIoU score on Occ3D-nuScenes, and sets state-of-the-art open-loop planning metric on nuScenes benchmark, demonstrating its strong holistic capability.

</details>


### [35] [VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment](https://arxiv.org/abs/2601.06475)
*Kai Cheng,Ruoqi Wang,Qiong Luo*

Main category: cs.CV

TL;DR: VVTRec利用可见度引导的视觉与文本模态增强并结合VLMs，显著改善了稀疏射电干涉数据的成像效果，且计算代价低。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅利用单一模态的稀疏可见度进行重建，导致残留伪影和相关性建模不足。为更充分提取可见度信息并强调图像域输出质量，提出将可见度扩展为视觉与文本两种模态以丰富信息并利用VLMs的预训练知识。

Method: VVTRec首先将稀疏可见度（visibility）映射为图像形式与文本形式的特征表示，以增强空间与语义信息；然后将这些多模态特征与原始可见度联合用于重建网络，输出更清洁的图像；此外利用预训练的VLMs对生成结果进行训练-free的知识补充，以进一步提升成像质量。

Result: 实验证明VVTRec在保持较小计算开销下，能显著减少成像伪影、提升结构完整性与像素级精度，且使用VLMs能带来额外的零训练性能提升。

Conclusion: 本文提出的VVTRec通过将稀疏可见度数据同时转为视觉与文本特征，并引入视觉-语言模型（VLM）进行零训练增强，从而在保持计算开销低的前提下改善射电干涉测量成像质量。实验表明该方法在结构完整性和图像精度上均优于仅使用单模态可见度的传统重建方法。

Abstract: Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be transformed into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a multimodal radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is transformed into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting multimodal information without introducing excessive computational overhead.

</details>


### [36] [SRFlow: A Dataset and Regularization Model for High-Resolution Facial Optical Flow via Splatting Rasterization](https://arxiv.org/abs/2601.06479)
*JiaLin Zhang,Dong Li*

Main category: cs.CV

TL;DR: 引入高分辨率人脸光流数据集SRFlow与带掩码/梯度正则化的SRFlowNet，通过高斯splatting引导，显著降低EPE并提升微表情识别F1。


<details>
  <summary>Details</summary>
Motivation: 缺乏高分辨率人脸光流数据限制了面部运动分析进展，尤其在纹理稀少或重复区域存在噪声和错误，需新数据集与专门模型改进高分辨率皮肤运动捕捉。

Method: 构建高分辨率人脸光流数据集（SRFlow），并设计SRFlowNet模型；引入基于掩码和梯度（差分或Sobel算子）的正则化损失，利用高斯splatting光栅化引导以抑制高频噪声和大尺度误差。

Result: 使用SRFlow数据集训练可使多种光流模型的EPE最多降低42%（0.5081→0.2953）；SRFlowNet结合SRFlow在三个微表情数据集复合上F1值最多提升48%（0.4733→0.6947），证明数据集与方法有效。

Conclusion: 本文提出SRFlow数据集和SRFlowNet模型，显著提升高分辨率人脸光流估计与微表情识别性能。

Abstract: Facial optical flow supports a wide range of tasks in facial motion analysis. However, the lack of high-resolution facial optical flow datasets has hindered progress in this area. In this paper, we introduce Splatting Rasterization Flow (SRFlow), a high-resolution facial optical flow dataset, and Splatting Rasterization Guided FlowNet (SRFlowNet), a facial optical flow model with tailored regularization losses. These losses constrain flow predictions using masks and gradients computed via difference or Sobel operator. This effectively suppresses high-frequency noise and large-scale errors in texture-less or repetitive-pattern regions, enabling SRFlowNet to be the first model explicitly capable of capturing high-resolution skin motion guided by Gaussian splatting rasterization. Experiments show that training with the SRFlow dataset improves facial optical flow estimation across various optical flow models, reducing end-point error (EPE) by up to 42% (from 0.5081 to 0.2953). Furthermore, when coupled with the SRFlow dataset, SRFlowNet achieves up to a 48% improvement in F1-score (from 0.4733 to 0.6947) on a composite of three micro-expression datasets. These results demonstrate the value of advancing both facial optical flow estimation and micro-expression recognition.

</details>


### [37] [Learning Domain Agnostic Latent Embeddings of 3D Faces for Zero-shot Animal Expression Transfer](https://arxiv.org/abs/2601.06484)
*Yue Wang,Lawrence Amadi,Xiang Gao,Yazheng Chen,Yuanpeng Liu,Ning Lu,Xianfeng Gu*

Main category: cs.CV

TL;DR: 提出零样本框架，利用HKS/WKS与网格无关潜在表示分离身份与表情，仅用人类对训练并通过几何损失实现合理的跨物种表情迁移。


<details>
  <summary>Details</summary>
Motivation: 缩小人类与动物面部形状的几何差距，使得不依赖动物表情数据也能实现跨物种表情迁移。

Method: 结合HKS/WKS等固有几何描述子与与网格无关的潜在编码，分离并重组身份(ID)与表情(latent)表示。仅用人类表情对训练，通过Jacobian、顶点位置与Laplacian损失保证几何一致性。

Result: 在实验中实现了视觉上可信的跨物种表情迁移，表情解耦与重组有效，几何一致性损失提高了形变自然度。

Conclusion: 方法在无动物标签数据情况下，能将人类表情迁移到动物面部网格，结果在视觉上合理，但仍需进一步验证几何连贯性与泛化性。

Abstract: We present a zero-shot framework for transferring human facial expressions to 3D animal face meshes. Our method combines intrinsic geometric descriptors (HKS/WKS) with a mesh-agnostic latent embedding that disentangles facial identity and expression. The ID latent space captures species-independent facial structure, while the expression latent space encodes deformation patterns that generalize across humans and animals. Trained only with human expression pairs, the model learns the embeddings, decoupling, and recoupling of cross-identity expressions, enabling expression transfer without requiring animal expression data. To enforce geometric consistency, we employ Jacobian loss together with vertex-position and Laplacian losses. Experiments show that our approach achieves plausible cross-species expression transfer, effectively narrowing the geometric gap between human and animal facial shapes.

</details>


### [38] [3D CoCa v2: Contrastive Learners with Test-Time Search for Generalizable Spatial Intelligence](https://arxiv.org/abs/2601.06496)
*Hao Tang,Ting Huang,Zeyu Zhang*

Main category: cs.CV

TL;DR: 3D CoCa v2通过融合CLIP语义先验、空间感知编码和测试时搜索，实现了更强的3D场景描述泛化与鲁棒性，显著优于前代方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D字幕方法受点云稀疏不规则性和弱定位能力限制，且难以在室内外等不同分布环境间泛化，亟需一个能结合视觉-语言先验并提高推理鲁棒性的统一框架。

Method: 框架使用冻结的CLIP作为语义先验，采用空间感知的点云/场景编码器提取几何信息，并通过对比和字幕生成双重目标联合训练多模态解码器；推理阶段引入不更新参数的测试时搜索（TTS），生成多样字幕候选并用基于场景摘要的奖励函数选择最优描述，避免依赖外部检测器或手工提议。

Result: 在多个数据集上取得提升：相比3D CoCa在ScanRefer上CIDEr@0.5IoU提升+1.50，在Nr3D上+1.61；在零样本OOD评估TOD3Cap上CIDEr@0.25提升+3.8。

Conclusion: 本文提出了3D CoCa v2，一种结合对比视觉-语言学习与3D描述生成的通用3D字幕框架，通过冻结的CLIP语义先验、空间感知的3D编码器和联合优化的多模态解码器，提升了跨环境的泛化能力和弱监督定位能力。

Abstract: Spatial intelligence refers to the ability to perceive, reason about, and describe objects and their relationships within three-dimensional environments, forming a foundation for embodied perception and scene understanding. 3D captioning aims to describe 3D scenes in natural language; however, it remains challenging due to the sparsity and irregularity of point clouds and, more critically, the weak grounding and limited out-of-distribution (OOD) generalization of existing captioners across drastically different environments, including indoor and outdoor 3D scenes. To address this challenge, we propose 3D CoCa v2, a generalizable 3D captioning framework that unifies contrastive vision-language learning with 3D caption generation and further improves robustness via test-time search (TTS) without updating the captioner parameters. 3D CoCa v2 builds on a frozen CLIP-based semantic prior, a spatially-aware 3D scene encoder for geometry, and a multimodal decoder jointly optimized with contrastive and captioning objectives, avoiding external detectors or handcrafted proposals. At inference, TTS produces diverse caption candidates and performs reward-guided selection using a compact scene summary. Experiments show improvements over 3D CoCa of +1.50 CIDEr@0.5IoU on ScanRefer and +1.61 CIDEr@0.5IoU on Nr3D, and +3.8 CIDEr@0.25 in zero-shot OOD evaluation on TOD3Cap. Code will be released at https://github.com/AIGeeksGroup/3DCoCav2.

</details>


### [39] [Bridging Robustness and Efficiency: Real-Time Low-Light Enhancement via Attention U-Net GAN](https://arxiv.org/abs/2601.06518)
*Yash Thesia,Meera Suthar*

Main category: cs.CV

TL;DR: 通过将Attention Gate嵌入轻量U-Net并采用条件GAN训练，论文在单次前向传播中接近生成模型的纹理恢复能力，达到0.06s延迟和LPIPS 0.112，兼顾质量与速度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型虽在感知质量上优秀但推理极慢，传统CNN虽实时但过度平滑，缺乏在边缘设备可用且能恢复细节的模型。

Method: 在轻量U-Net骨干中集成Attention Gates，并在条件对抗训练框架下训练，从而在单次前向传播中恢复高频纹理；对比实验采用SID数据集并与SID、EnlightenGAN及扩散模型对比。

Result: 在SID数据集上取得了高效模型中最佳LPIPS 0.112，推理延迟0.06s，比潜在扩散模型快约40倍，优于高效基线如SID与EnlightenGAN。

Conclusion: 提出的混合Attention U-Net GAN在保留生成模型纹理恢复能力的同时实现了边缘部署级别的速度，表明扩散模型的迭代采样并非恢复高频细节的必要条件。

Abstract: Recent advancements in Low-Light Image Enhancement (LLIE) have focused heavily on Diffusion Probabilistic Models, which achieve high perceptual quality but suffer from significant computational latency (often exceeding 2-4 seconds per image). Conversely, traditional CNN-based baselines offer real-time inference but struggle with "over-smoothing," failing to recover fine structural details in extreme low-light conditions. This creates a practical gap in the literature: the lack of a model that provides generative-level texture recovery at edge-deployable speeds. In this paper, we address this trade-off by proposing a hybrid Attention U-Net GAN. We demonstrate that the heavy iterative sampling of diffusion models is not strictly necessary for texture recovery. Instead, by integrating Attention Gates into a lightweight U-Net backbone and training within a conditional adversarial framework, we can approximate the high-frequency fidelity of generative models in a single forward pass. Extensive experiments on the SID dataset show that our method achieves a best-in-class LPIPS score of 0.112 among efficient models, significantly outperforming efficient baselines (SID, EnlightenGAN) while maintaining an inference latency of 0.06s. This represents a 40x speedup over latent diffusion models, making our approach suitable for near real-time applications.

</details>


### [40] [BabyVision: Visual Reasoning Beyond Language](https://arxiv.org/abs/2601.06521)
*Liang Chen,Weichu Xie,Yiyan Liang,Hongfeng He,Hans Zhao,Zhibo Yang,Zhiqi Huang,Haoning Wu,Haoyu Lu,Y. charles,Yiping Bao,Yuantao Fan,Guopeng Li,Haiyang Shen,Xuanzhong Chen,Wendong Xu,Shuzheng Si,Zefan Cai,Wenhao Chai,Ziqi Huang,Fangfu Liu,Tianyu Liu,Baobao Chang,Xiaobo Hu,Kaiyuan Chen,Yixin Ren,Yang Liu,Yuan Gong,Kuan Li*

Main category: cs.CV

TL;DR: BabyVision exposes that leading MLLMs lack basic visual primitives compared to humans; benchmark and tools provided to drive progress


<details>
  <summary>Details</summary>
Motivation: State-of-the-art MLLMs rely on linguistic priors and fail on basic visual tasks that even young children can solve; need a benchmark measuring core visual skills without language dependence

Method: Introduce BabyVision benchmark and evaluate MLLMs' core visual abilities independent of language priors

Result: Created BabyVision with 388 items across 22 subclasses and 4 categories; found MLLMs perform far worse than humans (e.g., Gemini3-Pro-Preview scored 49.7 vs adult 94.1); proposed BabyVision-Gen and automatic evaluation toolkit; released code/data

Conclusion: Despite strong language-driven abilities, MLLMs still lack fundamental visual perception; BabyVision offers a path to evaluate and improve visual reasoning in MLLMs

Abstract: While humans develop core visual skills long before acquiring language, contemporary Multimodal LLMs (MLLMs) still rely heavily on linguistic priors to compensate for their fragile visual understanding. We uncovered a crucial fact: state-of-the-art MLLMs consistently fail on basic visual tasks that humans, even 3-year-olds, can solve effortlessly. To systematically investigate this gap, we introduce BabyVision, a benchmark designed to assess core visual abilities independent of linguistic knowledge for MLLMs. BabyVision spans a wide range of tasks, with 388 items divided into 22 subclasses across four key categories. Empirical results and human evaluation reveal that leading MLLMs perform significantly below human baselines. Gemini3-Pro-Preview scores 49.7, lagging behind 6-year-old humans and falling well behind the average adult score of 94.1. These results show despite excelling in knowledge-heavy evaluations, current MLLMs still lack fundamental visual primitives. Progress in BabyVision represents a step toward human-level visual perception and reasoning capabilities. We also explore solving visual reasoning with generation models by proposing BabyVision-Gen and automatic evaluation toolkit. Our code and benchmark data are released at https://github.com/UniPat-AI/BabyVision for reproduction.

</details>


### [41] [Toward Generalizable Deblurring: Leveraging Massive Blur Priors with Linear Attention for Real-World Scenarios](https://arxiv.org/abs/2601.06525)
*Yuanting Gao,Shuo Cao,Xiaohui Li,Yuandong Pu,Yihao Liu,Kai Zhang*

Main category: cs.CV

TL;DR: 通过在模拟数据上预训练丰富模糊模式并引入运动与语义引导，GLOWDeblur实现了在多基准上的鲁棒泛化与轻量高效的真实场景去模糊。


<details>
  <summary>Details</summary>
Motivation: 现有去模糊方法在现实场景上泛化差，原因在于数据集在真实感与模糊模式覆盖间的权衡，以及算法设计更多关注像素级细节而忽视结构和语义一致性，扩散方法在窄数据上也难以泛化。

Method: 通过在仿真数据上预训练以学习丰富的模糊模式（BPP），并通过与真实数据的联合微调迁移先验；在严重退化场景引入运动与语义引导（MoSeG）以增强先验；整体架构GLOWDeblur结合卷积预重建与域对齐模块，加上轻量级扩散骨干实现高效去模糊。

Result: 在六个常用基准和两个真实数据集上进行广泛实验，结果表明引入模糊模式先验和MoSeG能显著提高泛化性能，且GLOWDeblur在保持轻量的同时具有实际应用价值。

Conclusion: 本文提出的Blur Pattern Pretraining (BPP)和Motion and Semantic Guidance (MoSeG)显著提升了去模糊算法在真实场景下的泛化能力，并结合轻量扩散网络的GLOWDeblur在多数据集上验证了方法有效性。

Abstract: Image deblurring has advanced rapidly with deep learning, yet most methods exhibit poor generalization beyond their training datasets, with performance dropping significantly in real-world scenarios. Our analysis shows this limitation stems from two factors: datasets face an inherent trade-off between realism and coverage of diverse blur patterns, and algorithmic designs remain restrictive, as pixel-wise losses drive models toward local detail recovery while overlooking structural and semantic consistency, whereas diffusion-based approaches, though perceptually strong, still fail to generalize when trained on narrow datasets with simplistic strategies. Through systematic investigation, we identify blur pattern diversity as the decisive factor for robust generalization and propose Blur Pattern Pretraining (BPP), which acquires blur priors from simulation datasets and transfers them through joint fine-tuning on real data. We further introduce Motion and Semantic Guidance (MoSeG) to strengthen blur priors under severe degradation, and integrate it into GLOWDeblur, a Generalizable reaL-wOrld lightWeight Deblur model that combines convolution-based pre-reconstruction & domain alignment module with a lightweight diffusion backbone. Extensive experiments on six widely-used benchmarks and two real-world datasets validate our approach, confirming the importance of blur priors for robust generalization and demonstrating that the lightweight design of GLOWDeblur ensures practicality in real-world applications. The project page is available at https://vegdog007.github.io/GLOWDeblur_Website/.

</details>


### [42] [Towards Egocentric 3D Hand Pose Estimation in Unseen Domains](https://arxiv.org/abs/2601.06537)
*Wiktor Mucha,Michael Wray,Martin Kampel*

Main category: cs.CV

TL;DR: 通过在归一化虚拟相机空间预测深度并在测试时进行自监督3D一致性优化，V-HPOT显著提升了第一视角跨域3D手部姿态估计的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在同域有优异表现，但因训练数据、深度信息受限且过拟合于特定相机内参，导致对新环境泛化能力差。提出虚拟相机归一化以消除相机内参影响并在测试时自适应目标域。

Method: 在归一化的虚拟相机坐标系（以焦距和图像尺寸归一化）中预测关键点深度；利用相机内参不变性，在测试时进行无监督的3D一致性损失优化，按空间尺度变换预测的手部姿态以自适应目标域。

Result: 在跨域评估上显著提升：在H2O数据集上平均姿态误差减少71%，在AssemblyHands上减少41%；在单阶段方法中表现最优，与两阶段方法接近，同时所需训练数据少约3.5至14倍。

Conclusion: V-HPOT通过在虚拟相机空间中估计关键点的z坐标并在推理时采用自监督测试时优化，有效提升了从第一视角图像的跨域3D手部姿态估计的深度泛化能力。

Abstract: We present V-HPOT, a novel approach for improving the cross-domain performance of 3D hand pose estimation from egocentric images across diverse, unseen domains. State-of-the-art methods demonstrate strong performance when trained and tested within the same domain. However, they struggle to generalise to new environments due to limited training data and depth perception -- overfitting to specific camera intrinsics. Our method addresses this by estimating keypoint z-coordinates in a virtual camera space, normalised by focal length and image size, enabling camera-agnostic depth prediction. We further leverage this invariance to camera intrinsics to propose a self-supervised test-time optimisation strategy that refines the model's depth perception during inference. This is achieved by applying a 3D consistency loss between predicted and in-space scale-transformed hand poses, allowing the model to adapt to target domain characteristics without requiring ground truth annotations. V-HPOT significantly improves 3D hand pose estimation performance in cross-domain scenarios, achieving a 71% reduction in mean pose error on the H2O dataset and a 41% reduction on the AssemblyHands dataset. Compared to state-of-the-art methods, V-HPOT outperforms all single-stage approaches across all datasets and competes closely with two-stage methods, despite needing approximately x3.5 to x14 less data.

</details>


### [43] [LLMTrack: Semantic Multi-Object Tracking with Multi-modal Large Language Models](https://arxiv.org/abs/2601.06550)
*Pan Liao,Feng Yang,Di Wu,Jinwen Yu,Yuhua Zhu,Wenhui Zhao*

Main category: cs.CV

TL;DR: LLMTrack把“看”与“理解”分离：用检测器做定位，用多模态LLM做语义理解，通过时空融合与三阶段微调实现对目标描述、交互识别与视频摘要的SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统MOT擅长几何定位与数据关联，但缺乏语义理解（what、why）。为实现更认知化的跟踪，需要把检测器与多模态LLM结合，使系统既能精确跟踪又能理解行为与语义。

Method: 框架包括（1）使用Grounding DINO负责强定位；（2）基于LLaVA-OneVision的大模态LLM负责语义理解；（3）时空融合模块聚合实例交互特征与视频级上下文；（4）三阶段训练：视觉对齐、时间微调与通过LoRA注入语义能力。

Result: 提出了一种将大模型用于语义多目标跟踪的新框架LLMTrack，将目标检测（Grounding DINO）与多模态大模型（LLaVA-OneVision）结合，通过时空融合模块让LLM理解轨迹，并用三阶段训练策略适配模型，实验在BenSMOT上表现优异。

Conclusion: LLMTrack在保持跟踪稳定性的同时，显著提升了实例描述、交互识别和视频总结的语义能力，证明将大模型引入MOT能有效弥补传统方法在语义理解方面的不足。

Abstract: Traditional Multi-Object Tracking (MOT) systems have achieved remarkable precision in localization and association, effectively answering \textit{where} and \textit{who}. However, they often function as autistic observers, capable of tracing geometric paths but blind to the semantic \textit{what} and \textit{why} behind object behaviors. To bridge the gap between geometric perception and cognitive reasoning, we propose \textbf{LLMTrack}, a novel end-to-end framework for Semantic Multi-Object Tracking (SMOT). We adopt a bionic design philosophy that decouples strong localization from deep understanding, utilizing Grounding DINO as the eyes and the LLaVA-OneVision multimodal large model as the brain. We introduce a Spatio-Temporal Fusion Module that aggregates instance-level interaction features and video-level contexts, enabling the Large Language Model (LLM) to comprehend complex trajectories. Furthermore, we design a progressive three-stage training strategy, Visual Alignment, Temporal Fine-tuning, and Semantic Injection via LoRA to efficiently adapt the massive model to the tracking domain. Extensive experiments on the BenSMOT benchmark demonstrate that LLMTrack achieves state-of-the-art performance, significantly outperforming existing methods in instance description, interaction recognition, and video summarization while maintaining robust tracking stability.

</details>


### [44] [ArrowGEV: Grounding Events in Video via Learning the Arrow of Time](https://arxiv.org/abs/2601.06559)
*Fangxu Yu,Ziyao Lu,Liqiang Niu,Fandong Meng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出ArrowGEV，用强化学习区分时间敏感与不敏感事件，通过方向性奖励改进事件定位和时间方向识别，显著提升VLM视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在事件定位上仅学习正向视频对应，忽略事件的时间结构和方向性，导致鲁棒性和泛化能力不足。受物理学“时间之箭”概念启发，提出显式建模时间方向性的方案。

Method: 将事件分为时间敏感和时间不敏感两类；对时间敏感事件设计奖励以促使模型区分正向/反向视频；对时间不敏感事件则约束正反向定位一致性；使用VLM作为基础模型，在强化学习框架中优化奖励。

Result: 在大量实验中，ArrowGEV提升了定位精度、时间方向性识别能力，同时推进了整体视频理解和推理性能。

Conclusion: ArrowGEV通过强化学习显式建模事件的时间方向性，能够提升事件定位和时间方向识别能力，并增强视频理解与推理。

Abstract: Grounding events in videos serves as a fundamental capability in video analysis. While Vision-Language Models (VLMs) are increasingly employed for this task, existing approaches predominantly train models to associate events with timestamps in the forward video only. This paradigm hinders VLMs from capturing the inherent temporal structure and directionality of events, thereby limiting robustness and generalization. To address this limitation, inspired by the arrow of time in physics, which characterizes the intrinsic directionality of temporal processes, we propose ArrowGEV, a reinforcement learning framework that explicitly models temporal directionality in events to improve both event grounding and temporal directionality understanding in VLMs. Specifically, we categorize events into time-sensitive (e.g., putting down a bag) and time-insensitive (e.g., holding a towel in the left hand). The former denote events whose reversal substantially alters their meaning, while the latter remain semantically unchanged under reversal. For time-sensitive events, ArrowGEV introduces a reward that encourages VLMs to discriminate between forward and backward videos, whereas for time-insensitive events, it enforces consistent grounding across both directions. Extensive experiments demonstrate that ArrowGEV not only improves grounding precision and temporal directionality recognition, but also enhances general video understanding and reasoning ability.

</details>


### [45] [QCaption: Video Captioning and Q&A through Fusion of Large Multimodal Models](https://arxiv.org/abs/2601.06566)
*Jiale Wang,Gee Wah Ng,Lee Onn Mak,Randall Cher,Ng Ding Hei Ryan,Davis Wang*

Main category: cs.CV

TL;DR: 提出可本地部署的QCaption管线，通过关键帧+LMM+LLM融合显著提高视频字幕与QA性能，消融与基准比较支持其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕与多模态QA模型在性能与可部署性方面存在局限，提出通过模型融合提升跨模态理解并实现本地化部署。

Method: 从视频中抽取关键帧，使用LMM进行图像-文本多模态分析，随后用LLM进行文本层面的推理与融合，最终生成视频字幕与回答。进行了消融实验评估各模块贡献，并与其他方法进行基准比较。

Result: QCaption在视频字幕和QA任务上分别带来最多44.2%与48.9%的性能提升；消融实验展示了LLM在融合阶段的重要性，并提出并比较了额外的字幕生成方法。

Conclusion: QCaption通过融合关键帧提取、LMM与LLM，提出了一种自包含、可本地部署的视频字幕与问答管线，显著提升任务性能并验证了模型融合的有效性。

Abstract: This paper introduces QCaption, a novel video captioning and Q&A pipeline that enhances video analytics by fusing three models: key frame extraction, a Large Multimodal Model (LMM) for image-text analysis, and a Large Language Model (LLM) for text analysis. This approach enables integrated analysis of text, images, and video, achieving performance improvements over existing video captioning and Q&A models; all while remaining fully self-contained, adept for on-premises deployment. Experimental results using QCaption demonstrated up to 44.2% and 48.9% improvements in video captioning and Q&A tasks, respectively. Ablation studies were also performed to assess the role of LLM on the fusion on the results. Moreover, the paper proposes and evaluates additional video captioning approaches, benchmarking them against QCaption and existing methodologies. QCaption demonstrate the potential of adopting a model fusion approach in advancing video analytics.

</details>


### [46] [APEX: Learning Adaptive Priorities for Multi-Objective Alignment in Vision-Language Generation](https://arxiv.org/abs/2601.06574)
*Dongliang Chen,Xinlin Zhuang,Junjie Xu,Luojian Xie,Zehui Wang,Jiaxi Zhuang,Haolin Yang,Liang Dou,Xiao He,Xingjiao Wu,Ying Qian*

Main category: cs.CV

TL;DR: APEX通过双阶段归一化与基于潜力/冲突/进展的动态优先级调度，解决方差劫持与梯度冲突问题，在Stable Diffusion 3.5上实现更平衡的多目标文本到图像对齐改进。


<details>
  <summary>Details</summary>
Motivation: 静态线性标量化在异质奖励下常失败，导致模型偏向高方差、高响应性的目标（如OCR），忽视感知质量等目标。需要一种能稳定多目标训练并平衡权重的方法。

Method: 提出Dual-Stage Adaptive Normalization对不同奖励进行两阶段标准化以抑制方差劫持，并设计P^3 Adaptive Priorities基于学习潜力、冲突惩罚和进展需求动态调度目标权重。

Result: 在Stable Diffusion 3.5上，APEX在四个异质目标上取得更优Pareto权衡：PickScore+1.31、DeQA+0.35、Aesthetics+0.53，同时保持OCR准确性竞争力，缓解多目标对齐不稳定性。

Conclusion: APEX通过自适应归一化与优先级调度，降低异质奖励的不稳定性，从而实现更平衡的多目标图像生成对齐。

Abstract: Multi-objective alignment for text-to-image generation is commonly implemented via static linear scalarization, but fixed weights often fail under heterogeneous rewards, leading to optimization imbalance where models overfit high-variance, high-responsiveness objectives (e.g., OCR) while under-optimizing perceptual goals. We identify two mechanistic causes: variance hijacking, where reward dispersion induces implicit reweighting that dominates the normalized training signal, and gradient conflicts, where competing objectives produce opposing update directions and trigger seesaw-like oscillations. We propose APEX (Adaptive Priority-based Efficient X-objective Alignment), which stabilizes heterogeneous rewards with Dual-Stage Adaptive Normalization and dynamically schedules objectives via P^3 Adaptive Priorities that combine learning potential, conflict penalty, and progress need. On Stable Diffusion 3.5, APEX achieves improved Pareto trade-offs across four heterogeneous objectives, with balanced gains of +1.31 PickScore, +0.35 DeQA, and +0.53 Aesthetics while maintaining competitive OCR accuracy, mitigating the instability of multi-objective alignment.

</details>


### [47] [Sissi: Zero-shot Style-guided Image Synthesis via Semantic-style Integration](https://arxiv.org/abs/2601.06605)
*Yingying Deng,Xiangyu He,Fan Tang,Weiming Dong,Xucheng Yin*

Main category: cs.CV

TL;DR: 本文提出一种训练-free的ReFlow-based修复框架，结合参考风格图与掩码目标图，通过DSSI动态重权多模态注意力，实现高保真且语义一致的风格引导图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本与视觉示例的风格化方法通常依赖任务特定的再训练或昂贵的逆向优化（inversion），这会损害内容完整性、降低风格保真度，并在语义提示遵从性与风格一致性间产生不理想权衡。作者希望设计一个训练Free、避免反演且能在语义保持与风格对齐之间取得更好平衡的方法。

Method: 方法包括：1) 将风格引导重构为一个上下文学习任务，输入为参考风格图和被掩码的目标图；2) 使用预训练ReFlow-based inpainting模型，该模型通过多模态注意力融合文本语义和视觉风格信息；3) 提出DSSI机制，根据语义与风格的重要性和噪声水平动态调整两种模态的注意力权重，从而平衡语义保持与风格迁移；4) 全程无需额外训练或昂贵逆向过程，保持内容完整性并减少伪影。

Result: 实验表明，该方法在保持语义一致性的同时实现高保真风格化，较先前方法在语义-风格平衡和视觉质量上具有优势，同时避免了复杂或易产生伪影的过程。

Conclusion: 该论文提出了一种无需训练的风格引导图像生成框架，通过将任务视为上下文学习，利用预训练的ReFlow基于修复（inpainting）模型，将参考风格图与带掩码的目标图拼接输入，并通过多模态注意力融合实现语义内容与风格的无缝整合。为解决多模态注意力融合中的不平衡与噪声敏感问题，提出了动态语义-风格整合（DSSI）机制，对文本语义与视觉风格注意力进行重权，缓解引导冲突，提升输出一致性。

Abstract: Text-guided image generation has advanced rapidly with large-scale diffusion models, yet achieving precise stylization with visual exemplars remains difficult. Existing approaches often depend on task-specific retraining or expensive inversion procedures, which can compromise content integrity, reduce style fidelity, and lead to an unsatisfactory trade-off between semantic prompt adherence and style alignment. In this work, we introduce a training-free framework that reformulates style-guided synthesis as an in-context learning task. Guided by textual semantic prompts, our method concatenates a reference style image with a masked target image, leveraging a pretrained ReFlow-based inpainting model to seamlessly integrate semantic content with the desired style through multimodal attention fusion. We further analyze the imbalance and noise sensitivity inherent in multimodal attention fusion and propose a Dynamic Semantic-Style Integration (DSSI) mechanism that reweights attention between textual semantic and style visual tokens, effectively resolving guidance conflicts and enhancing output coherence. Experiments show that our approach achieves high-fidelity stylization with superior semantic-style balance and visual quality, offering a simple yet powerful alternative to complex, artifact-prone prior methods.

</details>


### [48] [Boosting Overlapping Organoid Instance Segmentation Using Pseudo-Label Unmixing and Synthesis-Assisted Learning](https://arxiv.org/abs/2601.06642)
*Gui Huang,Kangyuan Zheng,Xuan Cai,Jiaqi Wang,Jianjia Zhang,Kaida Ning,Wenbo Wei,Yujuan Zhu,Jiong Zhang,Mengting Liu*

Main category: cs.CV

TL;DR: 本文首次将合成辅助半监督学习应用于organoid实例分割，提出伪标签解混（PLU）与轮廓驱动合成及实例级增强，有效处理重叠伪标签噪声，在仅10%标注下实现接近全监督的性能并刷新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习在器官类实例分割上受限于标注稀缺和显著的实例重叠，常产生噪声伪标签，尤其无法正确分离交叠的器官样本，导致性能下降。作者旨在通过合成辅助的半监督框架解决重叠带来的伪标签偏差问题。

Method: 提出PLU模块识别并分解错误伪标签以重建重叠器官类实例；采用基于轮廓的合成策略高效生成重叠实例图像；在伪标签上施加实例级增强（IA）后再用于图像合成，并将合成数据纳入训练（SA-SSL框架扩展）。

Result: 在两个器官类数据集上进行严格实验，使用仅10%标注数据即可达到与全监督相当的性能，并实现了最先进的结果。消融实验证实了PLU、轮廓合成和增强感知训练各自的贡献。

Conclusion: 该论文提出了针对organoid实例分割的伪标签解混（PLU）方法，并结合轮廓驱动的合成与实例级增强，有效缓解了半监督学习中由于重叠导致的伪标签噪声，引导合成数据提升模型性能。

Abstract: Organoids, sophisticated in vitro models of human tissues, are crucial for medical research due to their ability to simulate organ functions and assess drug responses accurately. Accurate organoid instance segmentation is critical for quantifying their dynamic behaviors, yet remains profoundly limited by high-quality annotated datasets and pervasive overlap in microscopy imaging. While semi-supervised learning (SSL) offers a solution to alleviate reliance on scarce labeled data, conventional SSL frameworks suffer from biases induced by noisy pseudo-labels, particularly in overlapping regions. Synthesis-assisted SSL (SA-SSL) has been proposed for mitigating training biases in semi-supervised semantic segmentation. We present the first adaptation of SA-SSL to organoid instance segmentation and reveal that SA-SSL struggles to disentangle intertwined organoids, often misrepresenting overlapping instances as a single entity. To overcome this, we propose Pseudo-Label Unmixing (PLU), which identifies erroneous pseudo-labels for overlapping instances and then regenerates organoid labels through instance decomposition. For image synthesis, we apply a contour-based approach to synthesize organoid instances efficiently, particularly for overlapping cases. Instance-level augmentations (IA) on pseudo-labels before image synthesis further enhances the effect of synthetic data (SD). Rigorous experiments on two organoid datasets demonstrate our method's effectiveness, achieving performance comparable to fully supervised models using only 10% labeled data, and state-of-the-art results. Ablation studies validate the contributions of PLU, contour-based synthesis, and augmentation-aware training. By addressing overlap at both pseudo-label and synthesis levels, our work advances scalable, label-efficient organoid analysis, unlocking new potential for high-throughput applications in precision medicine.

</details>


### [49] [eSkiTB: A Synthetic Event-based Dataset for Tracking Skiers](https://arxiv.org/abs/2601.06647)
*Krishna Vinod,Joseph Raj Vishal,Kaustav Chanda,Prithvi Jai Ramesh,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: eSkiTB通过视频到事件的等信息合成，建立了首个冬季运动事件跟踪基准；基准实验显示事件Transformer在有静态广播覆盖的场景里显著优于RGB。


<details>
  <summary>Details</summary>
Motivation: RGB广播视频中快速运动滑雪选手常被运动模糊、静态叠加物和背景杂乱遮挡，事件相机的时间对比特性可天然抵抗这些伪影，但缺乏受控的冬季运动事件跟踪基准。

Method: 从现有SkiTB视频通过直接视频到事件（video-to-event）转换生成合成事件数据集eSkiTB，保持模态信息量等价；使用两个Transformer基模型进行基准测试：SDTrack（脉冲/事件型Transformer）与STARK（RGB Transformer）。

Result: 在含大量静态覆盖的场景中，SDTrack在eSkiTB上达到0.685 IoU，较RGB方法高20.0个百分点；在全集上平均IoU为0.711，表明时间对比信号在弹道运动跟踪中可靠。数据集与代码将开源。

Conclusion: 该工作证明了事件相机在有大量静态广播覆盖与视觉杂乱的滑雪场景中对快速运动目标跟踪有显著优势；在受覆盖主导的场景中，事件跟踪器优于RGB跟踪器约20个百分点IoU。

Abstract: Tracking skiers in RGB broadcast footage is challenging due to motion blur, static overlays, and clutter that obscure the fast-moving athlete. Event cameras, with their asynchronous contrast sensing, offer natural robustness to such artifacts, yet a controlled benchmark for winter-sport tracking has been missing. We introduce event SkiTB (eSkiTB), a synthetic event-based ski tracking dataset generated from SkiTB using direct video-to-event conversion without neural interpolation, enabling an iso-informational comparison between RGB and event modalities. Benchmarking SDTrack (spiking transformer) against STARK (RGB transformer), we find that event-based tracking is substantially resilient to broadcast clutter in scenes dominated by static overlays, achieving 0.685 IoU, outperforming RGB by +20.0 points. Across the dataset, SDTrack attains a mean IoU of 0.711, demonstrating that temporal contrast is a reliable cue for tracking ballistic motion in visually congested environments. eSkiTB establishes the first controlled setting for event-based tracking in winter sports and highlights the promise of event cameras for ski tracking. The dataset and code will be released at https://github.com/eventbasedvision/eSkiTB.

</details>


### [50] [Quantification and Classification of Carbon Nanotubes in Electron Micrographs using Vision Foundation Models](https://arxiv.org/abs/2601.06673)
*Sanjay Pradeep,Chen Wang,Matthew M. Dahm,Jeff D. Eldredge,Candace S. J. Tsai*

Main category: cs.CV

TL;DR: 将SAM零样本分割与DINOv2自监督特征学习结合，能在TEM图像中以实例级精度和高通量自动识别与分类CNT形态，准确率95.5%，可处理混合样本，极大提高分析效率与可重复性。


<details>
  <summary>Details</summary>
Motivation: 当前CNT在电子显微镜图像中的形态学量化依赖人工分割，耗时且可变，影响暴露评估和毒理学研究；因此需要一个高通量、可重复且自动化的分析框架。

Method: 先用Segment Anything Model（SAM）构建交互式分割工具，借助最少的人机交互获得高精度实例掩码；随后用这些掩码在实例区域内提取特征，并将其输入DINOv2视觉变换器进行表征与分类，从而抑制背景噪声并专注粒子信息。

Result: 在1800张TEM图像的数据集上，提出的框架对四类CNT形态的分类准确率达95.5%，显著优于现有基线方法，且仅用到少量训练数据；此外可以在单个视野内识别并区分共存的不同粒子类型。

Conclusion: 该工作展示了将零样本分割（SAM）与自监督特征提取（DINOv2）结合，可实现快速、可重复且高精度的碳纳米管（CNT）形态学分析，从而替代耗时且主观的手工分割流程。

Abstract: Accurate characterization of carbon nanotube morphologies in electron microscopy images is vital for exposure assessment and toxicological studies, yet current workflows rely on slow, subjective manual segmentation. This work presents a unified framework leveraging vision foundation models to automate the quantification and classification of CNTs in electron microscopy images. First, we introduce an interactive quantification tool built on the Segment Anything Model (SAM) that segments particles with near-perfect accuracy using minimal user input. Second, we propose a novel classification pipeline that utilizes these segmentation masks to spatially constrain a DINOv2 vision transformer, extracting features exclusively from particle regions while suppressing background noise. Evaluated on a dataset of 1,800 TEM images, this architecture achieves 95.5% accuracy in distinguishing between four different CNT morphologies, significantly outperforming the current baseline despite using a fraction of the training data. Crucially, this instance-level processing allows the framework to resolve mixed samples, correctly classifying distinct particle types co-existing within a single field of view. These results demonstrate that integrating zero-shot segmentation with self-supervised feature learning enables high-throughput, reproducible nanomaterial analysis, transforming a labor-intensive bottleneck into a scalable, data-driven process.

</details>


### [51] [When Humans Judge Irises: Pupil Size Normalization as an Aid and Synthetic Irises as a Challenge](https://arxiv.org/abs/2601.06725)
*Mahsa Mitcheff,Adam Czajka*

Main category: cs.CV

TL;DR: Pupil alignment helps humans verify irises; modern synthetic same-eye images often cause humans to misclassify them as different-eye, reducing accuracy in mixed authentic-synthetic comparisons


<details>
  <summary>Details</summary>
Motivation: Determine how pupil-size variation and synthetic image generation affect human ability to verify iris identity, with relevance to forensic examination and presentation attack detection

Method: Human study comparing iris verification with pupil-size alignment and synthetic images

Result: Pupil-size normalization via an autoencoder-based identity-preserving translation model improves human verification accuracy; participants can distinguish same vs different for authentic/authentic and synthetic/synthetic pairs, but struggle more when comparing authentic to high-quality same-eye synthetic images (accuracy drops)

Conclusion: Pupil-size alignment is important for human-involved iris matching; despite high fidelity of generative models, humans more often judge same-eye synthetic images as different than same-eye authentic pairs; data and judgments are provided for replicability

Abstract: Iris recognition is a mature biometric technology offering remarkable precision and speed, and allowing for large-scale deployments to populations exceeding a billion enrolled users (e.g., AADHAAR in India). However, in forensic applications, a human expert may be needed to review and confirm a positive identification before an iris matching result can be presented as evidence in court, especially in cases where processed samples are degraded (e.g., in post-mortem cases) or where there is a need to judge whether the sample is authentic, rather than a result of a presentation attack.
  This paper presents a study that examines human performance in iris verification in two controlled scenarios: (a) under varying pupil sizes, with and without a linear/nonlinear alignment of the pupil size between compared images, and (b) when both genuine and impostor iris image pairs are synthetically generated. The results demonstrate that pupil size normalization carried out by a modern autoencoder-based identity-preserving image-to-image translation model significantly improves verification accuracy. Participants were also able to determine whether iris pairs corresponded to the same or different eyes when both images were either authentic or synthetic. However, accuracy declined when subjects were comparing authentic irises against high-quality, same-eye synthetic counterparts. These findings (a) demonstrate the importance of pupil-size alignment for iris matching tasks in which humans are involved, and (b) indicate that despite the high fidelity of modern generative models, same-eye synthetic iris images are more often judged by humans as different-eye images, compared to same-eye authentic image pairs.
  We offer data and human judgments along with this paper to allow full replicability of this study and future works.

</details>


### [52] [Benchmarking Egocentric Clinical Intent Understanding Capability for Medical Multimodal Large Language Models](https://arxiv.org/abs/2601.06750)
*Shaonan Liu,Guo Yu,Xiaoling Luo,Shiyi Zheng,Wenting Chen,Jie Liu,Linlin Shen*

Main category: cs.CV

TL;DR: MedGaze-Bench用医生凝视作为认知光标，提出三维临床意图框架并加入Trap QA，揭示当前医疗多模态LLM在空间、时间和规范性意图理解上的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法评估模型在现实临床场景中关键的自我中心（egocentric）意图理解能力，而临床决策高度依赖医生的注视焦点、时间顺序因果关系和严格的安全规范，因此需要新的基准来量化和促进模型在这些维度的可靠性。

Method: 构建基于临床凝视数据的基准，设计三维临床意图框架（空间意图、时间意图、规范意图），并引入Trap QA机制以惩罚幻觉和谄媚式认知。通过在手术、急诊模拟和诊断影像解释三种场景上评估多模态LLM，分析其在视觉同质性、时间因果依赖和安全协议隐含性带来的挑战。

Result: 实验表明当前MLLMs在区分相似解剖结构的精确目标（空间意图）、回溯与预测因果推理（时间意图）以及遵循安全协议（规范意图）上表现不足；模型倾向于利用全局视觉线索、生成捏造观察及无批判接受错误指令。Trap QA成功揭示并惩罚了幻觉与盲从行为，从而更严格地评估临床可靠性。

Conclusion: 该论文提出了MedGaze-Bench，通过使用临床医生的凝视作为‘认知光标’来评估医疗多模态大模型在具有自我中心意图理解的任务中的表现，是对现有基准的有益补充。实验证明现有MLLMs在空间、时间和规范性意图理解上存在明显不足，容易依赖全局特征并产生虚构信息或盲从不合理指令。

Abstract: Medical Multimodal Large Language Models (Med-MLLMs) require egocentric clinical intent understanding for real-world deployment, yet existing benchmarks fail to evaluate this critical capability. To address these challenges, we introduce MedGaze-Bench, the first benchmark leveraging clinician gaze as a Cognitive Cursor to assess intent understanding across surgery, emergency simulation, and diagnostic interpretation. Our benchmark addresses three fundamental challenges: visual homogeneity of anatomical structures, strict temporal-causal dependencies in clinical workflows, and implicit adherence to safety protocols. We propose a Three-Dimensional Clinical Intent Framework evaluating: (1) Spatial Intent: discriminating precise targets amid visual noise, (2) Temporal Intent: inferring causal rationale through retrospective and prospective reasoning, and (3) Standard Intent: verifying protocol compliance through safety checks. Beyond accuracy metrics, we introduce Trap QA mechanisms to stress-test clinical reliability by penalizing hallucinations and cognitive sycophancy. Experiments reveal current MLLMs struggle with egocentric intent due to over-reliance on global features, leading to fabricated observations and uncritical acceptance of invalid instructions.

</details>


### [53] [The Normalized Difference Layer: A Differentiable Spectral Index Formulation for Deep Learning](https://arxiv.org/abs/2601.06777)
*Ali Lotfi,Adam Carter,Mohammad Meysami,Thuan Ha,Kwabena Nketia,Steve Shirtliffe*

Main category: cs.CV

TL;DR: 引入可学习系数的规范差异层，保留传统归一差优点并减少参数量、提升噪声鲁棒性，与标准MLP相比参数减少约75%，在乘性噪声下性能更稳健。


<details>
  <summary>Details</summary>
Motivation: 提供一种灵活的规范差异表示，可学习波段系数，保留光照不变性和有界输出，提高在特定任务上的适应性。

Method: 定义可微分层及其前向/反向传递算法，使用softplus确保正系数与有界分母，扩展支持有符号输入以便堆叠进深度网络；通过端到端反向传播学习系数并在实验中验证性能。

Result: 提出了可微分的规范差异层（Normalized Difference Layer），使用softplus重参数化保证系数为正且分母有界，支持正负输入，能端到端训练。

Conclusion: 该层在保持照明不变性和输出[-1,1]约束前提下，通过学习波段权重实现任务专用优化，减少模型参数并提升噪声鲁棒性，且学得的系数模式在不同深度间一致。

Abstract: Normalized difference indices have been a staple in remote sensing for decades. They stay reliable under lighting changes produce bounded values and connect well to biophysical signals. Even so, they are usually treated as a fixed pre processing step with coefficients set to one, which limits how well they can adapt to a specific learning task. In this study, we introduce the Normalized Difference Layer that is a differentiable neural network module. The proposed method keeps the classical idea but learns the band coefficients from data. We present a complete mathematical framework for integrating this layer into deep learning architectures that uses softplus reparameterization to ensure positive coefficients and bounded denominators. We describe forward and backward pass algorithms enabling end to end training through backpropagation. This approach preserves the key benefits of normalized differences, namely illumination invariance and outputs bounded to $[-1,1]$ while allowing gradient descent to discover task specific band weightings. We extend the method to work with signed inputs, so the layer can be stacked inside larger architectures. Experiments show that models using this layer reach similar classification accuracy to standard multilayer perceptrons while using about 75\% fewer parameters. They also handle multiplicative noise well, at 10\% noise accuracy drops only 0.17\% versus 3.03\% for baseline MLPs. The learned coefficient patterns stay consistent across different depths.

</details>


### [54] [CliffordNet: All You Need is Geometric Algebra](https://arxiv.org/abs/2601.06793)
*Zhongping Ji*

Main category: cs.CV

TL;DR: 本文提出了基于几何代数（Clifford代数）的视觉骨干网络CliffordNet，通过几何乘积统一替代空间混合（注意力/卷积）和通道混合（FFN）模块，采用稀疏滚动机制实现线性复杂度，并在CIFAR-100上以更少参数达到或超越现有轻量模型性能。


<details>
  <summary>Details</summary>
Motivation: 质疑当前视觉架构普遍采用的“空间混合+通道混合”工程范式，回到数学第一原理，寻找一个代数上完备且统一的特征交互机制以提高表征效率和参数利用率。

Method: 提出将Clifford（几何）代数中的Geometric Product（uv = u·v + u∧v）作为核心交互算子，通过稀疏滚动实现局部邻域的高效计算，省略或弱化FFN模块，设计Nano和Base两种变体并在CIFAR-100上训练和评估。

Result: 在CIFAR-100上，Nano（1.4M参数）达76.41%，接近ResNet-18（11.2M）；Base变体达78.05%，为tiny模型新SOTA；证明了几何交互使FFN冗余并实现良好参数-性能权衡。

Conclusion: CliffordNet展示出几何乘积作为单一、完备的局部交互算子，可以在无需传统FFN的情况下实现强大的表征能力，在参数效率和性能上优于同类轻量模型，暗示几何代数在视觉建模中的潜力。

Abstract: Modern computer vision architectures, from CNNs to Transformers, predominantly rely on the stacking of heuristic modules: spatial mixers (Attention/Conv) followed by channel mixers (FFNs). In this work, we challenge this paradigm by returning to mathematical first principles. We propose the \textbf{Clifford Algebra Network (CAN)}, also referred to as CliffordNet, a vision backbone grounded purely in Geometric Algebra. Instead of engineering separate modules for mixing and memory, we derive a unified interaction mechanism based on the \textbf{Clifford Geometric Product} ($uv = u \cdot v + u \wedge v$). This operation ensures algebraic completeness regarding the Geometric Product by simultaneously capturing feature coherence (via the generalized inner product) and structural variation (via the exterior wedge product).
  Implemented via an efficient sparse rolling mechanism with \textbf{strict linear complexity $\mathcal{O}(N)$}, our model reveals a surprising emergent property: the geometric interaction is so representationally dense that standard Feed-Forward Networks (FFNs) become redundant. Empirically, CliffordNet establishes a new Pareto frontier: our \textbf{Nano} variant achieves \textbf{76.41\%} accuracy on CIFAR-100 with only \textbf{1.4M} parameters, effectively matching the heavy-weight ResNet-18 (11.2M) with \textbf{$8\times$ fewer parameters}, while our \textbf{Base} variant sets a new SOTA for tiny models at \textbf{78.05\%}. Our results suggest that global understanding can emerge solely from rigorous, algebraically complete local interactions, potentially signaling a shift where \textit{geometry is all you need}. Code is available at https://github.com/ParaMind2025/CAN.

</details>


### [55] [SpatialNav: Leveraging Spatial Scene Graphs for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2601.06806)
*Jiwen Zhang,Zejun Li,Siyuan Wang,Xiangyu Shi,Zhongyu Wei,Qi Wu*

Main category: cs.CV

TL;DR: 提出SpatialNav，在零-shot VLN下先探索构建Spatial Scene Graph，再用agent-centric地图、罗盘对齐视觉表示和远程目标定位策略进行导航，显著提升性能，缩小与有学习方法差距。


<details>
  <summary>Details</summary>
Motivation: 零-shot VLN缺乏从大量数据中隐式学习空间知识，主要依赖局部观察，导致探索低效和性能差距；通过允许预先探索并显式建模全局空间结构，可弥补这一缺陷。

Method: 先在环境中完全探索以构建Spatial Scene Graph（捕捉全局空间结构与语义）；基于SSG引入SpatialNav，包括：agent-centric空间地图、罗盘对齐的视觉表征以及远程物体定位策略用于规划与导航。

Result: 在离散与连续环境上进行实验，SpatialNav显著优于现有零-shot基线，并明显缩小与有学习方法的差距，验证全局空间表示的作用。

Conclusion: 在允许预先探索的零-shot设置中，显式的全局空间表示（SSG）和配套策略能显著提升零-shot VLN性能，证明全局空间表征对可迁移导航的重要性。

Abstract: Although learning-based vision-and-language navigation (VLN) agents can learn spatial knowledge implicitly from large-scale training data, zero-shot VLN agents lack this process, relying primarily on local observations for navigation, which leads to inefficient exploration and a significant performance gap. To deal with the problem, we consider a zero-shot VLN setting that agents are allowed to fully explore the environment before task execution. Then, we construct the Spatial Scene Graph (SSG) to explicitly capture global spatial structure and semantics in the explored environment. Based on the SSG, we introduce SpatialNav, a zero-shot VLN agent that integrates an agent-centric spatial map, a compass-aligned visual representation, and a remote object localization strategy for efficient navigation. Comprehensive experiments in both discrete and continuous environments demonstrate that SpatialNav significantly outperforms existing zero-shot agents and clearly narrows the gap with state-of-the-art learning-based methods. Such results highlight the importance of global spatial representations for generalizable navigation.

</details>


### [56] [SARA: Scene-Aware Reconstruction Accelerator](https://arxiv.org/abs/2601.06831)
*Jee Won Lee,Hansol Lim,Minhyeok Im,Dohyeon Lee,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: SARA 提出了一种基于几何的配对选择模块，通过预匹配估计重建信息量（重叠度与视差的乘积），构建信息加权生成树并添加环路/长基线/弱视角边，显著减少配对数量和匹配开销，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统SfM多依赖视觉相似性选择配对，造成大量冗余匹配与高昂计算开销；SARA 希望通过先利用几何线索评估配对对重建的“信息贡献”来筛除低价值配对，提升效率同时保持重建准确性。

Method: 方法包括：1) 轻量级预匹配阶段用互为最近邻和RANSAC快速估计图像间重叠与视差；2) 定义重建信息性评分 = 重叠 × 视差；3) 基于该评分构建信息加权生成树（IWST）；4) 在IWST上额外添加用于环路闭合、长基线锚点与弱视角增强的目标边；5) 仅对评分高的配对进行昂贵的特征匹配，从而将复杂度从二次降为准线性。

Result: 在多个现代学习检测器上与穷尽匹配比较，SARA 将旋转误差降低约46.5±5.5%、平移误差降低约12.5±6.5%；匹配对从30,848降至580（约98%减少），匹配加速最多50x；对3D Gaussian Splatting 与 SVRaster 的重建指标保持在基线±3%。

Conclusion: SARA 在大幅降低匹配对数量（最高98%减少）和计算成本（最多50x加速）的同时，能保留接近基线的重建质量（在 3D Gaussian Splatting 与 SVRaster 中误差在±3%以内），并显著降低旋转误差（约46.5%）与平移误差（约12.5%）。

Abstract: We present SARA (Scene-Aware Reconstruction Accelerator), a geometry-driven pair selection module for Structure-from-Motion (SfM). Unlike conventional pipelines that select pairs based on visual similarity alone, SARA introduces geometry-first pair selection by scoring reconstruction informativeness - the product of overlap and parallax - before expensive matching. A lightweight pre-matching stage uses mutual nearest neighbors and RANSAC to estimate these cues, then constructs an Information-Weighted Spanning Tree (IWST) augmented with targeted edges for loop closure, long-baseline anchors, and weak-view reinforcement. Compared to exhaustive matching, SARA reduces rotation errors by 46.5+-5.5% and translation errors by 12.5+-6.5% across modern learned detectors, while achieving at most 50x speedup through 98% pair reduction (from 30,848 to 580 pairs). This reduces matching complexity from quadratic to quasi-linear, maintaining within +-3% of baseline reconstruction metrics for 3D Gaussian Splatting and SVRaster.

</details>


### [57] [Enhancing Low-resolution Image Representation Through Normalizing Flows](https://arxiv.org/abs/2601.06834)
*Chenglong Bao,Tongyao Pang,Zuowei Shen,Dihan Zheng,Yihang Zou*

Main category: cs.CV

TL;DR: 本文提出LR2Flow，一种将小波紧框架与可逆流模型结合的非线性低分辨率图像表示方法。通过在小波域构建可逆网络并进行重建误差分析，证明了在小波紧框架域设计可逆网络的必要性。实验证明在图像缩放、压缩和去噪等任务上表现出色且鲁棒性强。


<details>
  <summary>Details</summary>
Motivation: 目标是学习一种稀疏的低分辨率图像表示，既能减小存储和传输成本，又能保留重建原图所需的关键信息，因此结合小波紧框架的频域分离能力与可逆流的可逆性与概率建模优势，提出LR2Flow。

Method: 方法将小波紧框架分解与正则化流（normalizing flows）结合：先用小波紧框架提取低频表示和高频细节块，在低频子带上构建可逆神经流以学习紧凑低分辨率表示，并通过逆变换与高频块重建图像。对网络进行了重建误差分析，推导出在小波域保证可逆性的必要条件，并据此设计网络结构。

Result: 在图像重缩放（rescaling）、有损压缩和去噪任务上，LR2Flow在重建误差、主观视觉质量和鲁棒性方面均超越或接近现有方法；分析表明其低分辨率表示在兼顾压缩效率与可逆重建能力上具有优势。

Conclusion: LR2Flow能在保留重要视觉信息的同时实现高质量重建，且在多种图像处理任务（重缩放、压缩、去噪）中均取得优异结果，证明在小波紧框架域应用可逆流模型是有效且必要的。

Abstract: Low-resolution image representation is a special form of sparse representation that retains only low-frequency information while discarding high-frequency components. This property reduces storage and transmission costs and benefits various image processing tasks. However, a key challenge is to preserve essential visual content while maintaining the ability to accurately reconstruct the original images. This work proposes LR2Flow, a nonlinear framework that learns low-resolution image representations by integrating wavelet tight frame blocks with normalizing flows. We conduct a reconstruction error analysis of the proposed network, which demonstrates the necessity of designing invertible neural networks in the wavelet tight frame domain. Experimental results on various tasks, including image rescaling, compression, and denoising, demonstrate the effectiveness of the learned representations and the robustness of the proposed framework.

</details>


### [58] [OSCAR: Optical-aware Semantic Control for Aleatoric Refinement in Sar-to-Optical Translation](https://arxiv.org/abs/2601.06835)
*Hyunseo Lee,Sang Min Kim,Ho Kyung Shin,Taeheon Kim,Woo-Jeoung Nam*

Main category: cs.CV

TL;DR: Proposes S2O translation with cross-modal semantic distillation, ControlNet guidance combining text and visual prompts, and uncertainty-aware loss to reduce speckle artifacts, achieving better perceptual and semantic results


<details>
  <summary>Details</summary>
Motivation: To overcome semantic and visual issues in SAR-to-optical translation by leveraging cross-modal learning, semantic guidance, and uncertainty modeling

Method: Extract and summarize methods

Result: A framework with Optical-Aware SAR Encoder via distillation, Semantically-Grounded ControlNet with class-aware text and hierarchical visual prompts, and an Uncertainty-Aware Objective modeling aleatoric uncertainty

Conclusion: The method improves perceptual quality and semantic consistency over state-of-the-art methods by combining semantic distillation, guided generation, and uncertainty modeling.

Abstract: Synthetic Aperture Radar (SAR) provides robust all-weather imaging capabilities; however, translating SAR observations into photo-realistic optical images remains a fundamentally ill-posed problem. Current approaches are often hindered by the inherent speckle noise and geometric distortions of SAR data, which frequently result in semantic misinterpretation, ambiguous texture synthesis, and structural hallucinations. To address these limitations, a novel SAR-to-Optical (S2O) translation framework is proposed, integrating three core technical contributions: (i) Cross-Modal Semantic Alignment, which establishes an Optical-Aware SAR Encoder by distilling robust semantic priors from an Optical Teacher into a SAR Student (ii) Semantically-Grounded Generative Guidance, realized by a Semantically-Grounded ControlNet that integrates class-aware text prompts for global context with hierarchical visual prompts for local spatial guidance; and (iii) an Uncertainty-Aware Objective, which explicitly models aleatoric uncertainty to dynamically modulate the reconstruction focus, effectively mitigating artifacts caused by speckle-induced ambiguity. Extensive experiments demonstrate that the proposed method achieves superior perceptual quality and semantic consistency compared to state-of-the-art approaches.

</details>


### [59] [PRISM: Color-Stratified Point Cloud Sampling](https://arxiv.org/abs/2601.06839)
*Hansol Lim,Minhyeok Im,Jongseong Brad Choi*

Main category: cs.CV

TL;DR: PRISM通过在RGB色空间上进行分层并为每个颜色桶限制采样容量k，实现颜色驱动的下采样，保留纹理丰富区域并压缩同质表面，从而得到更稀疏但信息量高的点云。


<details>
  <summary>Details</summary>
Motivation: 观察到独特场景特征通常伴随色彩多样性，而重复冗余的特征在颜色上较为同质，传统空间均匀的下采样方法忽略了这种光度学信息。

Method: 将RGB空间划分为颜色分箱（stratification domain），并对每个颜色桶设置最大容量k，按照颜色多样性分配采样密度，从而保留色彩变化大的区域并下采样色彩单一区域。

Result: 生成的点云在保持关键视觉特征（纹理边界、细节）上优于随机采样和体素网格等方法，同时点数更少，适合3D重建任务。

Conclusion: PRISM通过利用颜色导向的分层采样，在保持纹理丰富区域的同时显著减少颜色均匀表面上的点数，从而在视觉复杂性上优先保留对3D重建有用的特征。

Abstract: We present PRISM, a novel color-guided stratified sampling method for RGB-LiDAR point clouds. Our approach is motivated by the observation that unique scene features often exhibit chromatic diversity while repetitive, redundant features are homogeneous in color. Conventional downsampling methods (Random Sampling, Voxel Grid, Normal Space Sampling) enforce spatial uniformity while ignoring this photometric content. In contrast, PRISM allocates sampling density proportional to chormatic diversity. By treating RGB color space as the stratification domain and imposing a maximum capacity k per color bin, the method preserves texture-rich regions with high color variation while substantially reducing visually homogeneous surfaces. This shifts the sampling space from spatial coverage to visual complexity to produce sparser point clouds that retain essential features for 3D reconstruction tasks.

</details>


### [60] [Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models](https://arxiv.org/abs/2601.06843)
*Junyan Lin,Junlong Tong,Hao Wu,Jialiang Zhang,Jinming Liu,Xin Jin,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种并行流式框架，针对多模态大模型在实时视频理解中的延迟瓶颈，通过放宽全局位置信息连续性约束实现输入-输出并行处理，提出三种设计（Overlapped, Group-Decoupled, Gap-Isolated），并指出Group-Decoupled在效率和性能上取得最佳平衡，能够在感知与生成负载均衡时实现最多2x加速。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在实时视频理解场景中受限于标准位置编码的全局连续性约束，导致感知与生成必须串行，从而增加延迟，阻碍实时交互。

Method: 提出Parallel Streaming框架，通过三种松弛位置连续性的策略（重叠、组解耦、间隔隔离）允许模型在接收新输入的同时生成输出；实验比较了三种设计的延迟和性能，验证Group-Decoupled最优。

Result: 在多组实验中，Group-Decoupled在准确性和流畅性保持接近离线或串行流式方法的同时，将系统延迟在平衡负载下提高了约2倍；代码已开源。

Conclusion: Group-Decoupled设计在保持流畅性与准确性的前提下，显著降低了延迟，为实时“看边说”系统提供了可行路径；整体框架证明放宽全局位置连续性可实现感知与生成并行，提升实时交互效率。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance across many tasks, yet most systems remain limited to offline inference, requiring complete inputs before generating outputs. Recent streaming methods reduce latency by interleaving perception and generation, but still enforce a sequential perception-generation cycle, limiting real-time interaction. In this work, we target a fundamental bottleneck that arises when extending MLLMs to real-time video understanding: the global positional continuity constraint imposed by standard positional encoding schemes. While natural in offline inference, this constraint tightly couples perception and generation, preventing effective input-output parallelism. To address this limitation, we propose a parallel streaming framework that relaxes positional continuity through three designs: Overlapped, Group-Decoupled, and Gap-Isolated. These designs enable simultaneous perception and generation, allowing the model to process incoming inputs while producing responses in real time. Extensive experiments reveal that Group-Decoupled achieves the best efficiency-performance balance, maintaining high fluency and accuracy while significantly reducing latency. We further show that the proposed framework yields up to 2x acceleration under balanced perception-generation workloads, establishing a principled pathway toward speak-while-watching real-time systems. We make all our code publicly available: https://github.com/EIT-NLP/Speak-While-Watching.

</details>


### [61] [MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data](https://arxiv.org/abs/2601.06847)
*Mengmeng Zhang,Xiaoping Wu,Hao Luo,Fan Wang,Yisheng Lv*

Main category: cs.CV

TL;DR: 提出MedGround，用专家分割掩码生成高质量医学指称定位数据并通过多阶段验证得到MedGround-35K，显著提升VLM视觉定位和多对象消歧能力。


<details>
  <summary>Details</summary>
Motivation: VLM在临床叙述中常缺乏视觉锚定，原因是缺少高质量大规模临床指称-定位对，需数据驱动方法改善视觉定位能力。

Method: 基于专家分割掩码提取定位目标、形状与空间线索，生成自然临床查询；并通过格式检查、几何与医学先验规则、图像视觉判定的多阶段验证过滤样本。

Result: MedGround: automated pipeline converting segmentation masks to referring-localization data; produces MedGround-35K dataset; improves VLM grounding.

Conclusion: MedGround能可扩展地将医学语言锚定到可验证视觉证据，通过自动化生成和严格筛选提升VLM的指称定位性能与泛化。

Abstract: Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.

</details>


### [62] [MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation](https://arxiv.org/abs/2601.06874)
*Changli Wu,Haodong Wang,Jiayi Ji,Yutian Yao,Chunsai Du,Jihua Kang,Yanwei Fu,Liujuan Cao*

Main category: cs.CV

TL;DR: 在稀疏多视图图像下进行3D指称分割，提出端到端双分支Transformer（MVGGT）与PVSO优化策略，并建立MVRefer基准。


<details>
  <summary>Details</summary>
Motivation: 现有3DRES依赖稠密点云，真实场景多为稀疏视图且受时延限制，需直接从多视图图像恢复结构并分割目标。

Method: MVGGT采用双分支设计融合语言与稀疏视图几何推理；引入PVSO缓解Foreground Gradient Dilution，并端到端训练。

Result: 提出MV-3DRES问题与MVGGT模型

Conclusion: MVGGT在MV-3DRES上实现了高精度与快速推理，优于传统两阶段方法。

Abstract: Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.

</details>


### [63] [Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation](https://arxiv.org/abs/2601.06882)
*Dillan Imans,Phuoc-Nguyen Bui,Duc-Tai Le,Hyunseung Choo*

Main category: cs.CV

TL;DR: Proposes SAM-RefiSeR, integrating SAM prompts with refinement and self-regularization for unsupervised domain adaptation, yielding superior brain tumor segmentation across domains.


<details>
  <summary>Details</summary>
Motivation: Address domain shift in brain tumor segmentation by combining SAM (Segment Anything Model) with a refinement and self-regularization approach to improve unsupervised domain adaptation performance.

Method: Unsupervised Domain Adaptation with SAM-RefiSeR

Result: Improved segmentation accuracy on target domains for brain tumor images, demonstrating robustness to domain shifts and better boundary delineation compared to baseline methods.

Conclusion: SAM-RefiSeR effectively mitigates domain shift, leveraging SAM-based prompts and refinement/self-regularization to achieve improved and robust brain tumor segmentation in unsupervised domain adaptation settings.

Abstract: Unsupervised Domain Adaptation with SAM-RefiSeR for Enhanced Brain Tumor Segmentation

</details>


### [64] [MixRI: Mixing Features of Reference Images for Novel Object Pose Estimation](https://arxiv.org/abs/2601.06883)
*Xinhang Liu,Jiawei Shi,Zheng Dang,Yuchao Dai*

Main category: cs.CV

TL;DR: MixRI是一种即时适用于新物体的轻量级CAD基姿态估计网络，通过参考图像融合和基于多视图点匹配的设计，减少参考图像与参数量，在BOP数据集上实现了与大型方法相当的性能但资源消耗更小。


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中对内存与推理速度的需求，提出轻量级网络并减少参考图像数量，以便即时在测试时应用到新物体而无需微调。

Method: 提出MixRI，基于多视图信息直接进行点匹配的轻量级网络；采用参考图像融合策略减少参考图像数量；设计网络以降低参数量与内存占用，从而加快推理。

Result: 在BOP挑战的七个核心数据集上，使用更少参考图像和更少参数的情况下，MixRI取得了可比于需更多参考图像和更大模型的方法的结果，同时显著降低了内存需求与推理时间。

Conclusion: MixRI在无需微调的前提下，通过参考图像融合和轻量化设计，实现了在保持准确性的同时显著降低内存与推理时间，适合资源受限或实时的姿态估计场景。

Abstract: We present MixRI, a lightweight network that solves the CAD-based novel object pose estimation problem in RGB images. It can be instantly applied to a novel object at test time without finetuning. We design our network to meet the demands of real-world applications, emphasizing reduced memory requirements and fast inference time. Unlike existing works that utilize many reference images and have large network parameters, we directly match points based on the multi-view information between the query and reference images with a lightweight network. Thanks to our reference image fusion strategy, we significantly decrease the number of reference images, thus decreasing the time needed to process these images and the memory required to store them. Furthermore, with our lightweight network, our method requires less inference time. Though with fewer reference images, experiments on seven core datasets in the BOP challenge show that our method achieves comparable results with other methods that require more reference images and larger network parameters.

</details>


### [65] [CLIMP: Contrastive Language-Image Mamba Pretraining](https://arxiv.org/abs/2601.06891)
*Nimrod Shabtay,Itamar Zimerman,Eli Schwartz,Raja Giryes*

Main category: cs.CV

TL;DR: CLIMP用Mamba替换视觉与文本编码器，提升了跨模态检索和OOD鲁棒性，支持可变分辨率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: Transformer在注意力机制上易受虚假相关性影响且计算复杂度随分辨率二次增长；因此探索更具空间归纳偏置、可扩展且更鲁棒的架构（Mamba）用于CLIP式预训练。

Method: 提出全Mamba架构（VMamba视觉编码器与Mamba文本编码器），在对比学习框架下训练，评估包括ImageNet-O鲁棒性、跨模态检索精度、不同训练/测试分辨率下的性能、计算资源（内存/FLOPs）等。

Result: CLIMP提出将Transformer替换为Mamba以改进CLIP在鲁棒性和可扩展性方面的表现。论文展示了在ImageNet-O上较CLIP-ViT-B提升7.5%的鲁棒性，支持可变分辨率输入并在高分辨率下显著节省内存与计算，同时引入自回归文本编码器以实现密集检索。

Conclusion: Mamba作为Transformer的替代方案，在视觉语言学习中具有优势：提高鲁棒性、检索性能和计算效率，且支持可变分辨率与自回归文本检索。

Abstract: Contrastive Language-Image Pre-training (CLIP) relies on Vision Transformers whose attention mechanism is susceptible to spurious correlations, and scales quadratically with resolution. To address these limitations, We present CLIMP, the first fully Mamba-based contrastive vision-language model that replaces both the vision and text encoders with Mamba. The new architecture encodes sequential structure in both vision and language, with VMamba capturing visual spatial inductive biases, reducing reliance on spurious correlations and producing an embedding space favorable for cross-modal retrieval and out-of-distribution robustness-surpassing OpenAI's CLIP-ViT-B by 7.5% on ImageNet-O. CLIMP naturally supports variable input resolutions without positional encoding interpolation or specialized training, achieving up to 6.6% higher retrieval accuracy at 16x training resolution while using 5x less memory and 1.8x fewer FLOPs. The autoregressive text encoder further overcomes CLIP's fixed context limitation, enabling dense captioning retrieval. Our findings suggest that Mamba exhibits advantageous properties for vision-language learning, making it a compelling alternative to Transformer-based CLIP.

</details>


### [66] [UDPNet: Unleashing Depth-based Priors for Robust Image Dehazing](https://arxiv.org/abs/2601.06909)
*Zengyuan Zuo,Junjun Jiang,Gang Wu,Xianming Liu*

Main category: cs.CV

TL;DR: 使用预训练深度估计模型的深度先验，通过DGAM与DPFM模块高效融合深度信息，UDPNet在多项去雾基准上实现显著PSNR提升。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法多关注RGB单模态特征，忽视场景深度与雾分布的内在关联；已有联合优化深度和去雾的方法未能充分利用高精度深度信息。

Method: 提出两大模块：Depth-Guided Attention Module (DGAM)利用轻量级深度引导通道注意力自适应调制特征；Depth Prior Fusion Module (DPFM)通过双滑窗多头交叉注意力实现多尺度深度图特征的层次融合。

Result: 在SOTS上提升0.85 dB PSNR，Haze4K上提升1.19 dB，NHR上提升1.79 dB，证明了在合成与真实场景下的显著效果。

Conclusion: UDPNet通过引入大规模预训练深度估计模型DepthAnything V2的深度先验，显著提升了图像去雾性能，在多个数据集上优于现有方法。

Abstract: Image dehazing has witnessed significant advancements with the development of deep learning models. However, a few methods predominantly focus on single-modal RGB features, neglecting the inherent correlation between scene depth and haze distribution. Even those that jointly optimize depth estimation and image dehazing often suffer from suboptimal performance due to inadequate utilization of accurate depth information. In this paper, we present UDPNet, a general framework that leverages depth-based priors from large-scale pretrained depth estimation model DepthAnything V2 to boost existing image dehazing models. Specifically, our architecture comprises two typical components: the Depth-Guided Attention Module (DGAM) adaptively modulates features via lightweight depth-guided channel attention, and the Depth Prior Fusion Module (DPFM) enables hierarchical fusion of multi-scale depth map features by dual sliding-window multi-head cross-attention mechanism. These modules ensure both computational efficiency and effective integration of depth priors. Moreover, the intrinsic robustness of depth priors empowers the network to dynamically adapt to varying haze densities, illumination conditions, and domain gaps across synthetic and real-world data. Extensive experimental results demonstrate the effectiveness of our UDPNet, outperforming the state-of-the-art methods on popular dehazing datasets, such as 0.85 dB PSNR improvement on the SOTS dataset, 1.19 dB on the Haze4K dataset and 1.79 dB PSNR on the NHR dataset. Our proposed solution establishes a new benchmark for depth-aware dehazing across various scenarios. Pretrained models and codes will be released at our project https://github.com/Harbinzzy/UDPNet.

</details>


### [67] [RenderFlow: Single-Step Neural Rendering via Flow Matching](https://arxiv.org/abs/2601.06928)
*Shenghao Zhang,Runtao Liu,Christopher Schroers,Yang Zhang*

Main category: cs.CV

TL;DR: Proposes RenderFlow, a deterministic single-step neural renderer using flow matching and sparse keyframe guidance, offering near real-time photorealism and an adapter for inverse intrinsic decomposition.


<details>
  <summary>Details</summary>
Motivation: Conventional PBR is accurate but slow; diffusion-based neural rendering improves speed but has latency and stochasticity issues. The paper aims to create a deterministic, single-step, fast neural renderer.

Method: Use flow matching paradigm to train a deterministic forward model; incorporate sparse keyframe guidance module; optionally use adapter-based module to repurpose model for intrinsic decomposition.

Result: RenderFlow: a flow-matching-based, single-step deterministic renderer with optional sparse keyframe guidance; near real-time photorealistic results and an adapter for intrinsic decomposition.

Conclusion: RenderFlow bridges efficiency of generative models and accuracy of PBR by enabling fast, physically plausible rendering with optional keyframe guidance and supporting inverse tasks via adapters.

Abstract: Conventional physically based rendering (PBR) pipelines generate photorealistic images through computationally intensive light transport simulations. Although recent deep learning approaches leverage diffusion model priors with geometry buffers (G-buffers) to produce visually compelling results without explicit scene geometry or light simulation, they remain constrained by two major limitations. First, the iterative nature of the diffusion process introduces substantial latency. Second, the inherent stochasticity of these generative models compromises physical accuracy and temporal consistency. In response to these challenges, we propose a novel, end-to-end, deterministic, single-step neural rendering framework, RenderFlow, built upon a flow matching paradigm. To further strengthen both rendering quality and generalization, we propose an efficient and effective module for sparse keyframe guidance. Our method significantly accelerates the rendering process and, by optionally incorporating sparsely rendered keyframes as guidance, enhances both the physical plausibility and overall visual quality of the output. The resulting pipeline achieves near real-time performance with photorealistic rendering quality, effectively bridging the gap between the efficiency of modern generative models and the precision of traditional physically based rendering. Furthermore, we demonstrate the versatility of our framework by introducing a lightweight, adapter-based module that efficiently repurposes the pretrained forward model for the inverse rendering task of intrinsic decomposition.

</details>


### [68] [Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos](https://arxiv.org/abs/2601.06931)
*Haodong Chen,Qiang Huang,Jiaqi Zhao,Qiuping Jiang,Xiaojun Chang,Jun Yu*

Main category: cs.CV

TL;DR: Proposes face-only counterfactual paradigm, builds FOCUS dataset and REFLECT benchmark; finds persistent demographic bias in VLMs even under strict visual control; task design affects measured bias


<details>
  <summary>Details</summary>
Motivation: real-world images confound demographic cues with background/clothing; need controlled attribution to measure VLM social bias

Method: face-only counterfactual evaluation; generate face-edited variants from real photos while keeping background/clothing fixed

Result: FOCUS dataset: 480 scene-matched counterfactual images across six occupations and ten demographic groups; REFLECT benchmark with three tasks; experiments on five VLMs show demographic disparities persist and vary by task

Conclusion: Controlled counterfactual audits are necessary; task formulation critically influences bias measurement in multimodal models

Abstract: Vision-Language Models (VLMs) are increasingly deployed in socially consequential settings, raising concerns about social bias driven by demographic cues. A central challenge in measuring such social bias is attribution under visual confounding: real-world images entangle race and gender with correlated factors such as background and clothing, obscuring attribution. We propose a \textbf{face-only counterfactual evaluation paradigm} that isolates demographic effects while preserving real-image realism. Starting from real photographs, we generate counterfactual variants by editing only facial attributes related to race and gender, keeping all other visual factors fixed. Based on this paradigm, we construct \textbf{FOCUS}, a dataset of 480 scene-matched counterfactual images across six occupations and ten demographic groups, and propose \textbf{REFLECT}, a benchmark comprising three decision-oriented tasks: two-alternative forced choice, multiple-choice socioeconomic inference, and numeric salary recommendation. Experiments on five state-of-the-art VLMs reveal that demographic disparities persist under strict visual control and vary substantially across task formulations. These findings underscore the necessity of controlled, counterfactual audits and highlight task design as a critical factor in evaluating social bias in multimodal models.

</details>


### [69] [Watching, Reasoning, and Searching: A Video Deep Research Benchmark on Open Web for Agentic Video Reasoning](https://arxiv.org/abs/2601.06943)
*Chengwen Liu,Xiaomin Yu,Zhuoyue Chang,Zhe Huang,Shuo Zhang,Heng Lian,Kunyi Wang,Rui Xu,Sen Hu,Jianheng Hou,Hao Peng,Chengwei Qin,Xiaobin Hu,Hong Peng,Ronghao Chen,Huacan Wang*

Main category: cs.CV

TL;DR: 该文提出VideoDR，这是首个面向视频条件下开放域问答的基准，强调跨帧线索提取、交互式网络检索与多跳推理的结合。通过人工标注构建高质量样本并评估多模型与两种范式（Workflow与Agentic），发现Agentic并非总优于Workflow，其效果受维持初始视频锚点的能力影响；目标漂移与长程一致性是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 在真实视频QA场景中，视频通常只包含局部线索，可验证答案散布在开放网络上，模型需联合视觉线索抽取、迭代检索与多跳推理来验证答案，现有基准不足以评估该能力，故提出VideoDR。

Method: 作者构建数据集VideoDR，包含六个语义域的高质量样本；任务要求模型执行跨帧视觉锚点提取、交互式网络检索及基于视频与网络联合证据的多跳推理。评估了多种闭源与开源多模态大模型，并在Workflow与Agentic两种执行范式下比较性能，分析了失败原因如目标漂移与长程一致性问题。

Result: 评测显示Agentic并非显著优于Workflow，取决于模型维持视频锚点的能力；目标漂移与长程一致性制约性能，表明需要在检索与推理链条中增强锚点保持与一致性机制。

Conclusion: VideoDR构建了系统性的基准并揭示视频-网络联合证据场景下的关键挑战：需要更强的跨帧锚点保持、检索链一致性与多跳验证能力。

Abstract: In real-world video question answering scenarios, videos often provide only localized visual cues, while verifiable answers are distributed across the open web; models therefore need to jointly perform cross-frame clue extraction, iterative retrieval, and multi-hop reasoning-based verification. To bridge this gap, we construct the first video deep research benchmark, VideoDR. VideoDR centers on video-conditioned open-domain video question answering, requiring cross-frame visual anchor extraction, interactive web retrieval, and multi-hop reasoning over joint video-web evidence; through rigorous human annotation and quality control, we obtain high-quality video deep research samples spanning six semantic domains. We evaluate multiple closed-source and open-source multimodal large language models under both the Workflow and Agentic paradigms, and the results show that Agentic is not consistently superior to Workflow: its gains depend on a model's ability to maintain the initial video anchors over long retrieval chains. Further analysis indicates that goal drift and long-horizon consistency are the core bottlenecks. In sum, VideoDR provides a systematic benchmark for studying video agents in open-web settings and reveals the key challenges for next-generation video deep research agents.

</details>


### [70] [SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models](https://arxiv.org/abs/2601.06944)
*Yuhang Su,Mei Wang,Yaoyao Zhong,Guozhang Li,Shixing Li,Yihan Feng,Hua Huang*

Main category: cs.CV

TL;DR: SketchJudge is a 1,015-sample benchmark testing MLLMs' grading/diagnostic skills on hand-drawn STEM diagrams; current MLLMs lag humans, highlighting weaknesses in handling sketches.


<details>
  <summary>Details</summary>
Motivation: Evaluate MLLMs' ability to grade and diagnose errors in hand-drawn STEM diagrams, addressing limitations in handling unstructured/ambiguous sketches and complex reasoning.

Method: Collected diverse hand-drawn student responses with annotated error types across four domains; evaluated multiple advanced MLLMs against human performance using public dataset and scripts.

Result: Introduced SketchJudge benchmark with 1,015 hand-drawn responses across geometry, physics, charts, flowcharts; evaluations show advanced MLLMs underperform humans, revealing fragility in vision-language alignment for symbolic/noisy contexts.

Conclusion: SketchJudge effectively exposes gaps in current MLLMs for symbolic and noisy visual reasoning; more research needed to improve structural, semantic, and metacognitive reasoning in multimodal models.

Abstract: While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge.

</details>


### [71] [Unified Personalized Understanding, Generating and Editing](https://arxiv.org/abs/2601.06965)
*Yu Zhong,Tianwei Lin,Ruike Zhu,Yuqian Yuan,Haoyu Zheng,Liang Liang,Wenqiao Zhang,Feifei Shao,Haoyuan Li,Wanggui He,Hao Jiang,Yueting Zhuang*

Main category: cs.CV

TL;DR: 该论文提出OmniPersona，一种用于统一大多模态模型（LMMs）的端到端个性化框架，通过结构上解耦的概念token和知识回放机制，在单一架构内同时支持个性化理解、生成与图像编辑，并提出评测集OmniPBench。


<details>
  <summary>Details</summary>
Motivation: 现有统一LMMs缺乏用户特定概念的一致可控建模；现有个性化方法依赖外部检索或软提示但存在跨任务耦合、训练复杂或知识模糊等问题，需一个端到端且在多任务中一致的个性化方案。

Method: 引入结构解耦的概念token，分配专门子空间给不同任务以最小化干扰；加入显式知识回放机制将个性化属性知识在任务间传播；在统一LMM架构中端到端训练，实现理解、生成和编辑一体化。

Result: 提出了OmniPersona与评测套件OmniPBench，实验显示在理解、生成、编辑及跨任务一致性评估中，OmniPersona表现稳健且优于或接近现有方法，证明结构解耦与知识回放可减轻干扰并提高一致性。

Conclusion: OmniPersona在多项个性化任务上表现出有竞争力且稳健的性能，能有效减少跨任务干扰并实现一致的个性化行为，提供了统一个性化研究的强基线。

Abstract: Unified large multimodal models (LMMs) have achieved remarkable progress in general-purpose multimodal understanding and generation. However, they still operate under a ``one-size-fits-all'' paradigm and struggle to model user-specific concepts (e.g., generate a photo of \texttt{<maeve>}) in a consistent and controllable manner. Existing personalization methods typically rely on external retrieval, which is inefficient and poorly integrated into unified multimodal pipelines. Recent personalized unified models introduce learnable soft prompts to encode concept information, yet they either couple understanding and generation or depend on complex multi-stage training, leading to cross-task interference and ultimately to fuzzy or misaligned personalized knowledge. We present \textbf{OmniPersona}, an end-to-end personalization framework for unified LMMs that, for the first time, integrates personalized understanding, generation, and image editing within a single architecture. OmniPersona introduces structurally decoupled concept tokens, allocating dedicated subspaces for different tasks to minimize interference, and incorporates an explicit knowledge replay mechanism that propagates personalized attribute knowledge across tasks, enabling consistent personalized behavior. To systematically evaluate unified personalization, we propose \textbf{\texttt{OmniPBench}}, extending the public UnifyBench concept set with personalized editing tasks and cross-task evaluation protocols integrating understanding, generation, and editing. Experimental results demonstrate that OmniPersona delivers competitive and robust performance across diverse personalization tasks. We hope OmniPersona will serve as a strong baseline and spur further research on controllable, unified personalization.

</details>


### [72] [Can Textual Reasoning Improve the Performance of MLLMs on Fine-grained Visual Classification?](https://arxiv.org/abs/2601.06993)
*Jie Zhu,Yiyang Su,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文系统分析了 CoT 在 FGVC 上退化的根源——推理长度引起的“思考代价”，并通过多奖励归一化（alg）和 ReFine-RFT 框架有效缓解该问题，达成 SOTA 成绩。


<details>
  <summary>Details</summary>
Motivation: 尽管 MLLMs 能力强，但在需要细粒度视觉辨识的 FGVC 任务上仍表现不佳；已有用 CoT 提升复杂任务的方法在视觉感知任务上反而可能损害性能，故需系统性探究原因并提出解决方案。

Method: 系统性地在零样本评估和多种训练范式下实验，分析 CoT 对 FGVC 的影响；提出 alg（多奖励平衡归一化方法）和 ReFine-RFT（结合集成奖励与归一化以限制推理长度并优化分类精度），并在多项 FGVC 基准上验证。

Result: 发现“思考代价”（Cost of Thinking）：推理文本越长，分类精度越差。采用 alg 和 ReFine-RFT 后，在多个 FGVC 基准上取得 SOTA 性能。

Conclusion: CoT 在 FGVC 上的退化主要由推理长度导致，较长的文本推理会降低分类精度。作者提出了 alg 方法用于多奖励优化的归一化，以及 ReFine-RFT 框架结合集成奖励和 alg 来约束推理长度并提供稠密的精度反馈，显著提升 FGVC 性能。

Abstract: Multi-modal large language models (MLLMs) exhibit strong general-purpose capabilities, yet still struggle on Fine-Grained Visual Classification (FGVC), a core perception task that requires subtle visual discrimination and is crucial for many real-world applications. A widely adopted strategy for boosting performance on challenging tasks such as math and coding is Chain-of-Thought (CoT) reasoning. However, several prior works have reported that CoT can actually harm performance on visual perception tasks. These studies, though, examine the issue from relatively narrow angles and leave open why CoT degrades perception-heavy performance. We systematically re-examine the role of CoT in FGVC through the lenses of zero-shot evaluation and multiple training paradigms. Across these settings, we uncover a central paradox: the degradation induced by CoT is largely driven by the reasoning length, in which longer textual reasoning consistently lowers classification accuracy. We term this phenomenon the ``Cost of Thinking''. Building on this finding, we make two key contributions: (1) \alg, a simple and general plug-and-play normalization method for multi-reward optimization that balances heterogeneous reward signals, and (2) ReFine-RFT, a framework that combines ensemble rewards with \alg to constrain reasoning length while providing dense accuracy-oriented feedback. Extensive experiments demonstrate the effectiveness of our findings and the proposed ReFine-RFT, achieving state-of-the-art performance across FGVC benchmarks. Code and models are available at \href{https://github.com/jiezhu23/ReFine-RFT}{Project Link}.

</details>


### [73] [Spatial Multi-Task Learning for Breast Cancer Molecular Subtype Prediction from Single-Phase DCE-MRI](https://arxiv.org/abs/2601.07001)
*Sen Zeng,Hong Zhou,Zheng Zhu,Yang Liu*

Main category: cs.CV

TL;DR: 提出基于单相DCE-MRI的多任务空间注意力网络，同时预测ER/PR/HER2和Ki-67，利用ROI加权与多尺度注意力捕获肿瘤及其周围信息，在960例验证上表现优越，支持临床可行的无创分子分型。


<details>
  <summary>Details</summary>
Motivation: 动机是减少侵入性活检采样偏差并利用临床实际常采的单相DCE-MRI对乳腺肿瘤进行非侵入性分子分型，以支持个体化治疗决策。

Method: 方法构建了一个深度特征提取网络，结合多尺度空间注意力模块以捕获肿瘤内及围肿瘤特征，并加入ROI加权模块强调肿瘤核心、边缘和周围组织；采用多任务学习共享表示并配备任务特定分支，同时对ER、PR、HER2进行分类和Ki-67进行回归。

Result: 在960例数据（886例内部分7:1:2划分训练/验证/测试，74例外部数据五折交叉验证）上，ER/PR/HER2分类的AUC分别为0.893、0.824、0.857，Ki-67回归的平均绝对误差为8.2%，显著优于传统放射组学和单任务深度学习方法。

Conclusion: 该论文提出的空间多任务学习框架在单相DCE-MRI上实现了非侵入性分子分型预测的可行性，显著优于放射组学与单任务深度学习基线，展示了临床可接受的ER/PR/HER2分类AUC和Ki-67回归精度。

Abstract: Accurate molecular subtype classification is essential for personalized breast cancer treatment, yet conventional immunohistochemical analysis relies on invasive biopsies and is prone to sampling bias. Although dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) enables non-invasive tumor characterization, clinical workflows typically acquire only single-phase post-contrast images to reduce scan time and contrast agent dose. In this study, we propose a spatial multi-task learning framework for breast cancer molecular subtype prediction from clinically practical single-phase DCE-MRI. The framework simultaneously predicts estrogen receptor (ER), progesterone receptor (PR), human epidermal growth factor receptor 2 (HER2) status, and the Ki-67 proliferation index -- biomarkers that collectively define molecular subtypes. The architecture integrates a deep feature extraction network with multi-scale spatial attention to capture intratumoral and peritumoral characteristics, together with a region-of-interest weighting module that emphasizes the tumor core, rim, and surrounding tissue. Multi-task learning exploits biological correlations among biomarkers through shared representations with task-specific prediction branches. Experiments on a dataset of 960 cases (886 internal cases split 7:1:2 for training/validation/testing, and 74 external cases evaluated via five-fold cross-validation) demonstrate that the proposed method achieves an AUC of 0.893, 0.824, and 0.857 for ER, PR, and HER2 classification, respectively, and a mean absolute error of 8.2\% for Ki-67 regression, significantly outperforming radiomics and single-task deep learning baselines. These results indicate the feasibility of accurate, non-invasive molecular subtype prediction using standard imaging protocols.

</details>


### [74] [Adversarial Attacks on Medical Hyperspectral Imaging Exploiting Spectral-Spatial Dependencies and Multiscale Features](https://arxiv.org/abs/2601.07056)
*Yunrui Gu,Zhenzhe Gao,Cong Kong,Zhaoxia Yin*

Main category: cs.CV

TL;DR: 论文提出结合局部像素依赖与多尺度干扰的对抗攻击框架，揭示医疗HSI模型的结构性脆弱性并在两个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 发现医疗HSI在临床诊断中越来越依赖深度学习模型，但这些模型对对抗样本敏感，作者旨在揭示这种脆弱性的根本原因并设计更具针对性的攻击以促进鲁棒防御研究。

Method: 提出一个包含两部分的攻击框架：局部像素依赖攻击（利用相邻像素的空间相关性构造扰动）和多尺度信息攻击（在不同光谱-空间层次上扰乱特征），通过联合优化在保证视觉不可察觉的同时最大化模型分类错误。

Result: 在Brain和MDC数据集上，所提攻击显著降低分类性能，尤其在肿瘤区域表现出更强的破坏力，且扰动在视觉上难以辨认；与现有方法相比，显示出更高的攻击成功率和更强的结构破坏能力。

Conclusion: 该论文总结了医疗高光谱成像（HSI）模型对对抗攻击高度脆弱，尤其依赖于局部像素依赖性和多尺度光谱-空间表示是其脆弱性根源，提出了针对性的对抗攻击框架并在数据集上表现有效。

Abstract: Medical hyperspectral imaging (HSI) enables accurate disease diagnosis by capturing rich spectral-spatial tissue information, but recent advances in deep learning have exposed its vulnerability to adversarial attacks. In this work, we identify two fundamental causes of this fragility: the reliance on local pixel dependencies for preserving tissue structure and the dependence on multiscale spectral-spatial representations for hierarchical feature encoding. Building on these insights, we propose a targeted adversarial attack framework for medical HSI, consisting of a Local Pixel Dependency Attack that exploits spatial correlations among neighboring pixels, and a Multiscale Information Attack that perturbs features across hierarchical spectral-spatial scales. Experiments on the Brain and MDC datasets demonstrate that our attacks significantly degrade classification performance, especially in tumor regions, while remaining visually imperceptible. Compared with existing methods, our approach reveals the unique vulnerabilities of medical HSI models and underscores the need for robust, structure-aware defenses in clinical applications.

</details>


### [75] [Billboard in Focus: Estimating Driver Gaze Duration from a Single Image](https://arxiv.org/abs/2601.07073)
*Carlos Pizarroso,Zuzana Berger Haladová,Zuzana Černeková,Viktor Kocur*

Main category: cs.CV

TL;DR: 提出一个两阶段自动化管线：YOLO检测广告牌（mAP@50=94%），结合DINOv2特征的分类器估计驾驶员在单帧上是否注视广告牌（准确率68.1%），并在Google街景上验证。


<details>
  <summary>Details</summary>
Motivation: 传统评估广告牌对驾驶员的影响依赖人工标注或眼动追踪，成本高且难以大规模应用；因此需要一种无需人工标注且可自动估计驾驶员注视广告牌的可扩展方法。

Method: 第一阶段使用基于YOLO的目标检测模型，先在Mapillary Vistas数据集上训练，然后在BillboardLamac上微调以检测广告牌；第二阶段在检测到的广告牌边界框位置上提取DINOv2特征并训练分类器，用于判断驾驶员是否注视广告牌（从单帧估计注视时长/相关性）。

Result: 在BillboardLamac数据集上，YOLO检测模型达到94% mAP@50；注视分类器在单帧上实现68.1%准确率；并在Google街景图像上对方法进行了验证，展示了跨域泛化能力。

Conclusion: 论文提出了一个完全自动化的两阶段管线，用于检测路边广告牌并估计驾驶员注视时长，从而无需人工标注或眼动仪评估广告牌相关性。模型在广告牌检测任务上表现优异（mAP@50=94%），在基于帧的注视时长估计上达到68.1%准确率，并在Google街景图像上进行了验证。

Abstract: Roadside billboards represent a central element of outdoor advertising, yet their presence may contribute to driver distraction and accident risk. This study introduces a fully automated pipeline for billboard detection and driver gaze duration estimation, aiming to evaluate billboard relevance without reliance on manual annotations or eye-tracking devices. Our pipeline operates in two stages: (1) a YOLO-based object detection model trained on Mapillary Vistas and fine-tuned on BillboardLamac images achieved 94% mAP@50 in the billboard detection task (2) a classifier based on the detected bounding box positions and DINOv2 features. The proposed pipeline enables estimation of billboard driver gaze duration from individual frames. We show that our method is able to achieve 68.1% accuracy on BillboardLamac when considering individual frames. These results are further validated using images collected from Google Street View.

</details>


### [76] [Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression](https://arxiv.org/abs/2601.07092)
*Yuliang Cai,Dongqiangzi Ye,Zitian Chen,Chongruo Wu*

Main category: cs.CV

TL;DR: 通过对早期帧进行令牌压缩并保留近期帧的完整令牌，SRC-Pipeline将VLM在自动驾驶VQA上的FLOPs降低约66%，在性能基本不损失的情况下显著提升了实时部署可行性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶VQA对实时性和低延迟有严格要求，但现有大型VLM通常处理每帧的密集patch tokens，导致FLOPs高、推理延迟大，限制了在安全关键场景中的实用部署。论文旨在在保持问答性能的同时降低计算资源消耗。

Method: SRC-Pipeline在输入视频帧序列中区分早期帧与近期帧：对早期帧通过学习的压缩模块将密集patch tokens汇聚成少量高层tokens（token compression/summary），而对近期帧保留原始patch tokens以保留细节信息。随后将压缩后的高层tokens与完整近期tokens输入到VLM进行联合编码和问答预测。

Result: 在自动驾驶视频问答任务上，SRC-Pipeline实现了约66%的FLOPs降低，同时性能与基线VLM相当，表明该方法在提高实时性和计算效率方面效果显著，适合安全关键的自动驾驶应用。

Conclusion: 该论文提出了面向自动驾驶视觉问答（VQA）的高效视觉语言模型框架SRC-Pipeline，通过对早期帧的低级令牌进行压缩为少量高层令牌，同时对近期帧保留完整补丁令牌，从而在保持性能的前提下显著减少计算量与推理延迟。

Abstract: Autonomous driving increasingly relies on Visual Question Answering (VQA) to enable vehicles to understand complex surroundings by analyzing visual inputs and textual queries. Currently, a paramount concern for VQA in this domain is the stringent requirement for fast latency and real-time processing, as delays directly impact real-world safety in this safety-critical application. However, current state-of-the-art VQA models, particularly large vision-language models (VLMs), often prioritize performance over computational efficiency. These models typically process dense patch tokens for every frame, leading to prohibitive computational costs (FLOPs) and significant inference latency, especially with long video sequences. This focus limits their practical deployment in real-time autonomous driving scenarios. To tackle this issue, we propose an efficient VLM framework for autonomous driving VQA tasks, SRC-Pipeline. It learns to compress early frame tokens into a small number of high-level tokens while retaining full patch tokens for recent frames. Experiments on autonomous driving video question answering tasks show that our approach achieves 66% FLOPs reduction while maintaining comparable performance, enabling VLMs to operate more effectively in real-time, safety-critical autonomous driving settings.

</details>


### [77] [3D Wavelet-Based Structural Priors for Controlled Diffusion in Whole-Body Low-Dose PET Denoising](https://arxiv.org/abs/2601.07093)
*Peiyuan Jing,Yue Tang,Chun-Wun Cheng,Zhenxuan Zhang,Liutao Yang,Thiago V. Lima,Klaus Strobel,Antoine Leimgruber,Angelica Aviles-Rivero,Guang Yang,Javier Montoya*

Main category: cs.CV

TL;DR: 提出WCC-Net：将小波频域结构先验通过轻量控制分支注入冻结的3D扩散骨干，以提高低剂量全身PET的去噪性能和解剖一致性，优于多种基线并具备剂量泛化能力。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET噪声增大，传统方法（CNN/GAN）或扩散模型虽能去噪但难以保证解剖结构一致性，尤其在低信噪比和全身体积成像下；因此需要显式频域结构先验来引导扩散去噪。

Method: 提出了一个完全3D的扩散模型框架，使用轻量级控制分支将小波域的结构先验注入到冻结的预训练扩散网络（ControlNet风格），维持生成表达能力同时强化三维连续性。

Result: 在内部1/20剂量测试集上，相较于强扩散基线，WCC-Net在PSNR上提升+1.21 dB，SSIM提升+0.008，同时降低GMSD与NMAE；并在未见剂量水平（1/50和1/4）上展示了更好的泛化性与体积解剖一致性。

Conclusion: WCC-Net通过在冻结的预训练扩散骨干中注入基于小波的结构引导，实现了将解剖结构与噪声解耦，从而在低剂量PET体积去噪任务中提升结构一致性与图像质量。

Abstract: Low-dose Positron Emission Tomography (PET) imaging reduces patient radiation exposure but suffers from increased noise that degrades image quality and diagnostic reliability. Although diffusion models have demonstrated strong denoising capability, their stochastic nature makes it challenging to enforce anatomically consistent structures, particularly in low signal-to-noise regimes and volumetric whole-body imaging. We propose Wavelet-Conditioned ControlNet (WCC-Net), a fully 3D diffusion-based framework that introduces explicit frequency-domain structural priors via wavelet representations to guide volumetric PET denoising. By injecting wavelet-based structural guidance into a frozen pretrained diffusion backbone through a lightweight control branch, WCC-Net decouples anatomical structure from noise while preserving generative expressiveness and 3D structural continuity. Extensive experiments demonstrate that WCC-Net consistently outperforms CNN-, GAN-, and diffusion-based baselines. On the internal 1/20-dose test set, WCC-Net improves PSNR by +1.21 dB and SSIM by +0.008 over a strong diffusion baseline, while reducing structural distortion (GMSD) and intensity error (NMAE). Moreover, WCC-Net generalizes robustly to unseen dose levels (1/50 and 1/4), achieving superior quantitative performance and improved volumetric anatomical consistency.

</details>


### [78] [MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning](https://arxiv.org/abs/2601.07107)
*Meng Lu,Yuxing Lu,Yuchen Zhuang,Megan Mullins,Yang Xie,Guanghua Xiao,Charles Fleming,Wenqi Shi,Xuan Wang*

Main category: cs.CV

TL;DR: 提出MedVistaGym，通过交互式、工具集成和端到端强化学习训练，使医学VLM能够进行多步、可验证的视觉推理，显著提升医学VQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学VLM依赖静态视觉嵌入和单次前向推理，无法在多步推理中重检验或细化视觉证据；开源模型缺乏学习有效工具选择与协调的训练基础设施。

Method: 构建一个可扩展的交互式训练环境（MedVistaGym），支持工具选择/调用/协调、任务相关区域定位以及在统一接口中整合多子图像证据，通过轨迹采样和端到端强化学习训练模型（MedVistaGym-R1），实现交错的工具使用与推理。

Result: 在六个医学VQA基准上，MedVistaGym-R1-8B相比类似规模的工具增强基线，性能提升19.10%到24.21%。

Conclusion: MedVistaGym能通过结构化的智能体训练提升医学视觉-语言模型在多步、工具集成推理任务中的表现，相较仅有工具访问的模型，训练出的系统在六个医学VQA基准上有显著提升。

Abstract: Vision language models (VLMs) achieve strong performance on general image understanding but struggle to think with medical images, especially when performing multi-step reasoning through iterative visual interaction. Medical VLMs often rely on static visual embeddings and single-pass inference, preventing models from re-examining, verifying, or refining visual evidence during reasoning. While tool-integrated reasoning offers a promising path forward, open-source VLMs lack the training infrastructure to learn effective tool selection, invocation, and coordination in multi-modal medical reasoning. We introduce MedVistaGym, a scalable and interactive training environment that incentivizes tool-integrated visual reasoning for medical image analysis. MedVistaGym equips VLMs to determine when and which tools to invoke, localize task-relevant image regions, and integrate single or multiple sub-image evidence into interleaved multimodal reasoning within a unified, executable interface for agentic training. Using MedVistaGym, we train MedVistaGym-R1 to interleave tool use with agentic reasoning through trajectory sampling and end-to-end reinforcement learning. Across six medical VQA benchmarks, MedVistaGym-R1-8B exceeds comparably sized tool-augmented baselines by 19.10% to 24.21%, demonstrating that structured agentic training--not tool access alone--unlocks effective tool-integrated reasoning for medical image analysis.

</details>


### [79] [Few-shot Class-Incremental Learning via Generative Co-Memory Regularization](https://arxiv.org/abs/2601.07117)
*Kexin Bao,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 利用生成式编码器的领域自适应微调和双重类记忆（均值表征与权重）进行协同记忆正则化，以提高少样本类增量学习的表示与抗遗忘能力，实验优于现有方法。


<details>
  <summary>Details</summary>
Motivation: FSCIL需在少样本情况下既保持对旧类的记忆（防止灾难性遗忘）又能快速适应新类（防止对新类过拟合），因此需要增强表示学习与增量适应能力。

Method: 基于预训练生成式编码器进行领域自适应微调，采用MAE解码器和全连接分类器共同训练以获得通用且可适配的特征。构建两类类记忆（表征记忆存均值、权重记忆存分类器权重），在增量阶段通过同时优化分类损失与协同记忆正则化来训练分类器，并以类增量方式更新记忆。

Result: 在多个常用基准上广泛实验，结果显示该方法在准确率和抗遗忘性上均优于最新方法。

Conclusion: 该文提出了联合生成器记忆正则化的方法，用于解决少样本增量学习中的遗忘与过拟合问题，实验表明方法优于现有方法。

Abstract: Few-shot class-incremental learning (FSCIL) aims to incrementally learn models from a small amount of novel data, which requires strong representation and adaptation ability of models learned under few-example supervision to avoid catastrophic forgetting on old classes and overfitting to novel classes. This work proposes a generative co-memory regularization approach to facilitate FSCIL. In the approach, the base learning leverages generative domain adaptation finetuning to finetune a pretrained generative encoder on a few examples of base classes by jointly incorporating a masked autoencoder (MAE) decoder for feature reconstruction and a fully-connected classifier for feature classification, which enables the model to efficiently capture general and adaptable representations. Using the finetuned encoder and learned classifier, we construct two class-wise memories: representation memory for storing the mean features for each class, and weight memory for storing the classifier weights. After that, the memory-regularized incremental learning is performed to train the classifier dynamically on the examples of few-shot classes in each incremental session by simultaneously optimizing feature classification and co-memory regularization. The memories are updated in a class-incremental manner and they collaboratively regularize the incremental learning. In this way, the learned models improve recognition accuracy, while mitigating catastrophic forgetting over old classes and overfitting to novel classes. Extensive experiments on popular benchmarks clearly demonstrate that our approach outperforms the state-of-the-arts.

</details>


### [80] [Motion Focus Recognition in Fast-Moving Egocentric Video](https://arxiv.org/abs/2601.07154)
*Daniel Hong,James Tribble,Hao Wang,Chaoyi Zhou,Ashish Bastola,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

TL;DR: 提出一种基于相机位姿估计与系统级优化的实时第一视角运动意图识别方法，适用于体育/快速运动场景，支持边缘部署。


<details>
  <summary>Details</summary>
Motivation: 现有第一视角数据集和VLA系统侧重动作识别，忽视了在体育及快速运动场景中运动分析的重要性，作者希望填补该空白，使运动中心分析在边缘设备上可行。

Method: 方法利用基础模型进行相机位姿估计，并通过系统级优化（如滑动批处理推理策略）提升效率与可扩展性，实现实时处理和可控内存消耗。

Result: 在作者收集的第一视角动作数据集上评估，方法达到了实时性能并在内存占用方面表现可控，验证了滑动批推理的有效性。

Conclusion: 该论文提出了一种基于第一视角视频的实时运动关注识别方法，能从任意第一视角视频估计主体的移动意图，并在实时约束和内存限制下实现边缘部署可行性。

Abstract: From Vision-Language-Action (VLA) systems to robotics, existing egocentric datasets primarily focus on action recognition tasks, while largely overlooking the inherent role of motion analysis in sports and other fast-movement scenarios. To bridge this gap, we propose a real-time motion focus recognition method that estimates the subject's locomotion intention from any egocentric video. Our approach leverages the foundation model for camera pose estimation and introduces system-level optimizations to enable efficient and scalable inference. Evaluated on a collected egocentric action dataset, our method achieves real-time performance with manageable memory consumption through a sliding batch inference strategy. This work makes motion-centric analysis practical for edge deployment and offers a complementary perspective to existing egocentric studies on sports and fast-movement activities.

</details>


### [81] [Test-time Adaptive Hierarchical Co-enhanced Denoising Network for Reliable Multimodal Classification](https://arxiv.org/abs/2601.07163)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出了 TAHCD，通过全局稳定子空间对齐、样本自适应置信度对齐及测试时协同自适应更新，实现模态内与跨模态噪声联合去除，在多个基准上显著提升多模态低质量数据的鲁棒性与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时可靠地去除异质噪声并应对未见噪声，限制了多模态表示学习在低质量数据上的可靠性与安全性。

Method: TAHCD 包括两个主要机制：1）Adaptive Stable Subspace Alignment（全局层面）和 Sample-Adaptive Confidence Alignment（实例层面），分别用于发现稳定子空间对齐并基于样本自适应置信度进行噪声抑制，实现模态内与跨模态噪声联合去除；2）Test-time cooperative enhancement（测试时自适应增强），在无标签情况下根据输入噪声自适应更新模型参数，提升对未见噪声的适应性与泛化。

Result: 在多个基准数据集上的实验显示，TAHCD 在分类准确率、鲁棒性和对未见噪声的泛化性能上均优于当前领先的可靠多模态学习方法。

Conclusion: TAHCD 提出了一种在全局与实例层面协同去噪的测试时自适应网络，可同时处理模态特有噪声与跨模态噪声，从而提升多模态数据在低质量环境下的鲁棒性与泛化能力。

Abstract: Reliable learning on low-quality multimodal data is a widely concerning issue, especially in safety-critical applications. However, multimodal noise poses a major challenge in this domain and leads existing methods to suffer from two key limitations. First, they struggle to reliably remove heterogeneous data noise, hindering robust multimodal representation learning. Second, they exhibit limited adaptability and generalization when encountering previously unseen noise. To address these issues, we propose Test-time Adaptive Hierarchical Co-enhanced Denoising Network (TAHCD). On one hand, TAHCD introduces the Adaptive Stable Subspace Alignment and Sample-Adaptive Confidence Alignment to reliably remove heterogeneous noise. They account for noise at both global and instance levels and enable jointly removal of modality-specific and cross-modality noise, achieving robust learning. On the other hand, TAHCD introduces test-time cooperative enhancement, which adaptively updates the model in response to input noise in a label-free manner, improving adaptability and generalization. This is achieved by collaboratively enhancing the joint removal process of modality-specific and cross-modality noise across global and instance levels according to sample noise. Experiments on multiple benchmarks demonstrate that the proposed method achieves superior classification performance, robustness, and generalization compared with state-of-the-art reliable multimodal learning approaches.

</details>


### [82] [DIVER: Dynamic Iterative Visual Evidence Reasoning for Multimodal Fake News Detection](https://arxiv.org/abs/2601.07178)
*Weilin Zhou,Zonghao Ying,Chunlei Meng,Jiahui Liu,Hengyang Zhou,Quanchen Zou,Deyue Zhang,Dongdong Yang,Xiangzheng Zhang*

Main category: cs.CV

TL;DR: DIVER通过文本优先+按需视觉证据提取与不确定性感知融合，实现了更准确且更高效的多模态虚假新闻检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么静态融合导致计算冗余，要么依赖大模型但因视觉基础薄弱而产生幻觉。需要一种按需、可解释且高效的多模态虚假信息检测策略。

Method: 先构建强文本基线并利用文本内一致性筛除不可靠/幻觉声明；若文本不足则进行跨模态对齐验证；对存在显著语义差异的样本选择性调用OCR、密集描述等细粒度视觉工具；使用不确定性感知融合迭代聚合证据进行多模态推理。

Result: 在Weibo、Weibo21和GossipCop上平均比SOTA提升2.72%，并将推理延迟降低至4.12秒，证明了方法在性能和效率上的优势。

Conclusion: DIVER提出了基于文本优先、按需引入视觉证据的动态递进推理框架，有效减少冗余计算并降低视觉幻觉风险，最终实现更高准确性和更低推理延迟。

Abstract: Multimodal fake news detection is crucial for mitigating adversarial misinformation. Existing methods, relying on static fusion or LLMs, face computational redundancy and hallucination risks due to weak visual foundations. To address this, we propose DIVER (Dynamic Iterative Visual Evidence Reasoning), a framework grounded in a progressive, evidence-driven reasoning paradigm. DIVER first establishes a strong text-based baseline through language analysis, leveraging intra-modal consistency to filter unreliable or hallucinated claims. Only when textual evidence is insufficient does the framework introduce visual information, where inter-modal alignment verification adaptively determines whether deeper visual inspection is necessary. For samples exhibiting significant cross-modal semantic discrepancies, DIVER selectively invokes fine-grained visual tools (e.g., OCR and dense captioning) to extract task-relevant evidence, which is iteratively aggregated via uncertainty-aware fusion to refine multimodal reasoning. Experiments on Weibo, Weibo21, and GossipCop demonstrate that DIVER outperforms state-of-the-art baselines by an average of 2.72\%, while optimizing inference efficiency with a reduced latency of 4.12 s.

</details>


### [83] [ShowUI-Aloha: Human-Taught GUI Agent](https://arxiv.org/abs/2601.07181)
*Yichun Zhang,Xiangwu Guo,Yauhong Goh,Jessica Hu,Zhiheng Chen,Xin Wang,Difei Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 本文提出ShowUI-Aloha，一个将长期、无结构的桌面屏幕录像转化为结构化、可执行GUI任务的全流程框架。包含录制、语义理解、规划与执行四部分，实现从原始人类演示到操作执行的闭环。


<details>
  <summary>Details</summary>
Motivation: 现有自动化GUI智能体受限于高质量、可扩展训练数据的缺乏；人类屏幕录像是丰富的数据源但通常无结构且缺乏注释，难以直接用于训练智能体。

Method: 框架由四个模块构成：Recorder（采集屏幕视频与交互事件：点击、键盘、滚动）、Learner（将原始交互与视觉上下文语义化，生成自然语言描述）、Planner（解析演示、维护任务状态并基于上下文生成下一步高层行动计划）、Executor（在操作系统层面精确执行计划并提供安全检测与实时反馈）。

Result: 该系统实现了从人类观察到可执行操作的闭环转换，验证了从真实人类桌面录像中收集并解析数据以训练通用GUI代理的可行性。

Conclusion: ShowUI-Aloha 提供了一个可扩展的解决方案，通过录制精确交互、语义解析、动态任务规划与安全执行，将真实人类桌面录像转化为可供GUI智能体学习的结构化数据，推动了通用GUI代理的构建。

Abstract: Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans.

</details>


### [84] [SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model](https://arxiv.org/abs/2601.07209)
*Yu Guo,Zhiqiang Lao,Xiyun Song,Yubin Zhou,Heather Yu*

Main category: cs.CV

TL;DR: 作者通过路径追踪合成真实感玻璃反射数据，并用拼接输入+联合captioning的方式利用LMM，结合LoRA微调，显著提升单图反射去除与分离效果。


<details>
  <summary>Details</summary>
Motivation: 单张图像反射去除受限于现有数据集：合成数据物理真实性不足，真实采集数据规模又不足，导致模型难以泛化。为此需要更真实且多样的训练数据以及高效利用大模型的方法。

Method: 1) 构建路径追踪渲染流程：使用3D玻璃模型、真实背景图像，并控制玻璃属性、相机参数和后处理效果进行批量合成。2) 数据输入处理：将图像层级拼接成单一复合输入，并进行联合captioning以提供多模态监督。3) 模型训练：基于LMM采用LoRA进行任务特定微调，而非全参数训练，以充分利用大模型的表示能力并降低训练成本。

Result: 方法在反射去除和反射-透过分离任务上相比现有最先进方法表现更好，展示了合成数据框架与LMM+LoRA策略在提升泛化性和性能方面的有效性。

Conclusion: 本文提出了一个基于路径追踪的合成数据生成框架，通过在真实背景图像上渲染三维玻璃模型以生成物理上更真实的反射场景，并结合大型多模态模型（LMM）进行联合输入与任务特定LoRA微调，从而提升单张图像反射去除与分离性能。

Abstract: Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.

</details>


### [85] [SceneNAT: Masked Generative Modeling for Language-Guided Indoor Scene Synthesis](https://arxiv.org/abs/2601.07218)
*Jeongjun Choi,Yeonsoo Park,H. Jin Kim*

Main category: cs.CV

TL;DR: SceneNAT: masked non-autoregressive Transformer for 3D scene synthesis from text; discretized attributes, dual-level masking, triplet relation predictor; better accuracy and efficiency on 3D-FRONT than autoregressive/diffusion baselines.


<details>
  <summary>Details</summary>
Motivation: To generate accurate 3D indoor scenes from natural language efficiently, capturing both intra-object details and inter-object relationships while reducing computational cost compared to autoregressive and diffusion models.

Method: Trains on fully discretized semantic and spatial attributes using masked modeling. Applies attribute-level and instance-level masking. Uses a triplet predictor mapping learnable relation queries to symbolic triplets for relational reasoning. Single-stage masked non-autoregressive Transformer with few parallel decoding passes.

Result: On 3D-FRONT, SceneNAT outperforms state-of-the-art autoregressive and diffusion baselines in semantic compliance and spatial arrangement accuracy, with substantially lower computational cost.

Conclusion: SceneNAT is a powerful single-stage masked non-autoregressive Transformer that can synthesize complete 3D indoor scenes from language with few parallel decoding passes, improving efficiency and performance over prior methods.

Abstract: We present SceneNAT, a single-stage masked non-autoregressive Transformer that synthesizes complete 3D indoor scenes from natural language instructions through only a few parallel decoding passes, offering improved performance and efficiency compared to prior state-of-the-art approaches. SceneNAT is trained via masked modeling over fully discretized representations of both semantic and spatial attributes. By applying a masking strategy at both the attribute level and the instance level, the model can better capture intra-object and inter-object structure. To boost relational reasoning, SceneNAT employs a dedicated triplet predictor for modeling the scene's layout and object relationships by mapping a set of learnable relation queries to a sparse set of symbolic triplets (subject, predicate, object). Extensive experiments on the 3D-FRONT dataset demonstrate that SceneNAT achieves superior performance compared to state-of-the-art autoregressive and diffusion baselines in both semantic compliance and spatial arrangement accuracy, while operating with substantially lower computational cost.

</details>


### [86] [VENUS: Visual Editing with Noise Inversion Using Scene Graphs](https://arxiv.org/abs/2601.07219)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: VENUS is a fast, training-free scene graph-guided editing method using split prompts and noise inversion that improves fidelity, semantic consistency, and runtime over state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Text-based editing struggles to preserve background and maintain semantic consistency; scene graphs offer structured controllability but prior methods require costly fine-tuning.

Method: VENUS uses split prompt conditioning to separate target object and background, leverages noise inversion for fidelity, and integrates scene graphs from multimodal LLMs with diffusion backbones without additional training.

Result: On PIE-Bench VENUS raises PSNR to 24.80, SSIM to 0.84, lowers LPIPS to 0.070, improves CLIP similarity to 24.97, achieves 0.87 DINO on EditVal, and cuts runtime to 20-30s per image, outperforming SGEdit, LEDIT++, and P2P+DirInv.

Conclusion: VENUS is a training-free, scene graph-guided image editing framework that improves background preservation and semantic alignment while reducing runtime compared to prior methods.

Abstract: State-of-the-art text-based image editing models often struggle to balance background preservation with semantic consistency, frequently resulting either in the synthesis of entirely new images or in outputs that fail to realize the intended edits. In contrast, scene graph-based image editing addresses this limitation by providing a structured representation of semantic entities and their relations, thereby offering improved controllability. However, existing scene graph editing methods typically depend on model fine-tuning, which incurs high computational cost and limits scalability. To this end, we introduce VENUS (Visual Editing with Noise inversion Using Scene graphs), a training-free framework for scene graph-guided image editing. Specifically, VENUS employs a split prompt conditioning strategy that disentangles the target object of the edit from its background context, while simultaneously leveraging noise inversion to preserve fidelity in unedited regions. Moreover, our proposed approach integrates scene graphs extracted from multimodal large language models with diffusion backbones, without requiring any additional training. Empirically, VENUS substantially improves both background preservation and semantic alignment on PIE-Bench, increasing PSNR from 22.45 to 24.80, SSIM from 0.79 to 0.84, and reducing LPIPS from 0.100 to 0.070 relative to the state-of-the-art scene graph editing model (SGEdit). In addition, VENUS enhances semantic consistency as measured by CLIP similarity (24.97 vs. 24.19). On EditVal, VENUS achieves the highest fidelity with a 0.87 DINO score and, crucially, reduces per-image runtime from 6-10 minutes to only 20-30 seconds. Beyond scene graph-based editing, VENUS also surpasses strong text-based editing baselines such as LEDIT++ and P2P+DirInv, thereby demonstrating consistent improvements across both paradigms.

</details>


### [87] [Language-Grounded Multi-Domain Image Translation via Semantic Difference Guidance](https://arxiv.org/abs/2601.07221)
*Jongwon Ryu,Joonhyung Park,Jaeho Han,Yeong-Seok Kim,Hye-rin Kim,Sunjae Yoon,Junyeong Kim*

Main category: cs.CV

TL;DR: LACE通过GLIP-Adapter和按属性的多域控制引导，把语言描述的语义差异具体化为可调节的视觉翻译向量，实现了可解释且结构保真的多域图像翻译。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在多域设置中同时保证结构完整性和细粒度属性可控性，尤其是当涉及多个属性时。需要一种将自然语言提示所表达的语义差异准确落地到视觉变换的方法。

Method: 采用两大模块：1) GLIP-Adapter将全局语义与局部结构特征融合以保持一致性；2) 多域控制引导（Multi-Domain Control Guidance）将源-目标提示的语义增量映射为每个属性的翻译向量，并允许独立强度调节。

Result: 在CelebA(Dialog)和BDD100K数据集上，LACE在视觉保真度、结构保持以及可解释的域特异性控制方面均优于先前基线方法。

Conclusion: LACE通过将语言语义与逐属性视觉变化对齐，实现了可解释的多域图像到图像翻译，兼顾结构保持与细粒度属性控制。

Abstract: Multi-domain image-to-image translation re quires grounding semantic differences ex pressed in natural language prompts into corresponding visual transformations, while preserving unrelated structural and seman tic content. Existing methods struggle to maintain structural integrity and provide fine grained, attribute-specific control, especially when multiple domains are involved. We propose LACE (Language-grounded Attribute Controllable Translation), built on two compo nents: (1) a GLIP-Adapter that fuses global semantics with local structural features to pre serve consistency, and (2) a Multi-Domain Control Guidance mechanism that explicitly grounds the semantic delta between source and target prompts into per-attribute translation vec tors, aligning linguistic semantics with domain level visual changes. Together, these modules enable compositional multi-domain control with independent strength modulation for each attribute. Experiments on CelebA(Dialog) and BDD100K demonstrate that LACE achieves high visual fidelity, structural preservation, and interpretable domain-specific control, surpass ing prior baselines. This positions LACE as a cross-modal content generation framework bridging language semantics and controllable visual translation.

</details>


### [88] [Universal Adversarial Purification with DDIM Metric Loss for Stable Diffusion](https://arxiv.org/abs/2601.07253)
*Li Zheng,Liangbin Xie,Jiantao Zhou,He YiMin*

Main category: cs.CV

TL;DR: UDAP利用DDIM反演的重构差异最小化进行对抗净化，并通过动态迭代策略在效率与效果间取得平衡，对多种针对Stable Diffusion的攻击有效且具有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗净化方法多针对分类器，未考虑Stable Diffusion中特有的攻击目标（如VAE编码器与UNet噪声预测器）和反演特性，因此需要一个通用且高效的净化框架以应对多样的SD攻击策略。

Method: 基于DDIM反演，UDAP利用干净图像与被攻击图像在反演过程中的重构差异作为优化目标，定义DDIM度量损失并对输入图像进行迭代优化以去除对抗噪声。同时引入动态epoch调整策略，根据重构误差自适应调整优化轮数以提高效率。

Result: 在针对PID、Anti-DreamBooth、MIST及其强化变体Anti-DF、MetaCloak等攻击的测试中，UDAP显著降低了对抗影响，恢复生成质量，并在不同SD版本与提示文本下表现出良好泛化能力且计算效率较高。

Conclusion: 本文提出UDAP框架，有效净化针对Stable Diffusion模型的对抗噪声，通过最小化DDIM度量损失并结合动态迭代调整，能在保持效率的同时去除VAE、UNet或混合攻击的对抗扰动。实验表明UDAP对多种对抗攻击和增强型变体具有鲁棒性，并能跨SD版本与文本提示泛化。

Abstract: Stable Diffusion (SD) often produces degraded outputs when the training dataset contains adversarial noise. Adversarial purification offers a promising solution by removing adversarial noise from contaminated data. However, existing purification methods are primarily designed for classification tasks and fail to address SD-specific adversarial strategies, such as attacks targeting the VAE encoder, UNet denoiser, or both. To address the gap in SD security, we propose Universal Diffusion Adversarial Purification (UDAP), a novel framework tailored for defending adversarial attacks targeting SD models. UDAP leverages the distinct reconstruction behaviors of clean and adversarial images during Denoising Diffusion Implicit Models (DDIM) inversion to optimize the purification process. By minimizing the DDIM metric loss, UDAP can effectively remove adversarial noise. Additionally, we introduce a dynamic epoch adjustment strategy that adapts optimization iterations based on reconstruction errors, significantly improving efficiency without sacrificing purification quality. Experiments demonstrate UDAP's robustness against diverse adversarial methods, including PID (VAE-targeted), Anti-DreamBooth (UNet-targeted), MIST (hybrid), and robustness-enhanced variants like Anti-Diffusion (Anti-DF) and MetaCloak. UDAP also generalizes well across SD versions and text prompts, showcasing its practical applicability in real-world scenarios.

</details>


### [89] [From Landslide Conditioning Factors to Satellite Embeddings: Evaluating the Utilisation of Google AlphaEarth for Landslide Susceptibility Mapping using Deep Learning](https://arxiv.org/abs/2601.07268)
*Yusen Cheng,Qinfeng Zhu,Lei Fan*

Main category: cs.CV

TL;DR: AE嵌入（尤其是完整64波段）作为预测变量，可显著优于传统LCF，提升地滑敏感性制图的准确性与空间一致性，推荐作为标准化替代或补充。


<details>
  <summary>Details</summary>
Motivation: 传统LCF存在可用性、异质性及预处理不确定性，限制LSM可靠性；AE嵌入作为多源遥感融合的统一表征，可能提供更丰富、标准化的预测信息。

Method: 在三个区域（台湾南投、香港、意大利艾米利亚-罗马涅）使用CNN1D、CNN2D和Vision Transformer比较了两种AE表示（主成分保留和完整64波段）与常规LCF，并通过多种评估指标、ROC-AUC、误差统计和空间模式分析进行评估。

Result: AE基模型在所有区域和模型中均优于LCF，64波段AE表现最佳，F1提升约4%-15%，AUC提升0.04-0.11；生成的敏感性图更贴合观测滑坡分布，且对局部高危条件更敏感。时间一致性影响显著，南投和艾米利亚改进更明显。

Conclusion: AE嵌入可作为替代或补充传统LCF，提高了数据驱动地滑敏感性制图的性能。

Abstract: Data-driven landslide susceptibility mapping (LSM) typically relies on landslide conditioning factors (LCFs), whose availability, heterogeneity, and preprocessing-related uncertainties can constrain mapping reliability. Recently, Google AlphaEarth (AE) embeddings, derived from multi-source geospatial observations, have emerged as a unified representation of Earth surface conditions. This study evaluated the potential of AE embeddings as alternative predictors for LSM. Two AE representations, including retained principal components and the full set of 64 embedding bands, were systematically compared with conventional LCFs across three study areas (Nantou County, Taiwan; Hong Kong; and part of Emilia-Romagna, Italy) using three deep learning models (CNN1D, CNN2D, and Vision Transformer). Performance was assessed using multiple evaluation metrics, ROC-AUC analysis, error statistics, and spatial pattern assessment. Results showed that AE-based models consistently outperformed LCFs across all regions and models, yielding higher F1-scores, AUC values, and more stable error distributions. Such improvement was most pronounced when using the full 64-band AE representation, with F1-score improvements of approximately 4% to 15% and AUC increased ranging from 0.04 to 0.11, depending on the study area and model. AE-based susceptibility maps also exhibited clearer spatial correspondence with observed landslide occurrences and enhanced sensitivity to localised landslide-prone conditions. Performance improvements were more evident in Nantou and Emilia than in Hong Kong, revealing that closer temporal alignment between AE embeddings and landslide inventories may lead to more effective LSM outcomes. These findings highlight the strong potential of AE embeddings as a standardised and information-rich alternative to conventional LCFs for LSM.

</details>


### [90] [PALUM: Part-based Attention Learning for Unified Motion Retargeting](https://arxiv.org/abs/2601.07272)
*Siqi Liu,Maoyu Wang,Bo Dai,Cewu Lu*

Main category: cs.CV

TL;DR: PALUM 通过语义分部与时空注意力学习骨骼无关的运动表示，配合目标结构信息和循环一致性，实现对不同骨骼拓扑间高保真、语义一致的动作重定向，并在广泛实验中表现优越。


<details>
  <summary>Details</summary>
Motivation: 当源与目标角色的骨骼结构差异很大时，直接映射易丢失运动语义或产生失真；需要一种能在多种拓扑间保持语义一致且具泛化能力的表示学习方法。

Method: 将关节按语义分块为身体部位，构建骨骼无关的运动表示；使用时空注意力捕获各部位之间的动态关系；在转移到目标骨骼时融合目标的结构信息以恢复特定姿态；引入循环一致性损失以保持语义和运动保真度。

Result: 大量实验表明，PALUM 在处理多样骨骼结构时优于现有方法，能维持动作真实感与语义一致性，并能推广到未见过的骨骼-动作组合；实现将在未来公开。

Conclusion: PALUM 提出了一种通过分割语义身体部分并使用注意力机制学习跨不同骨骼拓扑的通用运动表示，从而实现高质量的动作重定向的方法。该方法结合无关骨骼的表示与目标特定结构信息，并通过循环一致性机制保持语义一致性，实验显示在多样骨骼结构上具有优越性能并能推广到未见过的组合。

Abstract: Retargeting motion between characters with different skeleton structures is a fundamental challenge in computer animation. When source and target characters have vastly different bone arrangements, maintaining the original motion's semantics and quality becomes increasingly difficult. We present PALUM, a novel approach that learns common motion representations across diverse skeleton topologies by partitioning joints into semantic body parts and applying attention mechanisms to capture spatio-temporal relationships. Our method transfers motion to target skeletons by leveraging these skeleton-agnostic representations alongside target-specific structural information. To ensure robust learning and preserve motion fidelity, we introduce a cycle consistency mechanism that maintains semantic coherence throughout the retargeting process. Extensive experiments demonstrate superior performance in handling diverse skeletal structures while maintaining motion realism and semantic fidelity, even when generalizing to previously unseen skeleton-motion combinations. We will make our implementation publicly available to support future research.

</details>


### [91] [GenDet: Painting Colored Bounding Boxes on Images via Diffusion Model for Object Detection](https://arxiv.org/abs/2601.07273)
*Chen Min,Chengyang Li,Fanjie Kong,Qi Zhu,Dawei Zhao,Liang Xiao*

Main category: cs.CV

TL;DR: GenDet用Stable Diffusion把检测问题视为条件生成，直接在潜在空间生成带语义的边界框，达到与判别式检测器相当的性能并提供更高灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统判别式检测器与生成模型存在范式差异；作者旨在探索能否利用生成模型的表达能力和预训练资源，将检测任务统一到生成框架，从而获得更好的泛化与灵活性。

Method: 基于大规模预训练的Stable Diffusion，设计条件生成架构：以输入图像作为条件，在潜在表征空间中用语义约束直接生成边界框位置和类别属性，允许对位置和类别进行精确控制，同时保留生成模型的灵活性。

Result: 实验表明GenDet在准确率上与判别式检测器具有可比性，同时保留了生成方法在灵活性和可控性方面的优势（比如对位置/类别的控制、在原始图像空间生成）。

Conclusion: GenDet将目标检测转化为条件图像生成问题，通过在Stable Diffusion的潜在空间中直接生成带语义标签的边界框，实现了检测与生成模型的融合。

Abstract: This paper presents GenDet, a novel framework that redefines object detection as an image generation task. In contrast to traditional approaches, GenDet adopts a pioneering approach by leveraging generative modeling: it conditions on the input image and directly generates bounding boxes with semantic annotations in the original image space. GenDet establishes a conditional generation architecture built upon the large-scale pre-trained Stable Diffusion model, formulating the detection task as semantic constraints within the latent space. It enables precise control over bounding box positions and category attributes, while preserving the flexibility of the generative model. This novel methodology effectively bridges the gap between generative models and discriminative tasks, providing a fresh perspective for constructing unified visual understanding systems. Systematic experiments demonstrate that GenDet achieves competitive accuracy compared to discriminative detectors, while retaining the flexibility characteristic of generative methods.

</details>


### [92] [Focal Guidance: Unlocking Controllability from Semantic-Weak Layers in Video Diffusion Models](https://arxiv.org/abs/2601.07287)
*Yuanyang Yin,Yufan Deng,Shenghai Yuan,Kaipeng Zhang,Xiao Yang,Feng Zhao*

Main category: cs.CV

TL;DR: Identify Semantic-Weak Layers in DiT-based I2V, apply CLIP-based anchor guidance and attention map transfer to re-inject textual signals, improving instruction adherence and benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Existing I2V models struggle to balance strong visual consistency with faithful instruction following; intermediate DiT layers can become decoupled from text guidance (Condition Isolation), weakening adherence to prompts.

Method: FG consists of Fine-grained Semantic Guidance (FSG) using CLIP to identify anchor regions in the reference frame, and Attention Cache that transfers attention maps from semantically responsive layers to Semantic-Weak Layers to inject semantic signals.

Result: On the proposed instruction-following I2V benchmark, FG raises Wan2.1-I2V score to 0.7250 (+3.97%) and improves HunyuanVideo-I2V MMDiT-based score to 0.5571 (+7.44%).

Conclusion: Focal Guidance (FG) improves adherence to textual instructions in DiT-based I2V by strengthening weak semantic layers, reducing Condition Isolation and enhancing controllability, resulting in measurable benchmark gains.

Abstract: The task of Image-to-Video (I2V) generation aims to synthesize a video from a reference image and a text prompt. This requires diffusion models to reconcile high-frequency visual constraints and low-frequency textual guidance during the denoising process. However, while existing I2V models prioritize visual consistency, how to effectively couple this dual guidance to ensure strong adherence to the text prompt remains underexplored. In this work, we observe that in Diffusion Transformer (DiT)-based I2V models, certain intermediate layers exhibit weak semantic responses (termed Semantic-Weak Layers), as indicated by a measurable drop in text-visual similarity. We attribute this to a phenomenon called Condition Isolation, where attention to visual features becomes partially detached from text guidance and overly relies on learned visual priors. To address this, we propose Focal Guidance (FG), which enhances the controllability from Semantic-Weak Layers. FG comprises two mechanisms: (1) Fine-grained Semantic Guidance (FSG) leverages CLIP to identify key regions in the reference frame and uses them as anchors to guide Semantic-Weak Layers. (2) Attention Cache transfers attention maps from semantically responsive layers to Semantic-Weak Layers, injecting explicit semantic signals and alleviating their over-reliance on the model's learned visual priors, thereby enhancing adherence to textual instructions. To further validate our approach and address the lack of evaluation in this direction, we introduce a benchmark for assessing instruction following in I2V models. On this benchmark, Focal Guidance proves its effectiveness and generalizability, raising the total score on Wan2.1-I2V to 0.7250 (+3.97\%) and boosting the MMDiT-based HunyuanVideo-I2V to 0.5571 (+7.44\%).

</details>


### [93] [VideoLoom: A Video Large Language Model for Joint Spatial-Temporal Understanding](https://arxiv.org/abs/2601.07290)
*Jiapeng Shi,Junke Wang,Zuyao You,Bo He,Zuxuan Wu*

Main category: cs.CV

TL;DR: 提出VideoLoom和LoomData-8.7k及LoomBench，推动细粒度时空视频理解，显示出优秀的基准成绩与全面评测能力。


<details>
  <summary>Details</summary>
Motivation: 当前Video LLM在空间或时间理解上常各自为政，缺乏细粒度的时空定位能力与统一评测集；为推动多模态智能，需要一个能同时处理空间与时间定位的通用模型及匹配的数据与基准。

Method: 收集并标注LoomData-8.7k（带时间段和空间掩码的字幕），训练统一的VideoLoom模型以同时处理空间与时间定位问题，并在多项基准（如ReVOS、Charades-STA）上进行评估；同时构建LoomBench用于视频问答的综合评测。

Result: 在若干时空基准上取得最先进或高度竞争的结果：例如ReVOS上63.1 J&F（指涉视频对象分割），Charades-STA上48.3 R1@0.7（时间定位）；并通过LoomBench展示模型在时、空与组合问题上的综合能力。

Conclusion: VideoLoom通过构建人类中心、时序绑定且空间定位的训练数据（LoomData-8.7k），并设计统一的Video LLM框架，实现了在时空理解任务上的强劲性能，提出的LoomBench可用于更全面评估Video LLM的时、空与组合能力。

Abstract: This paper presents VideoLoom, a unified Video Large Language Model (Video LLM) for joint spatial-temporal understanding. To facilitate the development of fine-grained spatial and temporal localization capabilities, we curate LoomData-8.7k, a human-centric video dataset with temporally grounded and spatially localized captions. With this, VideoLoom achieves state-of-the-art or highly competitive performance across a variety of spatial and temporal benchmarks (e.g., 63.1 J&F on ReVOS for referring video object segmentation, and 48.3 R1@0.7 on Charades-STA for temporal grounding). In addition, we introduce LoomBench, a novel benchmark consisting of temporal, spatial, and compositional video-question pairs, enabling a comprehensive evaluation of Video LLMs from diverse aspects. Collectively, these contributions offer a universal and effective suite for joint spatial-temporal video understanding, setting a new standard in multimodal intelligence.

</details>


### [94] [A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model](https://arxiv.org/abs/2601.07291)
*Qi Zheng,Shuliang Liu,Yu Huang,Sihang Jia,Jungang Li,Lyuhao Chen,Junhao Chen,Hanqian Li,Aiwei Liu,Yibo Yan,Xuming Hu*

Main category: cs.CV

TL;DR: VISA-Mark通过轻量前缀微调提取图像驱动的证据权重，针对性扰动仅视觉支持的token，实现高检测率、强鲁棒性且不损视觉/语义保真与推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法要么视图无关导致引入视觉无关token并破坏视觉对齐，要么语义感知方法昂贵（如拒绝采样）导致推理延迟，需要一种既能被检测又不损视觉与语义保真的高效方法。

Method: 使用轻量前缀微调器（prefix-tuner）提取动态的Visual-Evidence Weights，根据图像对候选词的视觉支持程度给权重，基于此进行自适应词表划分与logits扰动，将水印强度集中在视觉支持的token上，从而对齐水印与视觉证据。

Result: 在视觉一致性指标（Chair-I）上相比传统方法提升7.8%，检测AUC达到96.88%，对抗鲁棒性99.3%，同时保持高效推理和更好的语义保真。

Conclusion: VISA-Mark在嵌入可检测信号的同时严格保持视觉保真，解决了现有视觉无关水印破坏视觉对齐和语义敏感方法推理慢的问题。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases, while some semantic-aware methods incur prohibitive inference latency due to rejection sampling. In this paper, we propose the VIsual Semantic Adaptive Watermark (VISA-Mark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. Our approach employs a lightweight, efficiently trained prefix-tuner to extract dynamic Visual-Evidence Weights, which quantify the evidentiary support for candidate tokens based on the visual input. These weights guide an adaptive vocabulary partitioning and logits perturbation mechanism, concentrating watermark strength specifically on visually-supported tokens. By actively aligning the watermark with visual evidence, VISA-Mark effectively maintains visual fidelity. Empirical results confirm that VISA-Mark outperforms conventional methods with a 7.8% improvement in visual consistency (Chair-I) and superior semantic fidelity. The framework maintains highly competitive detection accuracy (96.88% AUC) and robust attack resilience (99.3%) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multimodal watermarking.

</details>


### [95] [Inference-Time Scaling for Visual AutoRegressive modeling by Searching Representative Samples](https://arxiv.org/abs/2601.07293)
*Weidong Tang,Xinyan Wan,Siyu Li,Xiumei Wang*

Main category: cs.CV

TL;DR: 提出VAR-Scaling：在VQ离散潜空间用KDE构建近似连续特征空间，并结合Top-k和Random-k的密度自适应混合采样，针对VAR的尺度模式进行推理时扩放，显著提升生成质量与多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理时尺度化已在LLM和扩散模型中取得进展，但其在VQ视觉自回归建模中尚未探讨，原因在于离散潜空间阻碍了连续路径搜索和优化。作者旨在填补这一空白，提升VAR模型在推理阶段的生成质量与多样性。

Method: 通过在VQ离散潜空间上进行KDE映射得到近似连续特征分布，然后基于该分布设计密度自适应混合采样（高密度区域用Top-k保持质量，低密度区域用Random-k增加多样性），并识别VAR中的尺度模式（general与specific）来有针对性地在关键尺度应用扩展策略。

Result: 在类别条件与文本到图像的实验中，VAR-Scaling在生成质量与多样性上显著优于基线方法，证明了KDE映射与密度自适应混合采样在离散VQ空间中进行推理尺度化的有效性。

Conclusion: VAR-Scaling在VAR推理阶段通过将离散VQ空间映射到近似连续的核密度估计特征空间，结合密度自适应的Top-k与Random-k混合采样策略，有效优化了关键尺度下的样本保真度，从而提升了生成质量。

Abstract: While inference-time scaling has significantly enhanced generative quality in large language and diffusion models, its application to vector-quantized (VQ) visual autoregressive modeling (VAR) remains unexplored. We introduce VAR-Scaling, the first general framework for inference-time scaling in VAR, addressing the critical challenge of discrete latent spaces that prohibit continuous path search. We find that VAR scales exhibit two distinct pattern types: general patterns and specific patterns, where later-stage specific patterns conditionally optimize early-stage general patterns. To overcome the discrete latent space barrier in VQ models, we map sampling spaces to quasi-continuous feature spaces via kernel density estimation (KDE), where high-density samples approximate stable, high-quality solutions. This transformation enables effective navigation of sampling distributions. We propose a density-adaptive hybrid sampling strategy: Top-k sampling focuses on high-density regions to preserve quality near distribution modes, while Random-k sampling explores low-density areas to maintain diversity and prevent premature convergence. Consequently, VAR-Scaling optimizes sample fidelity at critical scales to enhance output quality. Experiments in class-conditional and text-to-image evaluations demonstrate significant improvements in inference process. The code is available at https://github.com/WD7ang/VAR-Scaling.

</details>


### [96] [Mimic Human Cognition, Master Multi-Image Reasoning: A Meta-Action Framework for Enhanced Visual Understanding](https://arxiv.org/abs/2601.07298)
*Jianghao Yin,Qingbin Li,Kun Sun,Cheng Ding,Jie Wang,Qin Chen,Jie Zhou,Nan Wang,Changqing Li,Pei Wu,Jian Xu,Zheming Yang,Liang He*

Main category: cs.CV

TL;DR: 提出CINEMA，通过五个认知元动作和检索式树采样+两阶段强化学习训练，解决多图像推理信息分散与关系复杂问题，在多模态基准上取得了领先或竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 多图像推理中关键信息散布在多张图像且图间关系复杂，现有多模态大模型在该场景表现显著下降。受人类认知过程启发，作者希望通过分解推理步骤与结构化行为空间，引导模型形成更可靠的多图像推理能力，同时设计训练策略改善学习稳定性与探索效率。

Method: 方法包括：1）将多图像推理分解为Global, Focus, Hint, Think, Answer五个有序元动作，明确认知步骤并约束模型行为；2）冷启动使用检索式树采样（Retrieval-Based Tree Sampling）生成高质量元动作轨迹以引导学习；3）强化学习采用两阶段策略：探索阶段用多样性保留策略避免熵塌陷，利用阶段用退火的DAPO方法逐步增强利用；4）构建57k冷启动与58k强化学习训练样本，覆盖多图像、多帧及单图任务；5）在多套基准上进行广泛评估。

Result: 在多图像推理基准、视频理解基准与单图基准上的实验显示，CINEMA在若干关键任务上达到或超越现有最先进方法：在MUIR和MVMath上超过GPT-4o，并在视频理解基准上显著优于专用视频推理模型，证明方法有效且具备较好泛化性。

Conclusion: 本文提出CINEMA框架，通过五个元动作（Global, Focus, Hint, Think, Answer）模拟人类认知步骤，解决多图像推理中的信息分散和关系复杂问题。结合检索式树采样进行冷启动训练，以及“多样性保留-退火利用”的两阶段强化学习策略，提升模型稳定探索与逐步利用能力。实验在多图像、视频和单图像基准上表现优异，在若干关键任务上超过GPT-4o并优于专用视频推理模型，证明了方法的有效性和泛化性。

Abstract: While Multimodal Large Language Models (MLLMs) excel at single-image understanding, they exhibit significantly degraded performance in multi-image reasoning scenarios. Multi-image reasoning presents fundamental challenges including complex inter-relationships between images and scattered critical information across image sets. Inspired by human cognitive processes, we propose the Cognition-Inspired Meta-Action Framework (CINEMA), a novel approach that decomposes multi-image reasoning into five structured meta-actions: Global, Focus, Hint, Think, and Answer which explicitly modeling the sequential cognitive steps humans naturally employ. For cold-start training, we introduce a Retrieval-Based Tree Sampling strategy that generates high-quality meta-action trajectories to bootstrap the model with reasoning patterns. During reinforcement learning, we adopt a two-stage paradigm: an exploration phase with Diversity-Preserving Strategy to avoid entropy collapse, followed by an annealed exploitation phase with DAPO to gradually strengthen exploitation. To train our model, we construct a dataset of 57k cold-start and 58k reinforcement learning instances spanning multi-image, multi-frame, and single-image tasks. We conduct extensive evaluations on multi-image reasoning benchmarks, video understanding benchmarks, and single-image benchmarks, achieving competitive state-of-the-art performance on several key benchmarks. Our model surpasses GPT-4o on the MUIR and MVMath benchmarks and notably outperforms specialized video reasoning models on video understanding benchmarks, demonstrating the effectiveness and generalizability of our human cognition-inspired reasoning framework.

</details>


### [97] [Revisiting the Ordering of Channel and Spatial Attention: A Comprehensive Study on Sequential and Parallel Designs](https://arxiv.org/abs/2601.07310)
*Zhongming Liu,Bingbing Jiang*

Main category: cs.CV

TL;DR: 系统比较18种通道-空间注意力融合拓扑，得出不同数据规模下的最佳策略并给出场景化设计指南与开源实现。


<details>
  <summary>Details</summary>
Motivation: 当前对通道注意力与空间注意力的融合策略多为经验性选择，缺乏系统化比较与统一原则，作者旨在填补这一空白，给出可复制的设计准则。

Method: 在统一框架下构建了包含18种拓扑（顺序、并行、多尺度、残差四类）的评估套件，在两类视觉与九个医学数据集上进行系统比较，并分析不同融合结构与数据规模之间的耦合关系。

Result: 发现了“数据规模-方法-性能”耦合规律：少样本任务首选Channel-多尺度Spatial级联；中等规模任务优选并行可学习融合结构；大规模任务优选带动态门控的并行结构；此外Spatial→Channel顺序对细粒度分类更稳定，残差连接有助于缓解梯度消失。代码已开源。

Conclusion: 本文总结了不同通道-空间注意力融合策略在不同数据规模下的表现规律，提出了场景化构建注意力模块的指导意见。

Abstract: Attention mechanisms have become a core component of deep learning models, with Channel Attention and Spatial Attention being the two most representative architectures. Current research on their fusion strategies primarily bifurcates into sequential and parallel paradigms, yet the selection process remains largely empirical, lacking systematic analysis and unified principles. We systematically compare channel-spatial attention combinations under a unified framework, building an evaluation suite of 18 topologies across four classes: sequential, parallel, multi-scale, and residual. Across two vision and nine medical datasets, we uncover a "data scale-method-performance" coupling law: (1) in few-shot tasks, the "Channel-Multi-scale Spatial" cascaded structure achieves optimal performance; (2) in medium-scale tasks, parallel learnable fusion architectures demonstrate superior results; (3) in large-scale tasks, parallel structures with dynamic gating yield the best performance. Additionally, experiments indicate that the "Spatial-Channel" order is more stable and effective for fine-grained classification, while residual connections mitigate vanishing gradient problems across varying data scales. We thus propose scenario-based guidelines for building future attention modules. Code is open-sourced at https://github.com/DWlzm.

</details>


### [98] [OSCAR: Open-Set CAD Retrieval from a Language Prompt and a Single Image](https://arxiv.org/abs/2601.07333)
*Tessa Pulli,Jean-Baptiste Weibel,Peter Hönig,Matthias Hirschmanner,Markus Vincze,Andreas Holzinger*

Main category: cs.CV

TL;DR: 无训练的两阶段多模态检索方法（文本过滤+图像精排），能从无标签3D库中自动找到与单图像/语言提示匹配的CAD，并有效支持零样本6D位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现实场景中对象集合不断变化，难以为每个实例收集或部署专用CAD模型；希望在不依赖实例训练的前提下，从大型无标签3D模型库中自动检索合适CAD以支持6D位姿估计。

Method: OSCAR在离线阶段为库中模型生成多视角渲染并用图像描述模型生成文本标签；在线阶段先用GroundedSAM在输入图像中定位目标，再计算RoI与库中文本标签的多模态嵌入，采用两阶段检索：先用CLIP基于文本过滤候选模型，再用DINOv2基于图像相似性精选最终模型。

Result: 在跨域3D模型检索基准MI3DOR上，OSCAR超越了所有现有方法；在YCB-V数据集检索平均精度达90.48%；并证明了用检索到的最相似模型结合Megapose进行位姿估计，性能优于基于重建的方法。

Conclusion: 本文提出了OSCAR，一种无需训练即可从无标签3D模型库中检索与给定单张图像和语言提示匹配的CAD模型的方法，能提升零样本6D位姿估计的模型获取与应用效率。

Abstract: 6D object pose estimation plays a crucial role in scene understanding for applications such as robotics and augmented reality. To support the needs of ever-changing object sets in such context, modern zero-shot object pose estimators were developed to not require object-specific training but only rely on CAD models. Such models are hard to obtain once deployed, and a continuously changing and growing set of objects makes it harder to reliably identify the instance model of interest. To address this challenge, we introduce an Open-Set CAD Retrieval from a Language Prompt and a Single Image (OSCAR), a novel training-free method that retrieves a matching object model from an unlabeled 3D object database. During onboarding, OSCAR generates multi-view renderings of database models and annotates them with descriptive captions using an image captioning model. At inference, GroundedSAM detects the queried object in the input image, and multi-modal embeddings are computed for both the Region-of-Interest and the database captions. OSCAR employs a two-stage retrieval: text-based filtering using CLIP identifies candidate models, followed by image-based refinement using DINOv2 to select the most visually similar object. In our experiments we demonstrate that OSCAR outperforms all state-of-the-art methods on the cross-domain 3D model retrieval benchmark MI3DOR. Furthermore, we demonstrate OSCAR's direct applicability in automating object model sourcing for 6D object pose estimation. We propose using the most similar object model for pose estimation if the exact instance is not available and show that OSCAR achieves an average precision of 90.48\% during object retrieval on the YCB-V object dataset. Moreover, we demonstrate that the most similar object model can be utilized for pose estimation using Megapose achieving better results than a reconstruction-based approach.

</details>


### [99] [Reconstruction Guided Few-shot Network For Remote Sensing Image Classification](https://arxiv.org/abs/2601.07335)
*Mohit Jaiswal,Naman Jain,Shivani Pathak,Mainak Singha,Nikunja Bihari Kar,Ankit Jha,Biplab Banerjee*

Main category: cs.CV

TL;DR: 通过掩码重建作为辅助任务，RGFS-Net在少样本遥感分类中提升了语义表达与泛化能力，实验结果优于基线，方法简单易扩展。


<details>
  <summary>Details</summary>
Motivation: 遥感影像类别间视觉差异大且标注昂贵，少样本设置下分类器易过拟合且泛化差。引入重建任务可强制模型学习更丰富的语义与空间上下文，使得在样本稀缺时仍能提取区分性特征。

Method: 在主分类任务外加入一个掩码重建分支：对输入图像随机遮挡若干区域，网络需重建被遮挡部分，训练时同时最小化分类损失与重建损失；该重建任务促进空间语义信息学习，提高特征判别性。实现上可在标准骨干（如ResNet）上并行接入重建头，训练时联合优化。

Result: 在EuroSAT与PatternNet的1-shot和5-shot对比实验中，RGFS-Net在多个基线方法上均取得稳健提升，显示了更好的未见类泛化与见类保持能力；方法简洁且可复现，代码已公开。

Conclusion: 本文提出的RGFS-Net通过引入掩码图像重建辅助任务，在少样本遥感图像分类中提升了语义特征表达，从而增强对未见类的泛化能力并保持对见类的一致性。实验在EuroSAT和PatternNet数据集的1-shot和5-shot设置下显示了优于基线的方法表现，方法简洁且兼容常见主干网络。

Abstract: Few-shot remote sensing image classification is challenging due to limited labeled samples and high variability in land-cover types. We propose a reconstruction-guided few-shot network (RGFS-Net) that enhances generalization to unseen classes while preserving consistency for seen categories. Our method incorporates a masked image reconstruction task, where parts of the input are occluded and reconstructed to encourage semantically rich feature learning. This auxiliary task strengthens spatial understanding and improves class discrimination under low-data settings. We evaluated the efficacy of EuroSAT and PatternNet datasets under 1-shot and 5-shot protocols, our approach consistently outperforms existing baselines. The proposed method is simple, effective, and compatible with standard backbones, offering a robust solution for few-shot remote sensing classification. Codes are available at https://github.com/stark0908/RGFS.

</details>


### [100] [PulseMind: A Multi-Modal Medical Model for Real-World Clinical Diagnosis](https://arxiv.org/abs/2601.07344)
*Jiao Xu,Junwei Liu,Jiangwei Lao,Qi Zhu,Yunpeng Zhao,Congyun Jin,Shinan Liu,Zhihong Lu,Lihe Zhang,Xin Chen,Jian Wang,Ping Wang*

Main category: cs.CV

TL;DR: PulseMind通过MediScope数据集、四维诊断基准和CRPO训练方法，推动了面向真实临床多模态多轮诊断的模型研究，取得了有竞争力的实验结果。


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型多集中于单一影像分析任务，缺乏对真实临床诊断中异质信息融合与多轮上下文持续理解的关注，导致模型在实际问诊场景中表现不足。

Method: 构建MediScope大规模多轮诊疗数据集（98k对话、601.5k影像，覆盖10+科室200+细分领域）；提出PulseMind Benchmark用于多轮诊断场景下的四维评估；设计CRPO（Comparison-based Reinforcement Policy Optimization）训练框架，利用相对偏好比较信号替代绝对分数进行强化学习优化。

Result: 在自建的PulseMind Benchmark上表现优异，同时在若干公开医学多模态/文本基准上达到或接近最先进水平，表明方法在临床多模态诊断任务上具有竞争力与泛化能力。

Conclusion: PulseMind提出了面向真实临床多模态诊断的整体解决方案，通过大规模多轮会诊数据集、针对性的评测基准和比较性强化策略优化训练框架，显著提升了模型在多维评价（主动性、准确性、实用性、语言质量）上的表现，并在公开医学基准上取得竞争力结果。

Abstract: Recent advances in medical multi-modal models focus on specialized image analysis like dermatology, pathology, or radiology. However, they do not fully capture the complexity of real-world clinical diagnostics, which involve heterogeneous inputs and require ongoing contextual understanding during patient-physician interactions. To bridge this gap, we introduce PulseMind, a new family of multi-modal diagnostic models that integrates a systematically curated dataset, a comprehensive evaluation benchmark, and a tailored training framework. Specifically, we first construct a diagnostic dataset, MediScope, which comprises 98,000 real-world multi-turn consultations and 601,500 medical images, spanning over 10 major clinical departments and more than 200 sub-specialties. Then, to better reflect the requirements of real-world clinical diagnosis, we develop the PulseMind Benchmark, a multi-turn diagnostic consultation benchmark with a four-dimensional evaluation protocol comprising proactiveness, accuracy, usefulness, and language quality. Finally, we design a training framework tailored for multi-modal clinical diagnostics, centered around a core component named Comparison-based Reinforcement Policy Optimization (CRPO). Compared to absolute score rewards, CRPO uses relative preference signals from multi-dimensional com-parisons to provide stable and human-aligned training guidance. Extensive experiments demonstrate that PulseMind achieves competitive performance on both the diagnostic consultation benchmark and public medical benchmarks.

</details>


### [101] [Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training](https://arxiv.org/abs/2601.07359)
*Shezheng Song,Shasha Li,Jie Yu*

Main category: cs.CV

TL;DR: 提出DualPD，一种无训练的双视角解码精炼方法，通过层间注意力对比与头部信息过滤修正注意力噪声，改善MLLMs“看对了但说错了”的问题，实验证明在多模型多任务上均有提升。


<details>
  <summary>Details</summary>
Motivation: 观察到MLLMs虽在较深层关注到正确视觉区域但最终预测被早期层的噪声注意力误导，导致内部理解与外部表达不一致，需在解码阶段无训练地增强视觉理解表现。

Method: DualPD由两模块组成：1) 基于层间注意力引导的对比logits模块，通过比较注意力变化最大的层之间的输出logits，捕捉正确答案信念的演化；2) 基于头部信息过滤模块，抑制关注无关区域的低贡献注意力头，提升每层注意力质量。

Result: 在LLaVA和Qwen-VL家族模型及多项多模态基准上，DualPD在无训练条件下持续提升准确率，验证了其有效性与普适性。

Conclusion: 本文指出多模态大模型内部常出现“看对了但说错了”的现象，提出DualPD方法在无训练下通过双视角解码精炼提升模型最终输出一致性与准确性。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong capabilities across a variety of vision-language tasks. However, their internal reasoning often exhibits a critical inconsistency: although deeper layers may attend to the correct visual regions, final predictions are frequently misled by noisy attention from earlier layers. This results in a disconnect between what the model internally understands and what it ultimately expresses, a phenomenon we describe as seeing it right but saying it wrong. To address this issue, we propose DualPD, a dual-perspective decoding refinement strategy that enhances the visual understanding without any additional training. DualPD consists of two components. (1) The layer-wise attention-guided contrastive logits module captures how the belief in the correct answer evolves by comparing output logits between layers that exhibit the largest attention shift. (2) The head-wise information filtering module suppresses low-contribution attention heads that focus on irrelevant regions, thereby improving attention quality within each layer. Experiments conducted on both the LLaVA and Qwen-VL model families across multiple multimodal benchmarks demonstrate that DualPD consistently improves accuracy without training, confirming its effectiveness and generalizability. The code will be released upon publication.

</details>


### [102] [HiVid-Narrator: Hierarchical Video Narrative Generation with Scene-Primed ASR-anchored Compression](https://arxiv.org/abs/2601.07366)
*Haoxuan Li,Mengyan Li,Junjun Zheng*

Main category: cs.CV

TL;DR: 作者构建E-HVC数据集并提出SPA-Compressor与HiVid-Narrator，通过ASR驱动的多模态压缩和阶段化注释，实现了高效且事实对齐的电商视频分层叙事生成。


<details>
  <summary>Details</summary>
Motivation: 电商视频信息密集且节奏快，现有模型难以同时把握细粒度视觉信息与组织成高层次连贯故事，且输入序列冗长影响训练效率。

Method: 构建E-HVC数据集（含Temporal Chain-of-Thought与Chapter Summary），采用阶段化注释流程：先收集ASR与帧级描述作为证据，再基于事件链细化章节边界与标题；提出SPA-Compressor，用ASR语义提示将多模态输入压缩为场景和事件层次表示；实现HiVid-Narrator框架用于生成层次化叙事。

Result: 在较少输入token的情况下，HiVid-Narrator在叙事质量上优于现有方法；E-HVC提供了时序对齐的双粒度注释，支持更可靠的事实驱动叙事生成。

Conclusion: 该论文提出了面向电商视频的分层叙事生成方法，通过双粒度、时间对齐的注释数据集和多模态压缩器，有效提升了叙事质量和效率。

Abstract: Generating structured narrations for real-world e-commerce videos requires models to perceive fine-grained visual details and organize them into coherent, high-level stories--capabilities that existing approaches struggle to unify. We introduce the E-commerce Hierarchical Video Captioning (E-HVC) dataset with dual-granularity, temporally grounded annotations: a Temporal Chain-of-Thought that anchors event-level observations and Chapter Summary that compose them into concise, story-centric summaries. Rather than directly prompting chapters, we adopt a staged construction that first gathers reliable linguistic and visual evidence via curated ASR and frame-level descriptions, then refines coarse annotations into precise chapter boundaries and titles conditioned on the Temporal Chain-of-Thought, yielding fact-grounded, time-aligned narratives. We also observe that e-commerce videos are fast-paced and information-dense, with visual tokens dominating the input sequence. To enable efficient training while reducing input tokens, we propose the Scene-Primed ASR-anchored Compressor (SPA-Compressor), which compresses multimodal tokens into hierarchical scene and event representations guided by ASR semantic cues. Built upon these designs, our HiVid-Narrator framework achieves superior narrative quality with fewer input tokens compared to existing methods.

</details>


### [103] [Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation](https://arxiv.org/abs/2601.07377)
*Jiao Xu,Xin Chen,Lihe Zhang*

Main category: cs.CV

TL;DR: 提出DiCo：通过动态教师-学生切换、多视角融合和对抗监督的半监督3D血管分割方法，显著提升分割性能并在多基准上实现SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统均值教师方法固定教师/学生角色，但在复杂的3D血管数据上教师不总是优于学生，导致认知偏差和性能瓶颈。为此需要一种动态角色切换与多视角形状约束的方法。

Method: 设计了动态协作网络(DiCo)，让两个模型在训练中动态交换教师与学生角色；加入多视角集成模块以从不同投影视角获得丰富特征；采用对抗监督并将3D体积投影为2D视图以约束无标签数据的血管形状并减少标签不一致影响。

Result: 在三个3D血管分割基准上，DiCo达到了新的最先进性能（论文中宣称并在代码库中提供实现）。

Conclusion: 本文提出的DiCo通过动态切换教师-学生角色、多视角融合和对抗监督，有效缓解半监督3D血管分割中教师主导带来的认知偏差，从而提升了性能。

Abstract: In this paper, we present a new dynamic collaborative network for semi-supervised 3D vessel segmentation, termed DiCo. Conventional mean teacher (MT) methods typically employ a static approach, where the roles of the teacher and student models are fixed. However, due to the complexity of 3D vessel data, the teacher model may not always outperform the student model, leading to cognitive biases that can limit performance. To address this issue, we propose a dynamic collaborative network that allows the two models to dynamically switch their teacher-student roles. Additionally, we introduce a multi-view integration module to capture various perspectives of the inputs, mirroring the way doctors conduct medical analysis. We also incorporate adversarial supervision to constrain the shape of the segmented vessels in unlabeled data. In this process, the 3D volume is projected into 2D views to mitigate the impact of label inconsistencies. Experiments demonstrate that our DiCo method sets new state-of-the-art performance on three 3D vessel segmentation benchmarks. The code repository address is https://github.com/xujiaommcome/DiCo

</details>


### [104] [Forecast the Principal, Stabilize the Residual: Subspace-Aware Feature Caching for Efficient Diffusion Transformers](https://arxiv.org/abs/2601.07396)
*Guantao Chen,Shikang Zheng,Yuqi Lin,Linfeng Zhang*

Main category: cs.CV

TL;DR: 基于SVD的子空间感知缓存（SVD-Cache）对DiT特征分别处理主/残子空间，使用EMA预测主成分并重用残差，实现大幅无损加速。


<details>
  <summary>Details</summary>
Motivation: 现有缓存方法对全部特征一视同仁，但DiT特征在时间维度上呈现主/残差子空间差异，主子空间平滑可预测，残差子空间波动且能量低，因而应分别处理以提高缓存效果。

Method: 提出SVD-Cache：对特征做SVD分解，将低秩主子空间用EMA预测而将残余子空间直接重用，从而缓存和重建特征以减少迭代计算。

Result: 在多个模型与方法上达到近无损的加速效果，具体包括FLUX和HunyuanVideo上的5.55×加速，并且兼容蒸馏、量化与稀疏注意力等加速技术。

Conclusion: 本文证明了对DiT特征进行子空间分解并对主成分预测可以显著加速推理且几乎无损质量。

Abstract: Diffusion Transformer (DiT) models have achieved unprecedented quality in image and video generation, yet their iterative sampling process remains computationally prohibitive. To accelerate inference, feature caching methods have emerged by reusing intermediate representations across timesteps. However, existing caching approaches treat all feature components uniformly. We reveal that DiT feature spaces contain distinct principal and residual subspaces with divergent temporal behavior: the principal subspace evolves smoothly and predictably, while the residual subspace exhibits volatile, low-energy oscillations that resist accurate prediction. Building on this insight, we propose SVD-Cache, a subspace-aware caching framework that decomposes diffusion features via Singular Value Decomposition (SVD), applies exponential moving average (EMA) prediction to the dominant low-rank components, and directly reuses the residual subspace. Extensive experiments demonstrate that SVD-Cache achieves near-lossless across diverse models and methods, including 5.55$\times$ speedup on FLUX and HunyuanVideo, and compatibility with model acceleration techniques including distillation, quantization and sparse attention. Our code is in supplementary material and will be released on Github.

</details>


### [105] [SDHSI-Net: Learning Better Representations for Hyperspectral Images via Self-Distillation](https://arxiv.org/abs/2601.07416)
*Prachet Dev Singh,Shyamsundar Paramasivam,Sneha Barman,Mainak Singha,Ankit Jha,Girish Mishra,Biplab Banerjee*

Main category: cs.CV

TL;DR: 将自我蒸馏用于HSI分类，通过把早期输出当软标签强制一致性，提升了分类性能与鲁棒性，验证于两个基准数据集，代码开源。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像具有高光谱维度且标注样本稀缺，传统深度模型易过拟合且计算开销大，因此希望采用无需外部教师的自我蒸馏来提高泛化能力与训练效率。

Method: 在深度网络中对中间层输出和最终输出进行一致性约束，将较早的预测作为软标签用于训练（自我蒸馏），结合光谱-空间特征提取模块，优化损失包含交叉熵与蒸馏损失，训练过程中无需外部教师网络。

Result: 在两个基准HSI数据集上，所提方法在分类精度和鲁棒性方面均有显著提升，实验表明特征空间中类别内样本更为紧凑、类别间更可分，代码已公开。

Conclusion: 本文通过自我蒸馏（SD）将网络的早期输出作为软目标，强制中间与最终预测一致，从而提升了HSI分类的性能与鲁棒性，结论为SD能显著提高类别内紧凑性和类别间可分性，并在两个基准数据集上验证效果优于传统方法。

Abstract: Hyperspectral image (HSI) classification presents unique challenges due to its high spectral dimensionality and limited labeled data. Traditional deep learning models often suffer from overfitting and high computational costs. Self-distillation (SD), a variant of knowledge distillation where a network learns from its own predictions, has recently emerged as a promising strategy to enhance model performance without requiring external teacher networks. In this work, we explore the application of SD to HSI by treating earlier outputs as soft targets, thereby enforcing consistency between intermediate and final predictions. This process improves intra-class compactness and inter-class separability in the learned feature space. Our approach is validated on two benchmark HSI datasets and demonstrates significant improvements in classification accuracy and robustness, highlighting the effectiveness of SD for spectral-spatial learning. Codes are available at https://github.com/Prachet-Dev-Singh/SDHSI.

</details>


### [106] [PanoSAMic: Panoramic Image Segmentation from SAM Feature Encoding and Dual View Fusion](https://arxiv.org/abs/2601.07447)
*Mahdi Chamseddine,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: PanoSAMic repurposes SAM as a multi-stage encoder and adds spatio-modal fusion plus a spherical-aware decoder to deliver SotA panoramic segmentation across multiple modalities.


<details>
  <summary>Details</summary>
Motivation: Existing foundation models trained on perspective images underperform on spherical/panoramic images due to distortions and edge discontinuities; leverage SAM's strong pretraining and multimodal cues to improve panoramic semantic segmentation.

Method: Modify SAM encoder to output multi-stage features; design spatio-modal fusion module to select relevant modalities/features spatially; employ a semantic decoder using spherical attention and dual view fusion to handle distortion and edge discontinuity in panoramas; train/evaluate on Stanford2D3DS and Matterport3D for RGB, RGB-D, RGB-D-N.

Result: Achieves SotA results on Stanford2D3DS for RGB, RGB-D, RGB-D-N and on Matterport3D for RGB and RGB-D. Public code available.

Conclusion: PanoSAMic successfully adapts a pre-trained Segment Anything (SAM) encoder for panoramic semantic segmentation, introducing multi-stage feature outputs, a spatio-modal fusion module, and a spherical-aware decoder with dual view fusion, resulting in state-of-the-art performance on Stanford2D3DS and Matterport3D across multiple modalities.

Abstract: Existing image foundation models are not optimized for spherical images having been trained primarily on perspective images. PanoSAMic integrates the pre-trained Segment Anything (SAM) encoder to make use of its extensive training and integrate it into a semantic segmentation model for panoramic images using multiple modalities. We modify the SAM encoder to output multi-stage features and introduce a novel spatio-modal fusion module that allows the model to select the relevant modalities and best features from each modality for different areas of the input. Furthermore, our semantic decoder uses spherical attention and dual view fusion to overcome the distortions and edge discontinuity often associated with panoramic images. PanoSAMic achieves state-of-the-art (SotA) results on Stanford2D3DS for RGB, RGB-D, and RGB-D-N modalities and on Matterport3D for RGB and RGB-D modalities. https://github.com/dfki-av/PanoSAMic

</details>


### [107] [Improving Video Question Answering through query-based frame selection](https://arxiv.org/abs/2601.07459)
*Himanshu Patil,Geo Jolly,Ramana Raja Buddala,Ganesh Ramakrishnan,Rohit Saluja*

Main category: cs.CV

TL;DR: 用基于问题的子模互信息（SMI）方法替换均匀抽帧，可在MVBench上使VideoQA准确率提升最多4%，并在定性上更好捕捉与问题相关的关键帧。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视觉语言模型因计算成本限制通常对视频进行固定数量的均匀抽帧，但这种方式无法保证所抽帧包含与问题相关的关键信息或上下文，导致VideoQA性能受限。

Method: 使用问句作为查询，利用SMI函数在视频所有帧中选择一组与问题最相关且信息互补的帧，并将所选帧输入已有VLM（Video-LLaVA、LLaVA-NeXT）进行问答。与传统的均匀抽帧进行对比实验，并在MVBench数据集上评估。

Result: 在MVBench上，使用SMI的查询驱动帧选择相比均匀抽帧可将VideoQA准确率提升最高约4%。定性分析显示，SMI选择的帧更符合问题关注的内容。

Conclusion: 本文提出了基于子模互信息（SMI）的查询驱动帧选择方法，以替代视频问答中常用的均匀抽帧策略，从而更好地挑选与问题相关且互补的重要帧，进而提升VideoQA性能。

Abstract: Video Question Answering (VideoQA) models enhance understanding and interaction with audiovisual content, making it more accessible, searchable, and useful for a wide range of fields such as education, surveillance, entertainment, and content creation. Due to heavy compute requirements, most large visual language models (VLMs) for VideoQA rely on a fixed number of frames by uniformly sampling the video. However, this process does not pick important frames or capture the context of the video. We present a novel query-based selection of frames relevant to the questions based on the submodular mutual Information (SMI) functions. By replacing uniform frame sampling with query-based selection, our method ensures that the chosen frames provide complementary and essential visual information for accurate VideoQA. We evaluate our approach on the MVBench dataset, which spans a diverse set of multi-action video tasks. VideoQA accuracy on this dataset was assessed using two VLMs, namely Video-LLaVA and LLaVA-NeXT, both of which originally employed uniform frame sampling. Experiments were conducted using both uniform and query-based sampling strategies. An accuracy improvement of up to \textbf{4\%} was observed when using query-based frame selection over uniform sampling. Qualitative analysis further highlights that query-based selection, using SMI functions, consistently picks frames better aligned with the question. We opine that such query-based frame selection can enhance accuracy in a wide range of tasks that rely on only a subset of video frames.

</details>


### [108] [From Sketch to Fresco: Efficient Diffusion Transformer with Progressive Resolution](https://arxiv.org/abs/2601.07462)
*Shikang Zheng,Guantao Chen,Lixuan He,Jiacheng Liu,Yuqi Lin,Chang Zou,Linfeng Zhang*

Main category: cs.CV

TL;DR: Fresco通过统一的re-noise与渐进式按区域上采样策略，解决动态分辨率采样中阶段不一致与误差累积问题，实现了在多模型多域上的近无损显著加速（如10×、5×，与蒸馏结合达22×）。


<details>
  <summary>Details</summary>
Motivation: 现有动态分辨率采样通过降低早期步骤分辨率加速，但其在分辨率切换时采用的启发式re-noise会破坏不同阶段间的一致性，使模型反复重学全局结构；且一次性整体上采样未检测区域是否收敛，导致误差累积与可见伪影。Fresco旨在解决这些问题以实现更高效且接近无损的加速。

Method: 提出Fresco框架：1) 统一目标分布的re-noise设计，避免每次分辨率切换后模型需重学全局结构；2) 渐进式（progressive）上采样策略，按区域逐步恢复分辨率而非一次性整体上采样；3) 基于局部收敛性检测的选择性上采样，只上采样已收敛或接近收敛的区域以减少误差传播；4) 与蒸馏、量化、特征缓存等加速技术兼容。

Result: 在多域与多模型上实现接近无损的加速效果：对FLUX达到10×加速、对HunyuanVideo达到5×加速；与蒸馏模型结合可达22×加速。保持生成质量的同时显著降低采样计算量，代码将开源。

Conclusion: Fresco通过在多阶段动态分辨率采样中统一re-noise策略与全局结构保持，并引入渐进式上采样与局部收敛检测，解决了现有方法在分辨率切换时破坏阶段间一致性和盲目整体上采样导致误差累积的问题，从而在保持低分辨率草稿效率的同时实现高分辨率细化的保真度。

Abstract: Diffusion Transformers achieve impressive generative quality but remain computationally expensive due to iterative sampling. Recently, dynamic resolution sampling has emerged as a promising acceleration technique by reducing the resolution of early sampling steps. However, existing methods rely on heuristic re-noising at every resolution transition, injecting noise that breaks cross-stage consistency and forces the model to relearn global structure. In addition, these methods indiscriminately upsample the entire latent space at once without checking which regions have actually converged, causing accumulated errors, and visible artifacts. Therefore, we propose \textbf{Fresco}, a dynamic resolution framework that unifies re-noise and global structure across stages with progressive upsampling, preserving both the efficiency of low-resolution drafting and the fidelity of high-resolution refinement, with all stages aligned toward the same final target. Fresco achieves near-lossless acceleration across diverse domains and models, including 10$\times$ speedup on FLUX, and 5$\times$ on HunyuanVideo, while remaining orthogonal to distillation, quantization and feature caching, reaching 22$\times$ speedup when combined with distilled models. Our code is in supplementary material and will be released on Github.

</details>


### [109] [FocalOrder: Focal Preference Optimization for Reading Order Detection](https://arxiv.org/abs/2601.07483)
*Fuyuan Liu,Dianyu Yu,He Ren,Nayu Liu,Xiaomian Kang,Delai Qiu,Fa Zhang,Genpeng Zhen,Shengping Liu,Jiaen Liang,Wei Huang,Yining Wang,Junnan Zhu*

Main category: cs.CV

TL;DR: FocalOrder introduces FPO to detect and emphasize hard reading-order transitions, mitigating positional disparity and achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Standard training assumes uniform difficulty and lets easy patterns dominate learning, causing models to fail on complex intermediate layout regions (Positional Disparity). The goal is to align optimization with intrinsic structural ambiguity.

Method: Use Focal Preference Optimization (FPO) with adaptive difficulty discovery via exponential moving average to identify difficult transitions, and a difficulty-calibrated pairwise ranking objective to enforce global logical consistency.

Result: State-of-the-art performance on OmniDocBench v1.0 and Comp-HRDoc; compact model outperforms specialized baselines and large-scale VLMs.

Conclusion: FocalOrder successfully addresses positional disparity in reading order detection by focusing training on hard-to-learn transitions, improving consistency and performance across layouts.

Abstract: Reading order detection is the foundation of document understanding. Most existing methods rely on uniform supervision, implicitly assuming a constant difficulty distribution across layout regions. In this work, we challenge this assumption by revealing a critical flaw: \textbf{Positional Disparity}, a phenomenon where models demonstrate mastery over the deterministic start and end regions but suffer a performance collapse in the complex intermediate sections. This degradation arises because standard training allows the massive volume of easy patterns to drown out the learning signals from difficult layouts. To address this, we propose \textbf{FocalOrder}, a framework driven by \textbf{Focal Preference Optimization (FPO)}. Specifically, FocalOrder employs adaptive difficulty discovery with exponential moving average mechanism to dynamically pinpoint hard-to-learn transitions, while introducing a difficulty-calibrated pairwise ranking objective to enforce global logical consistency. Extensive experiments demonstrate that FocalOrder establishes new state-of-the-art results on OmniDocBench v1.0 and Comp-HRDoc. Our compact model not only outperforms competitive specialized baselines but also significantly surpasses large-scale general VLMs. These results demonstrate that aligning the optimization with intrinsic structural ambiguity of documents is critical for mastering complex document structures.

</details>


### [110] [Anatomy Aware Cascade Network: Bridging Epistemic Uncertainty and Geometric Manifold for 3D Tooth Segmentation](https://arxiv.org/abs/2601.07499)
*Bing Yu,Liu Shi,Haitao Wang,Deran Qi,Xiang Cai,Wei Zhong,Qiegen Liu*

Main category: cs.CV

TL;DR: AACNet通过熵门控的边界修复与带符号距离映射的解剖注意力，在CBCT牙齿三维分割上同时提升边界精细度与拓扑一致性，实验验证了其高精度与泛化能力。


<details>
  <summary>Details</summary>
Motivation: CBCT图像中邻近牙体间对比度低且上下颌间界面不清导致的粘连伪影，使得三维牙齿分割难以获得高精度的边界与结构一致性。需要一种既能细化边界又能保持全局结构的方案。

Method: 提出了一个粗到细的级联网络AACNet，包含两个核心模块：Ambiguity Gated Boundary Refiner(基于熵的不确定性门控用于过渡区域的特征修正)和Signed Distance Map guided Anatomical Attention(利用带符号距离映射引入隐式几何约束以保持拓扑一致性)。

Result: 在125例CBCT数据集上，AACNet取得Dice 90.17%和HD95 3.63mm，显著优于现有方法；在外部测试集上HD95为2.19mm，显示良好的泛化性并适用于临床规划。

Conclusion: AACNet在处理CBCT中牙齿粘连与边界模糊方面表现优异，通过粗到细框架结合不确定性门控与带符号距离映射的结构性注意力，有效提升了边界精细化与拓扑一致性。

Abstract: Accurate three-dimensional (3D) tooth segmentation from Cone-Beam Computed Tomography (CBCT) is a prerequisite for digital dental workflows. However, achieving high-fidelity segmentation remains challenging due to adhesion artifacts in naturally occluded scans, which are caused by low contrast and indistinct inter-arch boundaries. To address these limitations, we propose the Anatomy Aware Cascade Network (AACNet), a coarse-to-fine framework designed to resolve boundary ambiguity while maintaining global structural consistency. Specifically, we introduce two mechanisms: the Ambiguity Gated Boundary Refiner (AGBR) and the Signed Distance Map guided Anatomical Attention (SDMAA). The AGBR employs an entropy based gating mechanism to perform targeted feature rectification in high uncertainty transition zones. Meanwhile, the SDMAA integrates implicit geometric constraints via signed distance map to enforce topological consistency, preventing the loss of spatial details associated with standard pooling. Experimental results on a dataset of 125 CBCT volumes demonstrate that AACNet achieves a Dice Similarity Coefficient of 90.17 \% and a 95\% Hausdorff Distance of 3.63 mm, significantly outperforming state-of-the-art methods. Furthermore, the model exhibits strong generalization on an external dataset with an HD95 of 2.19 mm, validating its reliability for downstream clinical applications such as surgical planning. Code for AACNet is available at https://github.com/shiliu0114/AACNet.

</details>


### [111] [Mon3tr: Monocular 3D Telepresence with Pre-built Gaussian Avatars as Amortization](https://arxiv.org/abs/2601.07518)
*Fangyu Lin,Yingdong Hu,Zhening Liu,Yufan Zhuang,Zehong Lin,Jun Zhang*

Main category: cs.CV

TL;DR: Mon3tr通过将3DGS参数化人模与单目驱动结合，采用离线重建+在线轻量传输与接收端动态属性修正，实现低带宽(<0.2 Mbps)、低延迟(~80 ms)、高帧率(~60 FPS)的移动端沉浸式3D远程呈现，效果优于现有多摄像头/点云方法。


<details>
  <summary>Details</summary>
Motivation: 目标是摆脱高成本多摄像头和高带宽体素/点云传输的限制，使沉浸式远程呈现可在移动设备(如Meta Quest 3)上实时运行。

Method: 方法包括：1) 离线阶段通过多视角重建构建用户特定的3DGS头像；2) 在线阶段仅用单目RGB摄像头实时捕捉身体和面部动作，提取运动与外观特征并以<0.2 Mbps通过WebRTC数据通道传输；3) 接收端在轻量级3DGS属性形变网络上对预构建avatar做动态属性校正以合成逼真运动与外观，目标约60 FPS。

Result: 实验显示在新姿势下PSNR>28 dB，端到端延迟约80 ms，带宽相比点云流降低>1000x，支持单目输入的实时合成与多场景鲁棒性。

Conclusion: Mon3tr提出了将3D Gaussian Splatting(3DGS)参数化人体建模首次应用于沉浸式远程呈现的框架，通过离线多视角重建+在线单目驱动的分工策略，实现低带宽、高帧率和低延迟的实时远程协作。

Abstract: Immersive telepresence aims to transform human interaction in AR/VR applications by enabling lifelike full-body holographic representations for enhanced remote collaboration. However, existing systems rely on hardware-intensive multi-camera setups and demand high bandwidth for volumetric streaming, limiting their real-time performance on mobile devices. To overcome these challenges, we propose Mon3tr, a novel Monocular 3D telepresence framework that integrates 3D Gaussian splatting (3DGS) based parametric human modeling into telepresence for the first time. Mon3tr adopts an amortized computation strategy, dividing the process into a one-time offline multi-view reconstruction phase to build a user-specific avatar and a monocular online inference phase during live telepresence sessions. A single monocular RGB camera is used to capture body motions and facial expressions in real time to drive the 3DGS-based parametric human model, significantly reducing system complexity and cost. The extracted motion and appearance features are transmitted at < 0.2 Mbps over WebRTC's data channel, allowing robust adaptation to network fluctuations. On the receiver side, e.g., Meta Quest 3, we develop a lightweight 3DGS attribute deformation network to dynamically generate corrective 3DGS attribute adjustments on the pre-built avatar, synthesizing photorealistic motion and appearance at ~ 60 FPS. Extensive experiments demonstrate the state-of-the-art performance of our method, achieving a PSNR of > 28 dB for novel poses, an end-to-end latency of ~ 80 ms, and > 1000x bandwidth reduction compared to point-cloud streaming, while supporting real-time operation from monocular inputs across diverse scenarios. Our demos can be found at https://mon3tr3d.github.io.

</details>


### [112] [ViewMorpher3D: A 3D-aware Diffusion Framework for Multi-Camera Novel View Synthesis in Autonomous Driving](https://arxiv.org/abs/2601.07540)
*Farhad G. Zanjani,Hong Cai,Amirhossein Habibian*

Main category: cs.CV

TL;DR: 提出 ViewMorpher3D，通过基于扩散模型的多视图联合增强，结合相机位姿与几何先验，提升驾驶场景渲染图像的真实感与跨视图一致性，有助于闭环仿真器的构建与评估。


<details>
  <summary>Details</summary>
Motivation: 现有基于 3D 重建的闭环仿真器（如使用 Gaussian Splatting）在渲染新视角时常出现伪影，特别是在视角外推或观测稀疏时，影响感知与规划模块的评估可靠性，因此需要一个能修复渲染缺陷并保证多视图一致性的图像增强方法。

Method: 设计一个基于图像扩散模型的多视图联合增强网络，输入为一组渲染视图及其相机位姿、3D 几何先验（如深度或点云）、以及时序相邻或空间重叠的参考视图。网络通过跨视图注意力或变换模块融合信息，利用扩散过程生成高质量图像，保持跨视图一致性。训练时采用感知损失、重建损失和跨视图一致性损失，或引入几何对齐损失以维持几何精度。

Result: 在真实驾驶数据集上，与单视图去伪影或修复方法对比，ViewMorpher3D 在 PSNR、SSIM、LPIPS 等图像质量指标上有显著提升；主观评估显示伪影减少且细节恢复更好，同时通过几何保真度度量（如深度一致性或相机再投影误差）验证未破坏场景几何。

Conclusion: 本论文提出了一种名为 ViewMorpher3D 的多视图图像增强框架，旨在提升自动驾驶场景中由 3D 重建技术（如 Gaussian Splatting）渲染出的图像的真实感与多视图一致性。通过联合处理多视角渲染视图，并结合相机位姿、3D 几何先验以及时序或空间相邻参考视图，模型能推断缺失细节、抑制渲染伪影并增强跨视图一致性。方法支持可变数量摄像头和灵活的参考/目标视图配置，适配多样传感器布局。实验表明在真实驾驶数据集上能显著提升图像质量指标，在降低伪影的同时保持几何保真度。

Abstract: Autonomous driving systems rely heavily on multi-view images to ensure accurate perception and robust decision-making. To effectively develop and evaluate perception stacks and planning algorithms, realistic closed-loop simulators are indispensable. While 3D reconstruction techniques such as Gaussian Splatting offer promising avenues for simulator construction, the rendered novel views often exhibit artifacts, particularly in extrapolated perspectives or when available observations are sparse.
  We introduce ViewMorpher3D, a multi-view image enhancement framework based on image diffusion models, designed to elevate photorealism and multi-view coherence in driving scenes. Unlike single-view approaches, ViewMorpher3D jointly processes a set of rendered views conditioned on camera poses, 3D geometric priors, and temporally adjacent or spatially overlapping reference views. This enables the model to infer missing details, suppress rendering artifacts, and enforce cross-view consistency.
  Our framework accommodates variable numbers of cameras and flexible reference/target view configurations, making it adaptable to diverse sensor setups. Experiments on real-world driving datasets demonstrate substantial improvements in image quality metrics, effectively reducing artifacts while preserving geometric fidelity.

</details>


### [113] [BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation](https://arxiv.org/abs/2601.07581)
*Ahmad AlMughrabi,Guillermo Rivo,Carlos Jiménez-Farfán,Umair Haroon,Farid Al-Areqi,Hyunjun Jung,Benjamin Busam,Ricardo Marques,Petia Radeva*

Main category: cs.CV

TL;DR: BenchSeg：55道菜、25k帧多视角食物视频分割基准；评测20种模型，记忆增强方法（SeTR-MLA+XMem2）在新视角泛化与时序一致性上表现最好，优于先前工作~2.63% mAP。


<details>
  <summary>Details</summary>
Motivation: 现有食物分割方法受限于多视角数据稀缺，且对新视角泛化能力差，影响食物体积与营养估算准确性，需构建多视角视频数据集并评估记忆增强方法的时序稳定性与泛化能力。

Method: 汇集来自 Nutrition5k、Vegetables & Fruits、MetaFood3D 和 FoodKit 的场景，收集自由360°相机运动的视频并逐帧精细标注；构建基准后评测20种最先进的分割模型（包含SAM、Transformer、CNN及大规模多模态模型），并将它们与视频记忆模块（如XMem2）结合进行对比实验。

Result: BenchSeg 提供了25,284帧标注的多视角视频基准；在该基准上，标准单帧分割器在新视角下性能显著下降，而记忆增强的视频分割方法表现更稳定。最佳模型SeTR-MLA+XMem2 在mAP上较 FoodMem 提升约2.63%。数据集与代码已公开。

Conclusion: BenchSeg 是一个针对多视角食物视频分割的新数据集和基准，包含55道菜场景与25,284帧标注，支持评估图像与视频记忆增强的分割模型。实验表明：单帧分割器在新视角下性能明显下降，而带记忆的视频方法能保持时序一致性；SeTR-MLA+XMem2 在mAP上优于先前工作约2.63%。

Abstract: Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.

</details>


### [114] [Robust Multicentre Detection and Classification of Colorectal Liver Metastases on CT: Application of Foundation Models](https://arxiv.org/abs/2601.07585)
*Shruti Atul Mali,Zohaib Salahuddin,Yumeng Zhang,Andre Aichert,Xian Zhong,Henry C. Woodruff,Maciej Bobowicz,Katrine Riklund,Juozas Kupčinskas,Lorenzo Faggioni,Roberto Francischello,Razvan L Miclea,Philippe Lambin*

Main category: cs.CV

TL;DR: 基于UMedPT的多任务AI管道在异构CT数据上实现了较高的CRLM病人级分类和病灶级检测性能，结合不确定性筛选与Grad-CAM能提升可靠性与可解释性，具有潜在临床应用价值。


<details>
  <summary>Details</summary>
Motivation: 目前多中心CT上可靠检测CRLM具有挑战性，数据异构性与可转移性差导致模型在实际临床场景中表现受限。研究旨在利用大型预训练模型（foundation models）提升鲁棒性，同时通过不确定性量化和可解释性增强临床信任度。

Method: 研究筛选并比较多种预训练模型，选择UMedPT作为骨干模型；在其上对分类任务添加MLP头，对病灶检测任务添加基于FCOS的检测头；使用EuCanImage（n=2437）和外部TCIA队列（n=197）进行训练与测试。引入不确定性度量用于置信度分层并结合Grad-CAM提供可解释性。

Result: 分类模型在合并测试集上AUC=0.90、灵敏度0.82，外部队列灵敏度0.85；剔除最不确定的20%病例后AUC提升至0.91、平衡准确率为0.86；决策曲线分析显示在阈值概率0.30-0.40区间有临床获益。检测模型总体检出率69.1%，按病灶大小四分位从30%增长到98%。Grad-CAM在高置信病例能突出病灶对应区域。

Conclusion: 该研究表明基于foundation model的AI管道在多中心、异构对比增强CT数据上，能够较为稳健且可解释地完成结直肠肝转移（CRLM）的病人级别分类与病灶级别检测，且加入不确定性量化与可解释性手段可提升可靠性与临床适用性。

Abstract: Colorectal liver metastases (CRLM) are a major cause of cancer-related mortality, and reliable detection on CT remains challenging in multi-centre settings. We developed a foundation model-based AI pipeline for patient-level classification and lesion-level detection of CRLM on contrast-enhanced CT, integrating uncertainty quantification and explainability. CT data from the EuCanImage consortium (n=2437) and an external TCIA cohort (n=197) were used. Among several pretrained models, UMedPT achieved the best performance and was fine-tuned with an MLP head for classification and an FCOS-based head for lesion detection. The classification model achieved an AUC of 0.90 and a sensitivity of 0.82 on the combined test set, with a sensitivity of 0.85 on the external cohort. Excluding the most uncertain 20 percent of cases improved AUC to 0.91 and balanced accuracy to 0.86. Decision curve analysis showed clinical benefit for threshold probabilities between 0.30 and 0.40. The detection model identified 69.1 percent of lesions overall, increasing from 30 percent to 98 percent across lesion size quartiles. Grad-CAM highlighted lesion-corresponding regions in high-confidence cases. These results demonstrate that foundation model-based pipelines can support robust and interpretable CRLM detection and classification across heterogeneous CT data.

</details>


### [115] [Diffusion in SPAD Signals](https://arxiv.org/abs/2601.07599)
*Lior Dvir,Nadav Torem,Yoav Y. Schechner*

Main category: cs.CV

TL;DR: 针对SPAD时间戳信号建立了似然与score函数，并结合扩散模型先验，利用时间信息提升低光子条件下的反问题重建效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于SPAD的成像多只利用总计数或积分强度，忽略了每个探测事件的精确时间信息。而这些时间戳包含关于入射光子到达统计的重要信息，可用于更精细地恢复场景或辐射率。结合扩散模型能够提供强大的先验，提升在极低光子条件下的重建性能。

Method: 首先基于SPAD探测事件的随机到达模型（泊松过程或指数间隔分布）建立原始时间戳序列的似然表达式；在此基础上计算对数似然的梯度（score函数）。将该score嵌入扩散模型框架中，将图像先验用预训练或联合训练的扩散模型表示，通过优化或采样实现反问题求解。比较了使用仅计数信息、使用时间戳信息以及不同光子计数情况下的重建差异。

Result: 推导出闭式或可计算形式的似然和对应的score函数；在合成和/或真实数据上展示，时间信息在低光子计数时显著提高重建精度，且在高计数时仍能带来边际收益。将score与扩散先验结合的策略在多个反问题（如成像重建）上优于仅用计数或传统先验的方法。

Conclusion: 本文导出了单光子雪崩二极管（SPAD）在固定光子通量下的原始信号似然函数，并基于此推导了该信号的score函数，用于反问题求解。结合扩散模型作为图像先验，提出了利用检测事件时间信息的重建策略。实验表明，在低光子计数和高光子计数条件下，利用时间信息能显著改善重建质量。

Abstract: We derive the likelihood of a raw signal in a single photon avalanche diode (SPAD), given a fixed photon flux. The raw signal comprises timing of detection events, which are nonlinearly related to the flux. Moreover, they are naturally stochastic. We then derive a score function of the signal. This is a key for solving inverse problems based on SPAD signals. We focus on deriving solutions involving a diffusion model, to express image priors. We demonstrate the effect of low or high photon counts, and the consequence of exploiting timing of detection events.

</details>


### [116] [UIKA: Fast Universal Head Avatar from Pose-Free Images](https://arxiv.org/abs/2601.07603)
*Zijian Wu,Boyao Zhou,Liangxiao Hu,Hongyu Liu,Yuan Sun,Xuan Wang,Xun Cao,Yujun Shen,Hao Zhu*

Main category: cs.CV

TL;DR: UIKA是一种从任意数量无姿态输入快速生成可动画高斯头部模型的方法：通过像素级UV对应将图像信息投影到UV空间，使用可学习的UV token及屏幕/UV层的注意力聚合多视图信息，解码为规范化高斯属性，并借助大规模合成数据训练，从而在单目与多视图场景均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统头像构建依赖于多视角工作室采集和长时间针对特定人物的优化，成本高且可扩展性差。作者希望实现从少量甚至单帧、非配准输入中快速、可动画地生成高质量头像模型，因此在表示、网络设计和数据准备三方面进行了重新思考与设计。

Method: 方法包括：1）UV引导的头像建模：对每张输入图像估计像素级面部对应，并将有效像素颜色从屏幕空间重投影到UV空间；2）可学习的UV token设计：在屏幕和UV两个层面应用注意力机制，聚合来自所有视图的UV信息；3）解码模块：将聚合后的UV token解码为规范化高斯属性（位置、颜色、大小等）；4）数据准备：构建大规模合成训练集以增强身份多样性和泛化能力；5）前向可推理的、高效的模型架构，避免长时优化。

Result: 在单目和多视图设置下，UIKA显著优于现有方法；能够从单张图像或手机视频等非专业采集源生成高质量、可动画的高斯头模型。作者还提供了项目页面和大规模合成训练数据支持。

Conclusion: UIKA提出了一种基于UV引导的可动画高斯头部模型，能从任意数量、无姿态标注的输入（单张图像、多视角采集、手机视频）快速生成头像模型。该方法通过像素级面部对应估计将屏幕空间像素重投影到与相机姿态和表情无关的UV空间，利用可学习的UV token并在屏幕与UV层面应用注意力机制，最终解码为规范化的高斯属性。此外，通过构建大规模合成身份丰富的数据集进行训练，取得了比现有方法更好的性能。

Abstract: We present UIKA, a feed-forward animatable Gaussian head model from an arbitrary number of unposed inputs, including a single image, multi-view captures, and smartphone-captured videos. Unlike the traditional avatar method, which requires a studio-level multi-view capture system and reconstructs a human-specific model through a long-time optimization process, we rethink the task through the lenses of model representation, network design, and data preparation. First, we introduce a UV-guided avatar modeling strategy, in which each input image is associated with a pixel-wise facial correspondence estimation. Such correspondence estimation allows us to reproject each valid pixel color from screen space to UV space, which is independent of camera pose and character expression. Furthermore, we design learnable UV tokens on which the attention mechanism can be applied at both the screen and UV levels. The learned UV tokens can be decoded into canonical Gaussian attributes using aggregated UV information from all input views. To train our large avatar model, we additionally prepare a large-scale, identity-rich synthetic training dataset. Our method significantly outperforms existing approaches in both monocular and multi-view settings. Project page: https://zijian-wu.github.io/uika-page/

</details>


### [117] [PARL: Position-Aware Relation Learning Network for Document Layout Analysis](https://arxiv.org/abs/2601.07620)
*Fuyuan Liu,Dianyu Yu,He Ren,Nayu Liu,Xiaomian Kang,Delai Qiu,Fa Zhang,Genpeng Zhen,Shengping Liu,Jiaen Liang,Wei Huang,Yining Wang,Junnan Zhu*

Main category: cs.CV

TL;DR: 请写出该论文的TL;DR摘要


<details>
  <summary>Details</summary>
Motivation: 请写出该论文的动机

Method: 请写出该论文的方法概述

Result: 请写出该论文的主要结果

Conclusion: 请写出该论文的结论

Abstract: Document layout analysis aims to detect and categorize structural elements (e.g., titles, tables, figures) in scanned or digital documents. Popular methods often rely on high-quality Optical Character Recognition (OCR) to merge visual features with extracted text. This dependency introduces two major drawbacks: propagation of text recognition errors and substantial computational overhead, limiting the robustness and practical applicability of multimodal approaches. In contrast to the prevailing multimodal trend, we argue that effective layout analysis depends not on text-visual fusion, but on a deep understanding of documents' intrinsic visual structure. To this end, we propose PARL (Position-Aware Relation Learning Network), a novel OCR-free, vision-only framework that models layout through positional sensitivity and relational structure. Specifically, we first introduce a Bidirectional Spatial Position-Guided Deformable Attention module to embed explicit positional dependencies among layout elements directly into visual features. Second, we design a Graph Refinement Classifier (GRC) to refine predictions by modeling contextual relationships through a dynamically constructed layout graph. Extensive experiments show PARL achieves state-of-the-art results. It establishes a new benchmark for vision-only methods on DocLayNet and, notably, surpasses even strong multimodal models on M6Doc. Crucially, PARL (65M) is highly efficient, using roughly four times fewer parameters than large multimodal models (256M), demonstrating that sophisticated visual structure modeling can be both more efficient and robust than multimodal fusion.

</details>


### [118] [GeoMotionGPT: Geometry-Aligned Motion Understanding with Large Language Models](https://arxiv.org/abs/2601.07632)
*Zhankai Ye,Bofan Li,Yukai Jin,Shuoqiu Li,Wei Wang,Yanfu Zhang,Shangqian Gao,Xin Liu*

Main category: cs.CV

TL;DR: 通过在码本与LLM嵌入上强制正交性并用稀疏投影连接两者，本文实现了运动与语言模态的几何对齐，在HumanML3D上实现约20%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将运动量化与语义嵌入学习解耦，仅通过token ID连接，无法对齐运动空间与嵌入空间的内在几何，限制了LLM的细粒度运动推理能力。

Method: 使用Decoder-only量化器结合Gumbel-Softmax实现可微训练与代码簿平衡使用；通过稀疏投影将运动代码映射到LLM嵌入空间并保持正交性；采用两阶段正交正则调度，在分词器训练和LLM微调阶段分别施加软约束以维持几何对齐同时不妨碍语义适配。

Result: 在HumanML3D数据集上，所提框架比现有最先进方法提升约20%的性能，证明统一的几何基础能有效增强LLM的运动推理能力。

Conclusion: 本文提出通过在运动代码簿和LLM嵌入空间上显式施加正交性来统一几何基，从而改善运动-语言对齐，提升LLM对细粒度动作推理能力。

Abstract: Discrete motion tokenization has recently enabled Large Language Models (LLMs) to serve as versatile backbones for motion understanding and motion-language reasoning. However, existing pipelines typically decouple motion quantization from semantic embedding learning, linking them solely via token IDs. This approach fails to effectively align the intrinsic geometry of the motion space with the embedding space, thereby hindering the LLM's capacity for nuanced motion reasoning. We argue that alignment is most effective when both modalities share a unified geometric basis. Therefore, instead of forcing the LLM to reconstruct the complex geometry among motion tokens from scratch, we present a novel framework that explicitly enforces orthogonality on both the motion codebook and the LLM embedding space, ensuring that their relational structures naturally mirror each other. Specifically, we employ a decoder-only quantizer with Gumbel-Softmax for differentiable training and balanced codebook usage. To bridge the modalities, we use a sparse projection that maps motion codes into the LLM embedding space while preserving orthogonality. Finally, a two-stage orthonormal regularization schedule enforces soft constraints during tokenizer training and LLM fine-tuning to maintain geometric alignment without hindering semantic adaptation. Extensive experiments on HumanML3D demonstrate that our framework achieves a 20% performance improvement over current state-of-the-art methods, validating that a unified geometric basis effectively empowers the LLM for nuanced motion reasoning.

</details>


### [119] [StdGEN++: A Comprehensive System for Semantic-Decomposed 3D Character Generation](https://arxiv.org/abs/2601.07660)
*Yuze He,Yanning Zhou,Wang Zhao,Jingwen Ye,Zhongkai Wu,Ran Yi,Yong-Jin Liu*

Main category: cs.CV

TL;DR: StdGEN++ 通过双分支语义重建、语义表面提取与视频扩散纹理分解，实现高分辨率、语义可控的可编辑三维角色生成，适合游戏与动画等工业生产流程。


<details>
  <summary>Details</summary>
Motivation: 现有三维生成方法缺乏结构化语义与组件独立性，不满足游戏与动画等工业流水线对部件可编辑性、物理动画和精细控制的需求。

Method: 提出 Dual-Branch S-LRM（双分支语义感知大重建模型），同时前向重建几何、颜色与组件语义；引入兼容混合隐式场的语义表面提取形式化方法，并结合粗到细提议方案以降低内存并支持高分辨率网格；提出基于视频扩散的视频-扩散纹理分解模块以将外观解耦为可编辑层。

Result: 在几何精度与语义解耦方面显著优于现有方法；生成的模型支持非破坏性编辑、符合物理的动画及视线追踪等下游能力，满足自动化角色资产生产需求。

Conclusion: StdGEN++ 提出了一种面向工业应用的角色生成系统，通过语义分解实现可编辑的高保真三维角色模型，弥补了现有方法单体网格的局限。

Abstract: We present StdGEN++, a novel and comprehensive system for generating high-fidelity, semantically decomposed 3D characters from diverse inputs. Existing 3D generative methods often produce monolithic meshes that lack the structural flexibility required by industrial pipelines in gaming and animation. Addressing this gap, StdGEN++ is built upon a Dual-branch Semantic-aware Large Reconstruction Model (Dual-Branch S-LRM), which jointly reconstructs geometry, color, and per-component semantics in a feed-forward manner. To achieve production-level fidelity, we introduce a novel semantic surface extraction formalism compatible with hybrid implicit fields. This mechanism is accelerated by a coarse-to-fine proposal scheme, which significantly reduces memory footprint and enables high-resolution mesh generation. Furthermore, we propose a video-diffusion-based texture decomposition module that disentangles appearance into editable layers (e.g., separated iris and skin), resolving semantic confusion in facial regions. Experiments demonstrate that StdGEN++ achieves state-of-the-art performance, significantly outperforming existing methods in geometric accuracy and semantic disentanglement. Crucially, the resulting structural independence unlocks advanced downstream capabilities, including non-destructive editing, physics-compliant animation, and gaze tracking, making it a robust solution for automated character asset production.

</details>


### [120] [Variational Contrastive Learning for Skeleton-based Action Recognition](https://arxiv.org/abs/2601.07666)
*Dang Dinh Nguyen,Decky Aspandi Latif,Titus Zaharia*

Main category: cs.CV

TL;DR: 把变分潜变量模型和对比自监督学习结合，获得了更具不确定性与语义性的动作骨架表示，在多个基准和低标签场景下优于传统对比方法，并在定性上更关注关键关节。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法为判别式，难以捕捉人体运动的变异性和不确定性；需要引入概率性表示以更好地刻画动作多样性并提升在不同数据集与低监督情况下的泛化。

Method: 在对比学习范式中加入变分潜变量建模（类似VAE），通过概率编码器对动作骨架序列进行潜空间分布建模，并在潜空间执行对比损失，强化结构化与不确定性表达；同时可能使用数据增强、动作为中心的采样与判别头进行下游评估。

Result: 在三个常用骨架动作识别基准（未指明具体数据集）上进行大量实验，结果持续优于现有方法，尤其在低标签设置下提升明显；定性分析显示学到的特征更关注重要关节、与动作和样本特性更相关。

Conclusion: 提出了一种变分对比学习框架，结合概率潜变量建模与对比自监督学习，能学习结构化、有语义的表示，泛化性好，在低标签环境下效果显著。

Abstract: In recent years, self-supervised representation learning for skeleton-based action recognition has advanced with the development of contrastive learning methods. However, most of contrastive paradigms are inherently discriminative and often struggle to capture the variability and uncertainty intrinsic to human motion. To address this issue, we propose a variational contrastive learning framework that integrates probabilistic latent modeling with contrastive self-supervised learning. This formulation enables the learning of structured and semantically meaningful representations that generalize across different datasets and supervision levels. Extensive experiments on three widely used skeleton-based action recognition benchmarks show that our proposed method consistently outperforms existing approaches, particularly in low-label regimes. Moreover, qualitative analyses show that the features provided by our method are more relevant given the motion and sample characteristics, with more focus on important skeleton joints, when compared to the other methods.

</details>


### [121] [Advancing Multinational License Plate Recognition Through Synthetic and Real Data Fusion: A Comprehensive Evaluation](https://arxiv.org/abs/2601.07671)
*Rayson Laroca,Valter Estevam,Gladston J. P. Moreira,Rodrigo Minetto,David Menotti*

Main category: cs.CV

TL;DR: 将大规模合成数据（模板、字符置换、GAN）与真实数据结合，能显著提升车牌OCR在多数据集和跨区域场景的性能，且在数据稀缺时依然有效，混合方法优于单一合成策略，并在速度-精度上提供最佳模型选择。


<details>
  <summary>Details</summary>
Motivation: 当前LPR研究尽管采用合成数据，但仍存在方法局限和效果不稳定。研究动机是系统性地探索真实与合成数据的结合方式，评估不同合成策略的贡献，并找到在多区域、多数据集情形下提升OCR性能的有效方案。

Method: 对16个OCR模型在12个公开数据集上进行基准测试；引入三类合成数据生成方法（模板、字符置换、GAN），并尝试不同的真实-合成数据混合策略；评估同数据集（intra）和跨数据集（cross）性能；分析少量真实训练数据情形下的表现；比较模型的精度-速度权衡。

Result: 大量合成数据显著提升模型性能；三种合成方法均有效，混合使用效果最好；合成数据能弥补训练数据不足，在仅使用少量真实数据时仍能获得高性能；在端到端评测中超过现有SOTA和商业系统；给出不同模型在精度-速度上的推荐。

Conclusion: 该研究表明，大规模合成数据与真实数据相结合能显著提升车牌识别（LPR）性能，在跨数据集和同数据集评估中均有显著提升。三种合成方法（模板生成、字符置换、GAN）各自有贡献，组合使用具有协同效应，且在数据稀缺场景下亦能获得良好效果。最终模型在端到端评估中优于现有最先进方法及商业系统，并分析了不同模型在速度与精度之间的权衡。

Abstract: Automatic License Plate Recognition is a frequent research topic due to its wide-ranging practical applications. While recent studies use synthetic images to improve License Plate Recognition (LPR) results, there remain several limitations in these efforts. This work addresses these constraints by comprehensively exploring the integration of real and synthetic data to enhance LPR performance. We subject 16 Optical Character Recognition (OCR) models to a benchmarking process involving 12 public datasets acquired from various regions. Several key findings emerge from our investigation. Primarily, the massive incorporation of synthetic data substantially boosts model performance in both intra- and cross-dataset scenarios. We examine three distinct methodologies for generating synthetic data: template-based generation, character permutation, and utilizing a Generative Adversarial Network (GAN) model, each contributing significantly to performance enhancement. The combined use of these methodologies demonstrates a notable synergistic effect, leading to end-to-end results that surpass those reached by state-of-the-art methods and established commercial systems. Our experiments also underscore the efficacy of synthetic data in mitigating challenges posed by limited training data, enabling remarkable results to be achieved even with small fractions of the original training data. Finally, we investigate the trade-off between accuracy and speed among different models, identifying those that strike the optimal balance in each intra-dataset and cross-dataset settings.

</details>


### [122] [Leveraging 3D Representation Alignment and RGB Pretrained Priors for LiDAR Scene Generation](https://arxiv.org/abs/2601.07692)
*Nicolas Sereyjol-Garros,Ellington Kirby,Victor Besnier,Nermin Samet*

Main category: cs.CV

TL;DR: R3DPA injects image-pretrained and self-supervised 3D features into LiDAR generative modeling, improving quality and control and achieving SOTA on KITTI-360.


<details>
  <summary>Details</summary>
Motivation: LiDAR data is scarce compared to large RGB datasets; existing LiDAR generative models are limited by data volume. The goal is to bring rich image-based generative priors and strong 3D self-supervision to LiDAR synthesis.

Method: Align intermediate features of a generative model with self-supervised 3D representations; transfer image-pretrained generative model knowledge to LiDAR generation; use an unconditional model with inference-time control for object inpainting and scene mixing.

Result: Improved generation quality and control, state-of-the-art performance on KITTI-360, and released code/models.

Conclusion: R3DPA successfully leverages image-pretrained generative priors and self-supervised 3D features to improve LiDAR scene synthesis, achieving state-of-the-art on KITTI-360. Its main limitations include reliance on pretrained models and potential domain gaps between images and LiDAR.

Abstract: LiDAR scene synthesis is an emerging solution to scarcity in 3D data for robotic tasks such as autonomous driving. Recent approaches employ diffusion or flow matching models to generate realistic scenes, but 3D data remains limited compared to RGB datasets with millions of samples. We introduce R3DPA, the first LiDAR scene generation method to unlock image-pretrained priors for LiDAR point clouds, and leverage self-supervised 3D representations for state-of-the-art results. Specifically, we (i) align intermediate features of our generative model with self-supervised 3D features, which substantially improves generation quality; (ii) transfer knowledge from large-scale image-pretrained generative models to LiDAR generation, mitigating limited LiDAR datasets; and (iii) enable point cloud control at inference for object inpainting and scene mixing with solely an unconditional model. On the KITTI-360 benchmark R3DPA achieves state of the art performance. Code and pretrained models are available at https://github.com/valeoai/R3DPA.

</details>


### [123] [Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model](https://arxiv.org/abs/2601.07695)
*Siwen Jiao,Tianxiong Lv,Kangan Qian,Chenxu Zhao,Xiuyuan Zhu,Tianlun Li,Xiaolong Cheng,Jinyu Li,Zhihao Liao,Yang Cai*

Main category: cs.CV

TL;DR: Introduce SNRA and AP-GRPO to overcome RL reward sparsity and advantage collapse for numerical 3D tasks; build Numerical3D-50k; achieve supervised-level performance with better data efficiency


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with precise numerical prediction for 3D; RL relative ranking causes reward sparsity and advantage collapse, losing near-miss samples

Method: Describe the methods used: SNRA and AP-GRPO, dataset Numerical3D-50k, experiments comparing to supervised methods

Result: AP-GRPO with SNRA yields dense rewards, preserves absolute gradients, matches supervised performance with higher data efficiency; dataset constructed

Conclusion: SNRA and AP-GRPO effectively activate 3D numerical reasoning in VLMs without architecture changes and mitigate data utilization bottlenecks

Abstract: Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes "near-miss" samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.

</details>


### [124] [Hidden Monotonicity: Explaining Deep Neural Networks via their DC Decomposition](https://arxiv.org/abs/2601.07700)
*Jakob Paul Zimmermann,Georg Loho*

Main category: cs.CV

TL;DR: 通过将ReLU网路分解为两单调凸部分并提出SplitCAM/SplitLRP，以及直接训练两个单调网络之差，作者在ImageNet-S上改进了显著性方法并获得更强自解释性。


<details>
  <summary>Details</summary>
Motivation: 动机是单调性已被证明能提升模型可解释性，但并非所有目标函数可由单调网络良好逼近，因而探索如何在非单调情形下仍能利用单调性来提高可解释性。

Method: 方法包括：1) 改进ReLU网络分解为两个单调凸函数的算法，解决权重爆炸的数值问题，并基于此提出SplitCAM与SplitLRP显著性映射方法；2) 直接训练形式为f=g-h的模型，其中g、h均为单调神经网络，以获得自解释性；在VGG16与ResNet18上于ImageNet-S数据集进行实验证明。

Result: 在ImageNet-S上，SplitCAM与SplitLRP在所有Quantus显著性评估指标类别上均超越现有方法；另外，差分单调网络（g-h）展示出强的自解释性属性。

Conclusion: 本文展示了即使目标函数非单调，单调性仍可用于提升神经网络的可解释性，通过两条途径：一是将已训练的ReLU网络分解为两个单调且凸的部分，并在数值稳定性上做改进，获得新的显著性方法SplitCAM和SplitLRP，在ImageNet-S上的Quantus指标上优于现有方法；二是直接以两个单调神经网络之差构建模型，从而得到具自解释性的系统。

Abstract: It has been demonstrated in various contexts that monotonicity leads to better explainability in neural networks. However, not every function can be well approximated by a monotone neural network. We demonstrate that monotonicity can still be used in two ways to boost explainability. First, we use an adaptation of the decomposition of a trained ReLU network into two monotone and convex parts, thereby overcoming numerical obstacles from an inherent blowup of the weights in this procedure. Our proposed saliency methods -- SplitCAM and SplitLRP -- improve on state of the art results on both VGG16 and Resnet18 networks on ImageNet-S across all Quantus saliency metric categories. Second, we exhibit that training a model as the difference between two monotone neural networks results in a system with strong self-explainability properties.

</details>


### [125] [FMAC: a Fair Fiducial Marker Accuracy Comparison Software](https://arxiv.org/abs/2601.07723)
*Guillaume J. Laurent,Patrick Sandoz*

Main category: cs.CV

TL;DR: 用高保真合成图像和低差异采样在6自由度上系统评估标志点姿态估计精度；基于物理光线追踪渲染相机失真与模糊，提供验证与评估方法并开源代码。


<details>
  <summary>Details</summary>
Motivation: 需要一个公平、可重复且细分到各自由度的评估框架，避免现实采集中不可控因素并探究各姿态自由度对估计误差的影响，从而比较不同fiducial标志的性能。

Method: 开发基于物理的光线追踪渲染器，直接采用相机标准标定参数，模拟像差、散焦和衍射模糊，并对锐边做亚像素采样；用低差异采样在6自由度空间生成大量样本，计算姿态误差并通过36对组合图检视自由度相关性；用已知标志进行实验验证。

Result: Method for fair comparisons of pose estimation accuracy using fiducial markers through high-fidelity synthetic images and low-discrepancy sampling across 6-DOF, rendering with PBR ray tracer reproducing camera calibration, distortions, defocus, diffraction, sub-pixel edge sampling; validates renderer; proposes evaluation metrics applied to known markers; code open-source.

Conclusion: 高保真渲染与系统化采样可实现公平、细粒度的姿态估计比较，能揭示不同标志在各自由度上的误差特性，工具可用于设计和选择更鲁棒的fiducial标志。

Abstract: This paper presents a method for carrying fair comparisons of the accuracy of pose estimation using fiducial markers. These comparisons rely on large sets of high-fidelity synthetic images enabling deep exploration of the 6 degrees of freedom. A low-discrepancy sampling of the space allows to check the correlations between each degree of freedom and the pose errors by plotting the 36 pairs of combinations. The images are rendered using a physically based ray tracing code that has been specifically developed to use the standard calibration coefficients of any camera directly. The software reproduces image distortions, defocus and diffraction blur. Furthermore, sub-pixel sampling is applied to sharp edges to enhance the fidelity of the rendered image. After introducing the rendering algorithm and its experimental validation, the paper proposes a method for evaluating the pose accuracy. This method is applied to well-known markers, revealing their strengths and weaknesses for pose estimation. The code is open source and available on GitHub.

</details>


### [126] [Evaluating the encoding competence of visual language models using uncommon actions](https://arxiv.org/abs/2601.07737)
*Chen Ling,Nai Ding*

Main category: cs.CV

TL;DR: 提出UAIT: 一个针对不合常理动作场景的图文理解评测集，通过LLM+图像生成合成反常常识样本，测试模型对语法正确但语义反常的判断。实验显示当前VLM远低于人类，微调可改进。


<details>
  <summary>Details</summary>
Motivation: 现有数据集偏向常见场景使模型依赖频率统计而非真实语义理解；需要一种挑战模型语义推理、代理-受体关系与物理可行性的基准，推动具有真实视觉语义推理能力模型的发展。

Method: 半自动化合成流程：使用大型语言模型进行few-shot提示工程生成不合常识动作描述，并用文本到图像生成模型合成视觉样本；为每对图文设计多项选择问题用于精细化推理测试；评估多种SOTA VLM及对比学习模型，并进行微调实验。

Result: UAIT dataset: benchmark of uncommon-sense action image-text pairs to test VLM semantic understanding; semi-automated synthesis via LLMs, prompts, text-to-image; multiple-choice questions; evaluation shows VLMs lag humans; fine-tuning helps lightweight models; provides diagnostics and directions.

Conclusion: UAIT揭示了VLM在区分语法合理与语义合理上的薄弱，提供了用于诊断与改进视觉语义推理能力的资源与方向。

Abstract: We propose UAIT (Uncommon-sense Action Image-Text) dataset, a new evaluation benchmark designed to test the semantic understanding ability of visual language models (VLMs) in uncommon-sense action scenes. Unlike previous datasets that focus on common visual scenes with statistical frequency advantages, UAIT challenges models with grammatically reasonable but semantically counter-common sense image-text pairs. Such tasks require models to go beyond superficial pattern recognition and demonstrate a deep understanding of agent-patient relationships and physical feasibility. To build UAIT, we designed a semi-automated process to synthesize high-quality uncommon-sense image-text samples using large language models, few-shot prompt engineering, and text-to-image generation. Each sample is accompanied by a carefully designed multiple-choice question to test the model's competence in fine-grained reasoning. We evaluate multiple state-of-the-art visual language models and compare them with models based on contrastive learning. Experiments show that all models perform significantly worse than humans in semantic judgment, especially in distinguishing grammatical correctness from semantic rationality. Further experiments show that even the lightweight model can improve its accuracy after fine-tuning, demonstrating the great potential of directional adaptation. This study not only reveals the key weaknesses of VLMs, but also provides diagnostic tools and research directions for the development of robust models with real visual semantic reasoning capabilities.

</details>


### [127] [On the application of the Wasserstein metric to 2D curves classification](https://arxiv.org/abs/2601.07749)
*Agnieszka Kaliszewska,Monika Syga*

Main category: cs.CV

TL;DR: 引入基于离散概率测度的加权Wasserstein距离变体，对曲线片段赋权以聚焦局部差异，在考古学2D曲线聚类任务中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的整体距离度量难以突出曲线的局部差异，而在考古学等应用中，某些曲线片段比整体更具判别力，需设计可聚焦片段的重要性并反映在距离度量中的方法。

Method: 通过引入若干离散概率测度来表示曲线片段的重要性，在Wasserstein距离计算中对不同片段赋予不同权重，形成多种变体；并将这些距离用于曲线的聚类分析。

Result: 在考古学数据上的聚类实验表明，带权重的Wasserstein变体能够提升针对指定片段的分类/聚类效果，验证了方法在强调局部信息方面的有效性。

Conclusion: 提出了一类基于加权Wasserstein距离的变体，能够在2D曲线分类时对指定片段加权关注，从而更好地体现局部差异。

Abstract: In this work we analyse a number of variants of the Wasserstein distance which allow to focus the classification on the prescribed parts (fragments) of classified 2D curves. These variants are based on the use of a number of discrete probability measures which reflect the importance of given fragments of curves. The performance of this approach is tested through a series of experiments related to the clustering analysis of 2D curves performed on data coming from the field of archaeology.

</details>


### [128] [Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding](https://arxiv.org/abs/2601.07761)
*Yanxiang Huang,Guohua Gao,Zhaoyang Wei,Jianyuan Ni*

Main category: cs.CV

TL;DR: 提出CoE：用轻量证据定位和强化学习优化的证据锚定协议，结合大规模双注释数据，提升视频理解的准确性并降低幻觉与计算成本。


<details>
  <summary>Details</summary>
Motivation: LVLM 在视频推理中要么进行冗长的计算以保证准确性，要么采用高效但未被证据支撑的方法导致幻觉。为在效率与可靠性之间取得平衡，需在架构上分离并协同优化感知与推理两个模块。

Method: CoE 包括两个核心创新模块：1) 轻量级证据定位模块（EGM），作为查询引导的过滤器，用于动态识别并提取紧凑且高保真度的视觉证据；2) 基于强化学习优化的证据锚定协议，通过复合奖励机制强制模型在推理时严格引用已识别的时间锚点以减少幻觉。

Result: 作者构建了包含双重标注的大规模CoE-Instruct数据集（164k 样本），并在Video-MME、MVBench、VSI-Bench 等五个基准上进行大量实验。CoE 增强模型在准确率上显著优于现有方法，达到了新的最先进水平。

Conclusion: 本文提出Chain of Evidence (CoE) 框架，通过将感知定位与推理效率架构上解耦并联合优化，解决了大视觉语言模型在视频推理中面临的高计算代价与幻觉风险的矛盾。

Abstract: Large Vision-Language Models (LVLMs) face a fundamental dilemma in video reasoning: they are caught between the prohibitive computational costs of verbose reasoning and the hallucination risks of efficient, ungrounded approaches. To resolve this, we introduce the Chain of Evidence (CoE), a novel framework that architecturally decouples and co-optimizes perceptual grounding and reasoning efficiency. CoE incorporates two core innovations: (1) A lightweight Evidence Grounding Module (EGM) that acts as a query-guided filter, dynamically identifying and extracting a compact set of high-fidelity visual evidence; and (2) An Evidence-Anchoring Protocol optimized via Reinforcement Learning. Crucially, we design a composite reward mechanism that enforces process alignment, compelling the model to strictly reference identified temporal anchors during deduction, thereby mitigating hallucinations. To enable this, we construct CoE-Instruct, a large-scale dataset (164k samples) featuring a novel dual-annotation schema for separate perception and reasoning supervision. Extensive experiments on five benchmarks, including Video-MME, MVBench, and VSI-Bench, demonstrate that CoE-enhanced models establish a new state-of-the-art. They significantly outperform existing methods in accuracy, proving CoE to be a powerful and practical paradigm for reliable video understanding.

</details>


### [129] [Beyond External Guidance: Unleashing the Semantic Richness Inside Diffusion Transformers for Improved Training](https://arxiv.org/abs/2601.07773)
*Lingchen Sun,Rongyuan Wu,Zhengqiang Zhang,Ruibin Li,Yujing Sun,Shuaizheng Liu,Lei Zhang*

Main category: cs.CV

TL;DR: 提出一种仅用模型内部特征自监督的DiT加速训练方法，通过短期用VAE对齐浅层、再用classifier-free guidance强化中间特征，生成内部监督以训练新DiT，实现无需外部预训练模型的快速高质量收敛。


<details>
  <summary>Details</summary>
Motivation: 现有通过外部语义特征（如DINO）引导DiT训练虽能加速收敛但引入外部依赖，降低灵活性；作者认为DiT本身有能力自我引导，从而避免外部依赖。

Method: 先短期（如40个epoch）用预训练VAE的潜在表示对齐DiT的浅层特征，然后对中间特征应用无分类器引导（classifier-free guidance）以增强判别性与语义表达能力，利用这些内部学习到的特征作为监督信号对新DiT进行训练，无需外部预训练网络。

Result: 在仅使用内部特征监督的前提下，方法显著提升收敛速度和生成质量，相较其它自包含方法有大幅提升，并可在质量和速度上超越REPA，同时具备更高的骨干网络适配性与广泛任务推广潜力。

Conclusion: 本文提出Self-Transcendence方法，通过仅利用模型内部特征监督加速DiT（Diffusion Transformer）训练，实现快速收敛并提升生成质量，甚至超越依赖外部预训练网络的REPA。

Abstract: Recent works such as REPA have shown that guiding diffusion models with external semantic features (e.g., DINO) can significantly accelerate the training of diffusion transformers (DiTs). However, this requires the use of pretrained external networks, introducing additional dependencies and reducing flexibility. In this work, we argue that DiTs actually have the power to guide the training of themselves, and propose \textbf{Self-Transcendence}, a simple yet effective method that achieves fast convergence using internal feature supervision only. It is found that the slow convergence in DiT training primarily stems from the difficulty of representation learning in shallow layers. To address this, we initially train the DiT model by aligning its shallow features with the latent representations from the pretrained VAE for a short phase (e.g., 40 epochs), then apply classifier-free guidance to the intermediate features, enhancing their discriminative capability and semantic expressiveness. These enriched internal features, learned entirely within the model, are used as supervision signals to guide a new DiT training. Compared to existing self-contained methods, our approach brings a significant performance boost. It can even surpass REPA in terms of generation quality and convergence speed, but without the need for any external pretrained models. Our method is not only more flexible for different backbones but also has the potential to be adopted for a wider range of diffusion-based generative tasks. The source code of our method can be found at https://github.com/csslc/Self-Transcendence.

</details>


### [130] [Vision-Language Model for Accurate Crater Detection](https://arxiv.org/abs/2601.07795)
*Patrick Bauer,Marius Schwinning,Florian Renk,Andreas Weinmann,Hichem Snoussi*

Main category: cs.CV

TL;DR: 该论文提出基于Vision Transformer的OWLv2检测器，结合LoRA微调和CIoU+对比损失，在IMPACT高分辨率LROC数据上实现了高鲁棒性的环形山检测（召回94.0%、精度73.1%）。


<details>
  <summary>Details</summary>
Motivation: ESA对Argonaut登月器的安全着陆高度依赖可靠的环形山检测，月球影像中环形山数量巨大、形状与尺寸多样且受光照与地形影响显著，传统方法难以在复杂成像条件下稳定工作，因此需要基于深度学习的鲁棒检测方法。

Method: 方法包括：1）采用OWLv2检测框架并基于Vision Transformer构建主干；2）使用IMPACT项目的人工标注数据进行微调；3）通过Low-Rank Adaptation嵌入可训练参数以减少微调成本；4）设计复合损失函数：CIoU用于定位，加入对比损失增强分类区分能力；5）在IMPACT测试集上评估性能并可视化检测结果。

Result: 在IMPACT测试集上取得了最高召回率94.0%和最高精度73.1%，并在复杂光照与崎岖地形条件下展示了满意的可视化检测结果，表明方法在各种成像条件下具有较好鲁棒性。

Conclusion: 本文提出了一种基于OWLv2（基于Vision Transformer）的深度学习月表环形山检测算法，结合低秩适应（LoRA）进行参数高效微调，并采用CIoU与对比损失的复合损失函数，实现了在IMPACT手工标注高分辨率LROC影像上的高召回率（最高94.0%）和较高精度（最高73.1%）。

Abstract: The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.

</details>


### [131] [Exchange Is All You Need for Remote Sensing Change Detection](https://arxiv.org/abs/2601.07805)
*Sijun Dong,Siming Fu,Kaiyu Li,Xiangyong Cao,Xiaoliang Meng,Bo Du*

Main category: cs.CV

TL;DR: 本文提出 SEED：通过无参数的双时相特征交换及权重共享的 Siamese 编码器-解码器，实现高效且信息保持的变化检测，理论证明其优越性并在多个数据集和骨干上达成或超过 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法依赖显式差分（如减法或拼接）进行双时相特征融合，可能导致信息损失或结构复杂；探索更简单且信息保持的融合机制以提升鲁棒性与可解释性。

Method: 提出 SEED：共享权重的双分支（Siamese）编码器与解码器，中间通过交换双时相的特征图（无参数操作）实现信息融合；在理论分析中将交换建模为正交置换算子，证明其信息保持性质；在五个数据集和三种骨干上做大规模对比实验；并提出将现成分割模型通过插入特征交换转换为变化检测（SEG2CD）。

Result: SEED 在 SYSU-CD、LEVIR-CD、PX-CLCD、WaterCD 和 CDD 等五个基准上，以及 SwinT、EfficientNet、ResNet 三个骨干上，取得与或优于现有最先进方法的性能；SEG2CD 证明普通语义分割模型仅需插入特征交换即可成为竞争力的变化检测器；理论和实验支持特征交换在保持信息与优化风险方面的优势。

Conclusion: SEED 通过参数共享的 Siamese 编码器-交换-解码器范式，用参数无关的特征交换替代显式差分，实验证明在多个基准和骨干网络上能匹配或超越 SOTA；理论上将特征交换形式化为正交置换算子，并证明在像素一致性下保持互信息和 Bayes 最优风险，而算术融合常造成信息损失。

Abstract: Remote sensing change detection fundamentally relies on the effective fusion and discrimination of bi-temporal features. Prevailing paradigms typically utilize Siamese encoders bridged by explicit difference computation modules, such as subtraction or concatenation, to identify changes. In this work, we challenge this complexity with SEED (Siamese Encoder-Exchange-Decoder), a streamlined paradigm that replaces explicit differencing with parameter-free feature exchange. By sharing weights across both Siamese encoders and decoders, SEED effectively operates as a single parameter set model. Theoretically, we formalize feature exchange as an orthogonal permutation operator and prove that, under pixel consistency, this mechanism preserves mutual information and Bayes optimal risk, whereas common arithmetic fusion methods often introduce information loss. Extensive experiments across five benchmarks, including SYSU-CD, LEVIR-CD, PX-CLCD, WaterCD, and CDD, and three backbones, namely SwinT, EfficientNet, and ResNet, demonstrate that SEED matches or surpasses state of the art methods despite its simplicity. Furthermore, we reveal that standard semantic segmentation models can be transformed into competitive change detectors solely by inserting this exchange mechanism, referred to as SEG2CD. The proposed paradigm offers a robust, unified, and interpretable framework for change detection, demonstrating that simple feature exchange is sufficient for high performance information fusion. Code and full training and evaluation protocols will be released at https://github.com/dyzy41/open-rscd.

</details>


### [132] [More Images, More Problems? A Controlled Analysis of VLM Failure Modes](https://arxiv.org/abs/2601.07812)
*Anurag Das,Adrian Bulat,Alberto Baldrati,Ioannis Maniadis Metaxas,Bernt Schiele,Georgios Tzimiropoulos,Brais Martinez*

Main category: cs.CV

TL;DR: 提出MIMIC基准，诊断出LVLM多图理解的关键缺陷，提出程序化数据合成与注意力掩码优化方案，显著提升跨图聚合与多图任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在单图任务上表现突出，但其在理解和推理多张图像的能力尚未充分评估与改进，需要系统的基准与方法来揭示弱点并提升性能。

Method: 提出两种互补方法：1）在数据方面，使用程序化生成策略，将单图标注组合成针对性的多图训练样本；2）在优化方面，分析模型层级注意力模式，设计适用于多图输入的注意力掩码方案。

Result: 在MIMIC基准和现有多图基准上，所提方法显著提升了跨图信息聚合能力，并在多个任务上超过先前最先进方法。

Conclusion: 本文提出了MIMIC基准，通过诊断实验发现大型视觉语言模型在多图场景下普遍存在跨图信息聚合和多概念跟踪困难的问题，并提出了数据合成和基于注意力掩码的优化策略以改进模型能力。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.

</details>


### [133] [MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head](https://arxiv.org/abs/2601.07832)
*Kewei Zhang,Ye Huang,Yufan Deng,Jincheng Yu,Junsong Chen,Huan Ling,Enze Xie,Daquan Zhou*

Main category: cs.CV

TL;DR: 提出 MHLA，通过在 token 维度分头计算保持表示多样性，解决线性注意力的全局上下文坍塌问题，在多任务上显著提升性能且保持线性复杂度。


<details>
  <summary>Details</summary>
Motivation: Transformer 的自注意力带来二次复杂度，线性注意力虽高效但直接替换会导致性能下降，现有补救方法通常通过增加额外模块（如深度可分离卷积）恢复性能，但同时引入了额外计算，违背了线性注意力的初衷。需要一种在不增加复杂度的前提下恢复表达能力的方法。

Method: 提出 MHLA，通过在 token 维度对注意力进行分头计算以保持表示多样性。理论上证明 MHLA 在保持线性时间复杂度的同时能近似恢复 softmax 注意力的表达能力。实验上在图像分类、NLP、图像生成和视频生成任务上进行验证，比较了与标准线性注意力和其他改进方法的性能。

Result: MHLA 在保持线性复杂度条件下，显著提升了任务表现：ImageNet 分类提升约3.6%，NLP 提升约6.3%，图像生成提升约12.6%，视频生成提升约41%，在相同时间复杂度下优于基线线性注意力和带额外模块的方法。

Conclusion: 本文识别并解决了线性注意力方法在大规模应用中表现下降的根本问题：全局上下文坍塌（representational diversity 丧失）。通过提出多头线性注意力（MHLA），在保证线性复杂度的同时恢复了类似 softmax 注意力的表达能力，从而在多个任务上显著提升性能。

Abstract: While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.

</details>


### [134] [Tuning-free Visual Effect Transfer across Videos](https://arxiv.org/abs/2601.07833)
*Maxwell Jones,Rameen Abdal,Or Patashnik,Ruslan Salakhutdinov,Sergey Tulyakov,Jun-Yan Zhu,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出RefVFX，通过大规模合成三元组数据和参考视频条件的训练，实现复杂时间效果从参考视频到目标的转移，效果优于提示仅方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以通过文本或关键帧描述复杂的时间效果（如动态光照、角色变换），因此通过直接以参考视频作为条件来转移时间动态可以更好地捕捉难以描述的动态变化。

Method: 构建大规模三元组数据集（参考效果视频、输入图像/视频、目标输出视频），并提出可扩展自动化流水线生成视频-视频配对；同时用LoRA生成的图像到视频效果和程序化合成的代码化时间效果进行数据增强；基于现有文本到视频的骨干网络训练参考条件模型。

Result: 实验表明RefVFX在视觉一致性和时间连贯性上表现良好，能泛化到未见效应类别，并在定量指标与人类偏好上优于仅文本提示的基线。

Conclusion: RefVFX提出了一种参考视频驱动的前向传递框架，将复杂的时间动态效果从参考视频转移到目标视频或图像。

Abstract: We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video's existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input's motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\ this\ URL}$.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [135] [Reflective Reasoning for SQL Generation](https://arxiv.org/abs/2601.06678)
*Isabelle Mohr,Joao Gandarela,John Dujany,Andre Freitas*

Main category: cs.DB

TL;DR: 通过将 text-to-SQL 生成分解为有类型阶段并对阶段生成器施加持久化反馈的反思—精炼循环，本文在无金标准 SQL 的情况下结合解释器与 LLM 判别器，实现了更稳定、可迁移且在少量迭代内收敛的 SQL 生成，且在 Spider/BIRD 上优于强提示基线。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在面对复杂真实数据库的 text-to-SQL 任务时仍脆弱：迭代性精炼容易引入语法与语义漂移，纠错不易跨查询迁移，且大上下文窗口代价高。需要一种能局部定位问题、保持已验证约束并在少量精炼步骤内稳定收敛的方法。

Method: 将生成过程拆分为多个有类型的阶段（例如 schema 解读、候选子句生成、合成与校验等），引入 Reflection-Refinement Loop：定位违反的阶段并将反馈作为该阶段的持久化更新，从而保留已验证的约束并实现单调改进。不使用 gold SQL，而是用解释器执行检查（语法/执行）与 LLM 进行语义覆盖判断作为 epistemic judges；在小预算内对阶段级生成器进行迭代调整。

Result: 在 Spider 与 BIRD 数据集上，相比强提示基线，所提方法在执行准确率和收敛性上均有一致性提升；精炼预算小（少次迭代）即可达到稳健收敛；方法对前沿与公开权重模型家族均有效。

Conclusion: 本文提出了一种基于反思-精炼的受控 text-to-SQL 框架，通过将生成分解为有类型的阶段并将反馈作为持久化的阶段级更新，从而避免重复改写当前 SQL 实例，减少语法/语义漂移，提高约束保留并实现单调改进。方法在无需金标准 SQL 的情况下，结合解释器检查与基于 LLM 的语义覆盖验证作为不确定性判别器。实验在 Spider 和 BIRD 上显示相较强提示基线的稳定提升、在小量精炼预算内的稳健收敛，以及对不同模型家族的执行准确率改进。

Abstract: Robust text-to-SQL over complex, real-world databases remains brittle even with modern LLMs: iterative refinement often introduces syntactic and semantic drift, corrections tend to be non-transferable across queries, and naive use of large context windows scales poorly. We propose a controlled text-to-SQL framework built around reflective refinement. Instead of repeatedly rewriting the current SQL instance, the system decomposes generation into typed stages and applies feedback as persistent updates to the stage-level generation mechanism. A Reflection-Refinement Loop localizes violations to the responsible stage maximize preservation of previously validated constraints and support monotonic improvement over a query set. The method operates without gold SQL by combining interpreter-based checks with LLM-based semantic coverage verification as epistemic judges. Experiments on Spider and BIRD demonstrate consistent gains over strong prompting baselines, robust convergence within a small refinement budget, and improved execution accuracy across both frontier and open-weight model families.

</details>


### [136] [Algorithm Support for Graph Databases, Done Right](https://arxiv.org/abs/2601.06705)
*Daan de Graaf,Robert Brijder,Soham Chakraborty,George Fletcher,Bram van de Wall,Nikolay Yakovets*

Main category: cs.DB

TL;DR: GraphAlg：一个以矩阵为基础的图算法 DSL，编译为关系代数并进行稀疏与循环优化，使图查询引擎能高效表达与执行图分析算法，简化工作流并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图数据库查询语言无法表达如 PageRank 等算法，现有替代方案（算法库、顶点中心 API、递归 CTE）在表达能力、性能与可用性之间无法兼顾，导致需要额外的数据清洗开销。

Method: 基于线性代数构建语言原语（矩阵操作），将高层图算法编译为关系代数并在查询引擎中优化执行；实现了稀疏性分析、循环不变代码移动和就地聚合等激进优化。

Result: 在 AvantGraph 中的实现显示相比于 SQL/Python 与 Pregel，大幅减少代码复杂度，并在 LDBC Graphalytics 基准上取得出色性能。

Conclusion: GraphAlg 证明了图数据库可以同时支撑查询与图算法分析，通过将图算法 DSL 编译为关系代数，消除了数据搬移并与查询处理管道无缝整合。

Abstract: Graph database query languages cannot express algorithms like PageRank, forcing costly data wrangling, while existing solutions such as algorithm libraries, vertex-centric APIs, and recursive CTEs lack the necessary combination of expressiveness, performance, and usability. We present GraphAlg: a domain-specific language for graph algorithms that compiles to relational algebra, enabling seamless integration with query processing pipelines. Built on linear algebra foundations, GraphAlg provides intuitive matrix operations that are amenable to aggressive optimization including sparsity analysis, loop-invariant code motion, and in-place aggregation. Our implementation in AvantGraph demonstrates significant code complexity reduction compared to SQL/Python and Pregel while achieving excellent performance on LDBC Graphalytics benchmarks. GraphAlg establishes that graph databases can serve as unified platforms for both queries and analytics.

</details>


### [137] [Vextra: A Unified Middleware Abstraction for Heterogeneous Vector Database Systems](https://arxiv.org/abs/2601.06727)
*Chandan Suri,Gursifath Bhasin*

Main category: cs.DB

TL;DR: Vextra是一个提供统一高层API和可插拔适配器的中间件，解决向量数据库API碎片化，提升可移植性与互操作性，同时仅带来很小的性能开销。


<details>
  <summary>Details</summary>
Motivation: 向量数据库生态多样导致API碎片化，阻碍可移植性、增加维护成本并造成厂商锁定，需一个中间层统一接口以降低开发成本并促进互操作性与优化。

Method: 提出一个高层统一API，覆盖数据upsert、相似性搜索与元数据过滤；设计可插拔适配器把统一调用映射到各后端原生协议；实现最小化运行时开销的适配层与缓存/批处理优化，并在实验中评估延迟与吞吐。

Result: Vextra在不同后端的评测中显示出对开发者友好的统一接口，保持接近原生的性能开销（低延迟提升、可接受吞吐下降），并通过适配器实现了无缝后端切换与更低的维护负担。

Conclusion: Vextra通过统一API和可插拔适配器成功缓解了向量数据库的API碎片化问题，兼顾可移植性与性能，推动生态互操作性与优化能力发展。

Abstract: The rapid integration of vector search into AI applications, particularly for Retrieval Augmented Generation (RAG), has catalyzed the emergence of a diverse ecosystem of specialized vector databases. While this innovation offers a rich choice of features and performance characteristics, it has simultaneously introduced a significant challenge: severe API fragmentation. Developers face a landscape of disparate, proprietary, and often volatile API contracts, which hinders application portability, increases maintenance overhead, and leads to vendor lock-in. This paper introduces Vextra, a novel middleware abstraction layer designed to address this fragmentation. Vextra presents a unified, high-level API for core database operations, including data upsertion, similarity search, and metadata filtering. It employs a pluggable adapter architecture to translate these unified API calls into the native protocols of various backend databases. We argue that such an abstraction layer is a critical step towards maturing the vector database ecosystem, fostering interoperability, and enabling higher-level query optimization, while imposing minimal performance overhead.

</details>


### [138] [The Complexity of Finding Missing Answer Repairs](https://arxiv.org/abs/2601.06764)
*Jesse Comer,Val Tannen*

Main category: cs.DB

TL;DR: TL;DR：论文给出查询结果缺失元组的最小修复问题在不同查询语言与复杂度度量下的精确复杂性边界：某些受限查询可多项式解决，而允许投影与连接或递归则导致NP-hard或更高复杂度；同时给出最小修复大小的精确复杂度和基于NP oracle的构造算法；半正Datalog在数据复杂度上可多项式处理。


<details>
  <summary>Details</summary>
Motivation: 动机：数据库查询结果可能缺少本应出现的元组（缺失答案），需要从数据库中识别并修复导致缺失的原因。研究不同查询表达能力下最小修复问题的可解性与复杂度边界，对数据库一致性修复与问答解释有理论与实践意义。

Method: 方法：形式化最小修复问题，分析不同查询语言（并联合的带否定原子的一阶查询、基于关系代数的子类、允许递归的Datalog）下问题的复杂性。通过复杂性理论归约（如从SAT、Set Cover）证明NP-困难和近似难度，以及构造性算法利用NP oracle实现接近最优的构造。对数据复杂度利用半正Datalog的性质给出多项式时间识别算法。

Result: 主要结果：1) 组合复杂度：存在性判定在允许弱投影和选择的查询类中与可满足性等价；对于某些关系代数子类，最小修复可多项式时间求解；若查询同时允许投影和连接，则问题NP-困难且近似困难；计算最小修复大小为OptP[log(n)]-完全；构造最小修复可用O(n^2)次NP查询。2) 允许递归时：复杂度显著上升，至少EXP下界。3) 数据复杂度：半正Datalog下可多项式识别最小修复。

Conclusion: 论文结论：给出在不同复杂度视角下关于查询结果缺失元组的最小修复问题的复杂性边界：在组合复杂度下，当查询允许弱形式的投影和选择时，判定修复是否存在与可满足性等价；限制查询为某些不使用投影或连接的子类时可多项式求解；而当查询允许同时使用投影和连接时问题为NP-困难并且近似性受限；计算最小修复大小为OptP[log(n)]-完全，构建最小修复可通过O(n^2)次对NP oracle查询实现；允许递归时复杂度至少为EXP下界；从数据复杂度角度，对于半正Datalog可在多项式时间内识别最小修复。

Abstract: We investigate the problem of identifying database repairs for missing tuples in query answers. We show that when the query is part of the input - the combined complexity setting - determining whether or not a repair exists is polynomial-time is equivalent to the satisfiability problem for classes of queries admitting a weak form of projection and selection. We then identify the sub-classes of unions of conjunctive queries with negated atoms, defined by the relational algebra operations permitted to appear in the query, for which the minimal repair problem can be solved in polynomial time. In contrast, we show that the problem is NP-hard, as well as set cover-hard to approximate via strict reductions, whenever both projection and join are permitted in the input query. Additionally, we show that finding the size of a minimal repair for unions of conjunctive queries (with negated atoms permitted) is OptP[log(n)]-complete, while computing a minimal repair is possible with O($n^2$) queries to an NP oracle. With recursion permitted, the combined complexity of all of these variants increases significantly, with an EXP lower bound. However, from the data complexity perspective, we show that minimal repairs can be identified in polynomial time for all queries expressible as semi-positive datalog programs.

</details>


### [139] [VISTA: Knowledge-Driven Interpretable Vessel Trajectory Imputation via Large Language Models](https://arxiv.org/abs/2601.06940)
*Hengyu Liu,Tianyi Li,Haoyu Wang,Kristian Torp,Tiancheng Zhang,Yushuai Li,Christian S. Jensen*

Main category: cs.DB

TL;DR: VISTA结合结构化数据知识与隐式LLM知识，通过知识图与并行工作流实现可解释、高效的AIS轨迹插补，显著提升精度与效率并支持下游分析。


<details>
  <summary>Details</summary>
Motivation: 解决现有轨迹插补方法缺乏可解释性与无法为下游任务提供有用知识的问题，提升插补结果的可解释性和对异常检测、航线规划等任务的支持能力。

Method: 提出了数据-知识-数据循环和基于结构化数据的知识图（SDK Graph），并引入工作流管理层实现大规模并行的知识抽取和插补流程，包含异常处理与冗余消除。

Result: 在两个大型AIS数据集上，VISTA在插补精度和计算效率上均显著优于现有方法，精度提升5%~94%，时间成本降低51%~93%，并能输出有助于下游任务的可解释知识线索。

Conclusion: 该论文提出了首个兼顾可解释性和下游知识支持的AIS轨迹插补框架VISTA，通过将结构化数据知识与隐式大模型知识相结合，实现了高精度且可解释的轨迹恢复。

Abstract: The Automatic Identification System provides critical information for maritime navigation and safety, yet its trajectories are often incomplete due to signal loss or deliberate tampering. Existing imputation methods emphasize trajectory recovery, paying limited attention to interpretability and failing to provide underlying knowledge that benefits downstream tasks such as anomaly detection and route planning. We propose knowledge-driven interpretable vessel trajectory imputation (VISTA), the first trajectory imputation framework that offers interpretability while simultaneously providing underlying knowledge to support downstream analysis. Specifically, we first define underlying knowledge as a combination of Structured Data-derived Knowledge (SDK) distilled from AIS data and Implicit LLM Knowledge acquired from large-scale Internet corpora. Second, to manage and leverage the SDK effectively at scale, we develop a data-knowledge-data loop that employs a Structured Data-derived Knowledge Graph for SDK extraction and knowledge-driven trajectory imputation. Third, to efficiently process large-scale AIS data, we introduce a workflow management layer that coordinates the end-to-end pipeline, enabling parallel knowledge extraction and trajectory imputation with anomaly handling and redundancy elimination. Experiments on two large AIS datasets show that VISTA is capable of state-of-the-art imputation accuracy and computational efficiency, improving over state-of-the-art baselines by 5%-94% and reducing time cost by 51%-93%, while producing interpretable knowledge cues that benefit downstream tasks. The source code and implementation details of VISTA are publicly available.

</details>


### [140] [Jasper: ANNS Quantized for Speed, Built for Change on GPU](https://arxiv.org/abs/2601.07048)
*Hunter McCoy,Zikun Wang,Prashant Pandey*

Main category: cs.DB

TL;DR: Jasper是一个GPU原生的可更新ANNS系统，通过批量并行构建、无随机访问的高效量化和优化搜索内核，在吞吐、构建速度和资源利用率上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GPU加速ANNS在实时更新、高维向量带来的内存带宽压力和数据依赖的随机访问导致性能下降等方面存在三大限制。

Method: 基于Vamana图索引，提出CUDA批量并行构建实现无锁流式插入，GPU高效RaBitQ量化，以及优化的贪婪搜索内核以提高计算利用率。

Result: 在五个数据集上的评估表明：与CAGRA相比查询吞吐最高提高1.93倍，构建速度平均2.4倍；与BANG相比查询速度提高19-131倍；峰值利用率达80%，量化使内存占用最多减少8倍。

Conclusion: Jasper在保持高吞吐的同时实现了可更新性，显著优于现有GPU ANNS系统。

Abstract: Approximate nearest neighbor search (ANNS) is a core problem in machine learning and information retrieval applications. GPUs offer a promising path to high-performance ANNS: they provide massive parallelism for distance computations, are readily available, and can co-locate with downstream applications.
  Despite these advantages, current GPU-accelerated ANNS systems face three key limitations. First, real-world applications operate on evolving datasets that require fast batch updates, yet most GPU indices must be rebuilt from scratch when new data arrives. Second, high-dimensional vectors strain memory bandwidth, but current GPU systems lack efficient quantization techniques that reduce data movement without introducing costly random memory accesses. Third, the data-dependent memory accesses inherent to greedy search make overlapping compute and memory difficult, leading to reduced performance.
  We present Jasper, a GPU-native ANNS system with both high query throughput and updatability. Jasper builds on the Vamana graph index and overcomes existing bottlenecks via three contributions: (1) a CUDA batch-parallel construction algorithm that enables lock-free streaming insertions, (2) a GPU-efficient implementation of RaBitQ quantization that reduces memory footprint up to 8x without the random access penalties, and (3) an optimized greedy search kernel that increases compute utilization, resulting in better latency hiding and higher throughput.
  Our evaluation across five datasets shows that Jasper achieves up to 1.93x higher query throughput than CAGRA and achieves up to 80% peak utilization as measured by the roofline model. Jasper's construction scales efficiently and constructs indices an average of 2.4x faster than CAGRA while providing updatability that CAGRA lacks. Compared to BANG, the previous fastest GPU Vamana implementation, Jasper delivers 19-131x faster queries.

</details>


### [141] [RAIRS: Optimizing Redundant Assignment and List Layout for IVF-Based ANN Search](https://arxiv.org/abs/2601.07183)
*Zehai Yang,Shimin Chen*

Main category: cs.DB

TL;DR: RAIRS通过AIR的方向+距离列表选择和SEIL的共享单元布局，减少漏检与重复计算，在欧氏IVF检索上显著提速。


<details>
  <summary>Details</summary>
Motivation: 现有冗余分配策略在欧氏空间表现差，且IVF在多列表访问中存在重复距离计算影响吞吐。

Method: 提出AIR度量用于欧氏空间的列表选择，结合方向信息与距离判断第二分配列表；提出SEIL列表布局，利用共享单元减少重复距离计算。

Result: 在真实数据集上，RAIRS优于已有冗余分配方案，最高比IVF-PQ Fast Scan（含重排）提升1.33倍。

Conclusion: RAIRS通过优化冗余分配和列表布局，有效提升了IVF在欧氏空间下的近邻搜索性能，综合性能优于现有方法。

Abstract: IVF is one of the most widely used ANNS (Approximate Nearest Neighbors Search) methods in vector databases. The idea of redundant assignment is to assign a data vector to more than one IVF lists for reducing the chance of missing true neighbors in IVF search. However, the naive strategy, which selects the second IVF list based on the distance between a data vector and the list centroids, performs poorly. Previous work focuses only on the inner product distance, while there is no optimized list selection study for the most popular Euclidean space. Moreover, the IVF search may access the same vector in more than one lists, resulting in redundant distance computation and decreasing query throughput. In this paper, we present RAIRS to address the above two challenges. For the challenge of the list selection, we propose an optimized AIR metric for the Euclidean space. AIR takes not only distances but also directions into consideration in order to support queries that are closer to the data vector but father away from the first chosen list's centroid. For the challenge of redundant distance computation, we propose SEIL, an optimized list layout that exploits shared cells to reduce repeated distance computations for IVF search. Our experimental results using representative real-world data sets show that RAIRS out-performs existing redundant assignment solutions and achieves up to 1.33x improvement over the best-performing IVF method, IVF-PQ Fast Scan with refinement.

</details>
