<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 126]
- [cs.DB](#cs.DB) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Leveraging AI multimodal geospatial foundation models for improved near-real-time flood mapping at a global scale](https://arxiv.org/abs/2512.02055)
*Mirela G. Tulbure,Julio Caineta,Mark Broich,Mollie D. Gaines,Philippe Rufin,Leon-Friedrich Thomas,Hamed Alemohammad,Jan Hemmerling,Patrick Hostert*

Main category: cs.CV

TL;DR: Fine-tuning TerraMind on a global multimodal flood dataset improves recall and operational mapping potential, with base-unfrozen best cost-accuracy trade-off; U-Net still yields highest recall but with precision/accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: Assess how Geospatial Foundation Models like TerraMind generalize to diverse global flood events and whether multimodal fine-tuning improves operational flood extent mapping.

Method: Fine-tuning TerraMind (base and large; frozen vs unfrozen backbones) on FloodsNet (harmonized Sentinel-1 SAR and Sentinel-2 optical for 85 events); compared to TerraMind Sen1Floods11 example and U-Net trained on FloodsNet and Sen1Floods11; evaluated accuracy, precision, recall and computational cost.

Result: Base-unfrozen TerraMind gave best trade-off of accuracy, precision, recall and lower computation; large-unfrozen had highest recall; FloodsNet-trained models had better recall than Sen1Floods11 example; U-Net had highest recall but lower accuracy/precision.

Conclusion: Fine-tuning TerraMind on a large multimodal global dataset improves flood mapping recall and supports near-real-time applications, but GFMs still lag U-Net in recall and have trade-offs with model size and computational cost.

Abstract: Floods are among the most damaging weather-related hazards, and in 2024, the warmest year on record, extreme flood events affected communities across five continents. Earth observation (EO) satellites provide critical, frequent coverage for mapping inundation, yet operational accuracy depends heavily on labeled datasets and model generalization. Recent Geospatial Foundation Models (GFMs), such as ESA-IBM's TerraMind, offer improved generalizability through large-scale self-supervised pretraining, but their performance on diverse global flood events remains poorly understood.
  We fine-tune TerraMind for flood extent mapping using FloodsNet, a harmonized multimodal dataset containing co-located Sentinel-1 (Synthetic Aperture Radar, SAR data) and Sentinel-2 (optical) imagery for 85 flood events worldwide. We tested four configurations (base vs. large models; frozen vs. unfrozen backbones) and compared against the TerraMind Sen1Floods11 example and a U-Net trained on both FloodsNet and Sen1Floods11. The base-unfrozen configuration provided the best balance of accuracy, precision, and recall at substantially lower computational cost than the large model. The large unfrozen model achieved the highest recall. Models trained on FloodsNet outperformed the Sen1Floods11-trained example in recall with similar overall accuracy. U-Net achieved higher recall than all GFM configurations, though with slightly lower accuracy and precision.
  Our results demonstrate that integrating multimodal optical and SAR data and fine-tuning a GFM can enhance near-real-time flood mapping. This study provides one of the first global-scale evaluations of a GFM for flood segmentation, highlighting both its potential and current limitations for climate adaptation and disaster resilience.

</details>


### [2] [Context-Enriched Contrastive Loss: Enhancing Presentation of Inherent Sample Connections in Contrastive Learning Framework](https://arxiv.org/abs/2512.02152)
*Haojin Deng,Yimin Yang*

Main category: cs.CV

TL;DR: 提出基于双重收敛目标的上下文增强对比损失，同时利用标签对比与源图像一致性，减少增强带来的信息失真，显著提高性能与收敛速度，尤其在偏置数据集上效果突出。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习依赖增强样本，但容易产生信息失真：模型过度依赖同类样本信息，忽视来自同一原始图像的正样本，尤其在大规模数据集上更严重。需要一种能同时利用类别信息和图像源信息的损失来缓解这一问题。

Method: 设计一种包含两部分的损失：一是对标签对比敏感的分离项，用于加强不同类别间的特征区分；二是拉近来自同一源图像的增强样本的聚合项，同时把其它样本拉远。结合常规对比学习框架进行训练，并在多个数据集上评估性能与收敛速度。

Result: 在八个大规模图像分类基准（CIFAR10/100、Caltech-101/256、ImageNet、BiasedMNIST、UTKFace、CelebA）上，相比16种最先进对比学习方法，该方法在泛化性能和收敛速度上均有提升。在偏置数据集（BiasedMNIST）上相对原始对比损失提高22.9%。

Conclusion: 提出的上下文增强对比损失通过双重收敛目标同时提高学习效果并缓解信息失真，从而在多数据集上优于现有方法，特别是在偏置数据集上提升显著。

Abstract: Contrastive learning has gained popularity and pushes state-of-the-art performance across numerous large-scale benchmarks. In contrastive learning, the contrastive loss function plays a pivotal role in discerning similarities between samples through techniques such as rotation or cropping. However, this learning mechanism can also introduce information distortion from the augmented samples. This is because the trained model may develop a significant overreliance on information from samples with identical labels, while concurrently neglecting positive pairs that originate from the same initial image, especially in expansive datasets. This paper proposes a context-enriched contrastive loss function that concurrently improves learning effectiveness and addresses the information distortion by encompassing two convergence targets. The first component, which is notably sensitive to label contrast, differentiates between features of identical and distinct classes which boosts the contrastive training efficiency. Meanwhile, the second component draws closer the augmented samples from the same source image and distances all other samples. We evaluate the proposed approach on image classification tasks, which are among the most widely accepted 8 recognition large-scale benchmark datasets: CIFAR10, CIFAR100, Caltech-101, Caltech-256, ImageNet, BiasedMNIST, UTKFace, and CelebA datasets. The experimental results demonstrate that the proposed method achieves improvements over 16 state-of-the-art contrastive learning methods in terms of both generalization performance and learning convergence speed. Interestingly, our technique stands out in addressing systematic distortion tasks. It demonstrates a 22.9% improvement compared to original contrastive loss functions in the downstream BiasedMNIST dataset, highlighting its promise for more efficient and equitable downstream training.

</details>


### [3] [FineGRAIN: Evaluating Failure Modes of Text-to-Image Models with Vision Language Model Judges](https://arxiv.org/abs/2512.02161)
*Kevin David Hayes,Micah Goldblum,Vikash Sehwag,Gowthami Somepalli,Ashwinee Panda,Tom Goldstein*

Main category: cs.CV

TL;DR: 提出一个包含27类失败模式的层级化评估框架与数据集，用以联合评估5个T2I模型与3个VLM，发现T2I在属性/对象保真度上存在系统性错误，当前评价指标不足。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型常在细粒度属性（例如指定颜色、数量）和对象表现上出错，而现有评估指标无法细致捕捉这些错误；同时VLM的基准未覆盖复杂场景的注释能力。故需一个联合且结构化的评估框架与数据集来系统分析这些错误。

Method: 构建了一套方法：收集具有挑战性的提示词生成图像（5个T2I模型），定义27个失败模式；用3个VLM对生成图像进行识别；并用LLM（Llama3）对VLM输出进行注释来判断VLM是否正确识别失败模式。数据集包含提示、生成图像和VLM标注。

Result: 在对多个T2I模型和VLMs的联合评估中，发现普遍存在属性保真度和对象表示的系统性错误；不同模型之间在识别某些失败模式上差异明显；现有度量不足以捕捉这些细微错误，强调了有针对性的基准重要性。

Conclusion: 该论文提出了一个层级化评估框架，用于联合评估Text-to-Image (T2I)模型与视觉语言模型(VLMs)，并通过27种具体失败模式检测VLM是否能识别T2I生成图像的错误。结果表明当前度量不足，需更具针对性的基准。

Abstract: Text-to-image (T2I) models are capable of generating visually impressive images, yet they often fail to accurately capture specific attributes in user prompts, such as the correct number of objects with the specified colors. The diversity of such errors underscores the need for a hierarchical evaluation framework that can compare prompt adherence abilities of different image generation models. Simultaneously, benchmarks of vision language models (VLMs) have not kept pace with the complexity of scenes that VLMs are used to annotate. In this work, we propose a structured methodology for jointly evaluating T2I models and VLMs by testing whether VLMs can identify 27 specific failure modes in the images generated by T2I models conditioned on challenging prompts. Our second contribution is a dataset of prompts and images generated by 5 T2I models (Flux, SD3-Medium, SD3-Large, SD3.5-Medium, SD3.5-Large) and the corresponding annotations from VLMs (Molmo, InternVL3, Pixtral) annotated by an LLM (Llama3) to test whether VLMs correctly identify the failure mode in a generated image. By analyzing failure modes on a curated set of prompts, we reveal systematic errors in attribute fidelity and object representation. Our findings suggest that current metrics are insufficient to capture these nuanced errors, highlighting the importance of targeted benchmarks for advancing generative model reliability and interpretability.

</details>


### [4] [Mapping of Lesion Images to Somatic Mutations](https://arxiv.org/abs/2512.02162)
*Rahul Mehta*

Main category: cs.CV

TL;DR: LLOST通过点云表示与双VAE+共享潜空间、条件正规化流先验，实现了从医学影像预测体细胞突变计数与发生的能力，验证于TCIA/TCGA数据并揭示影像-突变间的共享模式。


<details>
  <summary>Details</summary>
Motivation: 早期诊断和精准治疗对癌症疗效至关重要。医学影像能提供快速初筛，而基因突变信息有助于制定个体化治疗。目标是通过影像预测患者的体细胞突变谱，便于更早地指导基因层面的干预或检测。

Method: 提出点云表示的病变影像、双变分自编码器（分别针对影像和突变计数）以及一个将两者统一的共享潜在空间；三个潜空间均使用有条件正规化流（conditional normalizing flow）作为先验来表达各域多样化的分布。

Result: 在TCIA影像与TCGA突变数据上进行了定性与定量实验。模型在预测特定突变计数和突变发生（存在/不存在）方面表现良好，能发现影像与突变域间与癌种相关的共享模式。

Conclusion: 该论文提出的LLOST模型能够将医学影像（以病变点云表示）与体细胞突变计数通过共享潜在空间联合建模，展示了从影像预测突变计数和发生的能力，并揭示了影像与突变间反映癌种的共享模式。作者还指出了改进方向与扩展到其他遗传域的可能性。

Abstract: Medical imaging is a critical initial tool used by clinicians to determine a patient's cancer diagnosis, allowing for faster intervention and more reliable patient prognosis. At subsequent stages of patient diagnosis, genetic information is extracted to help select specific patient treatment options. As the efficacy of cancer treatment often relies on early diagnosis and treatment, we build a deep latent variable model to determine patients' somatic mutation profiles based on their corresponding medical images. We first introduce a point cloud representation of lesions images to allow for invariance to the imaging modality. We then propose, LLOST, a model with dual variational autoencoders coupled together by a separate shared latent space that unifies features from the lesion point clouds and counts of distinct somatic mutations. Therefore our model consists of three latent space, each of which is learned with a conditional normalizing flow prior to account for the diverse distributions of each domain. We conduct qualitative and quantitative experiments on de-identified medical images from The Cancer Imaging Archive and the corresponding somatic mutations from the Pan Cancer dataset of The Cancer Genomic Archive. We show the model's predictive performance on the counts of specific mutations as well as it's ability to accurately predict the occurrence of mutations. In particular, shared patterns between the imaging and somatic mutation domain that reflect cancer type. We conclude with a remark on how to improve the model and possible future avenues of research to include other genetic domains.

</details>


### [5] [SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting](https://arxiv.org/abs/2512.02172)
*Pranav Asthana,Alex Hanson,Allen Tu,Tom Goldstein,Matthias Zwicker,Amitabh Varshney*

Main category: cs.CV

TL;DR: SplatSuRe基于相机-几何信息有选择地在欠采样区域应用超分辨率，解决了统一SR带来的多视图不一致问题，显著提升了局部细节和整体渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有对每视图统一应用SR的方法会导致多视图不一致性和渲染模糊。观察到近景低分辨率视图可能包含高频信息且可被远景视图缺失，作者提出利用几何信息有选择地添加SR以避免不一致。

Method: 基于3D Gaussian Splatting框架，作者分析了视角与采样密度关联，使用相机位姿判断哪些像素由近景低分辨率视图提供高频信息，并仅在这些欠采样区域注入SR增强，而对充分采样区域保持原始分辨率。此外在训练/优化过程中对SR内容进行约束以保证多视图一致性。

Result: 在 Tanks & Temples、Deep Blending 和 Mip-NeRF 360 数据集上，SplatSuRe在保真度和感知质量上均优于基线方法，尤其在局部前景细节区域提升显著。

Conclusion: 该论文提出了SplatSuRe方法，通过根据相机视角与场景几何关系有选择地在欠采样区域添加超分辨率内容，从而在保持多视图一致性的同时提升局部细节。

Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.

</details>


### [6] [RobustSurg: Tackling domain generalisation for out-of-distribution surgical scene segmentation](https://arxiv.org/abs/2512.02188)
*Mansoor Ali,Maksim Richards,Gilberto Ochoa-Ruiz,Sharib Ali*

Main category: cs.CV

TL;DR: 通过在主干中引入实例归一化、特征协方差映射和restitution模块，RobustSurg显著提升了手术图像分割在跨中心与跨模态场景的泛化性，并通过新数据集验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有手术场景分割模型在单中心/单模态数据上表现良好，但在不同中心或模态（即分布外）上泛化性差；自然场景的域泛化方法难以直接迁移到视觉线索有限且场景多变的手术图像。

Method: 在ResNet主干中加入实例归一化和特征协方差映射以抑制外观差异，同时设计restitution模块恢复被抑制的任务相关特征；并构建新的多中心多类手术分割数据集用于训练与评价。

Result: 在CholecSeg8K上训练并在HeiCholSeg（未见中心）上测试，提出方法RobustSurg相比DeepLabv3+基线提升≈23% mean IoU，相比SOTA提升10–32%；在EndoUDA靶域上对息肉分割相比基线提升≈22%，相比最近SOTA提升≈11%。

Conclusion: 提出通过风格与内容分离（实例归一化与特征协方差映射）并结合回复模块（restitution）来提升外域与跨模态手术场景分割的泛化性。

Abstract: While recent advances in deep learning for surgical scene segmentation have demonstrated promising results on single-centre and single-imaging modality data, these methods usually do not generalise to unseen distribution (i.e., from other centres) and unseen modalities. Current literature for tackling generalisation on out-of-distribution data and domain gaps due to modality changes has been widely researched but mostly for natural scene data. However, these methods cannot be directly applied to the surgical scenes due to limited visual cues and often extremely diverse scenarios compared to the natural scene data. Inspired by these works in natural scenes to push generalisability on OOD data, we hypothesise that exploiting the style and content information in the surgical scenes could minimise the appearances, making it less variable to sudden changes such as blood or imaging artefacts. This can be achieved by performing instance normalisation and feature covariance mapping techniques for robust and generalisable feature representations. Further, to eliminate the risk of removing salient feature representation associated with the objects of interest, we introduce a restitution module within the feature learning ResNet backbone that can enable the retention of useful task-relevant features. To tackle the lack of multiclass and multicentre data for surgical scene segmentation, we also provide a newly curated dataset that can be vital for addressing generalisability in this domain. Our proposed RobustSurg obtained nearly 23% improvement on the baseline DeepLabv3+ and from 10-32% improvement on the SOTA in terms of mean IoU score on an unseen centre HeiCholSeg dataset when trained on CholecSeg8K. Similarly, RobustSurg also obtained nearly 22% improvement over the baseline and nearly 11% improvement on a recent SOTA method for the target set of the EndoUDA polyp dataset.

</details>


### [7] [Multifractal Recalibration of Neural Networks for Medical Imaging Segmentation](https://arxiv.org/abs/2512.02198)
*Miguel L. Martins,Miguel T. Coimbra,Francesco Renna*

Main category: cs.CV

TL;DR: 提出两种基于多重分形统计的通道注意力先验并嵌入U-Net，在三项医学影像分割任务上显著优于其他高阶统计注意力方法，且揭示了注意力响应随深度不增强的现象及其与全局变异性的关联。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多重分形方法依赖严重池化或特征空间降采样，限制了语义分割等任务的精细预测；希望设计不破坏空间结构且可嵌入主流架构的多重分形先验模块。

Method: 将概率质量函数与多重分形谱的关系转化为对编码器嵌入的统计描述，并实现为卷积网络中的通道注意力模块（Monofractal 和 Multifractal Recalibration）；在U-Net框架中加入这些注意力层，并与其他基于高阶统计的通道注意力机制比较。

Result: 在ISIC18、Kvasir-SEG 和 BUSI 三个公开医学影像数据集上，多重分形重校准较基线及其它高阶统计注意力机制带来显著提升；并分析了注意力层随编码器深度的不变性与其与实例全局统计变异性的关系。

Conclusion: 引入单重分形与多重分形重校准作为先验，有助于在U-Net架构中通过通道注意力利用多重分形统计特性，提升医学影像分割性能。

Abstract: Multifractal analysis has revealed regularities in many self-seeding phenomena, yet its use in modern deep learning remains limited. Existing end-to-end multifractal methods rely on heavy pooling or strong feature-space decimation, which constrain tasks such as semantic segmentation. Motivated by these limitations, we introduce two inductive priors: Monofractal and Multifractal Recalibration. These methods leverage relationships between the probability mass of the exponents and the multifractal spectrum to form statistical descriptions of encoder embeddings, implemented as channel-attention functions in convolutional networks.
  Using a U-Net-based framework, we show that multifractal recalibration yields substantial gains over a baseline equipped with other channel-attention mechanisms that also use higher-order statistics. Given the proven ability of multifractal analysis to capture pathological regularities, we validate our approach on three public medical-imaging datasets: ISIC18 (dermoscopy), Kvasir-SEG (endoscopy), and BUSI (ultrasound).
  Our empirical analysis also provides insights into the behavior of these attention layers. We find that excitation responses do not become increasingly specialized with encoder depth in U-Net architectures due to skip connections, and that their effectiveness may relate to global statistics of instance variability.

</details>


### [8] [Towards Unified Video Quality Assessment](https://arxiv.org/abs/2512.02224)
*Chen Feng,Tianhao Peng,Fan Zhang,David Bull*

Main category: cs.CV

TL;DR: 提出Unified-VQA：基于诊断性MoE和多代理训练的统一可解释VQA框架，能生成全局质量分与多维伪影向量，在多格式、多数据库上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有VQA方法多为单一分数预测的单片式模型，缺乏可诊断、可解释反馈，且多为特定格式的度量，不能泛化到不同感知域。需要一个统一、可解释且适用于多格式的VQA解决方案。

Method: 使用多个针对不同感知域的“感知专家”，采用多代理（multi-proxy）专家训练策略和基于排名的损失，结合最合适的代理度量引导训练；加入诊断多任务头以同时输出全局质量分和伪影向量，采用弱监督策略利用大规模训练库的已知属性进行优化。

Result: 在不更改模型参数的情况下，Unified-VQA 在17个包含HD、UHD、HDR、HFR等多样流媒体伪影的数据库上，对通用VQA和诊断性伪影检测任务均优于18+基线方法，表现稳定且优越。

Conclusion: Unified-VQA 提出了一种将通用视频质量评估重构为诊断性专家混合（MoE）的问题的框架，能在多种失真类型和视频格式上使用单一模型，并生成全局质量得分与多维可解释伪影向量。

Abstract: Recent works in video quality assessment (VQA) typically employ monolithic models that typically predict a single quality score for each test video. These approaches cannot provide diagnostic, interpretable feedback, offering little insight into why the video quality is degraded. Most of them are also specialized, format-specific metrics rather than truly ``generic" solutions, as they are designed to learn a compromised representation from disparate perceptual domains. To address these limitations, this paper proposes Unified-VQA, a framework that provides a single, unified quality model applicable to various distortion types within multiple video formats by recasting generic VQA as a Diagnostic Mixture-of-Experts (MoE) problem. Unified-VQA employs multiple ``perceptual experts'' dedicated to distinct perceptual domains. A novel multi-proxy expert training strategy is designed to optimize each expert using a ranking-inspired loss, guided by the most suitable proxy metric for its domain. We also integrated a diagnostic multi-task head into this framework to generate a global quality score and an interpretable multi-dimensional artifact vector, which is optimized using a weakly-supervised learning strategy, leveraging the known properties of the large-scale training database generated for this work. With static model parameters (without retraining or fine-tuning), Unified-VQA demonstrates consistent and superior performance compared to over 18 benchmark methods for both generic VQA and diagnostic artifact detection tasks across 17 databases containing diverse streaming artifacts in HD, UHD, HDR and HFR formats. This work represents an important step towards practical, actionable, and interpretable video quality assessment.

</details>


### [9] [See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2512.02231)
*Le Thien Phuc Nguyen,Zhuoran Yu,Samuel Low Yu Hang,Subin An,Jeongik Lee,Yohan Ban,SeungEun Chung,Thanh-Huy Nguyen,JuWan Maeng,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: AV-SpeakerBench：3,212道以说话者为中心的视听选择题基准，专注语音与画面对齐与时序精度，评测表明当前模型在视听融合上仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态基准多数可通过视觉线索解决或仅对语音做粗略评估，无法衡量模型是否能将“谁说、说了什么、何时发生”这类细粒度视听信息对齐与推理。

Method: 收集并构建了3,212个选择题，采用以说话者为中心的题目设计，融合听觉与视觉信息在题干语义中，配以专家级时间精确与跨模态有效性注释。然后对多种模型（包括闭源的Gemini系列和若干开源模型如Qwen3-Omni-30B）进行全面评估。

Result: 评测结果显示Gemini系列表现最好，Gemini 2.5 Pro领先；开源中Qwen3-Omni-30B接近Gemini 2.0 Flash但仍落后于Gemini 2.5 Pro，主要差距来自视听融合能力而非视觉感知。

Conclusion: 该论文提出了AV-SpeakerBench，为多模态大语言模型在说话者层面的视听推理提供了更精细的评测基准，填补了现有视频基准在语音细粒度推理上的空白。

Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.

</details>


### [10] [Exploring the Potentials of Spiking Neural Networks for Image Deraining](https://arxiv.org/abs/2512.02258)
*Shuang Chen,Tomas Krajnik,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: 提出面向低级视觉任务的VLIF神经元及配套模块，提升SNN在图像去雨任务的表现与能效，在五个数据集上优于现有方法且显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 现有生物启发、能效高的SNN在低级视觉任务（如图像去雨）应用不足，主要由于传统脉冲神经元倾向高通特征且缺乏空间上下文感知，导致频域表现受限，因此需要改进神经元模型和模块设计以提升表示能力与能效。

Method: 提出Visual LIF (VLIF)神经元，结合脉冲分解增强模块（Spiking Decomposition and Enhancement Module）与轻量级多尺度单元（Spiking Multi-scale Unit），用于层级多尺度表示学习，解决高通特性和频域饱和问题。

Result: 在五个去雨基准数据集上，提出的方法优于当前最先进的SNN去雨方法，并将能耗降至现有方法的13%，显示出在精度和能效方面的显著改进。

Conclusion: 论文提出的VLIF神经元有效弥补了传统脉冲神经元在空间上下文感知和频域表现上的不足，使SNN能够更好地处理低级视觉任务（以去雨为例）。实验表明，在五个基准数据集上优于现有SNN去雨方法，并以仅13%能耗实现更高性能，证明了方法在能效与效果间的优势。

Abstract: Biologically plausible and energy-efficient frameworks such as Spiking Neural Networks (SNNs) have not been sufficiently explored in low-level vision tasks. Taking image deraining as an example, this study addresses the representation of the inherent high-pass characteristics of spiking neurons, specifically in image deraining and innovatively proposes the Visual LIF (VLIF) neuron, overcoming the obstacle of lacking spatial contextual understanding present in traditional spiking neurons. To tackle the limitation of frequency-domain saturation inherent in conventional spiking neurons, we leverage the proposed VLIF to introduce the Spiking Decomposition and Enhancement Module and the lightweight Spiking Multi-scale Unit for hierarchical multi-scale representation learning. Extensive experiments across five benchmark deraining datasets demonstrate that our approach significantly outperforms state-of-the-art SNN-based deraining methods, achieving this superior performance with only 13\% of their energy consumption. These findings establish a solid foundation for deploying SNNs in high-performance, energy-efficient low-level vision tasks.

</details>


### [11] [Spatiotemporal Pyramid Flow Matching for Climate Emulation](https://arxiv.org/abs/2512.02268)
*Jeremy Andrew Irvin,Jiaqi Han,Zikui Wang,Abdulaziz Alharbi,Yufei Zhao,Nomin-Erdene Bayarsaikhan,Daniele Visioni,Andrew Y. Ng,Duncan Watson-Parris*

Main category: cs.CV

TL;DR: 提出分级时空流匹配方法SPF并构建大规模ClimateSuite数据集，实现快速、并行的多时尺度气候概率仿真并在多个基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于天气尺度自回归的方法在长时序气候仿真上计算开销大且在非平稳强迫下难以获得稳定滚动输出，需更高效且能处理多时尺度及情景变化的方法。

Method: SPF 将生成轨迹划分为时空金字塔，逐级提高空间分辨率并为每级配备相应时间尺度，可在任意时尺度直接采样；各级以物理强迫（如温室气体、气溶胶）为条件，实现并行多时尺度生成。

Result: 在 ClimateBench 上，SPF 在年尺度和月尺度均优于强基线和预训练模型，且在粗时尺度上采样更快。扩展至 ClimateSuite（33,000+ 模拟年，含10个气候模型和干预情景）后，SPF 对未见情景表现出较好泛化能力。代码与数据公开。

Conclusion: 本文提出了Spatiotemporal Pyramid Flows (SPF)，通过时空金字塔的分级流匹配实现高效鲁棒的气候仿真。

Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .

</details>


### [12] [Progressive Image Restoration via Text-Conditioned Video Generation](https://arxiv.org/abs/2512.02273)
*Peng Kang,Xijun Wang,Yu Yuan*

Main category: cs.CV

TL;DR: 将CogVideo微调用于生成从劣化到清晰的恢复序列，通过合成数据与场景自适应提示训练，能在定量指标和真实数据上实现有效、连贯且可解释的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 现代文本到视频模型具有强时间生成能力，但其在图像恢复方向的潜力未被充分挖掘；利用视频模型的时间建模能力生成逐步恢复序列，或可带来更鲁棒、可解释的恢复过程与零样本泛化能力。

Method: 构建合成数据集（超分、去模糊、低光增强），每个样本为从降质到干净帧的渐进序列；对比两种提示策略（统一文本提示与通过LLaVA+ChatGPT生成的场景自适应提示）；在CogVideo上微调以学习时间步与恢复质量的对应关系，并评估PSNR、SSIM、LPIPS等指标与时序一致性。

Result: 微调后的CogVideo能逐步提升图像质量，空间细节与光照一致性显著改善，PSNR/SSIM提升、LPIPS下降；在ReLoBlur等实景数据上实现零样本泛化，保持良好的时间连贯性与可解释性。

Conclusion: 本论文展示了将大型文本到视频模型 CogVideo 通过微调用于视觉逐步恢复任务的可行性，证明模型能生成从劣化到清晰的恢复轨迹，并在定量与实景测试中取得良好表现。

Abstract: Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.

</details>


### [13] [Enhancing Cross Domain SAR Oil Spill Segmentation via Morphological Region Perturbation and Synthetic Label-to-SAR Generation](https://arxiv.org/abs/2512.02290)
*Andre Juarez,Luis Salsavilca,Frida Coaquira,Celso Gonzales*

Main category: cs.CV

TL;DR: MORP--Synth augments Mediterranean-trained SAR oil-spill segmentation with curvature-guided morphological edits and INADE texture rendering, improving transfer to Peruvian domain and boosting minority-class IoU substantially.


<details>
  <summary>Details</summary>
Motivation: SAR oil spill models trained in Mediterranean fail to generalize to Peruvian coast due to different sea-state, backscatter, and morphology; scarce Peruvian labels. Synthetic augmentation can bridge domain gap by creating realistic geometric and texture variations.

Method: Two-stage synthetic augmentation (MORP--Synth). Stage A: Morphological Region Perturbation (curvature-guided label-space edits). Stage B: conditional INADE-based texture rendering.

Result: Compiled Peruvian dataset 2112 labeled 512x512 patches from 40 Sentinel-1 scenes (2014-2024). Pretrained Mediterranean models drop from 67.8% to 51.8% mIoU on Peruvian. MORP--Synth improves mIoU by up to +6 and increases minority-class IoU: +10.8 for oil, +14.6 for look-alike.

Conclusion: MORP--Synth effectively reduces domain gap between Mediterranean and Peruvian SAR oil-spill data, improving segmentation performance especially for minority classes; dataset and evaluations support efficacy.

Abstract: Deep learning models for SAR oil spill segmentation often fail to generalize across regions due to differences in sea-state, backscatter statistics, and slick morphology, a limitation that is particularly severe along the Peruvian coast where labeled Sentinel-1 data remain scarce. To address this problem, we propose \textbf{MORP--Synth}, a two-stage synthetic augmentation framework designed to improve transfer from Mediterranean to Peruvian conditions. Stage~A applies Morphological Region Perturbation, a curvature guided label space method that generates realistic geometric variations of oil and look-alike regions. Stage~B renders SAR-like textures from the edited masks using a conditional generative INADE model. We compile a Peruvian dataset of 2112 labeled 512$\times$512 patches from 40 Sentinel-1 scenes (2014--2024), harmonized with the Mediterranean CleanSeaNet benchmark, and evaluate seven segmentation architectures. Models pretrained on Mediterranean data degrade from 67.8\% to 51.8\% mIoU on the Peruvian domain; MORP--Synth improves performance up to +6 mIoU and boosts minority-class IoU (+10.8 oil, +14.6 look-alike).

</details>


### [14] [Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision](https://arxiv.org/abs/2512.02339)
*Chenshuang Zhang,Kang Zhang,Joon Son Chung,In So Kweon,Junmo Kim,Chengzhi Mao*

Main category: cs.CV

TL;DR: 论文发现预训练视频扩散模型的早期去噪阶段编码运动信息，利用该表示构建无监督跟踪器，在区分视觉相似目标上显著提升（最多+6点），并通过可视化验证其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前自监督跟踪器在外观模糊或视觉相似物体情况下表现不佳，需大量标注数据；寻找无需额外标注即可获得鲁棒运动表示的方法。

Method: 分析视频扩散模型的去噪过程，发现早期高噪声阶段编码运动信息，随后阶段细化外观；基于此提取扩散模型的运动表示并构建自监督跟踪器；在标准基准和新设计的视觉相似物体测试集上评估性能。

Result: 在既有基准和新引入的关注视觉相似项的测试上，相比最近自监督方法提升了最多6个百分点；可视化表明扩散衍生的运动表示在视角变化和形变下仍能鲁棒跟踪相同物体。

Conclusion: 该论文表明预训练视频扩散模型在去噪过程中天然学习到运动表示，能够无需任务特定训练就用于跟踪，从而在视觉相似目标区分上显著优于现有自监督跟踪方法。

Abstract: Distinguishing visually similar objects by their motion remains a critical challenge in computer vision. Although supervised trackers show promise, contemporary self-supervised trackers struggle when visual cues become ambiguous, limiting their scalability and generalization without extensive labeled data. We find that pre-trained video diffusion models inherently learn motion representations suitable for tracking without task-specific training. This ability arises because their denoising process isolates motion in early, high-noise stages, distinct from later appearance refinement. Capitalizing on this discovery, our self-supervised tracker significantly improves performance in distinguishing visually similar objects, an underexplored failure point for existing methods. Our method achieves up to a 6-point improvement over recent self-supervised approaches on established benchmarks and our newly introduced tests focused on tracking visually similar items. Visualizations confirm that these diffusion-derived motion representations enable robust tracking of even identical objects across challenging viewpoint changes and deformations.

</details>


### [15] [TALO: Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction](https://arxiv.org/abs/2512.02341)
*Fengyi Zhang,Tianjun Zhang,Kasra Khosoussi,Zheng Zhang,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: 提出TPS+全局控制点的长时序非刚性对齐框架与点不可知子图配准，显著提升在线3D重建的时间一致性与轨迹精度，兼容多模型与相机设置。


<details>
  <summary>Details</summary>
Motivation: 现有在线场景中对连续帧预测的对齐多依赖刚性或低自由度全局变换，忽略了空间变形与局部错位问题；而且对噪声几何敏感，导致时间一致性差。需要更高自由度和更鲁棒的对齐策略来保证长期连贯性。

Method: 框架使用全局传播的控制点结合TPS进行空间上可变（非刚性）对齐；同时采用点不可知的子图（submap）配准设计，以增强对噪声几何预测的鲁棒性；该方法为即插即用，兼容不同3D基础模型和相机配置（单目/环视）。

Result: 在多数据集、多骨干网络和多相机设置下，所提出方法在几何连贯性和轨迹误差上均持续优于基线，验证了其鲁棒性与泛化能力。

Conclusion: 本文提出了一种基于Thin Plate Spline（TPS）的高自由度、长时序对齐框架，用于修正在线3D视觉基础模型在时间上产生的空间不一致性，从而提升几何连贯性与轨迹精度。

Abstract: 3D vision foundation models have shown strong generalization in reconstructing key 3D attributes from uncalibrated images through a single feed-forward pass. However, when deployed in online settings such as driving scenarios, predictions are made over temporal windows, making it non-trivial to maintain consistency across time. Recent strategies align consecutive predictions by solving global transformation, yet our analysis reveals their fundamental limitations in assumption validity, local alignment scope, and robustness under noisy geometry. In this work, we propose a higher-DOF and long-term alignment framework based on Thin Plate Spline, leveraging globally propagated control points to correct spatially varying inconsistencies. In addition, we adopt a point-agnostic submap registration design that is inherently robust to noisy geometry predictions. The proposed framework is fully plug-and-play, compatible with diverse 3D foundation models and camera configurations (e.g., monocular or surround-view). Extensive experiments demonstrate that our method consistently yields more coherent geometry and lower trajectory errors across multiple datasets, backbone models, and camera setups, highlighting its robustness and generality. Codes are publicly available at \href{https://github.com/Xian-Bei/TALO}{https://github.com/Xian-Bei/TALO}.

</details>


### [16] [A multi-weight self-matching visual explanation for cnns on sar images](https://arxiv.org/abs/2512.02344)
*Siyuan Sun,Yongping Zhang,Hongcheng Zeng,Yamin Wang,Wei Yang,Wanting Yang,Jie Chen*

Main category: cs.CV

TL;DR: 提出MS-CAM结合通道和元素级权重，通过特征与梯度自匹配为SAR任务提供更精细的可视化解释，并可扩展用于弱监督定位，实验验证了其优越性与关键影响因素。


<details>
  <summary>Details</summary>
Motivation: CNN在SAR任务中表现好但内部机制复杂且不透明，影响高可靠性需求的满足，故需提升模型可解释性以促进在SAR领域的应用。

Method: 提出多权重自匹配类激活映射（MS-CAM），将SAR图像与CNN提取的特征图及对应梯度进行匹配，结合通道级和元素级权重生成可视化热图并用于定位。

Result: 在自建SAR目标分类数据集上进行大量实验，MS-CAM相比其他可视化方法更准确地标出感兴趣区域并捕捉目标细节，同时验证了其在弱监督定位任务中的可行性，并分析了影响定位精度的关键因素如像素阈值。

Conclusion: MS-CAM有效提升了CNN在SAR上的可解释性，能更准确地突出网络关注区域并捕捉目标细节，且可用于弱监督目标定位。

Abstract: In recent years, convolutional neural networks (CNNs) have achieved significant success in various synthetic aperture radar (SAR) tasks. However, the complexity and opacity of their internal mechanisms hinder the fulfillment of high-reliability requirements, thereby limiting their application in SAR. Improving the interpretability of CNNs is thus of great importance for their development and deployment in SAR. In this paper, a visual explanation method termed multi-weight self-matching class activation mapping (MS-CAM) is proposed. MS-CAM matches SAR images with the feature maps and corresponding gradients extracted by the CNN, and combines both channel-wise and element-wise weights to visualize the decision basis learned by the model in SAR images. Extensive experiments conducted on a self-constructed SAR target classification dataset demonstrate that MS-CAM more accurately highlights the network's regions of interest and captures detailed target feature information, thereby enhancing network interpretability. Furthermore, the feasibility of applying MS-CAM to weakly-supervised obiect localization is validated. Key factors affecting localization accuracy, such as pixel thresholds, are analyzed in depth to inform future work.

</details>


### [17] [Understanding and Harnessing Sparsity in Unified Multimodal Models](https://arxiv.org/abs/2512.02351)
*Shwai He,Chaorui Deng,Ang Li,Shen Yan*

Main category: cs.CV

TL;DR: 该文系统性评估了统一多模态模型组件在不需训练的剪枝下的可压缩性：理解模块可高度压缩，生成模块对压缩敏感。为解决生成模块脆弱性，提出MoE Adaptation，将生成模块划分为多专家并稀疏激活，通过专家冻结微调和全量可训练适配恢复性能。改造后的BAGEL在仅激活约一半参数下恢复到近全量性能。


<details>
  <summary>Details</summary>
Motivation: 统一多模态模型在推理上存在低效：不是所有任务或样本都需要全部模型能力，缺乏对各组件易压缩性和如何通过稀疏激活改善效率的系统理解。

Method: 使用训练-free pruning（深度剪枝与宽度减小）作为探针分析模型组件；基于观察设计Mixture-of-Experts Adaptation，先做expert-frozen tuning验证稀疏激活有效性，再做完全可训练的适配以进一步提升。

Result: Analyzed

Conclusion: 理解组件具有较高可压缩性，生成组件对压缩极为敏感；MoE Adaptation通过稀疏专家激活有效恢复生成性能，兼顾效率与效果。

Abstract: Large multimodal models have achieved remarkable progress in both understanding and generation. Recent efforts pursue unified multimodal models that integrate heterogeneous components to support both capabilities within a single framework. However, such unification introduces inference inefficiencies, e.g., specific tasks or samples may not require the full knowledge or capacity of the unified model. Yet, a systematic understanding of how these inefficiencies manifest across different components remains limited. In this work, we first conduct a systematic analysis of unified multimodal model components using training-free pruning as a probing methodology, considering both depth pruning and width reduction. Our study reveals that the understanding component exhibits notable compressibility in both understanding and generation tasks, which is more pronounced in the latter. In contrast, the generation components are highly sensitive to compression, with performance deteriorating sharply even under moderate compression ratios. To address this limitation, we propose the Mixture-of-Experts (MoE) Adaptation, inspired by the dynamic activation patterns observed across different samples. This approach partitions the generation module into multiple experts and enables sparse activation to restore generation quality. We validate the effectiveness of sparse activation through expert-frozen tuning and further demonstrate that a fully trainable adaptation delivers additional gains. As a result, the adapted BAGEL model achieves performance comparable to the full model while activating only about half of its parameters. The code is released at \href{https://github.com/Shwai-He/SparseUnifiedModel}{this link}.

</details>


### [18] [WSCF-MVCC: Weakly-supervised Calibration-free Multi-view Crowd Counting](https://arxiv.org/abs/2512.02359)
*Bin Li,Daijie Chen,Qi Zhang*

Main category: cs.CV

TL;DR: 提出一款弱监督、免标定的多视角人群计数方法，通过图像级人数监督、自监督多尺度排序损失及语义驱动的视图匹配，在弱标注条件下实现了优于现有方法的性能并开源代码。


<details>
  <summary>Details</summary>
Motivation: 现有多视角计数方法依赖昂贵的密度图标注和相机标定，且校准自由方法仍需图像级人数注释。目标是减少标注成本，提升部署灵活性，保持或超越现有方法的性能。

Method: 提出WSCF-MVCC框架：1) 单视图计数模块以图像级总人数作为监督，替代密度图监督；2) 引入自监督的多尺度排序损失（ranking loss），利用图像多尺度关系增强模型特征表达；3) 使用语义信息（如语义分割或特征）进行视图匹配，以便更准确地将不同视角的人群关联到场景平面进行计数；4) 训练中不需要相机标定或场景级注释，实用性更强。

Result: 在三个常用的多视角计数数据集上，WSCF-MVCC在弱监督设置下优于现有最先进方法，表明其在无需标定和密集注释的实际场景中更具适用性。作者已开源代码。

Conclusion: 该论文提出了一个弱监督、无需相机标定的多视角人群计数方法WSCF-MVCC，在单视图计数模块只使用总人数监督，避免密度图注释开销，并通过自监督多尺度排序损失提升感知能力，同时利用语义信息改善视图匹配，从而提高场景级计数精度。

Abstract: Multi-view crowd counting can effectively mitigate occlusion issues that commonly arise in single-image crowd counting. Existing deep-learning multi-view crowd counting methods project different camera view images onto a common space to obtain ground-plane density maps, requiring abundant and costly crowd annotations and camera calibrations. Hence, calibration-free methods are proposed that do not require camera calibrations and scene-level crowd annotations. However, existing calibration-free methods still require expensive image-level crowd annotations for training the single-view counting module. Thus, in this paper, we propose a weakly-supervised calibration-free multi-view crowd counting method (WSCF-MVCC), directly using crowd count as supervision for the single-view counting module rather than density maps constructed from crowd annotations. Instead, a self-supervised ranking loss that leverages multi-scale priors is utilized to enhance the model's perceptual ability without additional annotation costs. What's more, the proposed model leverages semantic information to achieve a more accurate view matching and, consequently, a more precise scene-level crowd count estimation. The proposed method outperforms the state-of-the-art methods on three widely used multi-view counting datasets under weakly supervised settings, indicating that it is more suitable for practical deployment compared with calibrated methods. Code is released in https://github.com/zqyq/Weakly-MVCC.

</details>


### [19] [VACoT: Rethinking Visual Data Augmentation with VLMs](https://arxiv.org/abs/2512.02361)
*Zhengzhuo Xu,Chong Sun,SiNan Du,Chen Li,Jing Lyu,Chun Yuan*

Main category: cs.CV

TL;DR: VACoT通过在推理时动态施加多样化后处理视觉增强，并用agent强化学习和条件奖励选择策略，显著提升VLM在困难及对抗OCR场景的鲁棒性，减少训练需求并在多基准上验证了效果。


<details>
  <summary>Details</summary>
Motivation: 现有VLM主要依赖大量真实或合成数据进行训练，训练成本高且对基础感知任务（如OCR）在对抗或分布外场景鲁棒性不足，继续训练回报递减，因此提出在推理阶段利用增强以提升鲁棒性。

Method: 在推理阶段使用结构化的视觉增强集合，结合基于agent的强化学习进行高效选择，采用条件奖励方案鼓励必要增强并惩罚冗长回答，从而减少训练需求与计算开销。

Result: 在13个感知基准上进行大量实验，VACoT表现优于先前方法；并提出AdvOCR数据集以展示后处理视觉增强在对抗场景下的泛化优势。

Conclusion: VACoT在推理时动态调用图像增强，通过后处理变换（如去噪）提高VLM在困难和分布外输入上的鲁棒性，尤其是OCR对抗场景。

Abstract: While visual data augmentation remains a cornerstone for training robust vision models, it has received limited attention in visual language models (VLMs), which predominantly rely on large-scale real data acquisition or synthetic diversity. Consequently, they may struggle with basic perception tasks that conventional models handle reliably. Given the substantial cost of pre-training and fine-tuning VLMs, continue training on augmented data yields limited and diminishing returns. In this paper, we present Visual Augmentation Chain-of-Thought (VACoT), a framework that dynamically invokes image augmentations during model inference. By incorporating post-hoc transformations such as denoising, VACoT substantially improves robustness on challenging and out-of-distribution inputs, especially in OCR-related adversarial scenarios. Distinct from prior approaches limited to local cropping, VACoT integrates a structured collection of general visual augmentations, broadening the query image views while reducing training complexity and computational overhead with efficient agentic reinforcement learning. We propose a conditional reward scheme that encourages necessary augmentation while penalizing verbose responses, ensuring concise and effective reasoning in perception tasks. We demonstrate the superiority of VACoT with extensive experiments on 13 perception benchmarks and further introduce AdvOCR to highlight the generalization benefits of post-hoc visual augmentations in adversarial scenarios.

</details>


### [20] [Tackling Tuberculosis: A Comparative Dive into Machine Learning for Tuberculosis Detection](https://arxiv.org/abs/2512.02364)
*Daanish Hindustani,Sanober Hindustani,Preston Nguyen*

Main category: cs.CV

TL;DR: 在4200张胸片上，轻量级SqueezeNet比预训练ResNet-50表现更好，适合移动端TB筛查，但仍需进一步提高速度、小型化与准确性以满足临床应用要求。


<details>
  <summary>Details</summary>
Motivation: 传统TB检测方法（痰涂片、培养）效率低且在资源有限地区难以推广，故探索深度学习与计算机视觉用于自动化、快速的X光胸片TB筛查，以便实现早期识别与治疗并便于部署到移动设备。

Method: 使用Kaggle提供的4200张胸部X光图像，进行了数据划分、数据增强和图像重采样；构建并训练两个模型：一个预训练的ResNet-50微调模型和一个通用SqueezeNet模型；使用损失、准确率、精确率、召回率和混淆矩阵评估模型性能。

Result: SqueezeNet：损失32%、准确率89%、精确率98%、召回率80%、F1为87%；ResNet-50：损失54%、准确率73%、精确率88%、召回率52%、F1为65%。

Conclusion: 该研究表明在胸片TB检测任务中，轻量化模型SqueezeNet在本文数据集上优于预训练ResNet-50，具有更高的准确率、精确率和F1分数，且更适合部署到资源受限设备。

Abstract: This study explores the application of machine learning models, specifically a pretrained ResNet-50 model and a general SqueezeNet model, in diagnosing tuberculosis (TB) using chest X-ray images. TB, a persistent infectious disease affecting humanity for millennia, poses challenges in diagnosis, especially in resource-limited settings. Traditional methods, such as sputum smear microscopy and culture, are inefficient, prompting the exploration of advanced technologies like deep learning and computer vision. The study utilized a dataset from Kaggle, consisting of 4,200 chest X-rays, to develop and compare the performance of the two machine learning models. Preprocessing involved data splitting, augmentation, and resizing to enhance training efficiency. Evaluation metrics, including accuracy, precision, recall, and confusion matrix, were employed to assess model performance. Results showcase that the SqueezeNet achieved a loss of 32%, accuracy of 89%, precision of 98%, recall of 80%, and an F1 score of 87%. In contrast, the ResNet-50 model exhibited a loss of 54%, accuracy of 73%, precision of 88%, recall of 52%, and an F1 score of 65%. This study emphasizes the potential of machine learning in TB detection and possible implications for early identification and treatment initiation. The possibility of integrating such models into mobile devices expands their utility in areas lacking TB detection resources. However, despite promising results, the need for continued development of faster, smaller, and more accurate TB detection models remains crucial in contributing to the global efforts in combating TB.

</details>


### [21] [Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention](https://arxiv.org/abs/2512.02368)
*Wenyi Xiong,Jian Chen*

Main category: cs.CV

TL;DR: 提出一种基于MoE的频域-时域-空域联合建模的无地图轨迹预测方法，通过频谱分量选择、选择性注意力和多模态解码器，在NuScenes上表现优越。


<details>
  <summary>Details</summary>
Motivation: 动机是现有方法在复杂交互场景下难以从冗余数据中高效提取有价值的场景信息，导致计算资源浪费与预测精度下降。通过在频域选择关键分量并结合时空选择性注意力，期望提升效率与准确性，尤其应对复杂代理交互。

Method: 方法包括：1) 在时域引入Mixture of Experts（MoE）机制自适应挑选关键频率分量，并结合频域特征与多尺度时间嵌入；2) 设计选择性注意力模块，用于在时间序列和空间交互中筛除冗余信息，提高信息利用效率；3) 构建多模态解码器生成多条候选轨迹；4) 使用patch-level和point-level损失进行监督，提升局部与点级精度。

Result: 在NuScenes数据集上的实验结果显示，所提方法在关键评估指标上优于对比方法，表明在复杂交互场景下具有更高的预测精度和效率（论文摘要未给出具体数值）。

Conclusion: 本论文提出了一种无地图的轨迹预测算法，能够在时域、空域和频域联合建模，显著提升复杂交互场景下的预测性能。采用专家混合（MoE）选择重要频谱分量、多尺度时间特征融合、选择性注意力模块去除冗余时序与交互信息，并通过多模态解码器与补丁级/点级损失监督训练，实验表明在NuScenes数据集上优于对比方法。

Abstract: Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.

</details>


### [22] [SAGE: Style-Adaptive Generalization for Privacy-Constrained Semantic Segmentation Across Domains](https://arxiv.org/abs/2512.02369)
*Qingmei Li,Yang Zhang,Peifeng Zhang,Haohuan Fu,Juepeng Zheng*

Main category: cs.CV

TL;DR: 提出SAGE：通过风格迁移生成多样样式并自适应融合为动态视觉提示，改善冻结语义分割模型的域泛化，实验验证优于或匹配现有方法且胜过全量微调。


<details>
  <summary>Details</summary>
Motivation: 在无法访问模型参数或架构、无法进行微调的隐私/安全场景下，需发展仅在输入层进行的策略来提升模型在未见目标域的泛化能力。

Method: 利用风格迁移生成源域的多样样式表示，学习一组风格特征；对每个输入图像根据其视觉上下文自适应融合这些风格线索，生成动态的视觉提示并施加于输入以调整外观，而不修改模型内部参数。

Result: 在五个基准数据集上的大量实验表明，SAGE在隐私受限条件下相比最先进的方法具有竞争力或更优的性能，并在所有设置中优于完整微调基线。

Conclusion: SAGE在隐私约束下通过合成视觉提示（visual prompts）以样式自适应的方式弥合冻结模型与未见域多样性之间的差距，从而提升语义分割的域泛化性能。

Abstract: Domain generalization for semantic segmentation aims to mitigate the degradation in model performance caused by domain shifts. However, in many real-world scenarios, we are unable to access the model parameters and architectural details due to privacy concerns and security constraints. Traditional fine-tuning or adaptation is hindered, leading to the demand for input-level strategies that can enhance generalization without modifying model weights. To this end, we propose a \textbf{S}tyle-\textbf{A}daptive \textbf{GE}neralization framework (\textbf{SAGE}), which improves the generalization of frozen models under privacy constraints. SAGE learns to synthesize visual prompts that implicitly align feature distributions across styles instead of directly fine-tuning the backbone. Specifically, we first utilize style transfer to construct a diverse style representation of the source domain, thereby learning a set of style characteristics that can cover a wide range of visual features. Then, the model adaptively fuses these style cues according to the visual context of each input, forming a dynamic prompt that harmonizes the image appearance without touching the interior of the model. Through this closed-loop design, SAGE effectively bridges the gap between frozen model invariance and the diversity of unseen domains. Extensive experiments on five benchmark datasets demonstrate that SAGE achieves competitive or superior performance compared to state-of-the-art methods under privacy constraints and outperforms full fine-tuning baselines in all settings.

</details>


### [23] [On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning](https://arxiv.org/abs/2512.02375)
*Liyuan Lou,Wanyun Li,Wentian Gan,Yifei Yu,Tengfei Wang,Xin Wang,Zongqian Zhan*

Main category: cs.CV

TL;DR: 提出一套实时SfM反馈闭环：动态粗网格、在线质量评估、预测路径优化，实验证明能在近实时下提升覆盖完整性并降低复飞成本；代码开源。


<details>
  <summary>Details</summary>
Motivation: 传统实时摄影测量多关注帧或图像的处理，缺乏对在飞重建质量的评估与对采集过程的引导，无法在任务进行中主动纠正覆盖盲区，影响时效性与数据完整性。

Method: 基于实时SfM，方法包含三个模块：(1) 在线增量粗网格生成，用于动态扩展稀疏点云；(2) 在线网格质量评估，输出可操作性指标以量化重建质量和覆盖缺口；(3) 预测性路径规划，根据评估结果即时优化无人机航迹以填补未覆盖或质量欠佳区域。

Result: 实验表明，该方法能在近实时完成原位重建与质量评估，生成可执行反馈显著减少覆盖缺口与复飞次数，验证了在支持主动式、智能化采集工作流方面的有效性。

Conclusion: 本文提出了一种用于实时无人机摄影测量的On-the-fly Feedback SfM框架，通过在飞行中对已观测区域进行质量评估并提供可操作反馈，实现探索与利用的迭代流程，从而改善覆盖率并降低复飞成本。

Abstract: Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.

</details>


### [24] [From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking](https://arxiv.org/abs/2512.02392)
*Yuqing Shao,Yuchen Yang,Rui Yu,Weilong Li,Xu Guo,Huaicheng Yan,Wei Wang,Xiao Sun*

Main category: cs.CV

TL;DR: 针对共享DETR生成的高互相相似嵌入，FDTA用空间、时间和身份三类适配器增强实例级可分性，从而显著改善端到端MOT的关联性能并在多项基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有端到端MOT虽检测性能强，但因共享DETR侧重类别级区分、帧内信息，导致对象嵌入在不同实例间相似度过高，关联效果不足；需要增强实例级的时空连续性区分。

Method: 在共享DETR基础上加入显式特征精炼模块：Spatial Adapter引入深度感知的空间连续性信息，Temporal Adapter聚合历史信息建立时间依赖，Identity Adapter通过质量感知对比学习增强实例可分性；整体在嵌入层进行联合训练以提升关联准确率。

Result: 在DanceTrack、SportsMOT、BFT等多个MOT基准上，FDTA显著提高了关联准确率并实现或接近SOTA，与原始端到端方法相比在IDF1、MOTA等指标上有明显提升。

Conclusion: FDTA通过三种适配器（空间、时间、身份）显著提升了DETR类端到端MOT方法的关联性能，在多个基准上取得了SOTA效果。

Abstract: End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.

</details>


### [25] [Reproducing and Extending RaDelft 4D Radar with Camera-Assisted Labels](https://arxiv.org/abs/2512.02394)
*Kejia Hu,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 作者重现并改进 RaDelft 工作，提出一种基于相机语义分割和空间聚类的自动雷达标注方法，提升了雷达标签质量并评估了雾天影响。


<details>
  <summary>Details</summary>
Motivation: 当前雷达语义分割受限于开源数据集和标签的缺乏；RaDelft 数据集仅提供 LiDAR 注释且无公开生成雷达标签的代码，影响可重复性与后续研究。

Method: 将 4D 雷达点云投影到摄像头语义分割结果上，结合空间聚类算法，将相机语义标签传递给雷达点云，生成雷达点级语义标签；并在不同雾霾条件下评估标注性能。

Result: 重现 RaDelft 数值结果，并展示相机引导的标注方法能显著提升雷达点云标签准确率；此外量化了不同雾等级对标注性能的影响。

Conclusion: 该论文重现并扩展了 RaDelft 的工作，证明基于摄像头引导的雷达标注管道可在无需人工标注下，为 4D 雷达点云生成高质量语义标签，从而提高雷达语义分割研究的可重复性和数据可用性。

Abstract: Recent advances in 4D radar highlight its potential for robust environment perception under adverse conditions, yet progress in radar semantic segmentation remains constrained by the scarcity of open source datasets and labels. The RaDelft data set, although seminal, provides only LiDAR annotations and no public code to generate radar labels, limiting reproducibility and downstream research. In this work, we reproduce the numerical results of the RaDelft group and demonstrate that a camera-guided radar labeling pipeline can generate accurate labels for radar point clouds without relying on human annotations. By projecting radar point clouds into camera-based semantic segmentation and applying spatial clustering, we create labels that significantly enhance the accuracy of radar labels. These results establish a reproducible framework that allows the research community to train and evaluate the labeled 4D radar data. In addition, we study and quantify how different fog levels affect the radar labeling performance.

</details>


### [26] [Skywork-R1V4: Toward Agentic Multimodal Intelligence through Interleaved Thinking with Images and DeepResearch](https://arxiv.org/abs/2512.02395)
*Yifan Zhang,Liang Hu,Haofeng Sun,Peiyu Wang,Yichen Wei,Shukang Yin,Jiangbo Pei,Wei Shen,Peng Xia,Yi Peng,Tianyidan Xie,Eric Li,Yang Liu,Xuchen Song,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork-R1V4: a 30B multimodal agent trained via supervised fine-tuning on <30k execution-consistent trajectories achieves SOTA multimodal planning, image manipulation, and deep search without RL.


<details>
  <summary>Details</summary>
Motivation: Existing systems treat image manipulation and web search separately, rely on RL, and lack planning grounded in real tool traces; aim to unify capabilities and avoid RL.

Method: Trained a 30B A3B model via supervised fine-tuning on <30k high-quality trajectories with stepwise consistency filtering; model alternates between image manipulation and external knowledge retrieval; no RL used.

Result: State-of-the-art on perception and multimodal search benchmarks (MMSearch 66.1, FVQA 67.2), surpassing Gemini 2.5 Flash on 11 metrics; can orchestrate >10 tool calls for long-horizon tasks.

Conclusion: Skywork-R1V4 demonstrates that supervised fine-tuning on a curated set of planning-execution-consistent trajectories can produce a strong multimodal agent capable of interleaved visual reasoning and web search without RL.

Abstract: Despite recent progress in multimodal agentic systems, existing approaches often treat image manipulation and web search as disjoint capabilities, rely heavily on costly reinforcement learning, and lack planning grounded in real tool-execution traces. To address these limitations, we present Skywork-R1V4, a 30B (A3B) parameter multimodal agentic model that unifies multimodal planning, active image manipulation ("thinking with images"), deep multimodal search, and, most critically, interleaved reasoning that dynamically alternates between visual operations and external knowledge retrieval. Trained solely via supervised fine-tuning on fewer than 30,000 high-quality, planning-execution-consistent trajectories and validated through stepwise consistency filtering, Skywork-R1V4 achieves state-of-the-art results across perception and multimodal search benchmarks: it scores 66.1 on MMSearch and 67.2 on FVQA, surpassing Gemini 2.5 Flash on all 11 metrics. Skywork-R1V4 exhibits emergent long-horizon reasoning at inference time, successfully orchestrating more than 10 tool calls to solve complex, multi-step tasks. Our results demonstrate that sophisticated agentic multimodal intelligence can be achieved through carefully curated supervised learning alone, without any reliance on reinforcement learning.

</details>


### [27] [Nav-$R^2$ Dual-Relation Reasoning for Generalizable Open-Vocabulary Object-Goal Navigation](https://arxiv.org/abs/2512.02400)
*Wentao Xiang,Haokang Zhang,Tianhang Yang,Zedong Chu,Ruihang Chu,Shichao Xie,Yujian Yuan,Jian Sun,Zhining Gu,Junjie Wang,Xiaolong Wu,Mu Xu,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出Nav-R^2：用结构化CoT推理和相似性感知记忆明确建模目标-环境与环境-动作关系，通过NavR2-CoT数据集学习感知与规划，SA-Mem压缩与融合帧特征无新增参数，在定位未知物体上达SOTA，实时2Hz。


<details>
  <summary>Details</summary>
Motivation: Address opaque decision-making and poor generalization for object-goal navigation to unseen objects by modeling target-environment and environment-action relationships explicitly and preserving relevant memory.

Method: Structured CoT with SA-Mem and Nav-R^2 pipeline

Result: Nav-R^2 with NavR2-CoT dataset and Similarity-Aware Memory achieves state-of-the-art localization of unseen objects, real-time inference at 2Hz, avoids overfitting to seen categories, no extra parameters for SA-Mem.

Conclusion: Nav-R^2有效提升开放词汇对象导航对未知对象的定位能力，结合结构化CoT与无参数开销的相似性记忆实现可解释、高效和泛化的导航。

Abstract: Object-goal navigation in open-vocabulary settings requires agents to locate novel objects in unseen environments, yet existing approaches suffer from opaque decision-making processes and low success rate on locating unseen objects. To address these challenges, we propose Nav-$R^2$, a framework that explicitly models two critical types of relationships, target-environment modeling and environment-action planning, through structured Chain-of-Thought (CoT) reasoning coupled with a Similarity-Aware Memory. We construct a Nav$R^2$-CoT dataset that teaches the model to perceive the environment, focus on target-related objects in the surrounding context and finally make future action plans. Our SA-Mem preserves the most target-relevant and current observation-relevant features from both temporal and semantic perspectives by compressing video frames and fusing historical observations, while introducing no additional parameters. Compared to previous methods, Nav-R^2 achieves state-of-the-art performance in localizing unseen objects through a streamlined and efficient pipeline, avoiding overfitting to seen object categories while maintaining real-time inference at 2Hz. Resources will be made publicly available at \href{https://github.com/AMAP-EAI/Nav-R2}{github link}.

</details>


### [28] [WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate](https://arxiv.org/abs/2512.02405)
*Anoop Cherian,River Doyle,Eyal Ben-Dov,Suhas Lohit,Kuan-Chuan Peng*

Main category: cs.CV

TL;DR: WISE is a modular MAD framework (Solvers + Reflectors) plus a modified Dawid-Skene aggregation that boosts multimodal reasoning accuracy by 2–7% on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MAD focuses on language-only tasks; need to explore efficacy on multimodal vision-and-language problems and leverage diverse LLM strengths.

Method: Introduce WISE: partition agents into Solvers (generate solutions) and Reflectors (verify, weight, provide feedback); use iterative debate rounds; aggregate via modified Dawid-Skene algorithm integrating two-stage debate model.

Result: On SMART-840, VisualPuzzles, EvoChart-QA, and SMART-840++, WISE improves accuracy by 2–7% over state-of-the-art MAD setups and aggregation methods across tasks and LLM configurations.

Conclusion: WISE demonstrates that multi-agent debate with heterogeneous single- and multi-modal experts improves vision-and-language reasoning, yielding consistent accuracy gains over baselines.

Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.

</details>


### [29] [MitUNet: Enhancing Floor Plan Recognition using a Hybrid Mix-Transformer and U-Net Architecture](https://arxiv.org/abs/2512.02413)
*Dmitriy Parashchuk,Alexey Kapshitskiy,Yuriy Karyakin*

Main category: cs.CV

TL;DR: 提出MitUNet：Mix-Transformer编码器 + scSE增强的U-Net解码器 + 可调Tversky损失，提升墙体分割的边界精度与结构正确性，利于3D重建管线。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型难以检测细长墙体并生成规则边界，导致后续矢量化和三维重建出现结构错误，需一种能兼顾全局语义与局部边界精细化的方法。

Method: 采用层次化Mix-Transformer作为编码器捕捉全局上下文，结合带scSE注意力模块的U-Net解码器恢复边界；训练时使用可调节超参数的Tversky损失以平衡精度与召回，并优先抑制边界假阳性噪声。

Result: 在CubiCasa5k和一套区域性私有数据集上，MitUNet在边界准确性和结构正确性上优于标准单任务模型，尤其在抑制边界噪声和保持对薄墙体的敏感性方面表现突出。

Conclusion: MitUNet能在墙体分割任务中提供更高的边界精度和结构完整性，适合用于自动三维重建的数据准备。

Abstract: Automatic 3D reconstruction of indoor spaces from 2D floor plans requires high-precision semantic segmentation of structural elements, particularly walls. However, existing methods optimized for standard metrics often struggle to detect thin structural components and yield masks with irregular boundaries, lacking the geometric precision required for subsequent vectorization. To address this issue, we introduce MitUNet, a hybrid neural network architecture specifically designed for wall segmentation tasks in the context of 3D modeling. In MitUNet, we utilize a hierarchical Mix-Transformer encoder to capture global context and a U-Net decoder enhanced with scSE attention blocks for precise boundary recovery. Furthermore, we propose an optimization strategy based on the Tversky loss function to effectively balance precision and recall. By fine-tuning the hyperparameters of the loss function, we prioritize the suppression of false positive noise along wall boundaries while maintaining high sensitivity to thin structures. Our experiments on the public CubiCasa5k dataset and a proprietary regional dataset demonstrate that the proposed approach ensures the generation of structurally correct masks with high boundary accuracy, outperforming standard single-task models. MitUNet provides a robust tool for data preparation in automated 3D reconstruction pipelines.

</details>


### [30] [Generalizing Vision-Language Models with Dedicated Prompt Guidance](https://arxiv.org/abs/2512.02421)
*Xinyao Li,Yinjie Min,Hongbo Chen,Zhekai Du,Fengling Li,Jingjing Li*

Main category: cs.CV

TL;DR: 通过在分割的源域上训练参数高效的专家并用Cross-Modal Attention自适应集成，GuiDG在少样本和域泛化场景下提升了VLM微调的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统对大预训练视觉-语言模型的微调在领域专用性与泛化能力之间存在权衡，统一模型在所有源域上训练可能牺牲对未见域的泛化能力。理论分析表明专家分割训练可改善泛化。

Method: 提出两步GuiDG框架：1) 使用prompt tuning得到源域专家；2) 通过Cross-Modal Attention模块在视觉编码器微调中自适应集成专家信息。并构建ImageNet-DG用于少样本DG评估。

Result: 在标准DG基准和构建的ImageNet-DG上，GuiDG在保持参数效率的同时，优于现有最先进的微调方法。

Conclusion: 训练多个参数高效的领域专家模型并通过自适应集成指导视觉编码器微调，可以提高在未见域上的泛化能力。

Abstract: Fine-tuning large pretrained vision-language models (VLMs) has emerged as a prevalent paradigm for downstream adaptation, yet it faces a critical trade-off between domain specificity and domain generalization (DG) ability. Current methods typically fine-tune a universal model on the entire dataset, which potentially compromises the ability to generalize to unseen domains. To fill this gap, we provide a theoretical understanding of the generalization ability for VLM fine-tuning, which reveals that training multiple parameter-efficient expert models on partitioned source domains leads to better generalization than fine-tuning a universal model. Inspired by this finding, we propose a two-step domain-expert-Guided DG (GuiDG) framework. GuiDG first employs prompt tuning to obtain source domain experts, then introduces a Cross-Modal Attention module to guide the fine-tuning of the vision encoder via adaptive expert integration. To better evaluate few-shot DG, we construct ImageNet-DG from ImageNet and its variants. Extensive experiments on standard DG benchmarks and ImageNet-DG demonstrate that GuiDG improves upon state-of-the-art fine-tuning methods while maintaining efficiency.

</details>


### [31] [GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2512.02423)
*Haolong Yan,Yeqing Shen,Xin Huang,Jia Wang,Kaijun Tan,Zhixuan Liang,Hongxin Li,Zheng Ge,Osamu Yoshie,Si Li,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 提出GUI Exploration Lab仿真环境并采用“监督微调→单回合RL→多回合RL”训练流程，逐步提升GUI导航代理的记忆、泛化与探索能力，实验证明在多种基准和真实场景下均有效。


<details>
  <summary>Details</summary>
Motivation: 真实PC和移动App界面复杂且受限，难以获取完整环境信息，阻碍代理导航能力系统性研究与基准构建；需要一个可控且可访问的仿真环境以促进研究。

Method: 构建可组合的GUI仿真引擎，允许访问完整环境信息；采用三阶段训练流程：监督微调（记忆基础知识）、单回合强化学习（提升泛化）、多回合强化学习（促进探索策略）；在静态与交互基准上进行大量实验验证。

Result: 实验显示：监督微调能有效记忆基础知识；单回合RL改善对未见场景的泛化；多回合RL通过试错形成探索策略，进一步提升导航表现；方法在静态与交互基准以及真实场景上均表现出良好泛化。

Conclusion: 本文提出了GUI Exploration Lab模拟环境，通过灵活定义界面元素和导航图，解决真实GUI环境不可获取全局信息的问题；实验表明监督微调用于记忆基础知识，单回合强化学习提升对未见场景的泛化，多回合强化学习通过交互探索进一步提升导航性能，结果在静态与交互基准上均有效，验证了强化学习在GUI导航中的优势。

Abstract: With the rapid development of Large Vision Language Models, the focus of Graphical User Interface (GUI) agent tasks shifts from single-screen tasks to complex screen navigation challenges. However, real-world GUI environments, such as PC software and mobile Apps, are often complex and proprietary, making it difficult to obtain the comprehensive environment information needed for agent training and evaluation. This limitation hinders systematic investigation and benchmarking of agent navigation capabilities. To address this limitation, we introduce GUI Exploration Lab, a simulation environment engine for GUI agent navigation research that enables flexible definition and composition of screens, icons, and navigation graphs, while providing full access to environment information for comprehensive agent training and evaluation. Through extensive experiments, we find that supervised fine-tuning enables effective memorization of fundamental knowledge, serving as a crucial foundation for subsequent training. Building on this, single-turn reinforcement learning further enhances generalization to unseen scenarios. Finally, multi-turn reinforcement learning encourages the development of exploration strategies through interactive trial and error, leading to further improvements in screen navigation performance. We validate our methods on both static and interactive benchmarks, demonstrating that our findings generalize effectively to real-world scenarios. These findings demonstrate the advantages of reinforcement learning approaches in GUI navigation and offer practical guidance for building more capable and generalizable GUI agents.

</details>


### [32] [WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning](https://arxiv.org/abs/2512.02425)
*Woongyeong Yeo,Kangsan Kim,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.CV

TL;DR: WorldMM 通過融合文本與視覺記憶與自適應多尺度檢索，解決長視頻推理中細節丟失與檢索尺度限制問題，顯著提高了長視頻問答性能。


<details>
  <summary>Details</summary>
Motivation: 現有長視頻模型受制於上下文容量與抽象過程中丟失視覺細節，且僅依賴文本摘要無法在複雜場景中利用視覺證據，檢索尺度固定也限制了對變長事件的捕捉。

Method: 引入三類互補記憶（情節記憶、多尺度索引；語義記憶、持續更新概念知識；視覺記憶、保存場景細節），並設計一個能根據查詢迭代選擇記憶源與時間粒度的自適應檢索代理。

Result: 在五個長視頻問答基準上，WorldMM 平均超越先前最先進方法 8.4% 的性能提升，顯示其在長視頻推理中的有效性。

Conclusion: WorldMM 提出了一種多模態記憶代理，通過結合文本與視覺記憶並使用自適應檢索策略，有效提升長視頻問答表現。

Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.

</details>


### [33] [LightHCG: a Lightweight yet powerful HSIC Disentanglement based Causal Glaucoma Detection Model framework](https://arxiv.org/abs/2512.02437)
*Daeyoung Kim*

Main category: cs.CV

TL;DR: LightHCG：极轻量的因果表征CVAE模型，通过HSIC解缠与图自编码器学习因果潜在空间，实现高性能、低参数的青光眼检测并支持干预分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度视觉模型的青光眼检测虽有高性能，但存在参数过重、可靠性不足、易受伪相关影响且难以用于干预分析的缺陷。因此需要一种更轻量、具因果解释性且便于临床干预模拟的方法。

Method: 采用卷积VAE构建潜在表征；利用HSIC（Hilbert-Schmidt Independence Criterion）进行潜在空间解缠；使用图自编码器（Graph Autoencoder）实现无监督的因果表示学习；基于这些因果驱动的潜在表征进行青光眼分类与干预分析。

Result: LightHCG 在保留或超越主流视觉模型（如InceptionV3、MobileNetV2、VGG16）性能的同时，模型参数减少约93%~99%，分类性能优越并增强了进行AI驱动干预分析的可行性。

Conclusion: LightHCG 提出了一种轻量化的、基于卷积变分自编码器（CVAE）与因果表示学习的青光眼检测模型，能够在显著减少参数的同时提升分类性能，并支持干预分析与临床模拟可能性。

Abstract: As a representative optic degenerative condition, glaucoma has been a threat to millions due to its irreversibility and severe impact on human vision fields. Mainly characterized by dimmed and blurred visions, or peripheral vision loss, glaucoma is well known to occur due to damages in the optic nerve from increased intraocular pressure (IOP) or neovascularization within the retina. Traditionally, most glaucoma related works and clinical diagnosis focused on detecting these damages in the optic nerve by using patient data from perimetry tests, optic papilla inspections and tonometer-based IOP measurements. Recently, with advancements in computer vision AI models, such as VGG16 or Vision Transformers (ViT), AI-automatized glaucoma detection and optic cup segmentation based on retinal fundus images or OCT recently exhibited significant performance in aiding conventional diagnosis with high performance. However, current AI-driven glaucoma detection approaches still have significant room for improvement in terms of reliability, excessive parameter usage, possibility of spurious correlation within detection, and limitations in applications to intervention analysis or clinical simulations. Thus, this research introduced a novel causal representation driven glaucoma detection model: LightHCG, an extremely lightweight Convolutional VAE-based latent glaucoma representation model that can consider the true causality among glaucoma-related physical factors within the optic nerve region. Using HSIC-based latent space disentanglement and Graph Autoencoder based unsupervised causal representation learning, LightHCG not only exhibits higher performance in classifying glaucoma with 93~99% less weights, but also enhances the possibility of AI-driven intervention analysis, compared to existing advanced vision models such as InceptionV3, MobileNetV2 or VGG16.

</details>


### [34] [Boosting Medical Vision-Language Pretraining via Momentum Self-Distillation under Limited Computing Resources](https://arxiv.org/abs/2512.02438)
*Phuc Pham,Nhu Pham,Ngoc Quoc Ly*

Main category: cs.CV

TL;DR: 通过动量自蒸馏和动量+梯度累积扩大有效批次，本文在单GPU和小样本医疗场景下实现高效的VLM训练，取得接近或优于SOTA的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗数据注释昂贵且样本量有限；对比学习需要大批次且计算资源高，故寻找在单GPU和小数据下保持或提升VLM性能的高效方法。

Method: 提出Momentum Self-Distillation (MSD)，将动量编码器作为教师模型并在对比学习框架中执行蒸馏，同时通过动量机制与梯度累积结合以增大有效批次大小以节省内存开销。

Result: 在零样本分类与少样本微调中取得与SOTA接近或超越的结果：少样本AUC-ROC超过90%，检索任务提升2-3%。在单GPU上实现合理训练时间与高效性能。

Conclusion: 该论文提出将动量方法与自蒸馏结合以提升医疗视觉-语言模型的学习效率与性能，尤其在资源受限（单GPU）与小样本场景下表现优异。

Abstract: In medical healthcare, obtaining detailed annotations is challenging, highlighting the need for robust Vision-Language Models (VLMs). Pretrained VLMs enable fine-tuning on small datasets or zero-shot inference, achieving performance comparable to task-specific models. Contrastive learning (CL) is a key paradigm for training VLMs but inherently requires large batch sizes for effective learning, making it computationally demanding and often limited to well-resourced institutions. Moreover, with limited data in healthcare, it is important to prioritize knowledge extraction from both data and models during training to improve performance. Therefore, we focus on leveraging the momentum method combined with distillation to simultaneously address computational efficiency and knowledge exploitation. Our contributions can be summarized as follows: (1) leveraging momentum self-distillation to enhance multimodal learning, and (2) integrating momentum mechanisms with gradient accumulation to enlarge the effective batch size without increasing resource consumption. Our method attains competitive performance with state-of-the-art (SOTA) approaches in zero-shot classification, while providing a substantial boost in the few-shot adaption, achieving over 90% AUC-ROC and improving retrieval tasks by 2-3%. Importantly, our method achieves high training efficiency with a single GPU while maintaining reasonable training time. Our approach aims to advance efficient multimodal learning by reducing resource requirements while improving performance over SOTA methods. The implementation of our method is available at https://github.com/phphuc612/MSD .

</details>


### [35] [Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation](https://arxiv.org/abs/2512.02441)
*Junghwan Park,Woojin Cho,Junhyuk Heo,Darongsae Kwon,Kookjin Lee*

Main category: cs.CV

TL;DR: BOLT从多任务微调向量提取层级正交基底，在新任务只训练对角系数，提供训练前的良好初始化及参数高效微调，实验证明其在低数据/计算预算下对未见任务迁移有效。


<details>
  <summary>Details</summary>
Motivation: 传统元学习需额外的元训练且成本高、不稳定；同时大量任务特定预训练模型可用。问题是如何在有限数据与计算预算下，利用这些已有微调模型高效迁移到新任务。

Method: 离线阶段：从多个源任务的微调权重向量收集主奇异向量并逐层正交化形成基底。在线阶段：冻结基底，只训练每层的一组对角系数（低秩、可控秩），并提供基于源任务系数池的训练前初始化与轻量重标度步骤。

Result: 实验证明BOLT在参数效率微调与未经训练的初始化两条路径上均表现稳健，优于常见PEFT基线和代表性元学习初始化，表明在任务信息引导的正交子空间内受约束的适应是未见任务转移的有效替代方案。

Conclusion: BOLT通过在层级上提取不同任务微调向量的主奇异方向并正交化来构建任务信息基底，然后在新任务上仅训练对角系数，从而在参数效率和训练稳定性上优于常见PEFT方法与部分元学习初始化，证明了受约束的正交子空间对未见任务迁移有效。

Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.

</details>


### [36] [Temporal Dynamics Enhancer for Directly Trained Spiking Object Detectors](https://arxiv.org/abs/2512.02447)
*Fan Luo,Zeyu Gao,Xinhao Luo,Kai Zhao,Yanfeng Lu*

Main category: cs.CV

TL;DR: 提出Temporal Dynamics Enhancer (TDE)，通过Spiking Encoder和Attention Gating Module增强脉冲神经网络的时序建模，并用Spike-Driven Attention降低注意力能耗，在目标检测上显著提升mAP并节能。


<details>
  <summary>Details</summary>
Motivation: 现有SNN通常重复或固定帧聚合输入，导致时间步间刺激高度相似，限制模型在复杂任务（如目标检测）中的表达能力，需增强时序信息建模能力。

Method: TDE包含两个模块：Spiking Encoder（SE）用于在不同时间步生成多样化输入刺激；Attention Gating Module（AGM）根据时间依赖性指导SE的生成；并提出Spike-Driven Attention（SDA）以避免高能耗乘法操作，降低注意力模块的能量开销。

Result: 在静态PASCAL VOC上mAP50-95=57.7%，在神经形态EvDET200K上mAP50-95=47.6%；SDA的能耗仅为传统注意力模块的0.240倍。

Conclusion: TDE可无缝集成于现有SNN检测器，提升检测性能并显著降低注意力相关能耗，实验证明在PASCAL VOC和EvDET200K上均优于现有方法。

Abstract: Spiking Neural Networks (SNNs), with their brain-inspired spatiotemporal dynamics and spike-driven computation, have emerged as promising energy-efficient alternatives to Artificial Neural Networks (ANNs). However, existing SNNs typically replicate inputs directly or aggregate them into frames at fixed intervals. Such strategies lead to neurons receiving nearly identical stimuli across time steps, severely limiting the model's expressive power, particularly in complex tasks like object detection. In this work, we propose the Temporal Dynamics Enhancer (TDE) to strengthen SNNs' capacity for temporal information modeling. TDE consists of two modules: a Spiking Encoder (SE) that generates diverse input stimuli across time steps, and an Attention Gating Module (AGM) that guides the SE generation based on inter-temporal dependencies. Moreover, to eliminate the high-energy multiplication operations introduced by the AGM, we propose a Spike-Driven Attention (SDA) to reduce attention-related energy consumption. Extensive experiments demonstrate that TDE can be seamlessly integrated into existing SNN-based detectors and consistently outperforms state-of-the-art methods, achieving mAP50-95 scores of 57.7% on the static PASCAL VOC dataset and 47.6% on the neuromorphic EvDET200K dataset. In terms of energy consumption, the SDA consumes only 0.240 times the energy of conventional attention modules.

</details>


### [37] [nuScenes Revisited: Progress and Challenges in Autonomous Driving](https://arxiv.org/abs/2512.02448)
*Whye Kit Fong,Venice Erin Liong,Kok Seang Tan,Holger Caesar*

Main category: cs.CV

TL;DR: 全面回顾nuScenes系列数据集的创建细节、生态影响与基于其的研究进展，为自动驾驶数据集设计与方法评估提供资源性总结。


<details>
  <summary>Details</summary>
Motivation: 深度学习驱动的自动驾驶高度依赖大规模标注数据集。nuScenes作为具有多模态传感器、跨大陆采集与真实自动驾驶数据的标杆数据集，其创建细节与影响力未被充分公开，作者旨在填补这一空白并总结其对社区的长期影响。

Method: 通过整理与披露nuScenes系列数据集的采集流程、标注规范、传感器配置、数据质量控制及基准任务设置，汇总官方与非官方任务，检索并分析使用nuScenes的文献，归纳方法学进展与趋势。

Result: 披露了nuScenes数据集的具体采集与标注细节，列举了其对后续数据集设计与评估指标的影响，整理了基于nuScenes的主要任务与方法进展，并提出了数据集使用中的局限性与未来改进方向。

Conclusion: 本文回顾并详述了nuScenes数据集及其扩展（nuImages、Panoptic nuScenes），总结了数据集创建过程、技术细节、对后续数据集和研究社区标准的影响，并综述了基于这些数据集的任务与方法，强调其在多模态感知和自动驾驶研究中的基础性作用。

Abstract: Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.

</details>


### [38] [HouseLayout3D: A Benchmark and Training-Free Baseline for 3D Layout Estimation in the Wild](https://arxiv.org/abs/2512.02450)
*Valentin Bieri,Marie-Julie Rakotosaona,Keisuke Tateno,Francis Engelmann,Leonidas Guibas*

Main category: cs.CV

TL;DR: 提出面向多层真实建筑的HouseLayout3D基准和一个训练自由的强力基线MultiFloor3D，现有单层方法在该任务上表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D布局估计模型多在合成单房间或单层数据上训练，无法原生处理多层建筑，且拆分楼层会丢失连接楼层（如楼梯）的全局空间上下文。需要支持整栋建筑的布局估计。

Method: 构建真实世界多层建筑数据集HouseLayout3D；提出MultiFloor3D方法，基于现有场景理解模块（无训练）来推断多层布局；在新基准与已有数据集上评估对比。

Result: HouseLayout3D为多层复杂真实环境提供基准；MultiFloor3D在该基准及之前的数据集上均超过现有3D布局模型，强调了对多层布局研究的必要性。

Conclusion: 本论文提出了支持整栋建筑三维布局估计的新基准HouseLayout3D，并展示了一个无训练基线MultiFloor3D，表明现有单层模型不足以处理多层复杂建筑。

Abstract: Current 3D layout estimation models are primarily trained on synthetic datasets containing simple single room or single floor environments. As a consequence, they cannot natively handle large multi floor buildings and require scenes to be split into individual floors before processing, which removes global spatial context that is essential for reasoning about structures such as staircases that connect multiple levels. In this work, we introduce HouseLayout3D, a real world benchmark designed to support progress toward full building scale layout estimation, including multiple floors and architecturally intricate spaces. We also present MultiFloor3D, a simple training free baseline that leverages recent scene understanding methods and already outperforms existing 3D layout estimation models on both our benchmark and prior datasets, highlighting the need for further research in this direction. Data and code are available at: https://houselayout3d.github.io.

</details>


### [39] [ClusterStyle: Modeling Intra-Style Diversity with Prototypical Clustering for Stylized Motion Generation](https://arxiv.org/abs/2512.02453)
*Kerui Chen,Jianrong Zhang,Ming Li,Zhonglong Zheng,Hehe Fan*

Main category: cs.CV

TL;DR: 用原型簇构建全局/局部风格嵌入并通过SMA注入到预训练模型，显著提升风格化动作生成的多样性与质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉单一风格下的内部多样性，导致生成动作风格单一或无法反映风格的时序变化，需设计结构化表示来刻画风格多样性。

Method: 提出基于聚类的框架，使用一组原型表示同一风格类别内的多样化模式，构建全局与局部两类风格嵌入空间，并通过与非学习原型锚点对齐进行优化；同时引入Stylistic Modulation Adapter(SMA)将风格特征注入到预训练的文本到动作生成模型中。

Result: 在大规模实验中，ClusterStyle在风格化动作生成与动作风格迁移任务上优于现有最先进方法，表明原型化与双层嵌入策略能有效提升多样性与风格一致性。

Conclusion: ClusterStyle通过原型簇建模风格内部多样性，并用全局与局部双重结构化风格嵌入与不可学习的原型锚点对齐，从而改进了风格化动作生成与风格迁移效果。

Abstract: Existing stylized motion generation models have shown their remarkable ability to understand specific style information from the style motion, and insert it into the content motion. However, capturing intra-style diversity, where a single style should correspond to diverse motion variations, remains a significant challenge. In this paper, we propose a clustering-based framework, ClusterStyle, to address this limitation. Instead of learning an unstructured embedding from each style motion, we leverage a set of prototypes to effectively model diverse style patterns across motions belonging to the same style category. We consider two types of style diversity: global-level diversity among style motions of the same category, and local-level diversity within the temporal dynamics of motion sequences. These components jointly shape two structured style embedding spaces, i.e., global and local, optimized via alignment with non-learnable prototype anchors. Furthermore, we augment the pretrained text-to-motion generation model with the Stylistic Modulation Adapter (SMA) to integrate the style features. Extensive experiments demonstrate that our approach outperforms existing state-of-the-art models in stylized motion generation and motion style transfer.

</details>


### [40] [See, Think, Learn: A Self-Taught Multimodal Reasoner](https://arxiv.org/abs/2512.02456)
*Sourabh Sharma,Sonam Gupta,Sadbhawna*

Main category: cs.CV

TL;DR: STL通过结构化“先看后想”自训练和负向理由增强，低成本同时提升视觉感知与推理，显著提高VLM多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在多模态推理上受限于感知或推理能力，且高质量CoT数据昂贵或忽视视觉感知，需一种低成本同时提升感知与推理的方法。

Method: 提出See-Think-Learn框架，使用“先看后想”结构化模板：先将图像属性以文本形式抽取（See），再基于这些属性进行推理（Think）；在自训练循环中模型生成并学习自己的结构化理由；同时加入负向理由训练以提高判别能力。

Result: 在多领域实验中，STL在准确率和鲁棒性上均优于直接仅用答案或自生成推理训练的基线；定性分析显示其生成的理由质量高。

Conclusion: STL通过自训练生成结构化推理路径，同时提升视觉感知与多模态推理能力，是一种高性价比方法，能显著优于仅训练答案或自生成推理的基线。

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language understanding. However, effective multimodal reasoning requires both accurate perception and robust reasoning, and weakness in either limits the performance of VLMs. Prior efforts to enhance reasoning often depend on high-quality chain-of-thought (CoT) data, obtained via labor-intensive human annotations, costly proprietary models, or self-training methods that overlook perception. To address these limitations, we propose a simple yet effective self-training framework called See-Think-Learn (STL). At its core, STL introduces a structured reasoning template that encourages the model to see before thinking, first extracting visual attributes in textual form, then using them to guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn from its own structured rationales in a self-training loop. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently outperforms baselines trained directly only on answers or self-generated reasoning, while qualitative analysis confirms the high quality of its rationales. STL thus provides a cost-effective solution to enhance multimodal reasoning ability of VLMs.

</details>


### [41] [Does Hearing Help Seeing? Investigating Audio-Video Joint Denoising for Video Generation](https://arxiv.org/abs/2512.02457)
*Jianzong Wu,Hao Lian,Dachao Hao,Ye Tian,Qingyu Shi,Biaolong Chen,Hao Jiang*

Main category: cs.CV

TL;DR: AVFullDiT通过音频-视频联合去噪训练提升了视频生成质量，尤其在物体碰撞等动态场景中表现更好，表明跨模态联合训练有助于更物理化的世界建模。


<details>
  <summary>Details</summary>
Motivation: 探索音频-视频联合训练是否能在仅优化视频质量的情况下仍带来视频生成改进——即联合训练是否带来超越同步性的好处。

Method: 提出参数高效的AVFullDiT架构，结合预训练的文本到视频（T2V）和文本到音频（T2A）模块进行联合去噪；在相同设置下训练T2AV模型与仅T2V模型进行对比。

Result: 系统性证据表明联合去噪带来一致改进，尤其在包含大幅和接触运动的挑战性子集上；作者假设预测音频作为特权信号，促使模型内部学习视觉事件与其声学后果的因果关系，从而正则化视频动态。

Conclusion: 本文结论是：在音频-视频联合去噪训练下，即便只关心视频质量，视频生成也能得到显著提升，尤其在大尺度运动和物体接触场景。

Abstract: Recent audio-video generative systems suggest that coupling modalities benefits not only audio-video synchrony but also the video modality itself. We pose a fundamental question: Does audio-video joint denoising training improve video generation, even when we only care about video quality? To study this, we introduce a parameter-efficient Audio-Video Full DiT (AVFullDiT) architecture that leverages pre-trained text-to-video (T2V) and text-to-audio (T2A) modules for joint denoising. We train (i) a T2AV model with AVFullDiT and (ii) a T2V-only counterpart under identical settings. Our results provide the first systematic evidence that audio-video joint denoising can deliver more than synchrony. We observe consistent improvements on challenging subsets featuring large and object contact motions. We hypothesize that predicting audio acts as a privileged signal, encouraging the model to internalize causal relationships between visual events and their acoustic consequences (e.g., collision $\times$ impact sound), which in turn regularizes video dynamics. Our findings suggest that cross-modal co-training is a promising approach to developing stronger, more physically grounded world models. Code and dataset will be made publicly available.

</details>


### [42] [Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration](https://arxiv.org/abs/2512.02458)
*Zhongyi Cai,Yi Du,Chen Wang,Yu Kong*

Main category: cs.CV

TL;DR: 提出SEER-Bench与3DSPMR，首次将几何信息融入MLLM空间记忆用于序列化室内任务，显著提高了EQA与EMN任务性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中智能体常遇到连续子任务且需复用之前的空间记忆，且部分子任务可能不可行，现有单任务研究未充分考虑如何跨任务复用探索信息与几何约束。

Method: 构建SEER-Bench评测框架包含序列化EQA与EMN任务；提出3DSPMR方法，将已探索区域的关系（relational）、视觉特征和几何信息编码入3D空间记忆，并用于增强MLLM进行推理与探索决策。

Result: 在SEER-Bench的序列化EQA与EMN任务上，3DSPMR相比不利用几何信息或无空间记忆的基线方法取得了显著性能提升，验证了几何信息对MLLM空间理解的贡献。

Conclusion: 本文提出并验证了在连续任务下复用空间记忆以支持后续推理与探索的必要性与有效性，3DSPMR通过结合关系、视觉与几何线索提升了多模态大模型在序列化室内任务上的表现，显著优于基线。

Abstract: Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.

</details>


### [43] [TGDD: Trajectory Guided Dataset Distillation with Balanced Distribution](https://arxiv.org/abs/2512.02469)
*Fengli Ran,Xiao Pu,Bo Liu,Xiuli Bi,Bin Xiao*

Main category: cs.CV

TL;DR: TGDD通过沿模型训练轨迹动态对齐与分布约束，提高合成数据的语义多样性与代表性，显著提升数据蒸馏性能，无额外优化开销。


<details>
  <summary>Details</summary>
Motivation: 现有DM方法忽视训练过程中表征的演变，导致合成数据表达能力受限和下游性能下降。

Method: 将分布匹配重构为沿训练轨迹的动态对齐：在各训练阶段对齐合成集与原始集的特征分布；引入分布约束正则化以减少类间重叠；无需额外优化开销。

Result: 在十个数据集上实验，TGDD在高分辨率基准上取得约5.0%准确率提升，整体实现性能与效率的良好平衡，达到SOTA。

Conclusion: TGDD通过沿模型训练轨迹的动态对齐，使合成数据同时保留语义多样性与代表性，从而显著提升数据蒸馏的下游性能，达到SOTA水平。

Abstract: Dataset distillation compresses large datasets into compact synthetic ones to reduce storage and computational costs. Among various approaches, distribution matching (DM)-based methods have attracted attention for their high efficiency. However, they often overlook the evolution of feature representations during training, which limits the expressiveness of synthetic data and weakens downstream performance. To address this issue, we propose Trajectory Guided Dataset Distillation (TGDD), which reformulates distribution matching as a dynamic alignment process along the model's training trajectory. At each training stage, TGDD captures evolving semantics by aligning the feature distribution between the synthetic and original dataset. Meanwhile, it introduces a distribution constraint regularization to reduce class overlap. This design helps synthetic data preserve both semantic diversity and representativeness, improving performance in downstream tasks. Without additional optimization overhead, TGDD achieves a favorable balance between performance and efficiency. Experiments on ten datasets demonstrate that TGDD achieves state-of-the-art performance, notably a 5.0% accuracy gain on high-resolution benchmarks.

</details>


### [44] [WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling](https://arxiv.org/abs/2512.02473)
*Yuta Oshima,Yusuke Iwasawa,Masahiro Suzuki,Yutaka Matsuo,Hiroki Furuta*

Main category: cs.CV

TL;DR: WorldPack用轨迹打包+记忆检索构建压缩记忆，实现在短上下文下的高效长期视频生成，LoopNav基准上表现领先。


<details>
  <summary>Details</summary>
Motivation: 现有视频世界模型在长时、空间一致性上受限，长上下文计算代价高；因此需要更高效的记忆表征以支持长期生成。

Method: 提出Compressed Memory，包括轨迹打包（trajectory packing）提高上下文效率）与记忆检索（memory retrieval）用于回放一致性和长时空间推理。

Result: 在LoopNav（Minecraft）基准上，WorldPack明显优于强基线，在长期一致性、清晰度和保真度方面取得显著提升。

Conclusion: WorldPack通过引入压缩记忆（轨迹打包+记忆检索）在短上下文下仍能实现长期视频世界建模，提高空间一致性与生成质量。

Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.

</details>


### [45] [G-SHARP: Gaussian Surgical Hardware Accelerated Real-time Pipeline](https://arxiv.org/abs/2512.02482)
*Vishwesh Nath,Javier G. Tejero,Ruilong Li,Filippo Filicori,Mahdi Azizian,Sean D. Huver*

Main category: cs.CV

TL;DR: G-SHARP 是首个原生基于 GSplat 的商业兼容实时外科场景重建系统，兼顾高保真、遮挡鲁棒性与边缘硬件实时部署，适用于微创手术室。


<details>
  <summary>Details</summary>
Motivation: 现有实时内窥镜重建方法依赖非商业许可实现，限制临床部署。需要一个既满足实时性与高精度又具商业许可兼容性的解决方案，以便在手术室实际使用。

Method: 基于 GSplat（Apache-2.0）可微高斯光栅化器原生实现高斯点云散射法；引入有原则的变形建模和鲁棒遮挡处理；在 EndoNeRF pulling 基准上进行评估；提供 Holoscan SDK 应用，将系统部署到 NVIDIA IGX Orin 与 Thor 边缘硬件，实现实时可视化。

Result: 在 EndoNeRF pulling 基准上实现了最先进的重建质量，兼顾速度与精度，适用于术中使用；成功在 NVIDA 边缘硬件上实时部署。

Conclusion: G-SHARP 提出了一种基于商业兼容的实时手术场景重建框架，能在微创手术中对可变形组织进行快速准确的三维建模，具备可部署性和高保真重建能力。

Abstract: We propose G-SHARP, a commercially compatible, real-time surgical scene reconstruction framework designed for minimally invasive procedures that require fast and accurate 3D modeling of deformable tissue. While recent Gaussian splatting approaches have advanced real-time endoscopic reconstruction, existing implementations often depend on non-commercial derivatives, limiting deployability. G-SHARP overcomes these constraints by being the first surgical pipeline built natively on the GSplat (Apache-2.0) differentiable Gaussian rasterizer, enabling principled deformation modeling, robust occlusion handling, and high-fidelity reconstructions on the EndoNeRF pulling benchmark. Our results demonstrate state-of-the-art reconstruction quality with strong speed-accuracy trade-offs suitable for intra-operative use. Finally, we provide a Holoscan SDK application that deploys G-SHARP on NVIDIA IGX Orin and Thor edge hardware, enabling real-time surgical visualization in practical operating-room settings.

</details>


### [46] [UCAgents: Unidirectional Convergence for Visual Evidence Anchored Multi-Agent Medical Decision-Making](https://arxiv.org/abs/2512.02485)
*Qianhan Feng,Zhongzhen Huang,Yakun Zhu,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: UCAgents通过结构化、单向证据审计约束多代理讨论，兼顾诊断准确性与计算效率，有效抑制语言漂移并强化视觉证据利用。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在医学诊断上会出现推理与图像证据脱节；多代理开放式讨论虽能减偏但增长文本噪声和计算成本，且未充分锚定视觉证据。

Method: 提出分层多代理框架UCAgents：禁止立场改变、限制交互为针对性证据核验、引入单轮询问讨论以发现视文错配风险，并用信息论形式化视文双重噪声瓶颈。

Result: 在四个医疗VQA基准上取得领先性能：PathVQA准确率71.3%（+6.0%）且令牌成本降低87.7%，显示在提高准确性的同时显著节省计算资源并减少文本干扰。

Conclusion: UCAgents通过限制代理间交互并强制按证据审计的单向收敛，减少语言漂移并提升与图像证据的一致性，从而提高医疗视觉问答的诊断可靠性和计算效率。

Abstract: Vision-Language Models (VLMs) show promise in medical diagnosis, yet suffer from reasoning detachment, where linguistically fluent explanations drift from verifiable image evidence, undermining clinical trust. Recent multi-agent frameworks simulate Multidisciplinary Team (MDT) debates to mitigate single-model bias, but open-ended discussions amplify textual noise and computational cost while failing to anchor reasoning to visual evidence, the cornerstone of medical decision-making. We propose UCAgents, a hierarchical multi-agent framework enforcing unidirectional convergence through structured evidence auditing. Inspired by clinical workflows, UCAgents forbids position changes and limits agent interactions to targeted evidence verification, suppressing rhetorical drift while amplifying visual signal extraction. In UCAgents, a one-round inquiry discussion is introduced to uncover potential risks of visual-textual misalignment. This design jointly constrains visual ambiguity and textual noise, a dual-noise bottleneck that we formalize via information theory. Extensive experiments on four medical VQA benchmarks show UCAgents achieves superior accuracy (71.3% on PathVQA, +6.0% over state-of-the-art) with 87.7% lower token cost, the evaluation results further confirm that UCAgents strikes a balance between uncovering more visual evidence and avoiding confusing textual interference. These results demonstrate that UCAgents exhibits both diagnostic reliability and computational efficiency critical for real-world clinical deployment. Code is available at https://github.com/fqhank/UCAgents.

</details>


### [47] [Masking Matters: Unlocking the Spatial Reasoning Capabilities of LLMs for 3D Scene-Language Understanding](https://arxiv.org/abs/2512.02487)
*Yerim Jeon,Miso Lee,WonJun Moon,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 用空间自适应与指令感知的注意力掩码代替因果掩码，简单有效提升3D场景-语言任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有将LLM用于3D推理的方法沿用语言建模的因果掩码，导致序列偏置与对象对指令关注受限，妨碍任务特定推理能力。

Method: 提出两种掩码：基于几何自适应掩码（按空间密度约束对象间注意力）和指令感知掩码（允许对象直接访问指令上下文），无需改动模型结构或增加参数，在解码器处替换注意力掩码实现。

Result: 在多基准和多个LLM骨干上，3D-SLIM带来显著性能提升，证明解码器掩码设计对3D多模态推理至关重要。

Conclusion: 3D-SLIM通过替换因果注意力掩码为自适应的空间结构掩码，解决了3D场景中对象无序性与指令上下文访问受限的问题。

Abstract: Recent advances in 3D scene-language understanding have leveraged Large Language Models (LLMs) for 3D reasoning by transferring their general reasoning ability to 3D multi-modal contexts. However, existing methods typically adopt standard decoders from language modeling, which rely on a causal attention mask. This design introduces two fundamental conflicts in 3D scene understanding: sequential bias among order-agnostic 3D objects and restricted object-instruction attention, hindering task-specific reasoning. To overcome these limitations, we propose 3D Spatial Language Instruction Mask (3D-SLIM), an effective masking strategy that replaces the causal mask with an adaptive attention mask tailored to the spatial structure of 3D scenes. Our 3D-SLIM introduces two key components: a Geometry-adaptive Mask that constrains attention based on spatial density rather than token order, and an Instruction-aware Mask that enables object tokens to directly access instruction context. This design allows the model to process objects based on their spatial relationships while being guided by the user's task. 3D-SLIM is simple, requires no architectural modifications, and adds no extra parameters, yet it yields substantial performance improvements across diverse 3D scene-language tasks. Extensive experiments across multiple benchmarks and LLM baselines validate its effectiveness and underscore the critical role of decoder design in 3D multi-modal reasoning.

</details>


### [48] [YingVideo-MV: Music-Driven Multi-Stage Video Generation](https://arxiv.org/abs/2512.02492)
*Jiahui Chen,Weida Wang,Runhua Shi,Huan Yang,Chaofan Ding,Zihao Chen*

Main category: cs.CV

TL;DR: 提出YingVideo-MV：首个面向音乐驱动长视频的级联生成框架，包含MV-Director、摄像机适配器和时间感知动态窗口等创新，配套大规模数据集，提升了音乐表演视频的连贯性、摄像机控制与同步精度。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的长视频生成在音画同步和身份一致性方面取得进展，但对含摄像机运动的音乐表演视频尚未充分探索，且缺乏对摄像机运动的显式控制与长序列连贯性建模。

Method: 框架包括：音频语义分析、可解释的镜头规划模块MV-Director、时序感知的扩散Transformer、摄像机适配器（将相机位姿嵌入潜在噪声）以及基于音频嵌入自适应调整去噪范围的时间感知动态窗口策略；同时构建大规模Music-in-the-Wild数据集并采用级联生成流程。

Result: 在综合基准测试中，YingVideo-MV在生成连贯、富表情的音乐视频方面表现优异，实现了精确的音乐-动作-摄像机同步，并展示了多样、高质量的生成结果。

Conclusion: YingVideo-MV提出了首个用于音乐驱动长视频生成的级联框架，能自动从音频合成包含摄像机运动的高质量音乐表演视频，并通过数据集和模块设计提高连贯性与可控性。

Abstract: While diffusion model for audio-driven avatar video generation have achieved notable process in synthesizing long sequences with natural audio-visual synchronization and identity consistency, the generation of music-performance videos with camera motions remains largely unexplored. We present YingVideo-MV, the first cascaded framework for music-driven long-video generation. Our approach integrates audio semantic analysis, an interpretable shot planning module (MV-Director), temporal-aware diffusion Transformer architectures, and long-sequence consistency modeling to enable automatic synthesis of high-quality music performance videos from audio signals. We construct a large-scale Music-in-the-Wild Dataset by collecting web data to support the achievement of diverse, high-quality results. Observing that existing long-video generation methods lack explicit camera motion control, we introduce a camera adapter module that embeds camera poses into latent noise. To enhance continulity between clips during long-sequence inference, we further propose a time-aware dynamic window range strategy that adaptively adjust denoising ranges based on audio embedding. Comprehensive benchmark tests demonstrate that YingVideo-MV achieves outstanding performance in generating coherent and expressive music videos, and enables precise music-motion-camera synchronization. More videos are available in our project page: https://giantailab.github.io/YingVideo-MV/ .

</details>


### [49] [Attention-guided reference point shifting for Gaussian-mixture-based partial point set registration](https://arxiv.org/abs/2512.02496)
*Mizuki Kikkawa,Tatsuya Yatagawa,Yutaka Ohtake,Hiromasa Suzuki*

Main category: cs.CV

TL;DR: 本文发现并分析了基于深度学习+GMM的部分到部分点云配准在平移/旋转不变性方面的问题，提出基于注意力的参考点位移（ARPS）层来获得变换不变特征，并把它融入DeepGMR和UGMMReg，显著提升配准性能，优于现有注意力/Transformer方法。


<details>
  <summary>Details</summary>
Motivation: 动机是揭示并解决基于深度学习和高斯混合模型（GMM）的点云配准方法在部分到部分配准中，由于特征不具变换不变性而导致的性能下降与理论缺陷，提供可解释且鲁棒的改进方案。

Method: 作者分析了特征向量在输入点集平移与旋转下的不变性问题，理论推导并实验验证DeepGMR在部分到部分配准上的局限性；提出ARPS层，利用注意力模块寻找共同参考点而非重叠区域；将ARPS集成到DeepGMR与UGMMReg并与其他基于注意力/Transformer的方法进行对比评估。

Result: 实验结果显示：ARPS能稳健找到两部分点云的共同参考点并生成变换不变特征；将ARPS加入后，DeepGMR与UGMMReg在部分到部分配准任务上性能显著提高，且优于使用注意力模块或Transformer直接提取重叠区域的方法。

Conclusion: 本文结论为：现有基于深度学习与GMM的部分到部分点云配准方法在平移与旋转不变性方面存在理论与实践问题，且通过引入ARPS层（基于注意力的参考点位移）可稳定识别两组部分点云的共同参考点，获得变换不变特征，从而显著提升DeepGMR与UGMMReg在部分到部分配准任务上的性能，优于此前基于注意力或Transformer以提取重叠区域的方法。

Abstract: This study investigates the impact of the invariance of feature vectors for partial-to-partial point set registration under translation and rotation of input point sets, particularly in the realm of techniques based on deep learning and Gaussian mixture models (GMMs). We reveal both theoretical and practical problems associated with such deep-learning-based registration methods using GMMs, with a particular focus on the limitations of DeepGMR, a pioneering study in this line, to the partial-to-partial point set registration. Our primary goal is to uncover the causes behind such methods and propose a comprehensible solution for that. To address this, we introduce an attention-based reference point shifting (ARPS) layer, which robustly identifies a common reference point of two partial point sets, thereby acquiring transformation-invariant features. The ARPS layer employs a well-studied attention module to find a common reference point rather than the overlap region. Owing to this, it significantly enhances the performance of DeepGMR and its recent variant, UGMMReg. Furthermore, these extension models outperform even prior deep learning methods using attention blocks and Transformer to extract the overlap region or common reference points. We believe these findings provide deeper insights into registration methods using deep learning and GMMs.

</details>


### [50] [A Large Scale Benchmark for Test Time Adaptation Methods in Medical Image Segmentation](https://arxiv.org/abs/2512.02497)
*Wenjing Yu,Shuo Jiang,Yifei Chen,Shuo Chang,Yuanhan Wang,Beining Wu,Jie Dong,Mingxuan Liu,Shenghao Zhu,Feiwei Qin,Changmiao Wang,Qiyuan Tian*

Main category: cs.CV

TL;DR: MedSeg-TTA提供了统一且跨模态的TTA基准，揭示不同适配范式的优势与局限，为临床部署指明了方法选择的重要性，并公开了资源与排行榜。


<details>
  <summary>Details</summary>
Motivation: 当前TTA评估在模态覆盖、任务多样性与一致性上不足，难以系统比较不同方法在临床相关移位（中心/设备差异、外观变化）下的可靠性与适用性。

Method: 构建了MedSeg-TTA基准：统一预处理、主干网络及测试协议，对20种代表性TTA方法在7种医学成像模态（MRI、CT、超声、病理、皮肤镜、OCT、胸片）上评估，覆盖输入级变换、特征级对齐、输出级正则化和先验估计四大范式；并提供标准数据集、实现与排行榜。

Result: 实验表明：输入级方法在轻度外观变化下更稳定；特征级与输出级方法在边界相关指标上更有优势；先验方法表现强烈依赖模态；若遇大幅跨中心/设备移位，若干方法性能显著下降。

Conclusion: 本文综述性基准显示，在医学影像分割的测试时自适应（TTA）中，没有单一范式能在所有模态和移位条件下始终占优；方法选择需结合移位类型与任务特征。

Abstract: Test time Adaptation is a promising approach for mitigating domain shift in medical image segmentation; however, current evaluations remain limited in terms of modality coverage, task diversity, and methodological consistency. We present MedSeg-TTA, a comprehensive benchmark that examines twenty representative adaptation methods across seven imaging modalities, including MRI, CT, ultrasound, pathology, dermoscopy, OCT, and chest X-ray, under fully unified data preprocessing, backbone configuration, and test time protocols. The benchmark encompasses four significant adaptation paradigms: Input-level Transformation, Feature-level Alignment, Output-level Regularization, and Prior Estimation, enabling the first systematic cross-modality comparison of their reliability and applicability. The results show that no single paradigm performs best in all conditions. Input-level methods are more stable under mild appearance shifts. Feature-level and Output-level methods offer greater advantages in boundary-related metrics, whereas prior-based methods exhibit strong modality dependence. Several methods degrade significantly under large inter-center and inter-device shifts, which highlights the importance of principled method selection for clinical deployment. MedSeg-TTA provides standardized datasets, validated implementations, and a public leaderboard, establishing a rigorous foundation for future research on robust, clinically reliable test-time adaptation. All source codes and open-source datasets are available at https://github.com/wenjing-gg/MedSeg-TTA.

</details>


### [51] [dots.ocr: Multilingual Document Layout Parsing in a Single Vision-Language Model](https://arxiv.org/abs/2512.02498)
*Yumeng Li,Guang Yang,Hao Liu,Bowen Wang,Colin Zhang*

Main category: cs.CV

TL;DR: dots.ocr是一种统一的视觉-语言模型，通过大规模合成多语言数据端到端联合学习布局、识别与关系理解，在多基准上显著领先，尤其在126语言的XDocParse上领先7.4点。


<details>
  <summary>Details</summary>
Motivation: 当前文档布局解析方法多为碎片化、多阶段流程，误差累积并且无法通过联合训练发挥任务间协同效应，亟需一个统一的端到端解决方案。

Method: 构建单一的视觉-语言模型，并借助可扩展的数据引擎合成大规模多语言语料进行联合训练，从而实现布局解析的三大任务协同优化。

Result: 在OmniDocBench上取得SOTA，并在新建的XDocParse（126种语言）上以比第二名高7.4点的优势建立了新的基线，展现出优秀的多语言能力。

Conclusion: dots.ocr通过统一端到端框架联合学习布局检测、文本识别和关系理解，克服了多阶段流水线的误差传播问题。

Abstract: Document Layout Parsing serves as a critical gateway for Artificial Intelligence (AI) to access and interpret the world's vast stores of structured knowledge. This process,which encompasses layout detection, text recognition, and relational understanding, is particularly crucial for empowering next-generation Vision-Language Models. Current methods, however, rely on fragmented, multi-stage pipelines that suffer from error propagation and fail to leverage the synergies of joint training. In this paper, we introduce dots.ocr, a single Vision-Language Model that, for the first time, demonstrates the advantages of jointly learning three core tasks within a unified, end-to-end framework. This is made possible by a highly scalable data engine that synthesizes a vast multilingual corpus, empowering the model to deliver robust performance across a wide array of tasks, encompassing diverse languages, layouts, and domains. The efficacy of our unified paradigm is validated by state-of-the-art performance on the comprehensive OmniDocBench. Furthermore, to catalyze research in global document intelligence, we introduce XDocParse, a challenging new benchmark spanning 126 languages. On this testbed, dots.ocr establishes a powerful new baseline, outperforming the next-best competitor by a remarkable +7.4 point margin and proving its unparalleled multilingual capabilities.

</details>


### [52] [GeoDiT: A Diffusion-based Vision-Language Model for Geospatial Understanding](https://arxiv.org/abs/2512.02505)
*Jiaqi Liu,Ronghao Fu,Haoran Liu,Lang Sun,Bo Yang*

Main category: cs.CV

TL;DR: GeoDiT用并行的扩散细化替代自回归，以更符合地理数据的并行结构，从而提升了地理空间视觉语言任务的性能。


<details>
  <summary>Details</summary>
Motivation: 自回归模型强制对场景进行顺序叙述，与地理空间理解的并行性不符，导致在生成结构化和连贯输出时受限。

Method: 提出GeoDiT：首个面向地理空间领域的基于扩散的视觉-语言模型，采用自粗到细的并行细化生成策略，同时解析所有语义元素。

Result: 在图像描述、视觉定位和多目标检测等需要结构化输出的基准上，GeoDiT达到了新的最先进水平，显著优于自回归模型。

Conclusion: GeoDiT通过将地理空间生成重新表述为并行的细化过程，解决了自回归模型与地理空间理解并行性不一致的问题，并在结构化、面向对象任务上取得了更好表现。

Abstract: Autoregressive models are structurally misaligned with the inherently parallel nature of geospatial understanding, forcing a rigid sequential narrative onto scenes and fundamentally hindering the generation of structured and coherent outputs. We challenge this paradigm by reframing geospatial generation as a parallel refinement process, enabling a holistic, coarse-to-fine synthesis that resolves all semantic elements simultaneously. To operationalize this, we introduce GeoDiT, the first diffusion-based vision-language model tailored for the geospatial domain. Extensive experiments demonstrate that GeoDiT establishes a new state-of-the-art on benchmarks requiring structured, object-centric outputs. It achieves significant gains in image captioning, visual grounding, and multi-object detection, precisely the tasks where autoregressive models falter. Our work validates that aligning the generative process with the data's intrinsic structure is key to unlocking superior performance in complex geospatial analysis.

</details>


### [53] [Two-Stage Vision Transformer for Image Restoration: Colorization Pretraining + Residual Upsampling](https://arxiv.org/abs/2512.02512)
*Aditya Chaudhary,Prachet Dev Singh,Ankit Jha*

Main category: cs.CV

TL;DR: 论文用“彩色化自监督预训练 + SR微调”的两阶段方案改进了ViT在单图超分任务上的性能，DIV2K上达到PSNR 22.90 dB和SSIM 0.712，方法合理但需更全面对比与消融验证。


<details>
  <summary>Details</summary>
Motivation: 动机在于利用自监督预训练从大量未标注数据中学习更丰富、可迁移的特征，以提高在数据有限的超分场景中的表现，同时利用ViT的建模能力改进图像重建质量。

Method: 方法包括：1) 自监督预训练阶段：在彩色化任务上训练ViT以学习通用视觉表征；2) 微调阶段：在4x超分任务上训练，通过预测高频残差并加到双三次插值的低频基底上完成重建，简化残差学习。

Result: 在DIV2K上实验，4x超分任务得到SSIM=0.712、PSNR=22.90 dB，表明两阶段策略有效。作者还建议可通过更大模型或替代预训练任务进一步提升性能。

Conclusion: 该论文提出的两阶段训练（自监督上色预训练+有监督SR微调）对Vision Transformer在单图超分任务上有积极作用，但仍有若干局限与不确定性需要审慎评估。

Abstract: In computer vision, Single Image Super-Resolution (SISR) is still a difficult problem. We present ViT-SR, a new technique to improve the performance of a Vision Transformer (ViT) employing a two-stage training strategy. In our method, the model learns rich, generalizable visual representations from the data itself through a self-supervised pretraining phase on a colourization task. The pre-trained model is then adjusted for 4x super-resolution. By predicting the addition of a high-frequency residual image to an initial bicubic interpolation, this design simplifies residual learning. ViT-SR, trained and evaluated on the DIV2K benchmark dataset, achieves an impressive SSIM of 0.712 and PSNR of 22.90 dB. These results demonstrate the efficacy of our two-stage approach and highlight the potential of self-supervised pre-training for complex image restoration tasks. Further improvements may be possible with larger ViT architectures or alternative pretext tasks.

</details>


### [54] [SkyMoE: A Vision-Language Foundation Model for Enhancing Geospatial Interpretation with Mixture of Experts](https://arxiv.org/abs/2512.02517)
*Jiaqi Liu,Ronghao Fu,Lang Sun,Haoran Liu,Xiao Yang,Weipeng Zhang,Xu Na,Zhuoran Duan,Bo Yang*

Main category: cs.CV

TL;DR: 提出 SkyMoE：带任务与粒度感知路由的 MoE VLM，并用语境解耦增强与新基准 MGRS-Bench 验证，在 21 个数据集上实现遥感多任务、多粒度的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有通用 VLM 对遥感任务表现不足，且统一建模难以兼顾局部细节与全局语境，因而需要一种能区分任务类型与解释粒度的可扩展模型。

Method: 设计了自适应路由器生成任务与粒度指令，分配给专门的 LLM 专家；引入语境解耦的数据增强，构造局部-全局对比学习以促使专家学习特定粒度表征；构建 MGRS-Bench 作为综合评测基准并在 21 个公开数据集上进行大规模实验验证。

Result: 在包含多任务与多粒度的 MGRS-Bench 和 21 个公开数据集上，SkyMoE 达到或超过现有最优方法，验证了其在适配性、可扩展性以及多粒度理解能力方面的优势。

Conclusion: SkyMoE 提出了一种针对遥感任务的 Mixture-of-Experts VLM，通过任务与粒度感知的路由器和语境解耦增强，使专家模块专注不同尺度的表征，从而在多任务、多粒度遥感解释上取得领先性能。

Abstract: The emergence of large vision-language models (VLMs) has significantly enhanced the efficiency and flexibility of geospatial interpretation. However, general-purpose VLMs remain suboptimal for remote sensing (RS) tasks. Existing geospatial VLMs typically adopt a unified modeling strategy and struggle to differentiate between task types and interpretation granularities, limiting their ability to balance local detail perception and global contextual understanding. In this paper, we present SkyMoE, a Mixture-of-Experts (MoE) vision-language model tailored for multimodal, multi-task RS interpretation. SkyMoE employs an adaptive router that generates task- and granularity-aware routing instructions, enabling specialized large language model experts to handle diverse sub-tasks. To further promote expert decoupling and granularity sensitivity, we introduce a context-disentangled augmentation strategy that creates contrastive pairs between local and global features, guiding experts toward level-specific representation learning. We also construct MGRS-Bench, a comprehensive benchmark covering multiple RS interpretation tasks and granularity levels, to evaluate generalization in complex scenarios. Extensive experiments on 21 public datasets demonstrate that SkyMoE achieves state-of-the-art performance across tasks, validating its adaptability, scalability, and superior multi-granularity understanding in remote sensing.

</details>


### [55] [On the Problem of Consistent Anomalies in Zero-Shot Anomaly Detection](https://arxiv.org/abs/2512.02520)
*Tai Le-Gia*

Main category: cs.CV

TL;DR: 本论文针对“重复性一致异常”问题，从理论（相似性缩放、邻居衰竭）出发，提出图谱过滤CoDeGraph，扩展到无训练3D MRI分割，并用伪掩码连接提示驱动V-L模型，实现真正的零样本异常检测与分割。


<details>
  <summary>Details</summary>
Motivation: 零样本AC/AS在工业与医疗中需求高，但在高度相似对象场景下，重复性一致异常导致基于距离的方法失效，需要理论解释与算法修正。

Method: 分析预训练ViT的patch表示，识别相似性缩放与邻居衰竭现象；基于此构建多阶段图、社区检测与结构化精化的CoDeGraph；提出一种无训练的体积token化策略用于MRI；利用CoDeGraph伪掩码监督提示驱动的视觉-语言模型。

Result: 提出的CoDeGraph在抑制一致性异常上有效，扩展方法实现了无需3D训练样本的体积分割，并且可生成伪掩码用于提升文本提示方法的分割性能（论文声称的实验验证）。

Conclusion: 本文解决了零样本异常分类与分割中由一致性异常引起的系统性偏差，提出CoDeGraph框架并扩展至3D医疗影像与视觉-语言提示监督，提供理论分析与实用算法。

Abstract: Zero-shot anomaly classification and segmentation (AC/AS) aim to detect anomalous samples and regions without any training data, a capability increasingly crucial in industrial inspection and medical imaging. This dissertation aims to investigate the core challenges of zero-shot AC/AS and presents principled solutions rooted in theory and algorithmic design.
  We first formalize the problem of consistent anomalies, a failure mode in which recurring similar anomalies systematically bias distance-based methods. By analyzing the statistical and geometric behavior of patch representations from pre-trained Vision Transformers, we identify two key phenomena - similarity scaling and neighbor-burnout - that describe how relationships among normal patches change with and without consistent anomalies in settings characterized by highly similar objects.
  We then introduce CoDeGraph, a graph-based framework for filtering consistent anomalies built on the similarity scaling and neighbor-burnout phenomena. Through multi-stage graph construction, community detection, and structured refinement, CoDeGraph effectively suppresses the influence of consistent anomalies.
  Next, we extend this framework to 3D medical imaging by proposing a training-free, computationally efficient volumetric tokenization strategy for MRI data. This enables a genuinely zero-shot 3D anomaly detection pipeline and shows that volumetric anomaly segmentation is achievable without any 3D training samples.
  Finally, we bridge batch-based and text-based zero-shot methods by demonstrating that CoDeGraph-derived pseudo-masks can supervise prompt-driven vision-language models. Together, this dissertation provides theoretical understanding and practical solutions for the zero-shot AC/AS problem.

</details>


### [56] [WeMMU: Enhanced Bridging of Vision-Language Models and Diffusion Models via Noisy Query Tokens](https://arxiv.org/abs/2512.02536)
*Jian Yang,Dacheng Yin,Xiaoxuan He,Yong Li,Fengyun Rao,Jing Lyu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出Noisy Query Tokens在VLM与扩散模型间学习分布式表示，并加VAE线性投影分支以恢复细节，解决固定查询令牌的泛化崩溃，提升连续学习稳定性。


<details>
  <summary>Details</summary>
Motivation: 固定数量的可学习查询令牌虽然高效，但在面向远离预训练任务的新任务时会发生泛化崩溃，需一种更具可扩展性和连续学习能力的连接机制。

Method: 在端到端优化框架中引入带噪声的可学习查询令牌以形成分布式表示，连接预训练VLM与扩散模型；并增设VAE分支与线性投影以恢复细粒度图像信息。

Result: Proposes Noisy Query Tokens to bridge VLMs and Diffusion Models, plus VAE branch with linear projection; improves continual learning and prevents task generalization collapse.

Conclusion: Noisy Query Tokens和VAE分支能缓解任务泛化崩溃，支持在多样任务上的稳定连续学习并恢复图像细节。

Abstract: Recent progress in multimodal large language models (MLLMs) has highlighted the challenge of efficiently bridging pre-trained Vision-Language Models (VLMs) with Diffusion Models. While methods using a fixed number of learnable query tokens offer computational efficiency, they suffer from task generalization collapse, failing to adapt to new tasks that are distant from their pre-training tasks. To overcome this, we propose Noisy Query Tokens, which learn a distributed representation space between the VLM and Diffusion Model via end-to-end optimization, enhancing continual learning. Additionally, we introduce a VAE branch with linear projection to recover fine-grained image details. Experimental results confirm our approach mitigates generalization collapse and enables stable continual learning across diverse tasks.

</details>


### [57] [AVGGT: Rethinking Global Attention for Accelerating VGGT](https://arxiv.org/abs/2512.02541)
*Xianbing Sun,Zhikai Zhu,Zhengyu Lou,Bo Yang,Jinyang Tang,Liqing Zhang,He Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: Analyze global attention in multi-view 3D models, find early/mid/late layer roles, and propose a training-free two-step acceleration (frame attention replacement + K/V subsampling) that yields large speedups (8–10×) with maintained or improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Global self-attention in recent multi-view 3D models gives strong performance but is computationally expensive. Sparse-attention alternatives provide partial gains but lack systematic understanding of global attention's role in multi-view reasoning.

Method: Empirical analysis of attention behavior in VGGT and π^3 to identify role of layers; propose a training-free two-step acceleration: (1) replace early global layers with frame attention, (2) subsample K/V tokens for global attention with diagonal preservation and mean-fill. Implemented on VGGT and π^3, evaluated on pose and point-map benchmarks.

Result: Achieves up to 8–10× inference speedup while matching or slightly improving original model accuracy. Robust in very dense multi-view scenarios where prior sparse-attention baselines fail.

Conclusion: The paper concludes that global self-attention in multi-view models has distinct roles across layers, enabling safe replacement of early global layers with cheaper frame attention and subsampling K/V in later global layers without loss of accuracy. This yields large inference speedups while preserving or improving performance, especially in dense multi-view settings.

Abstract: Since DUSt3R, models such as VGGT and $π^3$ have shown strong multi-view 3D performance, but their heavy reliance on global self-attention results in high computational cost. Existing sparse-attention variants offer partial speedups, yet lack a systematic analysis of how global attention contributes to multi-view reasoning. In this paper, we first conduct an in-depth investigation of the global attention modules in VGGT and $π^3$ to better understand their roles. Our analysis reveals a clear division of roles in the alternating global-frame architecture: early global layers do not form meaningful correspondences, middle layers perform cross-view alignment, and last layers provide only minor refinements. Guided by these findings, we propose a training-free two-step acceleration scheme: (1) converting early global layers into frame attention, and (2) subsampling global attention by subsampling K/V over patch tokens with diagonal preservation and a mean-fill component. We instantiate this strategy on VGGT and $π^3$ and evaluate across standard pose and point-map benchmarks. Our method achieves up to $8$-$10\times$ speedup in inference time while matching or slightly improving the accuracy of the original models, and remains robust even in extremely dense multi-view settings where prior sparse-attention baselines fail.

</details>


### [58] [OmniPerson: Unified Identity-Preserving Pedestrian Generation](https://arxiv.org/abs/2512.02554)
*Changxiao Ma,Chao Yuan,Xincheng Shi,Yuzhuo Ma,Yongfei Zhang,Longkun Zhou,Yujia Zhang,Shangze Li,Yifan Xu*

Main category: cs.CV

TL;DR: OmniPerson是首个面向可见光/红外ReID的统一身份保持行人生成系统，结合Multi-Refer Fuser和大规模PersonSyn数据集，实现高可控、多模态的高保真行人生成，并能有效提升ReID模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人生成用于数据增强的方法在身份一致性和可控性上不足，限制了合成数据提升ReID性能的能力；需要一种能在多视角、多模态下生成高保真且身份一致行人的统一方法，并配套高质量训练/评估数据集。

Method: 构建统一生成模型OmniPerson，支持多模态（RGB/IR）图像/视频生成，输入可包含任意数量参考图像、两种人体姿态和文本，并具备RGB到IR转换和图像超分能力；设计Multi-Refer Fuser模块用于从多视角参考图像提取并融合身份表征以保持身份一致性；自动化构建PersonSyn数据集，将公开的仅含ID的ReID基准数据转为带密集多模态标注的大规模数据集。

Result: 在行人生成任务上，OmniPerson在视觉保真度和身份一致性指标上均达到了SoTA；将其生成的数据并入现有数据集后，能稳定提升常见ReID模型的性能。作者计划开源代码、预训练模型及PersonSyn数据集。

Conclusion: OmniPerson提出了一个统一、可控且能保持身份一致性的行人生成流水线，显著提升了可见光/红外（RGB/IR）ReID数据增强的质量与效果，并通过PersonSyn数据集推动该方向研究。

Abstract: Person re-identification (ReID) suffers from a lack of large-scale high-quality training data due to challenges in data privacy and annotation costs. While previous approaches have explored pedestrian generation for data augmentation, they often fail to ensure identity consistency and suffer from insufficient controllability, thereby limiting their effectiveness in dataset augmentation. To address this, We introduce OmniPerson, the first unified identity-preserving pedestrian generation pipeline for visible/infrared image/video ReID tasks. Our contributions are threefold: 1) We proposed OmniPerson, a unified generation model, offering holistic and fine-grained control over all key pedestrian attributes. Supporting RGB/IR modality image/video generation with any number of reference images, two kinds of person poses, and text. Also including RGB-to-IR transfer and image super-resolution abilities.2) We designed Multi-Refer Fuser for robust identity preservation with any number of reference images as input, making OmniPerson could distill a unified identity from a set of multi-view reference images, ensuring our generated pedestrians achieve high-fidelity pedestrian generation.3) We introduce PersonSyn, the first large-scale dataset for multi-reference, controllable pedestrian generation, and present its automated curation pipeline which transforms public, ID-only ReID benchmarks into a richly annotated resource with the dense, multi-modal supervision required for this task. Experimental results demonstrate that OmniPerson achieves SoTA in pedestrian generation, excelling in both visual fidelity and identity consistency. Furthermore, augmenting existing datasets with our generated data consistently improves the performance of ReID models. We will open-source the full codebase, pretrained model, and the PersonSyn dataset.

</details>


### [59] [From Panel to Pixel: Zoom-In Vision-Language Pretraining from Biomedical Scientific Literature](https://arxiv.org/abs/2512.02566)
*Kun Yuan,Min Woo Sun,Zhen Chen,Alejandro Lozano,Xiangteng He,Shi Li,Nassir Navab,Xiaoxiao Sun,Nicolas Padoy,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: Panel2Patch从多面板图和文本中挖掘层级结构，生成图/面板/patch三级对齐并配合粒度感知预训，使得用更少数据获得更强的生物医学视觉-语言表示。


<details>
  <summary>Details</summary>
Motivation: 现有生物医学视觉-语言预训练通常将复杂的科学图表压缩为粗粒度的图级配对，舍弃临床上依赖的细粒度区域对应信息，限制了模型捕捉局部结构的能力。

Method: 设计Panel2Patch数据流水线：解析figure布局、分割panel、检测visual markers，构建figure/panel/patch三级对应的视觉-文本配对；提出粒度感知预训练策略，整合粗粒度描述到细粒度区域短语的多任务目标。

Result: 用少量文献图像通过Panel2Patch生成的多粒度监督，比以往管道在更少预训练数据下提供更有效的监督，显著提升模型表现。

Conclusion: Panel2Patch通过从科学文献中解析多面板布局、标记和文本，将图像-文本对细化为图、面板和局部patch三级对齐，从而保留局部语义并改善预训练效率。

Abstract: There is a growing interest in developing strong biomedical vision-language models. A popular approach to achieve robust representations is to use web-scale scientific data. However, current biomedical vision-language pretraining typically compresses rich scientific figures and text into coarse figure-level pairs, discarding the fine-grained correspondences that clinicians actually rely on when zooming into local structures. To tackle this issue, we introduce Panel2Patch, a novel data pipeline that mines hierarchical structure from existing biomedical scientific literature, i.e., multi-panel, marker-heavy figures and their surrounding text, and converts them into multi-granular supervision. Given scientific figures and captions, Panel2Patch parses layouts, panels, and visual markers, then constructs hierarchical aligned vision-language pairs at the figure, panel, and patch levels, preserving local semantics instead of treating each figure as a single data sample. Built on this hierarchical corpus, we develop a granularity-aware pretraining strategy that unifies heterogeneous objectives from coarse didactic descriptions to fine region-focused phrases. By applying Panel2Patch to only a small set of the literature figures, we extract far more effective supervision than prior pipelines, enabling substantially better performance with less pretraining data.

</details>


### [60] [Co-speech Gesture Video Generation via Motion-Based Graph Retrieval](https://arxiv.org/abs/2512.02576)
*Yafei Song,Peng Zhang,Bang Zhang*

Main category: cs.CV

TL;DR: 本文提出先用扩散模型生成手势，再基于运动相似性在运动图中检索并拼接，解决音频-手势多对多映射问题，提升同步性和自然性。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动图的检索方法依赖音频与动作之间的一对一映射或共享特征空间，但音频与手势存在多对多关系，导致检索效果受限。提出通过生成式模型隐式学习联合分布以缓解映射问题。

Method: 方法包括：1) 使用扩散模型学习音频与运动的联合分布以生成上下文相关的手势；2) 从输入音频提取低层和高层特征以增强扩散模型训练；3) 设计运动级检索算法，基于全局和局部运动相似性在运动图中选取路径；4) 对检索到的非连续节点段进行拼接，生成连续视频。

Result: 实验表明，所提方法在同步准确性和生成手势的自然性上均显著优于先前方法。

Conclusion: 该方法通过先用扩散模型生成手势运动，再在运动图中检索并拼接最合适的轨迹，实现了比之前方法更好地同步性和自然性。

Abstract: Synthesizing synchronized and natural co-speech gesture videos remains a formidable challenge. Recent approaches have leveraged motion graphs to harness the potential of existing video data. To retrieve an appropriate trajectory from the graph, previous methods either utilize the distance between features extracted from the input audio and those associated with the motions in the graph or embed both the input audio and motion into a shared feature space. However, these techniques may not be optimal due to the many-to-many mapping nature between audio and gestures, which cannot be adequately addressed by one-to-one mapping. To alleviate this limitation, we propose a novel framework that initially employs a diffusion model to generate gesture motions. The diffusion model implicitly learns the joint distribution of audio and motion, enabling the generation of contextually appropriate gestures from input audio sequences. Furthermore, our method extracts both low-level and high-level features from the input audio to enrich the training process of the diffusion model. Subsequently, a meticulously designed motion-based retrieval algorithm is applied to identify the most suitable path within the graph by assessing both global and local similarities in motion. Given that not all nodes in the retrieved path are sequentially continuous, the final step involves seamlessly stitching together these segments to produce a coherent video output. Experimental results substantiate the efficacy of our proposed method, demonstrating a significant improvement over prior approaches in terms of synchronization accuracy and naturalness of generated gestures.

</details>


### [61] [Content-Aware Texturing for Gaussian Splatting](https://arxiv.org/abs/2512.02621)
*Panagiotis Papantonakis,Georgios Kopanas,Fredo Durand,George Drettakis*

Main category: cs.CV

TL;DR: 引入按内容与采样自适应的每原语纹理，动态调整纹理分辨率与原语数量，实现更高效的高斯点场景表示与渲染。


<details>
  <summary>Details</summary>
Motivation: 高斯点渲染需大量小高斯原语以表达细节，当几何和外观的频率特性不一致时效率低下。借鉴纹理映射思想，用纹理来承载细节能提高效率。

Method: 为二维高斯原语设计带纹理的外观表示，纹理分辨率根据输入图像采样频率和内容自适应上下采样，并在优化过程中调整纹素大小与原语数量的关系。

Result: 方法在图像质量和总参数量方面优于现有纹理化高斯原语方案，同时可在优化中控制原语数量与纹理分辨率。

Conclusion: 该论文提出在高斯点渲染（Gaussian Splatting）中引入每个原语的自适应纹理贴图，从而在保持细节表现的同时减少参数量。

Abstract: Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.
  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/

</details>


### [62] [RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence](https://arxiv.org/abs/2512.02622)
*Xuming He,Zehao Fan,Hengjia Li,Fan Zhuo,Hankun Xu,Senlin Cheng,Di Weng,Haifeng Liu,Can Ye,Boxi Wu*

Main category: cs.CV

TL;DR: RULER-Bench为视频生成模型提供规则化推理评估，覆盖40任务和622实例，使用GPT打分与人工85%对齐，揭示模型在规则一致性上仍有明显缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成评测侧重视觉得分与理解，但忽视了模型的规则化推理能力；需要细粒度分解和全面协议来衡量规则推理。

Method: 构建基准覆盖文本到视频和图像到视频两大范式，包含6类规则、40项任务、622个标注实例；为每个生成视频设计4项评估指标的检查表，使用GPT-3评估得分并与人工判断对齐（85%）。

Result: 实验显示最先进模型在规则一致性指标上仅得48.87%，表明规则推理能力尚不足。

Conclusion: 该论文提出了一个面向视频生成模型推理能力评估的新基准RULER-Bench，强调当前模型在规则推理方面存在显著不足。

Abstract: Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.

</details>


### [63] [PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding](https://arxiv.org/abs/2512.02624)
*Zheng Huang,Xukai Liu,Tianyu Hu,Kai Zhang,Ye Liu*

Main category: cs.CV

TL;DR: PPTBench：基于958份PPTX、4,439样本的多模态基准，揭示当前MLLM在幻灯片视觉布局推理和一致性生成方面的显著短板，提供数据与代码以促进未来研究。


<details>
  <summary>Details</summary>
Motivation: 现有基准只关注狭窄子任务，忽视布局中心的挑战，而布局理解对真实幻灯片创建与编辑至关重要，因此需要一个全面的多模态基准评估模型在PPT场景的能力。

Method: 从958份PPTX文件构建数据集，包含4,439个样本并划分为检测、理解、修改和生成四类任务；设计评测用于测量模型的视觉布局理解与API规划能力，进行消融实验与案例分析以揭示错误类型。

Result: 实验发现模型能够解释幻灯片内容但在产生空间排列方面表现差强人意，难以将视觉线索与基于JSON的布局结构结合，也无法在API规划中融合视觉信息；并通过案例展示了对齐错误与元素重叠等问题。

Conclusion: PPTBench指出当前多模态大模型在幻灯片的视觉-布局推理上存在明显不足，尽管语义理解能力较好，但在生成一致且布局合理的幻灯片时失败，导致错位、重叠等系统性错误。

Abstract: PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.

</details>


### [64] [Leveraging Large-Scale Pretrained Spatial-Spectral Priors for General Zero-Shot Pansharpening](https://arxiv.org/abs/2512.02643)
*Yongchuan Cui,Peng Liu,Yi Zeng*

Main category: cs.CV

TL;DR: 通过在大规模多样化模拟数据上预训练融合模型，学习泛化的空间-光谱先验，显著提升跨传感器遥感融合（如全色锐化）在零样本与一样本情形下的通用性与性能。


<details>
  <summary>Details</summary>
Motivation: Limited real paired remote sensing training data and domain gaps across sensors cause poor generalization; foundation models trained on diverse simulated degradations may learn robust spatial-spectral priors.

Method: Pretraining on large-scale simulated datasets for pansharpening/fusion

Result: Pretraining on simulated degradations from ImageNet and SkyScript and evaluating zero-/one-shot across six satellite datasets improves generalization for CNNs, Transformers, and Mamba; superior zero-shot performance and strong one-shot adaptation.

Conclusion: 基于模拟降质与增强构建的预训练策略能有效构建具备跨传感器泛化能力的基础模型，为遥感图像融合的跨域应用和基准研究提供实用方案。

Abstract: Existing deep learning methods for remote sensing image fusion often suffer from poor generalization when applied to unseen datasets due to the limited availability of real training data and the domain gap between different satellite sensors. To address this challenge, we explore the potential of foundation models by proposing a novel pretraining strategy that leverages large-scale simulated datasets to learn robust spatial-spectral priors. Specifically, our approach first constructs diverse simulated datasets by applying various degradation operations (blur, noise, downsampling) and augmentations (bands generation, channel shuffling, high-pass filtering, color jittering, etc.) to natural images from ImageNet and remote sensing images from SkyScript. We then pretrain fusion models on these simulated data to learn generalizable spatial-spectral representations. The pretrained models are subsequently evaluated on six datasets (WorldView-2/3/4, IKONOS, QuickBird, GaoFen-2) using zero-shot and one-shot paradigms, with both full- and freeze-tuning approaches for fine-tuning. Extensive experiments on different network architectures including convolutional neural networks, Transformer, and Mamba demonstrate that our pretraining strategy significantly improves generalization performance across different satellite sensors and imaging conditions for various fusion models. The pretrained models achieve superior results in zero-shot scenarios and show remarkable adaptation capability with minimal real data in one-shot settings. Our work provides a practical solution for cross-domain pansharpening, establishes a new benchmark for generalization in remote sensing image fusion tasks, and paves the way for leveraging foundation models through advanced training strategies.

</details>


### [65] [PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking](https://arxiv.org/abs/2512.02648)
*Dong Li,Jiahao Xiong,Yingda Huang,Le Chang*

Main category: cs.CV

TL;DR: PoreTrack3D：首个面向孔尺度的动态 3D Gaussian splatting 面部轨迹基准，拥有大量长轨迹与孔尺度关键点，建立了该领域首个评测基线并开源数据。


<details>
  <summary>Details</summary>
Motivation: 当前面部运动捕捉与重建多集中在传统关键点与整体形变，缺乏对皮肤微小运动（如毛孔、细纹）的细粒度动态捕捉与评测基准。PoreTrack3D 旨在填补这一空白。

Method: 通过高分辨率面部扫描与动态重建流水线，结合 3D Gaussian splatting 技术提取并标注面部传统关键点与孔尺度关键点轨迹；对现有动态 3D Gaussian splatting 方法在数据集上进行系统评估以建立基线。

Result: 数据集包含>440,000 条面部轨迹，>52,000 条长度>10 帧，68 条手工审阅并覆盖全部 150 帧；发布了首批基线结果与构建流水线，数据集公开在 GitHub。

Conclusion: PoreTrack3D 成功构建了首个面向孔尺度、非刚性面部三维轨迹跟踪的动态 3D Gaussian splatting 基准数据集，推进了细粒度面部表情及皮肤表面微动研究。

Abstract: We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D

</details>


### [66] [Hear What Matters! Text-conditioned Selective Video-to-Audio Generation](https://arxiv.org/abs/2512.02650)
*Junwon Lee,Juhan Nam,Jiyoung Lee*

Main category: cs.CV

TL;DR: SelVA enables text-guided selective generation of a single intended sound from multi-object videos by modulating the video encoder with supplementary tokens and self-augmentation, achieving better quality and alignment on a curated benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing V2A generates mixed audio from multi-object videos due to entangled visual features and insufficient source specification; need selective single-source audio for precise multimedia editing.

Method: SelVA: text-conditioned selective V2A

Result: SelVA uses text prompt as explicit selector, supplements tokens to modulate video encoder via cross-attention suppression of irrelevant activations, efficient parameter tuning, self-augmentation to handle lack of mono supervision; outperforms baselines on VGG-MONOAUDIO in audio quality, semantic alignment, and temporal sync.

Conclusion: Text-conditioned selector tokens and self-augmentation enable robust selective V2A generation, facilitating precise audio source control for multimedia tasks.

Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.

</details>


### [67] [Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation](https://arxiv.org/abs/2512.02660)
*Agathoklis Georgiou*

Main category: cs.CV

TL;DR: 将VLM的patch相似性作为空间过滤器应用到OCR区域，通过坐标映射与交集度量在推理时融合两者优点，开源实现Snappy。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的VLM检索擅长细粒度语义对齐但只返回整页，缺乏精确位置；OCR方法提供位置但语义相关性弱。两者各有优点与局限，结合可在不训练的情况下同时获得语义精确性与区域定位。

Method: 利用ColPali等VLM的视觉Transformer输出的patch-level相似性分数，构建patch网格到OCR边界框的坐标映射；定义patch与边界框的交集度量（基于覆盖面积或比例）用于按阈值传播相似性；在不额外训练的前提下做推理级别的融合。

Result: 提出坐标映射和交集传播方法，并给出检索精确率的理论界（基于patch覆盖与相似性阈值）。发布了开源实现Snappy并进行实用性测试，正在进行经验评估。

Conclusion: 本文提出将视觉-语言模型的Patch级相似性与OCR区域结合的混合检索架构，实现在推理时对OCR提取的文本区域进行空间相关性过滤，从而输出更精确的检索片段，提升RAG场景下的上下文精确度。

Abstract: Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.

</details>


### [68] [PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes](https://arxiv.org/abs/2512.02664)
*Derui Shan,Qian Qiao,Hao Lu,Tao Du,Peng Lu*

Main category: cs.CV

TL;DR: PolarGuide-GSDR embeds polarization priors into 3D Gaussian Splatting via a bidirectional coupling: 3DGS helps disambiguate polarization, polarization refines 3DGS normals/SH, enabling accurate reflection separation and real-time reconstruction without environment maps.


<details>
  <summary>Details</summary>
Motivation: Existing polarization-aware NeRFs are slow and assumption-heavy; 3DGS is fast but fails on reflections due to reflection-geometry entanglement and environment-map dependence. Need real-time, assumption-light method for reflective scenes.

Method: Introduce a bidirectional coupling: use 3DGS geometric priors to resolve polarization ambiguity, then use refined polarization cues to guide 3DGS normal and spherical harmonic optimization; incorporate polarization-forward guidance into 3DGS optimization loop for joint refinement.

Result: State-of-the-art performance on public and collected datasets for specular reconstruction, normal estimation, and novel view synthesis, with real-time rendering retained.

Conclusion: PolarGuide-GSDR successfully integrates polarization cues into 3D Gaussian Splatting, producing accurate specular separation, improved normals, and real-time novel-view synthesis without environment maps or restrictive material assumptions.

Abstract: Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.

</details>


### [69] [UAUTrack: Towards Unified Multimodal Anti-UAV Visual Tracking](https://arxiv.org/abs/2512.02668)
*Qionglin Ren,Dawei Zhang,Chunxu Tian,Dan Zhang*

Main category: cs.CV

TL;DR: 提出UAUTrack：单流单阶段端到端的多模态反无人机跟踪框架，结合文本先验提示，实现在多个数据集上的SOTA水平并兼顾速度与精度。


<details>
  <summary>Details</summary>
Motivation: 现有Anti-UAV跟踪方法多为针对单一模态或独立任务模型，缺乏跨模态协作机制与有效的多模态数据融合；因此需要一个统一框架以提升鲁棒性和效率。

Method: 单流单阶段端到端跟踪网络，输入融合多模态（RGB、TIR、RGB-T），通过文本先验提示引导注意力到无人机目标，训练和推理在统一模型中完成以实现跨模态协同。

Result: 在Anti-UAV、DUT Anti-UAV数据集上达到或超越现有最先进方法，在Anti-UAV410上在准确性与速度间取得良好折中，证明模型兼顾性能与实际效率。

Conclusion: UAUTrack提出了一个统一的单目标跟踪框架，通过单流、单阶段、端到端的架构实现多模态融合，并引入文本先验提示策略以增强对无人机的关注，实验在多个Anti-UAV数据集上取得了SOTA表现，同时在精度与速度上保持良好平衡。

Abstract: Research in Anti-UAV (Unmanned Aerial Vehicle) tracking has explored various modalities, including RGB, TIR, and RGB-T fusion. However, a unified framework for cross-modal collaboration is still lacking. Existing approaches have primarily focused on independent models for individual tasks, often overlooking the potential for cross-modal information sharing. Furthermore, Anti-UAV tracking techniques are still in their infancy, with current solutions struggling to achieve effective multimodal data fusion. To address these challenges, we propose UAUTrack, a unified single-target tracking framework built upon a single-stream, single-stage, end-to-end architecture that effectively integrates multiple modalities. UAUTrack introduces a key component: a text prior prompt strategy that directs the model to focus on UAVs across various scenarios. Experimental results show that UAUTrack achieves state-of-the-art performance on the Anti-UAV and DUT Anti-UAV datasets, and maintains a favourable trade-off between accuracy and speed on the Anti-UAV410 dataset, demonstrating both high accuracy and practical efficiency across diverse Anti-UAV scenarios.

</details>


### [70] [PGP-DiffSR: Phase-Guided Progressive Pruning for Efficient Diffusion-based Image Super-Resolution](https://arxiv.org/abs/2512.02681)
*Zhongbao Yang,Jiangxin Dong,Yazhou Yao,Jinhui Tang,Jinshan Pan*

Main category: cs.CV

TL;DR: PGP-DiffSR prunes redundant blocks from diffusion SR backbones and adds a phase-exchange adapter using input phase to maintain restoration quality, yielding competitive results with much lower cost.


<details>
  <summary>Details</summary>
Motivation: Large diffusion SR models (e.g., SDXL, DiT) are computationally expensive; remove redundancy and leverage phase information to build a lightweight, efficient diffusion-based SR method.

Method: Identify intra-block redundancy and progressively prune redundant blocks in diffusion backbones; introduce a phase-exchange adapter that uses input phase information to guide the pruned model; unify both into a single framework.

Result: Experiments show comparable restoration quality with significantly lower computational load and memory; code released on GitHub.

Conclusion: PGP-DiffSR successfully reduces computation and memory while keeping competitive SR quality by progressive pruning and a phase-exchange adapter.

Abstract: Although diffusion-based models have achieved impressive results in image super-resolution, they often rely on large-scale backbones such as Stable Diffusion XL (SDXL) and Diffusion Transformers (DiT), which lead to excessive computational and memory costs during training and inference. To address this issue, we develop a lightweight diffusion method, PGP-DiffSR, by removing redundant information from diffusion models under the guidance of the phase information of inputs for efficient image super-resolution. We first identify the intra-block redundancy within the diffusion backbone and propose a progressive pruning approach that removes redundant blocks while reserving restoration capability. We note that the phase information of the restored images produced by the pruned diffusion model is not well estimated. To solve this problem, we propose a phase-exchange adapter module that explores the phase information of the inputs to guide the pruned diffusion model for better restoration performance. We formulate the progressive pruning approach and the phase-exchange adapter module into a unified model. Extensive experiments demonstrate that our method achieves competitive restoration quality while significantly reducing computational load and memory consumption. The code is available at https://github.com/yzb1997/PGP-DiffSR.

</details>


### [71] [Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance](https://arxiv.org/abs/2512.02685)
*Huankun Sheng,Ming Li,Yixiang Wei,Yeying Fan,Yu-Hui Wen,Tieliang Gong,Yong-Jin Liu*

Main category: cs.CV

TL;DR: FASA通过先分离前景/背景再对前景进行masked slot attention，并借助自监督伪掩码，显著提升了无监督物体发现的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有slot attention方法对前景和背景无差别处理，背景干扰导致在真实数据上实例发现效果欠佳，需显式建模前景以提高鲁棒性。

Method: 两阶段方法：阶段一利用聚类初始化和双slot竞争粗分前景/背景，阶段二用masked slot attention使第一个slot表示背景，其余slots争夺前景物体，并用基于自监督特征的patch affinity图生成伪掩码引导前景slot学习。

Result: 在合成和真实数据集上，FASA优于最先进方法，展示了显式前景建模和伪掩码引导在场景分解与物体一致表示上的有效性。

Conclusion: 本文提出FASA，一种显式前景-背景分离的两阶段slot attention框架，通过双slot竞争和masked slot attention结合伪掩码引导，改进了实例发现性能。

Abstract: Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.

</details>


### [72] [ClimaOoD: Improving Anomaly Segmentation via Physically Realistic Synthetic Data](https://arxiv.org/abs/2512.02686)
*Yuxing Liu,Yong Liu*

Main category: cs.CV

TL;DR: ClimaDrive生成语义一致、天气多样且物理合理的合成异常数据，构建ClimaOoD基准，显著提升异常分割模型在不同天气/场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 实车异常数据稀缺且多样性不足，现有合成方法（copy-paste或扩散inpainting）在上下文一致性和物理真实感方面存在缺陷，导致域差距。

Method: 提出ClimaDrive框架：结合结构引导的多天气生成与基于提示的异常修补（inpainting），统一生成语义一致、天气多样的驾驶场景异常图像，并基于此构建ClimaOoD基准数据集。

Result: 在四种SOTA方法上进行实验，使用ClimaOoD训练均带来显著提升：如在Fishyscapes LAF上，RbA方法的FPR95从3.97降至3.52，AUROC、AP等指标也普遍提升。

Conclusion: ClimaDrive通过语义引导的图像到图像合成方法生成气候多样且物理合理的异常样本，从而提升异常分割模型在开放世界场景的鲁棒性。

Abstract: Anomaly segmentation seeks to detect and localize unknown or out-of-distribution (OoD) objects that fall outside predefined semantic classes a capability essential for safe autonomous driving. However, the scarcity and limited diversity of anomaly data severely constrain model generalization in open-world environments. Existing approaches mitigate this issue through synthetic data generation, either by copy-pasting external objects into driving scenes or by leveraging text-to-image diffusion models to inpaint anomalous regions. While these methods improve anomaly diversity, they often lack contextual coherence and physical realism, resulting in domain gaps between synthetic and real data. In this paper, we present ClimaDrive, a semantics-guided image-to-image framework for synthesizing semantically coherent, weather-diverse, and physically plausible OoD driving data. ClimaDrive unifies structure-guided multi-weather generation with prompt-driven anomaly inpainting, enabling the creation of visually realistic training data. Based on this framework, we construct ClimaOoD, a large-scale benchmark spanning six representative driving scenarios under both clear and adverse weather conditions. Extensive experiments on four state-of-the-art methods show that training with ClimaOoD leads to robust improvements in anomaly segmentation. Across all methods, AUROC, AP, and FPR95 show notable gains, with FPR95 dropping from 3.97 to 3.52 for RbA on Fishyscapes LAF. These results demonstrate that ClimaOoD enhances model robustness, offering valuable training data for better generalization in open-world anomaly detection.

</details>


### [73] [ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection](https://arxiv.org/abs/2512.02696)
*Omid Reza Heidari,Yang Wang,Xinxin Zuo*

Main category: cs.CV

TL;DR: ALDI++通过自蒸馏、特征对齐与增强训练，有效缓解安全X光图像的域偏移问题，在ED S上优于SOTA，且ViTDet主干表现最佳。


<details>
  <summary>Details</summary>
Motivation: 安全X光成像中因扫描设备和环境差异导致显著域偏移，传统检测模型在目标检测任务上性能下降，需设计鲁棒的域自适应方法提升跨设备/跨环境的泛化能力。

Method: 将自蒸馏、特征对齐和增强训练策略整合到统一框架ALDI++，并在ED S数据集上进行多场景的无/有监督域自适应实验；在实验中比较不同基线（CNN与ViTDet），并进行类别级别的性能分析。

Result: 在ED S数据集上的多种适配场景中，ALDI++优于其他域自适应方法，ViTDet主干的ALDI++取得最高mAP；类别级分析显示大多数类别的AP都有一致提升，证明了方法的稳健性。

Conclusion: ALDI++在安全X光图像的域自适应目标检测任务上有效，显著优于现有SOTA方法，并在使用ViTDet主干时达到最高mAP，展示了transformer架构的跨域优势。

Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.

</details>


### [74] [GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization](https://arxiv.org/abs/2512.02697)
*Zixuan Song,Jing Zhang,Di Wang,Zidie Zhou,Wenbin Liu,Haonan Guo,En Wang,Bo Du*

Main category: cs.CV

TL;DR: 提出GeoBridge和GeoLoc：通过文本语义锚点实现多视图跨模态对齐，改善跨视图地理定位性能与泛化，已开源。


<details>
  <summary>Details</summary>
Motivation: 传统以卫星为中心的跨视图地理定位在高分辨率或更新的卫星图像不可用时表现受限，且未充分利用不同视角（无人机、卫星、街景）与模态（图像、语言）之间的互补信息。

Method: 构建语义锚点（文本描述）将无人机、街景和卫星图像特征对齐；设计双向匹配的基础模型GeoBridge，支持跨视图和语言-图像检索；在GeoLoc数据集上进行预训练与评价。

Result: 在GeoLoc上预训练的GeoBridge显著提升地理定位精度，并增强跨域泛化与跨模态知识迁移。作者开源了数据集、代码和预训练模型。

Conclusion: GeoBridge提出通过语义锚点机制桥接多视图特征，并支持双向匹配与语言-图像检索，从而提高跨视图地理定位的鲁棒性与泛化能力。

Abstract: Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.

</details>


### [75] [VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm](https://arxiv.org/abs/2512.02700)
*Zhenkai Wu,Xiaowen Ma,Zhenliang Ni,Dengming Zhang,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.CV

TL;DR: 提出一种训练-free的冗余与空间感知视觉token剪枝方法（VLM-Pruner）：离心选择、BSS准则与并行贪心策略并结合信息融合，在高剪枝率下优于基线并加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法仅基于重要性忽视token间冗余或空间关系，导致保留重复或空间覆盖不足的token，降低剪枝效率与下游表现。

Method: 提出离心式(token selection near-to-far)剪枝范式、Buffering for Spatial Sparsity (BSS)准则延后选择空间远离的token，以及并行贪心策略进行高效选择；对被丢弃token的显著信息进行选择性融合以减小信息损失。

Result: 在五种VLM上以88.9%剪枝率下，VLM-Pruner持续优于强基线并实现端到端推理加速。

Conclusion: VLM-Pruner在不需训练的情况下，通过兼顾冗余与空间稀疏性，有效减少视觉token数量并保持性能，适用于多种VLM并实现实际加速。

Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.

</details>


### [76] [Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study](https://arxiv.org/abs/2512.02702)
*Yasemin Utkueri,Elin Lundström,Håkan Ahlström,Johan Öfverstedt,Joel Kullberg*

Main category: cs.CV

TL;DR: 用皮下脂肪和肌肉掩模辅助的图割配准能明显提升UK Biobank全身MRI跨主体配准质量，改善Dice分数与错误频率，并产生更可靠的体素级年龄相关性图谱。


<details>
  <summary>Details</summary>
Motivation: UK Biobank的大规模全身MRI若能实现稳健的跨主体配准，可对影像参数与非影像健康数据做全身、体素或区域层面的关联分析；现有强度驱动方法在全身组织对齐上欠佳，利用结构掩模可增强解剖一致性。

Method: 提出性别分层的基于图割的配准框架，利用VIBESegmentator生成的皮下脂肪与肌肉掩模作为额外约束，结合强度信息进行配准；在4000例子集中与强度仅方法、uniGradICON和MIRTK比较，评估指标为71个掩模的Dice评分与体素标记错误频率，并分析年龄与脂肪含量/组织体积的体素级相关图谱。

Result: 提出方法在71个掩模上平均Dice为男性0.77、女性0.75；相比强度仅方法提升约6个百分点，相比uniGradICON提升9/8pp（男/女），相比MIRTK提升12/13pp；多数组织区域标记错误频率降低；基于此方法的年龄相关图谱噪声更低、解剖对齐更好。

Conclusion: 使用皮下脂肪和肌肉掩模增强的图割配准能显著改善全身MRI在群体间的空间对齐，从而提高标签重叠和降低标注错误频率，利于基于体素的生物标志物研究。

Abstract: The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content). We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research. The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment. In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.

</details>


### [77] [GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding](https://arxiv.org/abs/2512.02715)
*Peirong Zhang,Yidan Zhang,Luxiao Xu,Jinliang Lin,Zonghao Guo,Fengxiang Wang,Xue Yang,Kaiwen Wei,Lei Wang*

Main category: cs.CV

TL;DR: GeoViS通过树状逐步搜索与奖励引导的探索，把遥感视觉定位任务从一次性预测变为可解释的迭代推理，显著提升小目标检测与复杂地理关系理解，在五个数据集上领先。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中目标往往在千米级场景中非常小且查询包含复杂的地理关系（相对位置、空间层级、远距对象的上下文依赖），现有MLLMs直接迁移效果欠佳。

Method: 提出Geospatially Rewarded Visual Search框架：树结构逐步视觉线索探索，结合多模态感知、空间推理与基于奖励的探索，迭代细化地理假设；非一步定位而是进化式搜索。

Result: 在五个遥感定位基准上实验，GeoViS在关键视觉定位指标上持续超越现有方法，表现出精确的地理理解、强跨域泛化与可解释性。

Conclusion: GeoViS通过将遥感视觉定位重构为逐步搜索推理过程，能更好处理小目标与复杂地理关系，提供更强的跨域泛化与可解释性。

Abstract: Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.

</details>


### [78] [DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions](https://arxiv.org/abs/2512.02727)
*Yifan Zhou,Takehiko Ohkawa,Guwenxiao Zhou,Kanoko Goto,Takumi Hirose,Yusuke Sekikawa,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 提出DF-Mamba：在Mamba基础上引入可变形状态扫描以增强全局上下文建模，改善遮挡下的3D手势估计并实现SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的特征提取（如ResNet）在建模全局上下文方面受限，导致在双手重叠等严重遮挡场景下3D手势估计鲁棒性不足，需要更有效的机制关联局部特征与全局提示。

Method: 在卷积局部特征基础上，采用Mamba对序列状态进行选择性建模，并引入可变形状态扫描（deformable state scanning）在图像范围内聚合局部特征，选择性保留对全局上下文有用的线索。该框架替代传统ResNet作为视觉特征提取器，兼顾效率与全局感知能力。

Result: 在五个多样化数据集（单手/双手、手-物体交互、RGB/深度）上广泛评估，DF-Mamba在所有数据集上均优于VMamba和Spatial-Mamba等最新骨干，达到SOTA性能，同时保持与ResNet-50相当的推理速度。

Conclusion: DF-Mamba通过将Mamba的状态空间建模与可变形状态扫描相结合，能更好地捕获全局上下文信息，从而在遮挡严重的3D手势估计任务中显著提升性能，且推理速度可与ResNet-50相比。

Abstract: Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.

</details>


### [79] [Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone](https://arxiv.org/abs/2512.02737)
*Tristan Amadei,Enric Meinhardt-Llopis,Benedicte Bascle,Corentin Abgrall,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 提出一种只用卫星图像训练、通过特定增强模拟视角差异的UAV定位方法（CAEVL），在新发布的ViLD数据集上表现接近有配对训练数据的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大规模配对UAV-卫星数据，获取成本高且难以获得，限制了实用性；因此希望消除训练时对UAV图像的依赖以提高可用性和泛化性。

Method: 通过设计专门的数据增强策略模拟卫星视角与真实UAV视角之间的视觉域差异，并提出轻量模型CAEVL来利用该训练范式。

Result: 在作者构建并公开的ViLD真实UAV图像数据集上，方法在没有配对训练数据的情况下取得了与使用配对数据训练的方法相当的性能，展示了有效性和强泛化能力。

Conclusion: 该论文提出了一种在训练阶段不依赖无人机(UAV)实景图像、仅利用卫星图像进行学习的图像定位方法，显著降低了配对数据需求。

Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.

</details>


### [80] [Reasoning-Aware Multimodal Fusion for Hateful Video Detection](https://arxiv.org/abs/2512.02743)
*Shuonan Yang,Tailin Chen,Jiangbei Yue,Guangliang Cheng,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: RAMF通过局部-全局融合、语义交叉注意力和对抗推理三大创新，有效增强多模态仇恨视频理解，在两个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容多模态与语境依赖性增强，现有方法在模态融合与理解细腻仇恨内容方面存在不足，需要更强的语义交互与推理能力。

Method: 提出Local-Global Context Fusion (LGCF)用于同时捕捉局部显著线索与全局时序结构，设计Semantic Cross Attention (SCA)实现细粒度多模态语义交互；引入对抗推理模块，使用视觉-语言模型生成三阶段文本（客观描述、假定仇恨推断、假定非仇恨推断）以丰富语义上下文。

Result: 在两个真实仇恨视频数据集上，RAMF在Macro-F1和仇恨类召回方面分别较最先进方法提升约3%和7%，显示出更好的泛化与检测能力。

Conclusion: 本文提出的RAMF框架有效提升了多模态仇恨言论检测的性能，通过局部-全局融合与语义交叉注意力增强模态间交互，同时引入对抗推理生成多视角语义信息，从而更好理解细微的仇恨意图。

Abstract: Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.

</details>


### [81] [AttMetNet: Attention-Enhanced Deep Neural Network for Methane Plume Detection in Sentinel-2 Satellite Imagery](https://arxiv.org/abs/2512.02751)
*Rakib Ahsan,MD Sadik Hossain Shanto,Md Sultanul Arifin,Tanzima Hashem*

Main category: cs.CV

TL;DR: 提出AttMetNet：将NDMI融入注意力加强的U-Net并结合focal loss，在真实Sentinel-2影像上实现更低误报和更高IoU的甲烷羽流检测。


<details>
  <summary>Details</summary>
Motivation: 现有基于波段差或比值的传统方法误报率高、需人工验证；现有CNN方法缺乏优先关注甲烷特征的机制。因此设计一种融合指标与注意力的专用架构以提高检测鲁棒性。

Method: 提出基于注意力的U-Net架构，输入包括B11、B12及NDMI，使用注意力模块引导网络关注甲烷吸收特征并抑制背景；训练时采用focal loss以缓解类别不平衡，并在真实甲烷羽流数据集上训练和评估。

Result: 在大量真实Sentinel-2测试中，AttMetNet相比近期方法显著降低假阳性率、提升精度-召回平衡及IoU，证明了融合NDMI与注意力机制的有效性。

Conclusion: AttMetNet通过将NDMI与注意力增强的U-Net融合并使用focal loss，在实际Sentinel-2影像上实现了更稳健的甲烷羽流检测，减少误报并提升精度-召回和IoU。

Abstract: Methane is a powerful greenhouse gas that contributes significantly to global warming. Accurate detection of methane emissions is the key to taking timely action and minimizing their impact on climate change. We present AttMetNet, a novel attention-enhanced deep learning framework for methane plume detection with Sentinel-2 satellite imagery. The major challenge in developing a methane detection model is to accurately identify methane plumes from Sentinel-2's B11 and B12 bands while suppressing false positives caused by background variability and diverse land cover types. Traditional detection methods typically depend on the differences or ratios between these bands when comparing the scenes with and without plumes. However, these methods often require verification by a domain expert because they generate numerous false positives. Recent deep learning methods make some improvements using CNN-based architectures, but lack mechanisms to prioritize methane-specific features. AttMetNet introduces a methane-aware architecture that fuses the Normalized Difference Methane Index (NDMI) with an attention-enhanced U-Net. By jointly exploiting NDMI's plume-sensitive cues and attention-driven feature selection, AttMetNet selectively amplifies methane absorption features while suppressing background noise. This integration establishes a first-of-its-kind architecture tailored for robust methane plume detection in real satellite imagery. Additionally, we employ focal loss to address the severe class imbalance arising from both limited positive plume samples and sparse plume pixels within imagery. Furthermore, AttMetNet is trained on the real methane plume dataset, making it more robust to practical scenarios. Extensive experiments show that AttMetNet surpasses recent methods in methane plume detection with a lower false positive rate, better precision recall balance, and higher IoU.

</details>


### [82] [Rethinking Surgical Smoke: A Smoke-Type-Aware Laparoscopic Video Desmoking Method and Dataset](https://arxiv.org/abs/2512.02780)
*Qifan Liang,Junlin Li,Zhen Han,Xihao Wang,Zhongyuan Wang,Bin Mei*

Main category: cs.CV

TL;DR: 提出以烟雾类型感知为核心的STANet，通过掩码分割与类型指导的重建以及粗到细解缠机制，实现更精确的腹腔镜视频去烟并提升下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中电灼和激光产生的手术烟雾破坏视觉引导，且不同运动模式（扩散烟与环境烟）产生不同时空特征，现有方法未考虑烟雾类型差异导致去烟效果与泛化性不足。

Method: 设计两分支子网络：1) 烟雾掩码分割子网，采用注意力加权掩码聚合同时预测烟雾存在与烟雾类型，并引入粗到细解缠模块，通过烟型感知的交叉注意力在非缠绕与缠绕区域间精细分离两种烟雾掩码；2) 无烟视频重建子网，基于两类烟雾掩码对被烟污染的特征进行类型专门化去烟重构。并构建合成带烟雾类型标注的大规模视频数据集用于训练与评估。

Result: 在构建的合成数据集与多项下游任务评估中，STANet在图像质量指标和下游任务（如器械检测、分割、导航等）的泛化能力上显著优于现有最先进方法，且解缠模块提高了掩码精度与类型区分率。

Conclusion: 本论文提出了首个考虑烟雾类型差异的腹腔镜视频去烟网络STANet，通过区分扩散烟与环境烟并在掩码分割与视频重建中分别建模，提升去烟效果与下游任务泛化能力。

Abstract: Electrocautery or lasers will inevitably generate surgical smoke, which hinders the visual guidance of laparoscopic videos for surgical procedures. The surgical smoke can be classified into different types based on its motion patterns, leading to distinctive spatio-temporal characteristics across smoky laparoscopic videos. However, existing desmoking methods fail to account for such smoke-type-specific distinctions. Therefore, we propose the first Smoke-Type-Aware Laparoscopic Video Desmoking Network (STANet) by introducing two smoke types: Diffusion Smoke and Ambient Smoke. Specifically, a smoke mask segmentation sub-network is designed to jointly conduct smoke mask and smoke type predictions based on the attention-weighted mask aggregation, while a smokeless video reconstruction sub-network is proposed to perform specially desmoking on smoky features guided by two types of smoke mask. To address the entanglement challenges of two smoke types, we further embed a coarse-to-fine disentanglement module into the mask segmentation sub-network, which yields more accurate disentangled masks through the smoke-type-aware cross attention between non-entangled and entangled regions. In addition, we also construct the first large-scale synthetic video desmoking dataset with smoke type annotations. Extensive experiments demonstrate that our method not only outperforms state-of-the-art approaches in quality evaluations, but also exhibits superior generalization across multiple downstream surgical tasks.

</details>


### [83] [LumiX: Structured and Coherent Text-to-Intrinsic Generation](https://arxiv.org/abs/2512.02781)
*Xu Han,Biao Zhang,Xiangjun Tang,Xianzhi Li,Peter Wonka*

Main category: cs.CV

TL;DR: LumiX 通过 Query-Broadcast Attention 与 Tensor LoRA 实现了结构一致且参数高效的联合内在图扩散生成，显著改善对齐性与主观偏好。


<details>
  <summary>Details</summary>
Motivation: 当前文本到内在属性生成缺乏多图之间的结构一致性与物理约束，且联合训练跨图关系难以高效建模和稳定训练，因此需要一种既保持一致性又参数高效的方法。

Method: 提出 Query-Broadcast Attention（跨图共享 queries 以保证结构一致性）和 Tensor LoRA（张量化低秩适配以有效建模跨图关系并节省参数），并在联合扩散训练中稳定收敛，实现统一生成与图像条件分解。

Result: 在实验中，LumiX 在一致性对齐性上提升约23%，用户偏好评分显著优于最先进方法（0.19 vs -0.41），并能在同一框架下支持图像条件的内在分解。

Conclusion: LumiX 提供了一种能联合生成多种内在图（albedo、irradiance、normal、depth、final color）的统一扩散框架，生成结果在结构一致性和物理合理性上优于现有方法。

Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.

</details>


### [84] [TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking](https://arxiv.org/abs/2512.02789)
*Tang Haonan,Chen Yanjun,Jiang Lezhi*

Main category: cs.CV

TL;DR: TrackNetV5 adds MDD and R-STR to resolve motion direction loss and occlusions, achieving new SOTA with minimal compute overhead.


<details>
  <summary>Details</summary>
Motivation: Previous TrackNet versions fail under occlusion (V1-V3) or discard motion polarity causing directional ambiguity (V4).

Method: Introduces Motion Direction Decoupling (MDD) to encode signed motion polarity and a Transformer-based Residual-Driven Spatio-Temporal Refinement (R-STR) head to recover occlusions via coarse-to-fine residual estimation.

Result: On TrackNetV2 dataset, achieves F1=0.9859 and accuracy=0.9733, outperforming prior versions with only 3.7% extra FLOPs versus V4 and real-time inference.

Conclusion: TrackNetV5 effectively addresses directional ambiguity and occlusion, offering state-of-the-art tracking with minor FLOPs increase.

Abstract: The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.

</details>


### [85] [UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits](https://arxiv.org/abs/2512.02790)
*Keming Ye,Zhipeng Huang,Canmiao Fu,Qingyang Liu,Jiani Cai,Zheqi Lv,Chen Li,Jing Lyu,Zhou Zhao,Shengyu Zhang*

Main category: cs.CV

TL;DR: 用Qwen-Verify做统一后验验证，替代复杂工具链，产出10M图像编辑数据集UnicEdit-10M并提出UnicBench与新指标，从而为开源图像编辑模型提供高质量训练数据与精细诊断基准。


<details>
  <summary>Details</summary>
Motivation: 封闭源模型在图像编辑任务上领先于开源模型，主要归因于大规模高质量训练数据与全面诊断基准的匮乏。现有方法在人力成本与自动化噪声间存在权衡，需一种可扩展且能保证质量的管线。

Method: 用单一端到端模型替换多工具链并加入统一后验验证阶段；训练一个7B的双任务模型Qwen-Verify用于高效失败检测和指令重述（recaptioning）；基于该验证机制筛选并修正自动生成的数据，最终构建10M规模数据集；设计UnicBench评测集并提出Non-edit Consistency和Reasoning Accuracy等指标评估模型表现。

Result: 构建了UnicEdit-10M大规模数据集与UnicBench通用评测基准；提出Qwen-Verify能有效检测失败并重写指令；通过新指标对主流模型进行分析，揭示其在空间推理和知识驱动编辑上的局限，并指出改进方向。

Conclusion: 该论文提出了一个轻量级的数据构建与质量控制流水线，通过训练7B双任务专家模型Qwen-Verify进行失败检测与指令重写，从而在规模与质量间取得平衡，生成了10M规模的图像编辑数据集UnicEdit-10M，并构建了评测基准UnicBench及新指标用于细粒度诊断，实验证明现有模型在复杂编辑与空间/知识推理上存在明显不足。

Abstract: With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \textit{Non-edit Consistency} and \textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research.

</details>


### [86] [HUD: Hierarchical Uncertainty-Aware Disambiguation Network for Composed Video Retrieval](https://arxiv.org/abs/2512.02792)
*Zhiwei Chen,Yupeng Hu,Zixu Li,Zhiheng Fu,Haokun Wen,Weili Guan*

Main category: cs.CV

TL;DR: 提出HUD，利用视频—文本信息密度差异，通过整体与原子级别的消歧与对齐，实现更精确的组合检索，取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 观察到视频比文本承载更多语义，现有方法忽视信息密度差异，导致代词指代模糊和细节关注不足，影响组合视频/图像检索效果。

Method: 提出HUD框架，包括整体代词消歧、原子不确定性建模与整体到原子对齐，分别负责跨模态整体交互、细粒度局部语义建模与对齐。

Result: HUD在CVR和CIR三个基准数据集上均达到了最新的SOTA性能。

Conclusion: HUD通过利用视频与文本信息密度差异，有效进行对象消歧与细粒度语义对齐，从而提升组合检索性能。

Abstract: Composed Video Retrieval (CVR) is a challenging video retrieval task that utilizes multi-modal queries, consisting of a reference video and modification text, to retrieve the desired target video. The core of this task lies in understanding the multi-modal composed query and achieving accurate composed feature learning. Within multi-modal queries, the video modality typically carries richer semantic content compared to the textual modality. However, previous works have largely overlooked the disparity in information density between these two modalities. This limitation can lead to two critical issues: 1) modification subject referring ambiguity and 2) limited detailed semantic focus, both of which degrade the performance of CVR models. To address the aforementioned issues, we propose a novel CVR framework, namely the Hierarchical Uncertainty-aware Disambiguation network (HUD). HUD is the first framework that leverages the disparity in information density between video and text to enhance multi-modal query understanding. It comprises three key components: (a) Holistic Pronoun Disambiguation, (b) Atomistic Uncertainty Modeling, and (c) Holistic-to-Atomistic Alignment. By exploiting overlapping semantics through holistic cross-modal interaction and fine-grained semantic alignment via atomistic-level cross-modal interaction, HUD enables effective object disambiguation and enhances the focus on detailed semantics, thereby achieving precise composed feature learning. Moreover, our proposed HUD is also applicable to the Composed Image Retrieval (CIR) task and achieves state-of-the-art performance across three benchmark datasets for both CVR and CIR tasks. The codes are available on https://zivchen-ty.github.io/HUD.github.io/.

</details>


### [87] [IC-World: In-Context Generation for Shared World Modeling](https://arxiv.org/abs/2512.02793)
*Fan Wu,Jiacheng Wei,Ruibo Li,Yi Xu,Junyou Li,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: IC-World通过激活大视频模型的in-context能力并用GRPO及两类一致性奖励微调，实现对一组输入视角并行生成一致的多视频，显著提升几何和运动一致性。


<details>
  <summary>Details</summary>
Motivation: 解决共享世界建模任务：从一组同一场景不同相机位姿的输入图像生成多段视频，要求在不同生成视频间保持场景几何与物体运动的一致性。

Method: 提出IC-World框架：利用大视频模型的in-context generation实现对所有输入图像并行生成；设计Group Relative Policy Optimization（GRPO）进行强化学习微调；提出两种奖励模型，分别用于场景几何一致性和物体运动一致性。

Result: 大量实验显示IC-World在几何一致性和运动一致性指标上明显优于现有最先进方法。作者称这是首个系统性探究基于视频的共享世界建模的工作。

Conclusion: IC-World有效提升了共享世界建模中多视角视频生成的一致性，通过激活大视频模型的in-context生成能力并引入GRPO强化学习微调，显著优于现有方法。

Abstract: Video-based world models have recently garnered increasing attention for their ability to synthesize diverse and dynamic visual environments. In this paper, we focus on shared world modeling, where a model generates multiple videos from a set of input images, each representing the same underlying world in different camera poses. We propose IC-World, a novel generation framework, enabling parallel generation for all input images via activating the inherent in-context generation capability of large video models. We further finetune IC-World via reinforcement learning, Group Relative Policy Optimization, together with two proposed novel reward models to enforce scene-level geometry consistency and object-level motion consistency among the set of generated videos. Extensive experiments demonstrate that IC-World substantially outperforms state-of-the-art methods in both geometry and motion consistency. To the best of our knowledge, this is the first work to systematically explore the shared world modeling problem with video-based world models.

</details>


### [88] [PhyCustom: Towards Realistic Physical Customization in Text-to-Image Generation](https://arxiv.org/abs/2512.02794)
*Fan Wu,Cheng Chen,Zhoujie Fu,Jiacheng Wei,Yi Xu,Deheng Ye,Guosheng Lin*

Main category: cs.CV

TL;DR: PhyCustom通过isometric与decouple两项正则化在扩散模型微调中引入物理知识，显著改善了文本提示中物理概念的图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的文本到图像定制方法在风格和形状等具体概念上表现良好，但在物理概念（如材料、重量、弹性等）上的定制能力不足，原因是训练中缺乏显式物理知识引导。

Method: 提出了两个正则化项：isometric loss用于激活模型学习物理概念（鼓励物理属性在潜空间或生成结果中保持一致对应），decouple loss用于消除独立概念之间的混合学习，从而提高定制化的可控性。整体以微调为框架，在多样化数据集上训练并评估。

Result: 在提出的基准与多样化数据集上，PhyCustom在物理定制相关的定量指标和主观质量评价上均优于现有最先进和流行方法，定性结果展示了更准确的物理属性表现。

Conclusion: PhyCustom通过在扩散模型微调中引入物理正则化损失，有效提升了模型对物理属性的控制能力，能够在生成图像中更准确地表现物理概念。

Abstract: Recent diffusion-based text-to-image customization methods have achieved significant success in understanding concrete concepts to control generation processes, such as styles and shapes. However, few efforts dive into the realistic yet challenging customization of physical concepts. The core limitation of current methods arises from the absence of explicitly introducing physical knowledge during training. Even when physics-related words appear in the input text prompts, our experiments consistently demonstrate that these methods fail to accurately reflect the corresponding physical properties in the generated results. In this paper, we propose PhyCustom, a fine-tuning framework comprising two novel regularization losses to activate diffusion model to perform physical customization. Specifically, the proposed isometric loss aims at activating diffusion models to learn physical concepts while decouple loss helps to eliminate the mixture learning of independent concepts. Experiments are conducted on a diverse dataset and our benchmark results demonstrate that PhyCustom outperforms previous state-of-the-art and popular methods in terms of physical customization quantitatively and qualitatively.

</details>


### [89] [Defense That Attacks: How Robust Models Become Better Attackers](https://arxiv.org/abs/2512.02830)
*Mohamed Awad,Mahmoud Akrm,Walid Gomaa*

Main category: cs.CV

TL;DR: 对抗训练虽提升鲁棒性，但意外地使生成的对抗样本更易跨模型迁移，提出需要同时评估模型易受迁移攻击与其生成可迁移攻击的倾向。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗训练可提高模型自身的鲁棒性，但其对攻击可迁移性的影响未被充分研究，需评估AT是否无意中增强了对抗样本的跨模型传播能力并带来更大的安全风险。

Method: 训练并构建包含36个模型（CNN与ViT）的模型库，在多对多迁移实验设置中评估对抗样本的可迁移性，比较AT与标准模型生成攻击在不同受害模型上的成功率，公开所有模型与代码以保证可复现。

Result: 实验证明由AT模型生成的对抗样本在迁移攻击中成功率显著高于由标准模型生成的样本，表明AT可能提高攻击可迁移性；因此应在鲁棒性评估中纳入模型产生可迁移对抗样本的倾向性衡量。

Conclusion: 本文发现对抗性训练（AT）模型产生的对抗扰动比标准训练模型更具可迁移性，从而带来新的生态风险。

Abstract: Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.

</details>


### [90] [ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning](https://arxiv.org/abs/2512.02835)
*Yifan Li,Yingda Yin,Lingting Zhu,Weikai Chen,Shengju Qian,Xin Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: ReVSeg将复杂视频分割推理分解为三步显式决策并用强化学习优化，既提升性能又带来可解释的推理过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法把动态、因果与时序交互等复杂推理压缩为隐式嵌入，导致推理链不可解释且难以优化。提出显式分解以对齐预训练VLM已有能力。

Method: 在预训练视觉语言模型原生接口上，以顺序决策方式执行三步操作（语义解释、时间证据选择、空间着陆），并用强化学习根据最终分割结果优化决策策略。

Result: 在标准视频目标分割基准上达到或超过最新方法的性能，并产出易于审查的推理轨迹。

Conclusion: ReVSeg通过将推理显式分解为语义理解、时间证据选择和空间定位三步，并使用强化学习优化多步决策链，能够提高视频目标分割的可解释性与性能。

Abstract: Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .

</details>


### [91] [Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?](https://arxiv.org/abs/2512.02846)
*Manuel Benavent-Lledo,Konstantinos Bacharidis,Victoria Manousaki,Konstantinos Papoutsakis,Antonis Argyros,Jose Garcia-Rodriguez*

Main category: cs.CV

TL;DR: 通过结合单帧RGB、深度与语言或预测先验，AAG实现了无需视频时间聚合的有竞争力的动作预期。


<details>
  <summary>Details</summary>
Motivation: 传统动作预期依赖视频时序信息聚合，但人类在有足够上下文时常能仅凭单一瞬间预测即将发生的动作。研究旨在探讨是否可替代时序聚合，利用视觉特征与语言推理实现单帧动作预期。

Method: 提出AAG（Action Anticipation at a Glimpse）框架：从单帧提取RGB特征与深度线索以增强空间推理，并引入长期先验上下文（来自视觉-语言模型的文本总结或单帧动作识别器的预测）以提供语义背景，从而在无须视频时间聚合的情况下进行动作预期。

Result: 在三个装配类数据集（IKEA-ASM、Meccano、Assembly101）上进行评估，结果显示AAG在许多设置下能与视频聚合基线及部分最新方法相当，证明单帧多模态方法的可行性。

Conclusion: 该论文表明在许多装配类任务中，通过单帧多模态信息（RGB、深度和长时上下文）即可实现与基于视频聚合的方法相近的动作预期性能，尤其在任务复杂度较低或具有明显语义线索的场景中效果显著。

Abstract: Anticipating actions before they occur is a core challenge in action understanding research. While conventional methods rely on extracting and aggregating temporal information from videos, as humans we can often predict upcoming actions by observing a single moment from a scene, when given sufficient context. Can a model achieve this competence? The short answer is yes, although its effectiveness depends on the complexity of the task. In this work, we investigate to what extent video aggregation can be replaced with alternative modalities. To this end, based on recent advances in visual feature extraction and language-based reasoning, we introduce AAG, a method for Action Anticipation at a Glimpse. AAG combines RGB features with depth cues from a single frame for enhanced spatial reasoning, and incorporates prior action information to provide long-term context. This context is obtained either through textual summaries from Vision-Language Models, or from predictions generated by a single-frame action recognizer. Our results demonstrate that multimodal single-frame action anticipation using AAG can perform competitively compared to both temporally aggregated video baselines and state-of-the-art methods across three instructional activity datasets: IKEA-ASM, Meccano, and Assembly101.

</details>


### [92] [Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study](https://arxiv.org/abs/2512.02850)
*Vishal Dubey,Pallavi Tyagi*

Main category: cs.CV

TL;DR: 微调提升了在训练生成器上的检测，但会降低对印度/南亚身份保留AIGC的泛化能力，需开发保持身份代表性的适应方法与印度感知的基准。


<details>
  <summary>Details</summary>
Motivation: 研究现实中身份保持的图像编辑（如换衣、换背景）是否能被现有检测器鲁棒识别，尤其关注印度/南亚等代表性不足群体的性能差异与公平性。

Method: 作者构建了印度聚焦的训练集（从FairFD和HAV-DF抽取）并用商用生成器（Gemini、ChatGPT）生成两个独立的IP-AIGC测试集（HIDF-img-ip-genai、HIDF-vid-ip-genai）；评估了两种SOTA检测器（AIDE、Effort）在预训练和微调下的表现，报告AUC、AP、EER和准确率。

Result: 微调在训练域内显著提升性能（例如Effort AUC从0.739到0.944），但在对抗IP-AIGC的保留身份测试集上表现显著下降（例如AIDE AUC从0.923降至0.563），表明模型过拟合训练生成器的特征。对非IP的HIDF图像，预训练性能仍高，说明问题特异于身份保持编辑。

Conclusion: 该论文表明现有AIGC检测器在保留身份（IP-AIGC）场景对印度/南亚人脸存在泛化不足和公平性问题。

Abstract: Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.

</details>


### [93] [RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association](https://arxiv.org/abs/2512.02860)
*Abdul Hannan,Furqan Malik,Hina Jabbar,Syed Suleman Sadiq,Mubashir Noman*

Main category: cs.CV

TL;DR: 提出基于融合与正交投影的跨模态对齐方法，专注提取两模态之间的语义相关信息，在英德评测集上取得EER 33.1并获FAME 2026第三名。


<details>
  <summary>Details</summary>
Motivation: 多语种场景下人脸-声音关联更具挑战性，语言差异可能导致模态间语义不一致，现有方法在跨语种迁移和语义对齐方面表现有限，需重访融合策略和投影方法以聚焦相关语义信息。

Method: 基于特征融合和正交投影的框架：首先对人脸与语音模态提取表示，然后通过融合模块结合双模态信息，应用正交投影去除冗余或无关成分，从而强化语义相关性，最后在英语-德语配对集上进行评估。

Result: 在英文-德文数据划分上方法表现良好，在FAME 2026挑战中排名第三，达到EER 33.1%。

Conclusion: 本文提出在多语环境中进行人脸-声音关联任务的改进方法，通过融合和正交投影聚焦于跨模态的语义相关信息，能在英语-德语数据上取得较好效果，并在FAME 2026挑战中取得第三名（EER 33.1%）。

Abstract: Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.

</details>


### [94] [MICCAI STSR 2025 Challenge: Semi-Supervised Teeth and Pulp Segmentation and CBCT-IOS Registration](https://arxiv.org/abs/2512.02867)
*Yaqi Wang,Zhi Li,Chengyu Wu,Jun Liu,Yifan Zhang,Jialuo Chen,Jiaxue Ni,Qian Luo,Jin Liu,Can Han,Changkai Ji,Zhi Qin Tan,Ajo Babu George,Liangyu Chen,Qianni Zhang,Dahong Qian,Shuai Wang,Huiyu Zhou*

Main category: cs.CV

TL;DR: 组织了STSR 2025 MICCAI挑战，评测半监督学习在CBCT与IOS的牙齿与根管分割和跨模态刚性配准。提供标注与未标注数据，顶级方法基于nnU-Net、状态空间模型、PointNetLK等，采用伪标签、一致性正则和几何增强，分割Dice=0.967，配准采用混合神经-经典精修。数据与代码公开。


<details>
  <summary>Details</summary>
Motivation: 临床牙科数字化依赖CBCT与IOS，但标注稀缺阻碍自动化，需评估半监督学习在少标注情形下的可行性并提供基准数据集。

Method: 分割任务：基于nnU-Net与Mamba-like State Space Models，结合伪标签生成、教师-学生一致性正则、数据增强与多尺度训练；注册任务：PointNetLK结合可微SVD，使用几何增强、混合神经与经典ICP/刚性精修，部分管道加入端到端微调与自监督损失。

Result: 挑战吸引大量参赛者，顶尖方法在隐藏测试集上分割Dice=0.967、Instance Affinity=0.738；注册方法实现高精度刚性配准。所有数据与代码在GitHub公开。

Conclusion: 半监督方法在牙科CBCT与IOS任务上表现优异：分割可达到接近完美的Dice，配准通过结合学习与经典算法实现可靠对齐。公开数据与代码促进可复现研究与未来进展。

Abstract: Cone-Beam Computed Tomography (CBCT) and Intraoral Scanning (IOS) are essential for digital dentistry, but annotated data scarcity limits automated solutions for pulp canal segmentation and cross-modal registration. To benchmark semi-supervised learning (SSL) in this domain, we organized the STSR 2025 Challenge at MICCAI 2025, featuring two tasks: (1) semi-supervised segmentation of teeth and pulp canals in CBCT, and (2) semi-supervised rigid registration of CBCT and IOS. We provided 60 labeled and 640 unlabeled IOS samples, plus 30 labeled and 250 unlabeled CBCT scans with varying resolutions and fields of view. The challenge attracted strong community participation, with top teams submitting open-source deep learning-based SSL solutions. For segmentation, leading methods used nnU-Net and Mamba-like State Space Models with pseudo-labeling and consistency regularization, achieving a Dice score of 0.967 and Instance Affinity of 0.738 on the hidden test set. For registration, effective approaches combined PointNetLK with differentiable SVD and geometric augmentation to handle modality gaps; hybrid neural-classical refinement enabled accurate alignment despite limited labels. All data and code are publicly available at https://github.com/ricoleehduu/STS-Challenge-2025 to ensure reproducibility.

</details>


### [95] [Taming Camera-Controlled Video Generation with Verifiable Geometry Reward](https://arxiv.org/abs/2512.02870)
*Zhaoqing Wang,Xiaobo Xia,Zhuolin Bie,Jinlin Liu,Dongdong Yu,Jia-Wang Bian,Changhu Wang*

Main category: cs.CV

TL;DR: 提出一种基于段级几何对齐奖励的在线RL后训练方法，能在多样数据上提升视频生成器的相机控制能力和几何/视觉表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散相机控制多依赖监督微调，缺乏在线RL后训练以进一步提升控制精度与几何一致性；需要稠密且可验证的奖励信号以有效指导生成模型。

Method: 设计了可验证的几何奖励：对生成与参考视频估计三维相机轨迹，将轨迹分割为短段，计算段级相对位姿；奖励通过比较生成-参考每对段的对齐得分，提供密集段级反馈以缓解稀疏奖励问题并提高优化效率。并构建包含大幅度相机运动和多样场景动态的数据集，进行在线RL后训练。

Result: 在线RL后训练在相机控制精度、几何一致性与视觉质量等多方面显著优于SFT基线。

Conclusion: 本工作提出了在线强化学习（RL）后训练框架，以优化预训练视频生成器实现精确的相机控制，优于仅用监督微调（SFT）的做法。

Abstract: Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.

</details>


### [96] [MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm](https://arxiv.org/abs/2512.02895)
*Wei Chen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Zide Liu,Xuhao Pan,Chang Ren,Xudong Rao,Chenfeng Wang,Tao Wei,Chengjun Yu,Pengfei Yu,Yufei Zheng,Chunpeng Zhou,Pan Zhou,Xuhan Zhu*

Main category: cs.CV

TL;DR: MindGPT-4ov提出信息密度驱动的数据生成、课程化微调与混合RL的通用后训练范式，配合工程优化，在低成本下显著提升多模态模型的基线能力与领域泛化，实验证明优于多项基准并具备良好工业部署潜力。


<details>
  <summary>Details</summary>
Motivation: 动机是以低成本提升多模态大模型的基础能力和领域泛化能力，构建一套可工程化、可复现的后训练（post-training）范式，从数据、训练到部署一体化解决领域适配难题，推动学术到工业的平滑过渡。

Method: 方法包括三大核心：1) 基于信息密度的数据生成方案，配合双维树状标签体系实现跨域高质量数据自动化构建；2) 协同课程化监督微调策略，按难度/领域顺序注入领域知识以保留模型通用性；3) 混合强化学习范式，在提升推理能力的同时，通过多目标优化（多样性探索、多模态感知保持、回答简洁性）实现平衡。此外辅以5D并行训练、算子优化和推理量化等工程优化。

Result: 在MMBench、MMStar、MathVision、MathVista等多模态与视觉数学基准上优于现有SOTA模型；在垂直领域任务中用户体验显著提升。Qwen3-VL变体的权重、数据与代码将开源，便于社区复现与扩展。

Conclusion: MindGPT-4ov提出的通用后训练范式在多模态能力和领域泛化上表现出显著提升，结合信息密度的数据生成、协同课程化微调与混合强化学习，有效平衡领域知识注入与通用能力保持。基础设施优化使训练与推理成本降低，实验证明在多个多模态基准上优于现有方法，并具备良好的工程部署潜力。

Abstract: We present MindGPT-4ov, a multimodal large language model (MLLM) that introduces a general post-training paradigm spanning data production, model training, and efficient deployment. It achieves state-of-the-art performance across multiple benchmarks at low cost, effectively enhancing the foundational capabilities of MLLMs and the generalization ability. Focusing on data construction, supervised fine-tuning strategies, and multimodal reinforcement learning methods, this work proposes three key innovations: (1) An information density-based data generation scheme, integrated with a dual-dimensional tree-structured label system, enabling automated generation of high-quality cross-domain data. (2) A collaborative curriculum supervised fine-tuning approach that balances the injection of domain-specific knowledge with the preservation of general capabilities. (3) A hybrid reinforcement learning paradigm that enhances reasoning ability while simultaneously addressing multi-objective optimization such as diversity exploration, maintenance of multimodal perception, and response conciseness. Moreover, we implement a series of infrastructure optimizations, such as 5D parallel training, operator optimization, and inference quantization to enhance training and inference efficiency while reducing the cost of domain adaptation. Experimental results demonstrate that the MindGPT-4ov model outperforms state-of-the-art models on benchmarks such as MMBench, MMStar, MathVision, and MathVista. In addition, MindGPT-4ov also demonstrates superior user experience in vertical domain tasks, enabling a seamless transition from academic research to industrial deployment. MindGPT-4ov provides a general post-training paradigm applicable to a wide range of MLLMs. The model weights, datasets, and code for the Qwen3-VL-based variants will be recently open-sourced to support the community's development of MLLMs.

</details>


### [97] [Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models](https://arxiv.org/abs/2512.02897)
*Pierpaolo Serio,Giulio Pisaneschi,Andrea Dan Ryals,Vincenzo Infantino,Lorenzo Gentilini,Valentina Donzella,Lorenzo Pollini*

Main category: cs.CV

TL;DR: 研究表明，针对LiDAR到图像的合理投影设计能显著影响并提升基于视觉模型的地点识别表现，成为端到端3D学习的有效替代方案。


<details>
  <summary>Details</summary>
Motivation: 动机是探究在已有强大视觉基础模型下，是否通过设计合理的2D投影即可充分利用LiDAR信息完成地点识别，从而避免复杂或昂贵的端到端3D网络训练与部署，兼顾性能与实时性。

Method: 提出一个模块化检索流水线，固定骨干网络、特征聚合及评估协议，仅改变LiDAR→图像的投影方式；在多个数据集和部署场景上比较不同投影，使用一致的几何与结构通道输入，定量评估判别力、对环境变化的鲁棒性与实时性。并将最佳投影整合进实际地点识别策略进行验证。

Result: 实验表明：不同投影在判别能力、对光照/天气/视角变化的鲁棒性以及计算效率上存在显著差异；某些精心设计的投影能在多个数据集上达到或接近端到端3D方法的效果，并能在实际策略中带来可用的性能提升。

Conclusion: 本论文结论指出：在使用先进视觉基础模型进行度量级地点识别时，LiDAR点云到图像的二维投影方式对性能有显著影响。通过设计恰当的投影（保留几何与结构通道），可以在很多场景中替代端到端3D学习方法，实现具有竞争力的判别能力、鲁棒性和实时性。

Abstract: This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.

</details>


### [98] [Glance: Accelerating Diffusion Models with 1 Sample](https://arxiv.org/abs/2512.02899)
*Zhuobai Dong,Rui Zhao,Songjie Wu,Junchao Yi,Linjie Li,Zhengyuan Yang,Lijuan Wang,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 通过Slow-LoRA和Fast-LoRA两个轻量适配器，在相位感知策略下对扩散模型进行差异化加速，达到高效（最多5x）且泛化好的少步推理效果，训练资源成本极低。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型推理成本高且需大量步骤，传统少步蒸馏需大量重训练且泛化差。作者提出相位差异化加速，利用不同阶段的冗余程度不同来分配加速比例。

Method: 设计并训练两种轻量级LoRA适配器（Slow-LoRA和Fast-LoRA），分别针对慢速语义去噪阶段和快速冗余去噪阶段；在基模型上插入LoRA而非重训学生模型，从而实现高效加速与良好泛化。

Result: 在多项基准上实现最多5倍加速，同时保持可比较的视觉质量。LoRA适配器仅使用1个样本并在单个V100上训练约1小时即可训练完成，且在未见提示上表现出良好泛化。

Conclusion: 本文提出了相位感知的加速策略，通过在不同去噪阶段采用不同加速比例以减少推理步骤同时保持图像质量。

Abstract: Diffusion models have achieved remarkable success in image generation, yet their deployment remains constrained by the heavy computational cost and the need for numerous inference steps. Previous efforts on fewer-step distillation attempt to skip redundant steps by training compact student models, yet they often suffer from heavy retraining costs and degraded generalization. In this work, we take a different perspective: we accelerate smartly, not evenly, applying smaller speedups to early semantic stages and larger ones to later redundant phases. We instantiate this phase-aware strategy with two experts that specialize in slow and fast denoising phases. Surprisingly, instead of investing massive effort in retraining student models, we find that simply equipping the base model with lightweight LoRA adapters achieves both efficient acceleration and strong generalization. We refer to these two adapters as Slow-LoRA and Fast-LoRA. Through extensive experiments, our method achieves up to 5 acceleration over the base model while maintaining comparable visual quality across diverse benchmarks. Remarkably, the LoRA experts are trained with only 1 samples on a single V100 within one hour, yet the resulting models generalize strongly on unseen prompts.

</details>


### [99] [MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding](https://arxiv.org/abs/2512.02906)
*Fan Yang,Kaihao Zhang*

Main category: cs.CV

TL;DR: MRD: fuse semantic similarity from multiple resolutions and apply open-vocabulary sliding-window detector to better localize and understand objects in high-resolution images without extra training.


<details>
  <summary>Details</summary>
Motivation: Crop-based processing fragments objects across crops and different object sizes need different resolutions; semantic similarity biased when objects split.

Method: Training-free framework: compute semantic similarity maps at multiple resolutions via pretrained RAG; fuse maps to preserve object integrity; use open-vocabulary detector with sliding-window for global localization; select relevant crops for MLLMs.

Result: Experiments on high-resolution benchmarks with various MLLMs show MRD outperforms baselines in localization and VQA metrics, handling varied object sizes and reducing irrelevant info.

Conclusion: MRD improves high-resolution image understanding by fusing multi-resolution semantic maps and using OVD for global localization, yielding better localization and QA on benchmarks.

Abstract: Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.

</details>


### [100] [DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation](https://arxiv.org/abs/2512.02931)
*Ying Yang,Zhengyao Lv,Tianlin Pan,Haofan Wang,Binxin Yang,Hubery Yin,Chen Li,Chenyang Si*

Main category: cs.CV

TL;DR: 提出 DiverseAR：用自适应 logits 缩放和平衡的能量基路径搜索，提升位级自回归模型的多样性并保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 位级视觉标记器中存在样本多样性不足的问题，主要由二分类建模限制预测空间和过于尖锐的 logits 分布导致采样塌缩。

Method: 提出自适应 logits 缩放机制，在采样过程中动态调整二值输出分布的尖锐度以平滑预测；并结合能量基生成路径搜索算法，避免采样到低置信度标记以维护保真度。

Result: 通过在多个实验上的验证，DiverseAR 显著提升了位级自回归图像生成的样本多样性，同时视觉质量无明显下降。

Conclusion: DiverseAR 有效提高了基于位（bitwise）自回归视觉标记器的样本多样性，同时保持视觉质量。

Abstract: In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.

</details>


### [101] [EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis](https://arxiv.org/abs/2512.02932)
*Yancheng Zhang,Guangyu Sun,Chen Chen*

Main category: cs.CV

TL;DR: Proposes EGGS, combining 2D and 3D Gaussian splats with hybrid rasterization, adaptive type exchange, and frequency-decoupled optimization, achieving better appearance, geometry, and speed than existing NVS methods.


<details>
  <summary>Details</summary>
Motivation: 3DGS provides high fidelity but inconsistent multi-view geometry; 2DGS enforces consistency but loses texture detail. Need a representation that combines strengths of both.

Method: Hybrid Gaussian Rasterization for unified rendering; Adaptive Type Exchange for dynamic switching between 2D and 3D Gaussians; Frequency-Decoupled Optimization; CUDA-accelerated implementation for efficiency.

Result: EGGS outperforms prior methods in rendering quality, geometric accuracy, and computational efficiency on extensive experiments.

Conclusion: EGGS achieves a balance between appearance fidelity and geometric accuracy by combining 2D and 3D Gaussian splats into a hybrid representation, validated by experiments showing improved rendering quality, geometry, and efficiency.

Abstract: Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.

</details>


### [102] [LoVoRA: Text-guided and Mask-free Video Object Removal and Addition with Learnable Object-aware Localization](https://arxiv.org/abs/2512.02933)
*Zhihan Xiao,Lin Liu,Yixin Gao,Xiaopeng Zhang,Haoxuan Che,Songping Mai,Qi Tian*

Main category: cs.CV

TL;DR: LoVoRA是一种无需外部掩码的文本引导视频对象删/加框架，依靠数据合成与可学习的对象定位+扩散掩码预测实现端到端时空一致编辑，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部掩码或参考图像，限制了可扩展性与泛化；同时视频编辑对空间精度与时间一致性要求高，需新的无掩码解决方案。

Method: 构建了结合图像到视频翻译、基于光流的掩码传播与视频修补的数据构建流水线；提出可学习的对象感知定位机制与扩散掩码预测器（Diffusion Mask Predictor），为插入与删除任务提供稠密时空监督并实现掩码预测，支持端到端编辑。

Result: 在大量实验与人工评估中，LoVoRA在编辑质量、时空一致性与自动性方面优于基线方法，且用户偏好度较高。

Conclusion: LoVoRA在无需外部掩码或参考图像的情况下，通过可学习的对象感知定位机制实现了高质量的文本引导视频中对象删除与添加，能保持时空一致性并实现端到端推理。

Abstract: Text-guided video editing, particularly for object removal and addition, remains a challenging task due to the need for precise spatial and temporal consistency. Existing methods often rely on auxiliary masks or reference images for editing guidance, which limits their scalability and generalization. To address these issues, we propose LoVoRA, a novel framework for mask-free video object removal and addition using object-aware localization mechanism. Our approach utilizes a unique dataset construction pipeline that integrates image-to-video translation, optical flow-based mask propagation, and video inpainting, enabling temporally consistent edits. The core innovation of LoVoRA is its learnable object-aware localization mechanism, which provides dense spatio-temporal supervision for both object insertion and removal tasks. By leveraging a Diffusion Mask Predictor, LoVoRA achieves end-to-end video editing without requiring external control signals during inference. Extensive experiments and human evaluation demonstrate the effectiveness and high-quality performance of LoVoRA.

</details>


### [103] [Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench](https://arxiv.org/abs/2512.02942)
*Lanxiang Hu,Abhilash Shankarampeta,Yixin Huang,Zilin Dai,Haoyang Yu,Yujie Zhao,Haoqiang Kang,Daniel Zhao,Tajana Rosing,Hao Zhang*

Main category: cs.CV

TL;DR: 提出VideoScience-Bench，包含200个本科水平科学场景提示，覆盖14个主题；通过专家注释和VLM评估验证，目前视频模型在科学推理任务上仍不足，VLM评判与人类评价高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要集中在物理常识，无法充分评估模型的科学推理能力；因此需要一个能测试模型在真实科学规律下零-shot推理和生成正确物理/化学现象的基准。

Method: 构建200条精心设计的提示，覆盖14个主题和103个概念，结合专家注释与自动评估（使用视觉语言模型作为评判者）在T2V和I2V设置下，按五个维度评估七种最先进视频模型。

Result: 在专家注释和VLM自动评估下，发现当前视频模型在科学推理上表现有限，VLM作为评判者与人工评估高度相关，表明自动评估可行。

Conclusion: 该论文提出了VideoScience-Bench基准，用于评估视频模型在本科水平的科学推理能力，特别是对物理和化学现象的理解和推断。

Abstract: The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.

</details>


### [104] [Layout Anything: One Transformer for Universal Room Layout Estimation](https://arxiv.org/abs/2512.02952)
*Md Sohag Mia,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: 提出基于OneFormer的端到端室内布局估计方法，结合拓扑感知的数据增强与可微分几何损失，去除后处理并在精度与速度上取得优异表现，适合AR与3D重建应用。


<details>
  <summary>Details</summary>
Motivation: 减少传统布局估计依赖繁复后处理与启发式优化，提升几何一致性和边界精确度，同时满足实时或近实时的应用需求（如AR与大规模3D重建）。

Method: 在OneFormer的task-conditioned查询和对比学习基础上引入两大模块：1) 布局退化策略：对训练数据进行拓扑感知变换以保持曼哈顿世界约束；2) 可微分几何损失：在训练中直接约束平面一致性和边界锐利度。整体为端到端训练。

Result: 在多个基准数据集上实现或接近SOTA：LSUN上PE=5.43% CE=4.02%；Hedau上PE=7.04% CE=5.17%；Matterport3D-Layout上PE=4.03% CE=3.15%；推理速度114ms。

Conclusion: 该论文提出了一种基于Transformer的室内平面布局估计框架“Layout Anything”，通过改造OneFormer架构实现从语义分割到几何结构预测的迁移，消除了复杂后处理并实现高效推理。

Abstract: We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.

</details>


### [105] [A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems](https://arxiv.org/abs/2512.02965)
*Yuhan Chen,Yicui Shi,Guofa Li,Guangrui Bai,Jinyuan Shao,Xiangfei Huang,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出UltraFast-LieNET，使用极少参数的位移卷积与多尺度残差块，在车载低光增强场景下实现实时且高质量的图像增强。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强算法计算开销大，不适用于车载场景，故需极小参数量同时保持足够增强效果的实时算法。

Method: 引入只有12个可学习参数的Dynamic Shifted Convolution (DSConv)，通过不同位移距离组合成Multi-scale Shifted Residual Block (MSRB)以扩展感受野；网络采用残差结构并配合多层梯度感知损失以稳定训练；支持灵活参数配置，最小模型仅36参数。

Result: 在LOLI-Street数据集上，模型以180参数实现PSNR 26.51 dB，比现有方法高4.6 dB，并在四个基准数据集上验证了在资源受限下的实时性与增强质量平衡。

Conclusion: 该论文提出了UltraFast-LieNET，一种极轻量多尺度位移卷积网络，在有限计算资源下实现实时低光增强，实验表明在LOLl-Street等数据集上性能优越。

Abstract: In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET

</details>


### [106] [BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection](https://arxiv.org/abs/2512.02972)
*Guowen Zhang,Chenhang He,Liyi Chen,Lei Zhang*

Main category: cs.CV

TL;DR: BEVDilation通过把图像BEV作为隐式语义/稠密引导，设计体素稠化与语义引导扩散模块，实现以LiDAR为主、对深度噪声更鲁棒且性能更优的BEV融合3D检测框架。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR-视觉BEV融合方法因传感器几何精度差异直接融合会因图像深度误差导致空间错配并降低性能，需一种以LiDAR为主并借助图像补充语义与稠密信息的融合机制。

Method: 提出Sparse Voxel Dilation Block以利用图像先验密化前景体素，和Semantic-Guided BEV Dilation Block以在LiDAR特征扩散过程中引入图像语义引导与长程上下文，同时总体保持LiDAR为主的融合策略以抵抗深度噪声。

Result: 在nuScenes基准上，BEVDilation在性能上优于现有最先进方法，并在计算效率上具有竞争力，同时对深度噪声表现出更强鲁棒性。

Conclusion: 本文提出BEVDilation，一种以LiDAR为中心的BEV融合框架，通过将图像BEV特征作为隐式引导而非直接拼接，减少图像深度误差导致的空间对齐问题，并利用图像语义先验缓解点云稀疏与语义匮乏，从而提升3D检测性能。

Abstract: Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.

</details>


### [107] [Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities](https://arxiv.org/abs/2512.02973)
*Yuan Xiong,Ziqi Miao,Lijun Li,Chen Qian,Jie Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出了一种图像中心的越狱攻击（CIA），通过多代理和四种可视化策略在图像中隐蔽植入有害查询，结合上下文增强与毒性混淆，在主流MLLM上获得高毒性分和高ASR，表明视觉模态是重要的攻击向量。


<details>
  <summary>Details</summary>
Motivation: 现有攻击多聚焦文本-图像交互，把视觉模态当作次要提示，未充分利用图像承载复杂语境信息的潜力，因而需要一种以图像为中心的攻击方法。

Method: 提出Contextual Image Attack (CIA)，采用多代理系统在图像中用四种可视化策略隐蔽嵌入有害查询，并结合上下文元素增强和自动毒性混淆技术。

Result: 在MMSafetyBench-tiny上，CIA对GPT-4o和Qwen2.5-VL-72B分别取得平均毒性分4.73和4.83，ASR分别为86.31%和91.07%，显著优于先前方法。

Conclusion: 该论文证明图像可作为有效的越狱向量，通过在视觉上下文中隐蔽嵌入有害查询，显著提升对多模态大模型的攻击成功率。

Abstract: While Multimodal Large Language Models (MLLMs) show remarkable capabilities, their safety alignments are susceptible to jailbreak attacks. Existing attack methods typically focus on text-image interplay, treating the visual modality as a secondary prompt. This approach underutilizes the unique potential of images to carry complex, contextual information. To address this gap, we propose a new image-centric attack method, Contextual Image Attack (CIA), which employs a multi-agent system to subtly embeds harmful queries into seemingly benign visual contexts using four distinct visualization strategies. To further enhance the attack's efficacy, the system incorporate contextual element enhancement and automatic toxicity obfuscation techniques. Experimental results on the MMSafetyBench-tiny dataset show that CIA achieves high toxicity scores of 4.73 and 4.83 against the GPT-4o and Qwen2.5-VL-72B models, respectively, with Attack Success Rates (ASR) reaching 86.31\% and 91.07\%. Our method significantly outperforms prior work, demonstrating that the visual modality itself is a potent vector for jailbreaking advanced MLLMs.

</details>


### [108] [InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration](https://arxiv.org/abs/2512.02981)
*Zhongyu Yang,Yingfang Yuan,Xuanming Jiang,Baoyi An,Wei Pang*

Main category: cs.CV

TL;DR: InEx：无需训练的多智能体框架，先用熵引导内省减少不确定性，再通过跨模态多智能体外部验证迭代修正，显著降低幻觉并提高可靠性（+4%–27%）。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs多模态幻觉问题，通过模仿人类先内省再外部验证的决策流程，降低不确定性并提升可信度。

Method: 基于熵的不确定性估计指导内省推理生成初始响应；随后通过编辑代理与若干自反思代理进行跨模态、多角度的外部验证与迭代修正，整个流程无需额外训练。

Result: 提出InEx框架：训练免费、多智能体系统，结合基于熵的不确定性估计进行内省推理，并通过编辑与自我反思等外部多智能体协作迭代验证和修正响应，在多项基准上提升4%-27%。

Conclusion: InEx能自主减轻MLLM幻觉问题，增强鲁棒性与可靠性，为无需额外训练的多智能体纠错提供有效范式。

Abstract: Hallucination remains a critical challenge in large language models (LLMs), hindering the development of reliable multimodal LLMs (MLLMs). Existing solutions often rely on human intervention or underutilize the agent's ability to autonomously mitigate hallucination. To address these limitations, we draw inspiration from how humans make reliable decisions in the real world. They begin with introspective reasoning to reduce uncertainty and form an initial judgment, then rely on external verification from diverse perspectives to reach a final decision. Motivated by this cognitive paradigm, we propose InEx, a training-free, multi-agent framework designed to autonomously mitigate hallucination. InEx introduces internal introspective reasoning, guided by entropy-based uncertainty estimation, to improve the reliability of the decision agent's reasoning process. The agent first generates a response, which is then iteratively verified and refined through external cross-modal multi-agent collaboration with the editing agent and self-reflection agents, further enhancing reliability and mitigating hallucination. Extensive experiments show that InEx consistently outperforms existing methods, achieving 4%-27% gains on general and hallucination benchmarks, and demonstrating strong robustness.

</details>


### [109] [U4D: Uncertainty-Aware 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2512.02982)
*Xiang Xu,Ao Liang,Youquan Liu,Linfeng Li,Lingdong Kong,Ziwei Liu,Qingshan Liu*

Main category: cs.CV

TL;DR: 通过估计空间不确定性并采取“先重构高不确定区域、再条件完成其余区域”的两阶段扩散生成，结合MoST时空融合模块，U4D实现了更真实且时序一致的4D LiDAR世界建模。


<details>
  <summary>Details</summary>
Motivation: 现有生成框架对场景各区域一视同仁，忽视实际场景中语义/几何不确定性差异，导致复杂区域出现伪影和时序不稳，限制了4D世界建模的逼真性与可靠性。

Method: 方法包括：从预训练分割模型估计空间不确定性图；基于“难到易”两阶段生成：先对高熵（不确定）区域进行重构，再在不确定性条件下完成剩余区域；引入MoST（时空混合）模块，在扩散模型中自适应融合空间与时间表示以保证时序一致性。

Result: U4D在多项指标上产生几何逼真且时间一致的LiDAR序列，减少了复杂区域的伪影，提升了4D建模的可靠性与用于自动驾驶/仿真的适用性。

Conclusion: 本文提出的U4D通过不确定性感知来提升4D LiDAR建模的几何精确性与时间一致性，实验显示其在复杂/模糊区域生成质量和时序稳定性上优于统一处理方法。

Abstract: Modeling dynamic 3D environments from LiDAR sequences is central to building reliable 4D worlds for autonomous driving and embodied AI. Existing generative frameworks, however, often treat all spatial regions uniformly, overlooking the varying uncertainty across real-world scenes. This uniform generation leads to artifacts in complex or ambiguous regions, limiting realism and temporal stability. In this work, we present U4D, an uncertainty-aware framework for 4D LiDAR world modeling. Our approach first estimates spatial uncertainty maps from a pretrained segmentation model to localize semantically challenging regions. It then performs generation in a "hard-to-easy" manner through two sequential stages: (1) uncertainty-region modeling, which reconstructs high-entropy regions with fine geometric fidelity, and (2) uncertainty-conditioned completion, which synthesizes the remaining areas under learned structural priors. To further ensure temporal coherence, U4D incorporates a mixture of spatio-temporal (MoST) block that adaptively fuses spatial and temporal representations during diffusion. Extensive experiments show that U4D produces geometrically faithful and temporally consistent LiDAR sequences, advancing the reliability of 4D world modeling for autonomous perception and simulation.

</details>


### [110] [GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection](https://arxiv.org/abs/2512.02991)
*Md Sohag Mia,Md Nahid Hasan,Tawhid Ahmed,Muhammad Abdullah Adnan*

Main category: cs.CV

TL;DR: GraphFusion3D通过ACMT融合图像与点云、GRM进行多尺度图注意力的邻域推理与级联解码，显著提高了室内3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 点云数据稀疏、结构不完整和语义信息有限；难以捕捉远处对象间的上下文关系，需通过多模态信息和图结构建模增强表示。

Method: 引入自适应跨模态Transformer(ACMT)将图像特征自适应地融合到点表示中；提出图推理模块(GRM)利用多尺度图注意力对候选框间的空间和特征相似性进行加权建模；采用级联解码器进行多阶段精细化预测。

Result: 在SUN RGB-D和ScanNetV2上分别取得显著提升：SUN RGB-D达到70.6% AP25和51.2% AP50；ScanNetV2达到75.1% AP25和60.8% AP50。

Conclusion: 提出的GraphFusion3D通过多模态融合和高级特征学习，有效提升了点云三维检测性能。

Abstract: Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\% AP$_{25}$ and 51.2\% AP$_{50}$) and ScanNetV2 (75.1\% AP$_{25}$ and 60.8\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.

</details>


### [111] [TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond](https://arxiv.org/abs/2512.02993)
*Yifei Zeng,Yajie Bao,Jiachen Qian,Shuang Wu,Youtian Lin,Hao Zhu,Buyu Li,Feihu Zhang,Xun Cao,Yao Yao*

Main category: cs.CV

TL;DR: 提出TEXTRIX：基于潜在3D属性网格和稀疏注意力扩散Transformer的原生3D纹理与语义生成框架，避免多视图融合问题，实验证明在纹理合成与精细分割上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于多视图融合的3D纹理生成方法常受视图间不一致和复杂表面覆盖不全的限制，导致生成质量和完整性不足，因而需要一种能够在原生3D表示上生成高保真纹理并支持下游精细分割的方案。

Method: 在体素化的3D属性网格上学习潜在表示，并设计了一个扩散Transformer（Diffusion Transformer）结合稀疏注意力机制来生成纹理/语义属性，直接在体积空间进行颜色与语义预测，省去多视图融合步骤。

Result: 在大量实验中，TEXTRIX在纹理生成和3D部件分割上均达到或超过现有最先进方法，生成无缝高保真纹理，并实现具有精确边界的高精度分割。

Conclusion: TEXTRIX提出了一种原生3D属性生成框架，通过在体素/体积空间上构建潜在3D属性网格并使用具备稀疏注意力的扩散Transformer，直接对3D模型上色，从根源上避免多视图融合带来的视图不一致和覆盖不全问题，且可自然扩展为高精度3D语义分割。

Abstract: Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.

</details>


### [112] [DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling](https://arxiv.org/abs/2512.03000)
*Kairun Wen,Yuzhi Huang,Runyu Chen,Hui Zheng,Yunlong Lin,Panwang Pan,Chenxin Li,Wenyan Cong,Jian Zhang,Junbin Lu,Chenguo Lin,Dilin Wang,Zhicheng Yan,Hongyu Xu,Justin Theiss,Yue Huang,Xinghao Ding,Rakesh Ranjan,Zhiwen Fan*

Main category: cs.CV

TL;DR: DynamicVerse 用大模型与窗口化 BA+全局优化将互联网单目长视频转为度量尺度的 4D 多模态数据集（100K+ 视频），显著提升了深度、位姿与内参估计的物理尺度准确性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖受限模拟器或基于传统 SfM 的上尺度标注，并缺乏丰富描述，无法支持基础模型从互联网单目视频准确理解真实世界动态与度量信息。

Method: 利用大视觉模型、几何模型与多模态模型解析度量尺度静态几何、动态运动、实例掩码与整体描述；结合窗口化 Bundle Adjustment 与全局优化，将长时序真实视频转换为 4D 多模态表示。

Result: 构建了包含 100K+ 视频、800K+ 注释掩码与 10M+ 帧的大规模数据集 DynamicVerse；在视频深度估计、相机位姿估计与相机内参估计三项基准任务上，展示了在度量尺度恢复与全局准确性方面优于现有方法的性能。

Conclusion: DynamicVerse 提供了一个基于真实单目视频的物理量级、多模态 4D 世界建模框架，能更准确地恢复度量尺度的静态几何、动态运动、实例级掩码与文本描述，从而弥补了现有合成/SfM 数据集的不足。

Abstract: Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.

</details>


### [113] [DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images](https://arxiv.org/abs/2512.03004)
*Xiaoxue Chen,Ziyi Xiong,Yuantao Chen,Gen Li,Nan Wang,Hongcheng Luo,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Hongyang Li,Ya-Qin Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: DGGT是一种无需相机位姿输入的统一前向框架，通过联合预测相机与3D高斯表示并结合动态与寿命模块及扩散渲染精化，实现快速可扩展的4D动态驾驶场景重建，实验证明在多数据集上性能与泛化性领先。


<details>
  <summary>Details</summary>
Motivation: 现有动态驾驶场景重建依赖场景特化优化、已知位姿或短帧窗，速度慢且不可扩展；希望开发一个可扩展、无需位姿先验、能处理长序列和稀疏视图的前向方法以满足自动驾驶训练与评估需求。

Method: 将相机位姿从输入改为模型输出，联合预测每帧的3D高斯图和相机参数；引入轻量动态头用于动态成分分离、寿命(head)用于调制随时间可见性以保持时序一致性；使用扩散基渲染精化来减少运动/插值伪影并提升新视角质量；单次前向推理支持任意视图数和长序列。

Result: 在Waymo、nuScenes、Argoverse2三大数据集训练和测试时，在定制训练和零样本跨数据集迁移下均优于现有方法；随着输入帧数增加能良好扩展；单次推理速度更快并达到SOTA性能。

Conclusion: 提出了一种无位姿依赖的单次前向动态场景重建方法DGGT，能从稀疏无位姿图像直接预测相机参数与3D高斯图并分离动态图与时间可见性，结合扩散渲染修正，提高了稀疏输入下新视角质量与速度，实验在Waymo、nuScenes和Argoverse2上优于前人并具跨数据集泛化能力。

Abstract: Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.

</details>


### [114] [SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting](https://arxiv.org/abs/2512.03010)
*Svenja Strobel,Matthias Innmann,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.CV

TL;DR: 提出SurfFill：基于高斯surfel的LiDAR点云补全方法，通过密度变化启发式识别不确定区域并在这些区域进行受约束的高斯surfel重建与采样，以补全细薄结构与边缘，适用于大规模建筑场景，性能优于已有方法。


<details>
  <summary>Details</summary>
Motivation: LiDAR在平坦区域精度高但常在薄结构和吸光材料处漏采，摄影测量能补充细节但整体精度不如LiDAR。论文动机是结合两者优势，利用相机恢复的细节弥补LiDAR的缺失，同时保留LiDAR在无特征区域的优势。

Method: 分析LiDAR采集的伪影来源，指出光束发散在细薄结构与边缘处造成缺失；提出基于点云密度变化的模糊性启发式方法识别可能缺失区域；对这些区域应用受约束的高斯surfel重建（参考Huang et al. 2024），进行点生长与密化；从重建得到的高斯基元中采样点以补全原始点云；为大规模建筑场景设计分而治之的处理管线。

Result: 在合成与实景数据集的LiDAR点云补全任务上，SurfFill在定量指标与定性视觉效果上均超过了现有重建方法，并能在建筑尺度场景中运行。

Conclusion: 该论文提出了一种将LiDAR与相机重建优势结合的点云补全方法，称为SurfFill，通过识别LiDAR采集中因光束发散导致的不确定区域并在这些区域使用受约束的高斯surfel重建生成额外点以补全扫描，最后通过高斯基元采样完成点云。方法在合成与真实场景的建筑尺度补全任务上表现优于现有方法。

Abstract: LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.

</details>


### [115] [In-Context Sync-LoRA for Portrait Video Editing](https://arxiv.org/abs/2512.03013)
*Sagi Polaczek,Or Patashnik,Ali Mahdavi-Amiri,Daniel Cohen-Or*

Main category: cs.CV

TL;DR: 提出 Sync-LoRA：通过训练在时间上严格同步的配对视频的 in-context LoRA，使基于图像到视频的扩散模型能从编辑后的首帧将外观改动精确传播到整段人像视频，保持帧级同步与身份一致性。


<details>
  <summary>Details</summary>
Motivation: 人像视频编辑需要在广泛修改（外观、表情、添加物体等）与精确保留原始时间行为之间取得平衡，确保每一帧与对应源帧严格同步。

Method: 采用 image-to-video diffusion 框架，编辑由第一帧修改并通过模型传播至全序列；训练阶段使用自动生成并经时间同步筛选的配对视频来学习将源视频的运动线索与第一帧的视觉变化结合；训练对象为小而高质量的同步人像数据集。

Result: 在未见身份和多样编辑上的强泛化能力，展示出高视觉保真度与时间一致性，能稳健处理姿态和表情变化，同时在编辑保真度与运动保持之间实现良好平衡。

Conclusion: Sync-LoRA 在视频人像编辑中通过训练同步的 in-context LoRA，实现在保留原始动作时间对齐的同时进行高质量视觉修改，兼顾编辑一致性和身份保持。

Abstract: Editing portrait videos is a challenging task that requires flexible yet precise control over a wide range of modifications, such as appearance changes, expression edits, or the addition of objects. The key difficulty lies in preserving the subject's original temporal behavior, demanding that every edited frame remains precisely synchronized with the corresponding source frame. We present Sync-LoRA, a method for editing portrait videos that achieves high-quality visual modifications while maintaining frame-accurate synchronization and identity consistency. Our approach uses an image-to-video diffusion model, where the edit is defined by modifying the first frame and then propagated to the entire sequence. To enable accurate synchronization, we train an in-context LoRA using paired videos that depict identical motion trajectories but differ in appearance. These pairs are automatically generated and curated through a synchronization-based filtering process that selects only the most temporally aligned examples for training. This training setup teaches the model to combine motion cues from the source video with the visual changes introduced in the edited first frame. Trained on a compact, highly curated set of synchronized human portraits, Sync-LoRA generalizes to unseen identities and diverse edits (e.g., modifying appearance, adding objects, or changing backgrounds), robustly handling variations in pose and expression. Our results demonstrate high visual fidelity and strong temporal coherence, achieving a robust balance between edit fidelity and precise motion preservation.

</details>


### [116] [Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks](https://arxiv.org/abs/2512.03014)
*Matthew Dutson,Nathan Labiosa,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 作者通过可插拔的稳定器和accuracy-stability-robustness损失，实现了对冻结基模型的高效训练，从而在多种视觉任务上同时提升视频时序稳定性与抗图像腐蚀能力。


<details>
  <summary>Details</summary>
Motivation: 帧级网络逐帧应用于视频时常出现时间不一致（如闪烁），在输入存在时间变化的扰动（噪声、压缩、天气等）时问题更严重，需要一种通用、资源高效的方法提升视频推理的时序稳定性与鲁棒性。

Method: 提出一类可插入到任意架构的稳定器（stability adapters）和一种高效训练流程，训练时冻结原模型，只训练稳定器；引入accuracy-stability-robustness损失并分析其理论性质以指导稳定器训练。

Result: 在去噪（NAFNet）、图像增强（HDRNet）、单目深度（Depth Anything v2）和语义分割（DeepLabv3+）等任务上，方法显著提高了时序稳定性并增强了对多种图像腐蚀（压缩伪影、噪声、恶劣天气等）的鲁棒性，同时保持或提升预测质量。

Conclusion: 该方法通过在现有帧级网络中插入轻量级稳定器并使用冻结基础网络的资源节约训练，实现视频推理的时序稳定性和抗腐蚀性，同时保持或提升预测质量。

Abstract: When applied sequentially to video, frame-based networks often exhibit temporal inconsistency - for example, outputs that flicker between frames. This problem is amplified when the network inputs contain time-varying corruptions. In this work, we introduce a general approach for adapting frame-based models for stable and robust inference on video. We describe a class of stability adapters that can be inserted into virtually any architecture and a resource-efficient training process that can be performed with a frozen base network. We introduce a unified conceptual framework for describing temporal stability and corruption robustness, centered on a proposed accuracy-stability-robustness loss. By analyzing the theoretical properties of this loss, we identify the conditions where it produces well-behaved stabilizer training. Our experiments validate our approach on several vision tasks including denoising (NAFNet), image enhancement (HDRNet), monocular depth (Depth Anything v2), and semantic segmentation (DeepLabv3+). Our method improves temporal stability and robustness against a range of image corruptions (including compression artifacts, noise, and adverse weather), while preserving or improving the quality of predictions.

</details>


### [117] [AutoBrep: Autoregressive B-Rep Generation with Unified Topology and Geometry](https://arxiv.org/abs/2512.03018)
*Xiang Xu,Pradeep Kumar Jayaraman,Joseph G. Lambourne,Yilin Liu,Durvesh Malpure,Pete Meltzer*

Main category: cs.CV

TL;DR: AutoBrep: autoregressive Transformer with unified tokenization for B-Rep generation — encodes geometry and topology, generates watertight, high-fidelity CAD models, supports autocompletion; beats baselines.


<details>
  <summary>Details</summary>
Motivation: Direct end-to-end generation of B-Reps with precise geometry and watertight topology is challenging; need a scalable representation and model to produce valid CAD-ready solids.

Method: Design a unified tokenization encoding surfaces and curves as latent geometry tokens and topology via special reference tokens; organize sequence by breadth-first traversal of face adjacency; autoregressive Transformer predicts next tokens, progressively generating neighboring faces and edges.

Result: AutoBrep outperforms baselines in quality and watertightness, scales to complex solids with good fidelity and speed, and supports B-Rep autocompletion enabling user-controllable CAD generation.

Conclusion: AutoBrep successfully generates high-quality, watertight B-Reps by unifying geometric and topological information into a token sequence and using an autoregressive Transformer.

Abstract: The boundary representation (B-Rep) is the standard data structure used in Computer-Aided Design (CAD) for defining solid models. Despite recent progress, directly generating B-Reps end-to-end with precise geometry and watertight topology remains a challenge. This paper presents AutoBrep, a novel Transformer model that autoregressively generates B-Reps with high quality and validity. AutoBrep employs a unified tokenization scheme that encodes both geometric and topological characteristics of a B-Rep model as a sequence of discrete tokens. Geometric primitives (i.e., surfaces and curves) are encoded as latent geometry tokens, and their structural relationships are defined as special topological reference tokens. Sequence order in AutoBrep naturally follows a breadth first traversal of the B-Rep face adjacency graph. At inference time, neighboring faces and edges along with their topological structure are progressively generated. Extensive experiments demonstrate the advantages of our unified representation when coupled with next-token prediction for B-Rep generation. AutoBrep outperforms baselines with better quality and watertightness. It is also highly scalable to complex solids with good fidelity and inference speed. We further show that autocompleting B-Reps is natively supported through our unified tokenization, enabling user-controllable CAD generation with minimal changes. Code is available at https://github.com/AutodeskAILab/AutoBrep.

</details>


### [118] [Unrolled Networks are Conditional Probability Flows in MRI Reconstruction](https://arxiv.org/abs/2512.03020)
*Kehan Qi,Saumya Gupta,Qingqiao Hu,Weimin Lyu,Chao Chen*

Main category: cs.CV

TL;DR: 将unrolled网络视为流ODE离散化，提出FLAT以ODE对齐约束中间态，达到比扩散模型更快、比unrolled网络更稳的MRI重建。


<details>
  <summary>Details</summary>
Motivation: 现有unrolled网络虽高效但中间步骤参数可自由学习导致演化不稳；扩散模型有理论稳定性但计算开销大。需兼顾稳定性与效率的MRI重建方法。

Method: 证明unrolled网络为条件概率流ODE的离散实现，基于该理论设计Flow-Aligned Training：用ODE离散化公式初始化/约束unrolled参数，并在训练中以理想ODE轨迹为参考对齐中间重建结果。

Result: 在三个MRI数据集上，FLAT在重建质量上能与扩散模型相媲美，且迭代次数最多减少3倍，同时比传统unrolled网络表现出显著更好的稳定性。

Conclusion: 本文将无卷积网络与流ODE联系，提出FLAT，通过ODE离散化导出参数并对齐中间状态，从而提升MRI重建稳定性与收敛速度。

Abstract: Magnetic Resonance Imaging (MRI) offers excellent soft-tissue contrast without ionizing radiation, but its long acquisition time limits clinical utility. Recent methods accelerate MRI by under-sampling $k$-space and reconstructing the resulting images using deep learning. Unrolled networks have been widely used for the reconstruction task due to their efficiency, but suffer from unstable evolving caused by freely-learnable parameters in intermediate steps. In contrast, diffusion models based on stochastic differential equations offer theoretical stability in both medical and natural image tasks but are computationally expensive. In this work, we introduce flow ODEs to MRI reconstruction by theoretically proving that unrolled networks are discrete implementations of conditional probability flow ODEs. This connection provides explicit formulations for parameters and clarifies how intermediate states should evolve. Building on this insight, we propose Flow-Aligned Training (FLAT), which derives unrolled parameters from the ODE discretization and aligns intermediate reconstructions with the ideal ODE trajectory to improve stability and convergence. Experiments on three MRI datasets show that FLAT achieves high-quality reconstructions with up to $3\times$ fewer iterations than diffusion-based generative models and significantly greater stability than unrolled networks.

</details>


### [119] [MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation](https://arxiv.org/abs/2512.03034)
*Youxin Pang,Jiajun Liu,Lingfeng Tan,Yong Zhang,Feng Gao,Xiang Deng,Zhuoliang Kang,Xiaoming Wei,Yebin Liu*

Main category: cs.CV

TL;DR: MAViD通过Conductor-Creator架构结合AR音频与扩散视频生成以及新型融合模块，实现了可控、连贯且长时一致的音视频对话生成与理解.


<details>
  <summary>Details</summary>
Motivation: 现有方法多为非交互式、生成的语音受限且不自然；核心问题是理解与生成能力的整合以及多模态音视频的无缝融合以实现交互式长时对话生成

Method: 提出Conductor负责理解、推理与生成指令（分解为动作与语音），Creator根据指令生成响应；Creator使用AR模型用于音频生成、扩散模型用于视频生成，并采用双DiT结构与新的融合模块以增强连续片段与模态间连接

Result: 大量实验表明MAViD能生成生动且语义连贯的长时对话交互，保持身份、音色与语调一致，并能准确理解用户的多模态查询

Conclusion: MAViD提出了一种分层的Conductor-Creator架构，通过将理解与生成分工、并结合自回归与扩散模型，能够在多模态音视频对话中实现更细粒度控制和长时一致性的生成

Abstract: We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.

</details>


### [120] [ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation](https://arxiv.org/abs/2512.03036)
*Mengchen Zhang,Qi Chen,Tong Wu,Zihan Liu,Dahua Lin*

Main category: cs.CV

TL;DR: 提出BiAudio数据集与ViSAudio端到端双耳音频生成方法，通过conditional flow matching和双分支架构实现准确的时空对齐与空间化，效果优于现有两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频研究多为单声道，缺乏空间沉浸感；现有双耳方法多为先生成单声道再空间化的两阶段流水线，导致误差累积与时空不一致，因此需要端到端解决方案。

Method: 构建约97K的视频—双耳音频配对数据集（BiAudio）；提出ViSAudio框架，采用conditional flow matching和双分支音频生成架构（两分支分别建模音频潜流），并加入条件时空模块以在保持通道一致性的同时保留空间特性。

Result: 在多种客观指标和主观评测中，ViSAudio优于现有方法，能够生成具有空间沉浸感的高质量双耳音频，并能适应视角变化、声源运动和不同声学环境。

Conclusion: 本文提出端到端从静音视频生成双耳空间音频的新任务，通过BiAudio数据集与ViSAudio模型实现更一致的空间化音频生成。

Abstract: Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.

</details>


### [121] [Video4Spatial: Towards Visuospatial Intelligence with Context-Guided Video Generation](https://arxiv.org/abs/2512.03040)
*Zeqi Xiao,Yiwei Zhao,Lingxiao Li,Yushi Lan,Yu Ning,Rahul Garg,Roshni Cooper,Mohammad H. Taghavi,Xingang Pan*

Main category: cs.CV

TL;DR: Video4Spatial证明视频扩散模型在纯视频条件下可实现复杂的视觉空间推理，推动视频生成模型向通用视觉空间理解前进。


<details>
  <summary>Details</summary>
Motivation: 评估视频生成模型是否具备类似人类的视觉空间智能，且仅用视觉数据验证其对几何一致性、语义定位和规划能力的掌握。

Method: 提出Video4Spatial框架：用视频条件化的扩散模型，结合简单有效的设计和数据筛选，仅用视频输入执行两类任务（场景导航与物体定位），不借助深度或位姿等辅助模态。

Result: 在场景导航和物体定位任务上取得强劲表现：能够按摄像机位姿指令保持3D几何一致性、端到端规划导航并定位目标，且能泛化到长时间上下文与域外环境。

Conclusion: 视频扩散模型在仅依赖视频场景上下文时，能够展示出显著的视觉空间推理能力，支持导航规划与目标定位等复杂任务。

Abstract: We investigate whether video generative models can exhibit visuospatial intelligence, a capability central to human cognition, using only visual data. To this end, we present Video4Spatial, a framework showing that video diffusion models conditioned solely on video-based scene context can perform complex spatial tasks. We validate on two tasks: scene navigation - following camera-pose instructions while remaining consistent with 3D geometry of the scene, and object grounding - which requires semantic localization, instruction following, and planning. Both tasks use video-only inputs, without auxiliary modalities such as depth or poses. With simple yet effective design choices in the framework and data curation, Video4Spatial demonstrates strong spatial understanding from video context: it plans navigation and grounds target objects end-to-end, follows camera-pose instructions while maintaining spatial consistency, and generalizes to long contexts and out-of-domain environments. Taken together, these results advance video generative models toward general visuospatial reasoning.

</details>


### [122] [MultiShotMaster: A Controllable Multi-Shot Video Generation Framework](https://arxiv.org/abs/2512.03041)
*Qinghe Wang,Xiaoyu Shi,Baolu Li,Weikang Bian,Quande Liu,Huchuan Lu,Xintao Wang,Pengfei Wan,Kun Gai,Xu Jia*

Main category: cs.CV

TL;DR: 提出 MultiShotMaster：通过两类 RoPE 变体与自动化注释数据管线，扩展单镜头模型实现高可控、多镜头、时空参考注入的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法擅长单镜头生成但难以生成叙事性的多镜头视频，缺乏灵活的镜头安排、跨镜头的一致性以及对参考对象和场景的可控性。

Method: 在预训练单镜头模型中引入 Multi-Shot Narrative RoPE（在镜头切换处施加相位偏移以保留叙事顺序并允许灵活排列）和 Spatiotemporal Position-Aware RoPE（引入参考 token 与定位信号以实现时空参考注入），并构建自动化数据注释管线提取多镜头视频与跨镜头地面实况与参考图像以缓解数据稀缺。

Result: 模型支持文本驱动的跨镜头一致性、定制主体与运动控制以及基于背景的场景定制，同时镜头数量与时长可配置。实验显示在可控性与生成质量上优于基线。

Conclusion: MultiShotMaster 提出了一种通过两种 RoPE 变体扩展单镜头视频模型以实现可控多镜头视频生成的方法，解决了镜头排列灵活性、叙事连贯性和参考注入的挑战。

Abstract: Current video generation techniques excel at single-shot clips but struggle to produce narrative multi-shot videos, which require flexible shot arrangement, coherent narrative, and controllability beyond text prompts. To tackle these challenges, we propose MultiShotMaster, a framework for highly controllable multi-shot video generation. We extend a pretrained single-shot model by integrating two novel variants of RoPE. First, we introduce Multi-Shot Narrative RoPE, which applies explicit phase shift at shot transitions, enabling flexible shot arrangement while preserving the temporal narrative order. Second, we design Spatiotemporal Position-Aware RoPE to incorporate reference tokens and grounding signals, enabling spatiotemporal-grounded reference injection. In addition, to overcome data scarcity, we establish an automated data annotation pipeline to extract multi-shot videos, captions, cross-shot grounding signals and reference images. Our framework leverages the intrinsic architectural properties to support multi-shot video generation, featuring text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot count and duration are flexibly configurable. Extensive experiments demonstrate the superior performance and outstanding controllability of our framework.

</details>


### [123] [PPTArena: A Benchmark for Agentic PowerPoint Editing](https://arxiv.org/abs/2512.03042)
*Michael Ofengenden,Yunze Man,Ziqi Pang,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: PPTArena 是面向 PowerPoint 原地编辑的综合基准；PPTPilot 通过结构感知规划与程序化操作显著提升编辑质量，但大规模、长时程的 PPT 编辑仍未解决。


<details>
  <summary>Details</summary>
Motivation: 当前研究常关注图像或文本到幻灯片生成，而缺乏针对对真实 PPT 进行可控、精确的原地编辑的基准和方法。作者旨在填补这一空白，评估及提升对复杂、跨页和样式一致性要求的编辑能力。

Method: 提出 PPTArena 数据集与双重 VLM 判分流水线（分别评估指令遵从与视觉质量），并设计 PPTPilot 编辑代理：基于结构的语义编辑规划、在高层程序化工具与确定性 XML 操作间路由，以及计划-编辑-校验的迭代循环。

Result: 在包含100个文档、2125页与800+编辑操作的基准上，PPTPilot 在复合编辑、版式敏感编辑与跨页编辑上比强大的专有代理和先进 VLM 系统高出10+百分点，在视觉保真度与全卷一致性方面提升显著。但总体上所有系统在人类级的文档规模长任务上仍表现较弱。

Conclusion: PPTArena 提供了一个针对 PowerPoint 原地编辑的严格评估基准，展示了在自然语言指令下对真实幻灯片进行可靠修改的能力边界。实验表明 PPTPilot 在复杂、版式敏感和跨幻灯片编辑上显著优于现有系统，但长时程、文档级任务仍有明显不足。

Abstract: We introduce PPTArena, a benchmark for PowerPoint editing that measures reliable modifications to real slides under natural-language instructions. In contrast to image-PDF renderings or text-to-slide generation, PPTArena focuses on in-place editing across 100 decks, 2125 slides, and over 800 targeted edits covering text, charts, tables, animations, and master-level styles. Each case includes a ground-truth deck, a fully specified target outcome, and a dual VLM-as-judge pipeline that separately scores instruction following and visual quality using both structural diffs and slide images. Building on this setting, we propose PPTPilot, a structure-aware slide-editing agent that plans semantic edit sequences, routes between high-level programmatic tools and deterministic XML operations for precise control, and verifies outputs through an iterative plan-edit-check loop against task-specific constraints. In our experiments, PPTPilot outperforms strong proprietary agents and frontier VLM systems by over 10 percentage points on compound, layout-sensitive, and cross-slide edits, with particularly large gains in visual fidelity and deck-wide consistency. Despite these improvements, existing agents still underperform on long-horizon, document-scale tasks in PPTArena, highlighting the remaining challenges in reliable PPT editing.

</details>


### [124] [OneThinker: All-in-one Reasoning Model for Image and Video](https://arxiv.org/abs/2512.03043)
*Kaituo Feng,Manyuan Zhang,Hongyu Li,Kaixuan Fan,Shuang Chen,Yilei Jiang,Dian Zheng,Peiwen Sun,Yiyuan Zhang,Haoze Sun,Yan Feng,Peng Pei,Xunliang Cai,Xiangyu Yue*

Main category: cs.CV

TL;DR: OneThinker is an RL-based unified model for image and video reasoning across QA, captioning, grounding, tracking, segmentation, trained on a 600k corpus with CoT annotations and optimized via EMA-GRPO to balance multi-task rewards, achieving strong multi-benchmark results and cross-task transfer.


<details>
  <summary>Details</summary>
Motivation: Existing methods train separate models per task and separate image/video domains, limiting scalability and knowledge sharing; goal is an all-in-one multimodal reasoning generalist.

Method: Constructed OneThinker-600k dataset, used commercial models for chain-of-thought annotations to create OneThinker-SFT-340k for supervised fine-tuning, then applied multi-task RL with EMA-GRPO, which tracks task-wise moving averages of reward standard deviations to balance heterogeneous rewards.

Result: Strong performance on 31 benchmarks across 10 visual tasks, effective knowledge transfer between tasks, preliminary zero-shot generalization; code, models, and data released.

Conclusion: OneThinker demonstrates that a single RL-trained multimodal model can unify image and video reasoning across many tasks, improving scalability and enabling knowledge transfer and some zero-shot generalization.

Abstract: Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.

</details>


### [125] [CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models](https://arxiv.org/abs/2512.03045)
*Minkyung Kwon,Jinhyeok Choi,Jiho Park,Seonghu Jeon,Jinhyuk Jang,Junyoung Seo,Minseop Kwak,Jin-Hwa Kim,Seungryong Kim*

Main category: cs.CV

TL;DR: CAMEO：用几何对应监督注意力图，显著提升多视角扩散模型的新视角合成质量和训练速度，且只需监督单层注意力，方法简洁且通用。


<details>
  <summary>Details</summary>
Motivation: 目前多视角扩散模型虽能合成视图一致的图像，但其内部如何保持几何一致性不清楚；且在大视角变换下对应性不足，影响效果和训练效率。作者旨在揭示注意力图在几何对应中的作用并通过显式监督改进模型。

Method: 作者先分析多视角扩散模型的注意力图，发现其在训练过程中会学习到参考视图与目标视图之间的几何对应，但在大视角变化下对应性变差。基于此，提出CAMEO训练技术：在训练时利用几何对应信息直接监督模型的注意力图（仅需监督单个注意力层），以增强对应性的学习，从而保存参考图像的几何结构并加速收敛。

Result: 实施CAMEO后，模型在相同迭代次数下性能更好，且收敛所需的训练迭代次数减少约一半；方法对不同多视角扩散模型均适用（模型无关）。

Conclusion: CAMEO通过在训练时对注意力图进行几何对应监督，提高了多视角扩散模型的视图一致性、训练效率和新视角合成质量，并且对单层注意力进行监督即可取得显著效果。

Abstract: Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.

</details>


### [126] [MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues](https://arxiv.org/abs/2512.03046)
*Zichen Liu,Yue Yu,Hao Ouyang,Qiuyu Wang,Shuailei Ma,Ka Leong Cheng,Wen Wang,Qingyan Bai,Yuxuan Zhang,Yanhong Zeng,Yixuan Li,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: MagicQuill V2通过内容/空间/结构/颜色四层可控提示，实现对生成图像的细粒度、可组合控制，显著改善局部编辑与用户意图表达。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型使用单一提示无法将用户关于内容、位置、外观等不同意图解耦，导致缺乏精细控制。引入分层表示使创作者可直接操控各个方面。

Method: 提出分层组合范式；设计上下文感知的数据生成管道；引入统一控制模块处理所有视觉线索；微调空间分支以实现精确局部编辑与目标移除。

Result: 通过大量实验验证，分层方法能更好地解析用户意图，提升局部编辑精度、位置控制与外观一致性，并支持直观的创作流程。

Conclusion: MagicQuill V2通过将创作意图分解为多层可控视觉线索（内容、空间、结构、颜色），在生成图像编辑中提供比单一提示更细粒度的控制，从而缩小了扩散模型语义能力与传统图形软件精细控制之间的差距。

Abstract: We propose MagicQuill V2, a novel system that introduces a \textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [127] [FCDB (Functorial-Categorical Database): A Compositional Framework for Information Preservation and Anti-Commutativity Reduction](https://arxiv.org/abs/2512.02021)
*Jun Kawasaki*

Main category: cs.DB

TL;DR: 提出基于函子范畴与保全投影族的FCDb，定义最小核F_core并通过伴随与纤维结构实现操作可交换与所有权/能力约束的保留，避免为一致性丢弃信息。


<details>
  <summary>Details</summary>
Motivation: 传统数据库为保证局部一致性常通过丢弃信息（如可见性、历史或关系细节）来纠缠正确性与丢失；目标是构建一个在范畴论与投影解释下保留全部语义/时序/关系熵的数据库数学框架。

Method: 将数据操作视为层化函子范畴中的态射，定义Complete Preserving Family（CPF）投影族覆盖内容不变性（CAS）、能力（Cap）与所有权（Own），并可选地引入观测投影（B+Tree、append-only/LSM、Graph）。通过识别最小核F_core = Own ∘ Cap ∘ CAS、使用伴随提升与纤维结构，证明在范畴极限下操作对可交换且维护所有权与能力约束。

Result: 构建了FCDb理论框架，给出最小核及投影族定义，证明在伴随提升与纤维范畴条件下操作可以在范畴极限中交换，连接信息几何解释并支持在不丢弃信息情况下进行经验验证。

Conclusion: FCDb提出通过函子范畴和投影族（CPF）来在不丢弃信息的前提下表达数据库操作与权限边界，从而将一致性语义化为保留信息与拥有/能力边界之间的范畴结构关系。

Abstract: Conventional database architectures often secure local consistency by discarding information, entangling correctness with loss. We introduce the Functorial-Categorical Database (FCDb), which models data operations as morphisms in a layered functor category and establishes a Complete Preserving Family (CPF) of projections spanning content invariance (CAS), capability, and ownership, with optional observational projections for local order (B+Tree), temporal history (append-only/LSM), and adjacency (Graph). We identify a minimal kernel (F_core = Own o Cap o CAS) that preserves information and collapses non-commutativity to the ethical grant/revoke boundary. Under adjoint lifts and a fibred structure, operational pairs commute in the categorical limit while ownership integrity and capability constraints are maintained. The framework connects to information geometry via projection interpretations and supports empirical validation without discarding semantic, temporal, or relational entropy.

</details>


### [128] [Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving](https://arxiv.org/abs/2512.02281)
*Yi Liu,Chen Qian*

Main category: cs.DB

TL;DR: 该论文提出Trinity框架，将所有向量检索集中到单一共享的GPU向量搜索池，并与Prefill-and-Decode（PD）解耦部署配合，提出GPU向量服务架构、连续批处理和阶段感知调度以降低尾延迟与提升GPU利用率。


<details>
  <summary>Details</summary>
Motivation: 目前PD解耦下推理与检索仍耦合（RAG、缓存等），导致异构检索请求拉高尾延迟和资源碎片化，因此需要一个能与PD架构配合的统一向量检索部署方案，以满足SLO并提升GPU利用率。

Method: 设计一个专用的GPU向量搜索服务与PD解耦架构；引入连续批（continuous batching）以聚合异构查询并提高GPU吞吐；采用阶段感知调度（stage-aware scheduling）在prefill与decode任务间进行检索请求的可抢占调度；整体把检索流量统一到共享池并与LLM推理阶段协同。

Result: 论文宣称Trinity在各种检索工作负载下提高GPU利用率与吞吐、降低尾延迟，能够在不违反SLO的情况下将所有检索集中在共享GPU池并与PD解耦的LLM服务匹配。

Conclusion: Trinity能在PD解耦的大规模LLM服务中高效整合向量检索，通过共享GPU池、连续批与阶段感知调度在异构检索负载下提升吞吐与利用率、减少尾延迟，从而满足SLO。

Abstract: Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.

</details>


### [129] [Multi-Objective Agentic Rewrites for Unstructured Data Processing](https://arxiv.org/abs/2512.02289)
*Lindsey Linxi Wei,Shreya Shankar,Sepanta Zeighami,Yeounoh Chung,Fatma Ozcan,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: MOAR augments DocETL with cost-aware rewrite directives and a global, bandit-guided search over pipeline rewrites, improving accuracy and reducing cost versus prior optimizers.


<details>
  <summary>Details</summary>
Motivation: DocETL pipelines suffer from LLM inaccuracies; prior rewrite directives improved accuracy but ignored cost. Need for an optimizer that jointly considers accuracy and cost across interacting operators.

Method: Expanded DocETL directives (over 30 total), adding two new cost-focused directive categories and extending existing ones; designed a global search algorithm using a multi-armed bandit framework to explore pipeline rewrites holistically rather than locally.

Result: Across six workloads, MOAR achieved 27% higher accuracy than ABACUS and matched ABACUS's best accuracy at only 55% of the cost.

Conclusion: MOAR effectively balances accuracy and cost in DocETL by introducing cost-aware directives and a global search optimizer, achieving substantial accuracy gains and cost savings compared to prior work.

Abstract: One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter("is this email sent from an executive and discussing fraud?") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both?
  We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.

</details>


### [130] [QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning](https://arxiv.org/abs/2512.02444)
*Ning Wang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: QJoin用强化学习学会可重用的变换链，以解决开放、异构数据集中复杂连接问题，既提高了召回与精度，也加速了大规模任务。


<details>
  <summary>Details</summary>
Motivation: 传统方法侧重等值连接或容忍小幅字符串差异，难以处理标识符格式不一致、嵌入或拆分的情况；需要自动学习系统性变换以扩展连接发现能力。

Method: 提出基于Q-learning的代理（agent），设计唯一性感知奖励函数以在相似度与键唯一性间权衡；使用变换操作序列搜索转换链；引入两种重用机制：代理迁移（agent transfer）和变换缓存（transformation reuse）。

Result: 在AutoJoin Web基准（31对表）上平均F1=91.0%；在NYC+Chicago 19,990个连接任务上通过重用机制减少运行时间最多7.4%（13,747秒）。

Conclusion: QJoin通过强化学习学习并重用字符串/字段变换策略，显著提升异构表连接发现的准确性与效率。

Abstract: Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.

</details>


### [131] [A Datalake for Data-driven Social Science Research](https://arxiv.org/abs/2512.02463)
*Puneet Arya,Ojas Sahasrabudhe,Adwaiya Srivastav,Partha Pratim Das,Maya Ramanath*

Main category: cs.DB

TL;DR: 论文提出并实现了一个面向社会科学的Datalake平台，通过数据整合、溯源、权限与可视化工具，示范了如何使非专业用户更易进行透明且可复现的数据分析。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究对数据驱动洞见的需求增长，但研究者常受限于技术能力、数据格式不一致和可靠数据可及性不足；因此需要一个降低门槛、标准化流程并提高透明性的基础设施。

Method: 设计并实现了一个支持多数据类型摄取与整合、自动溯源与版本追踪、基于角色的访问控制以及内置可视化与分析工具的Datalake系统；通过真实用例演示平台功能并详细回顾一个收入-教育-婴儿死亡率关系的分析流程。

Result: 通过多个真实世界用例（治理、健康、教育）展示了平台在简化研究流程、保证透明性与可复现性方面的效果，并讨论了未来扩展（ML管线、移动访问、公民数据反馈）。

Conclusion: 该论文提出了面向跨学科社会科学研究的Datalake基础设施，旨在降低数据驱动研究门槛，提升可复现性和透明性。

Abstract: Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets.Social science research increasingly demands data-driven insights, yet researchers often face barriers such as lack of technical expertise, inconsistent data formats, and limited access to reliable datasets. In this paper, we present a Datalake infrastructure tailored to the needs of interdisciplinary social science research. Our system supports ingestion and integration of diverse data types, automatic provenance and version tracking, role-based access control, and built-in tools for visualization and analysis. We demonstrate the utility of our Datalake using real-world use cases spanning governance, health, and education. A detailed walkthrough of one such use case -- analyzing the relationship between income, education, and infant mortality -- shows how our platform streamlines the research process while maintaining transparency and reproducibility. We argue that such infrastructure can democratize access to advanced data science practices, especially for NGOs, students, and grassroots organizations. The Datalake continues to evolve with plans to support ML pipelines, mobile access, and citizen data feedback mechanisms.

</details>


### [132] [Stress-Testing Causal Claims via Cardinality Repairs](https://arxiv.org/abs/2512.02491)
*Yarden Gabbay,Haoquan Guan,Shaull Almagor,El Kindi Rezig,Brit Youngmann,Babak Salimi*

Main category: cs.DB

TL;DR: SubCure is a framework that finds small subsets of records whose removal changes causal estimates, formalizes complexity, and provides scalable machine-unlearning-based algorithms that reveal fragility in causal analyses across datasets.


<details>
  <summary>Details</summary>
Motivation: To quantify sensitivity of causal claims to small, targeted data modifications and identify specific data regions driving conclusions, improving reliability of high-stakes decisions.

Method: Formulates cardinality-repair problem for tuple- and pattern-level deletions, proves NP-completeness, and develops scalable algorithms using machine unlearning to incrementally update causal estimates without retraining.

Result: On four real-world datasets, SubCure finds compact high-impact subsets whose removal shifts causal estimates into user-specified ranges, exposing vulnerabilities missed by traditional sensitivity methods.

Conclusion: SubCure shows that causal conclusions from observational data can be fragile and that small targeted deletions can change estimates, enabling robustness auditing.

Abstract: Causal analyses derived from observational data underpin high-stakes decisions in domains such as healthcare, public policy, and economics. Yet such conclusions can be surprisingly fragile: even minor data errors - duplicate records, or entry mistakes - may drastically alter causal relationships. This raises a fundamental question: how robust is a causal claim to small, targeted modifications in the data? Addressing this question is essential for ensuring the reliability, interpretability, and reproducibility of empirical findings. We introduce SubCure, a framework for robustness auditing via cardinality repairs. Given a causal query and a user-specified target range for the estimated effect, SubCure identifies a small set of tuples or subpopulations whose removal shifts the estimate into the desired range. This process not only quantifies the sensitivity of causal conclusions but also pinpoints the specific regions of the data that drive those conclusions. We formalize this problem under both tuple- and pattern-level deletion settings and show both are NP-complete. To scale to large datasets, we develop efficient algorithms that incorporate machine unlearning techniques to incrementally update causal estimates without retraining from scratch. We evaluate SubCure across four real-world datasets covering diverse application domains. In each case, it uncovers compact, high-impact subsets whose removal significantly shifts the causal conclusions, revealing vulnerabilities that traditional methods fail to detect. Our results demonstrate that cardinality repair is a powerful and general-purpose tool for stress-testing causal analyses and guarding against misleading claims rooted in ordinary data imperfections.

</details>


### [133] [PystachIO: Efficient Distributed GPU Query Processing with PyTorch over Fast Networks & Fast Storage](https://arxiv.org/abs/2512.02862)
*Jigao Luo,Nils Boeschen,Muhammad El-Hindi,Carsten Binnig*

Main category: cs.DB

TL;DR: 针对存储驻留的大规模 OLAP，PystachIO 在 PyTorch 上通过重叠计算与高带宽 I/O 并优化资源利用，实现最多3x的端到端加速。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心采用以 GPU 为中心的 HPC 风格架构，张量计算运行时已被证明能加速分析型工作负载，但此前研究多假设数据可完全驻留于 GPU 内存。本工作动机是研究当数据驻留在存储中且规模较大时，如何用 TCR 支持可扩展的分布式查询处理。

Method: 基于 PyTorch 构建分布式 OLAP 引擎，利用 RDMA 网络和高带宽 NVMe 存储，通过优化 I/O 路径、改进数据并行和流水线机制、以及对 GPU、网络和存储的联合调度与重叠，实现高效的数据移动与计算协同。

Result: 在多个基准上，PystachIO 在端到端查询延迟/吞吐方面相比现有分布式 GPU 查询处理方法最高可达3倍加速，同时显著提高 GPU、网络和存储的利用率。

Conclusion: PystachIO 能在大规模、存储驻留的 OLAP 工作负载上利用 PyTorch 等张量计算运行时，通过重叠计算与数据移动并优化网络/存储访问，提高整体吞吐与资源利用率，从而显著加速分布式 GPU 查询处理。

Abstract: The AI hardware boom has led modern data centers to adopt HPC-style architectures centered on distributed, GPU-centric computation. Large GPU clusters interconnected by fast RDMA networks and backed by high-bandwidth NVMe storage enable scalable computation and rapid access to storage-resident data. Tensor computation runtimes (TCRs), such as PyTorch, originally designed for AI workloads, have recently been shown to accelerate analytical workloads. However, prior work has primarily considered settings where the data fits in aggregated GPU memory. In this paper, we systematically study how TCRs can support scalable, distributed query processing for large-scale, storage-resident OLAP workloads. Although TCRs provide abstractions for network and storage I/O, naive use often underutilizes GPU and I/O bandwidth due to insufficient overlap between computation and data movement. As a core contribution, we present PystachIO, a PyTorch-based distributed OLAP engine that combines fast network and storage I/O with key optimizations to maximize GPU, network, and storage utilization. Our evaluation shows up to 3x end-to-end speedups over existing distributed GPU-based query processing approaches.

</details>


### [134] [From Administrative Chaos to Analytical Cohorts: A Three-Stage Normalisation Pipeline for Longitudinal University Administrative Records](https://arxiv.org/abs/2512.02936)
*H. R. Paz*

Main category: cs.DB

TL;DR: 该研究提出一个透明可复现的三阶段规范化管道，用于处理跨四十年高校管理记录。管道保留所有记录、完成地理编码并恢复部分学校类型；通过统计检验确认未恢复信息是由历史入学实践导致的结构性缺失，建议在下游分析中区分完整人群与具有中学信息的子队列。


<details>
  <summary>Details</summary>
Motivation: 长期高校行政数据在建模前常被直接使用，忽视了原始数据不一致性与缺失处理的影响；需提供透明、可复现的规范化方法，避免对结构性历史缺失进行错误插补，并为下游分析提供合理的队列定义。

Method: 设计并实现三阶段规范化流程：N1 CENSAL（统一人口学层）、N1b IDENTITY RESOLUTION（整合重复标识并保留审计追踪）、N1c GEO与SECONDARY-SCHOOL NORMALISATION（构建参考表、学校类型分类、不可恢复标注DATA_MISSING）；随后对结果进行法医分析，包括卡方检验和逻辑回归，以检验缺失机制与入学年代和地理的关联性。

Result: 管道保留了100%学生记录、实现全部地理编码、为56.6%学生恢复了有效学校类型，43.4%标注为结构性缺失；统计分析表明缺失性与入学年代和地理高度相关，支持结构性缺失假设。

Conclusion: 该论文提出并验证了一个针对高等教育长期行政记录的数据规范化三阶段管道，证明在保留全部学生记录的同时，能完成地理编码并对学校类型进行部分恢复，且揭示大量缺失为结构性历史原因而非随机缺失。

Abstract: The growing use of longitudinal university administrative records in data-driven decision-making often overlooks a critical layer: how raw, inconsistent data are normalised before modelling. This article presents a three-stage normalisation pipeline for a dataset of 24,133 engineering students at a Latin American public university, spanning four decades (1980-2019). The pipeline comprises: (i) N1 CENSAL, harmonising demographics into a single person-level layer; (ii) N1b IDENTITY RESOLUTION, consolidating duplicate identifiers into a canonical ID while preserving an audit trail; and (iii) N1c GEO and SECONDARY-SCHOOL NORMALISATION, which builds reference tables, classifies school types (state national, state provincial, private secular, private religious), and flags irrecoverable cases as DATA_MISSING. The pipeline preserves 100% of students, achieves full geocoding, and yields valid school types for 56.6% of the population. The remaining 43.4% are identified as structurally missing due to legacy enrolment practices rather than stochastic non-response. Forensic analysis (chi-square, logistic regression) shows missingness is highly predictable from entry decade and geography, confirming a structural, historically induced mechanism. The article contributes: (a) a transparent, reproducible normalisation pipeline tailored to higher education; (b) a framework for treating structurally missing information without speculative imputation; and (c) guidance on defining analytically coherent cohorts (full population vs. secondary-school-informed subcohorts) for downstream learning analytics and policy evaluation.

</details>
