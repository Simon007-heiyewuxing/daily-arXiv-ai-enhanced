<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Do Open-Vocabulary Detectors Transfer to Aerial Imagery? A Comparative Evaluation](https://arxiv.org/abs/2601.22164)
*Christos Tsourveloudis*

Main category: cs.CV

TL;DR: 在航拍影像上，现有OVD方法在零样本条件下表现很差，主要受限于语义冲突而非检测定位，提示工程效果有限，需发展域自适应策略。


<details>
  <summary>Details</summary>
Motivation: 验证当前基于视觉-语言的OVD方法在航拍影像上的零样本迁移能力，并解析性能瓶颈以指导后续研究。

Method: 构建严格零样本评估协议，在LAE-80C（3,592张、80类）上比较五种最先进OVD模型，使用Global、Oracle、Single-Category三种推理模式以区分语义混淆与视觉定位影响；评估还包含词汇规模缩减和提示工程（域前缀、同义词扩展）实验，并在不同航拍数据集（DIOR、FAIR1M）上测试稳健性。

Result: 最优模型OWLv2在LAE-80C仅达27.6% F1，误报率69%；将类别从80降到3.2个等效类使性能提升约15倍，说明语义混淆主导失败；提示工程无显著改进；不同数据集间性能波动大（DIOR 0.53 vs FAIR1M 0.12）。

Conclusion: 该论文揭示了开放词汇目标检测（OVD）在航拍图像域的显著迁移失败，语义混淆而非定位成为主要瓶颈，提示需要专门的域自适应方法。

Abstract: Open-vocabulary object detection (OVD) enables zero-shot recognition of novel categories through vision-language models, achieving strong performance on natural images. However, transferability to aerial imagery remains unexplored. We present the first systematic benchmark evaluating five state-of-the-art OVD models on the LAE-80C aerial dataset (3,592 images, 80 categories) under strict zero-shot conditions. Our experimental protocol isolates semantic confusion from visual localization through Global, Oracle, and Single-Category inference modes. Results reveal severe domain transfer failure: the best model (OWLv2) achieves only 27.6% F1-score with 69% false positive rate. Critically, reducing vocabulary size from 80 to 3.2 classes yields 15x improvement, demonstrating that semantic confusion is the primary bottleneck. Prompt engineering strategies such as domain-specific prefixing and synonym expansion, fail to provide meaningful performance gains. Performance varies dramatically across datasets (F1: 0.53 on DIOR, 0.12 on FAIR1M), exposing brittleness to imaging conditions. These findings establish baseline expectations and highlight the need for domain-adaptive approaches in aerial OVD.

</details>


### [2] [What Lies Beneath: A Call for Distribution-based Visual Question & Answer Datasets](https://arxiv.org/abs/2601.22218)
*Jill P. Naiman,Daniel J. Evans,JooYoung Seo*

Main category: cs.CV

TL;DR: 提出并发布一个用于科学直方图VQA的新数据集，强调图表与数据非一一对应带来的推理挑战，并提供全面标注与底层数据以便研究。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集主要面向真实图像或简单图表，缺乏处理图表为数据转换（分析、简化、修改）带来推理挑战的基准；需要一个能评估模型在没有图表与数据一一对应情况下的推理能力的数据集。

Method: 对现有VQA数据集进行调研，指出局限性；合成基于真实数据的直方图并保存底层数据与生成参数；设计问题集让人类与大型推理模型回答需要访问底层数据的问题；提供图像标注（图形标记与文本边界框）。

Result: 发布了开源数据集，包含图像、底层数据、分布参数以及所有图形标记和文本的边界框，展示人类与模型在需访问底层数据的问题上表现差异。

Conclusion: 作者提出了一个专门针对科学图表的VQA基准，强调现实中图表与原始数据并非一一对应，现有数据集忽视了这一点。

Abstract: Visual Question Answering (VQA) has become an important benchmark for assessing how large multimodal models (LMMs) interpret images. However, most VQA datasets focus on real-world images or simple diagrammatic analysis, with few focused on interpreting complex scientific charts. Indeed, many VQA datasets that analyze charts do not contain the underlying data behind those charts or assume a 1-to-1 correspondence between chart marks and underlying data. In reality, charts are transformations (i.e. analysis, simplification, modification) of data. This distinction introduces a reasoning challenge in VQA that the current datasets do not capture. In this paper, we argue for a dedicated VQA benchmark for scientific charts where there is no 1-to-1 correspondence between chart marks and underlying data. To do so, we survey existing VQA datasets and highlight limitations of the current field. We then generate synthetic histogram charts based on ground truth data, and ask both humans and a large reasoning model questions where precise answers depend on access to the underlying data. We release the open-source dataset, including figures, underlying data, distribution parameters used to generate the data, and bounding boxes for all figure marks and text for future research.

</details>


### [3] [Lost in Space? Vision-Language Models Struggle with Relative Camera Pose Estimation](https://arxiv.org/abs/2601.22228)
*Ken Deng,Yifu Qiu,Yoni Kasten,Shay B. Cohen,Yftah Ziser*

Main category: cs.CV

TL;DR: 提出现实化与诊断性基准评估VLM在相对相机位姿估计任务，发现其在3D与多视图推理上显著落后于几何方法与人类。


<details>
  <summary>Details</summary>
Motivation: 探究当前视觉-语言模型在2D感知与语义推理之外，对3D空间结构和多视图信息整合能力的局限性，采用RCPE作为切入点。

Method: 提出了两个基准：VRRPI-Bench（从无标签第一人称视频生成，包含相对相机运动的语言化标注）和VRRPI-Diag（诊断性基准，用于孤立单一运动自由度），用以评估VLM在相对相机位姿估计（RCPE）上的能力，并对多图像融合进行测试。

Result: 大部分VLM在RCPE上依赖浅层2D启发式规则，在深度变化和绕光轴的滚转变换上表现尤差。最先进的模型（如GPT-5）仍低于经典几何基线与人类表现；多图像推理也表现不稳定，表明VLM对3D与多视图空间推理的弱化。

Conclusion: VLMs在3D空间理解和多视图推理方面存在显著不足，无法超越基于2D启发式的方法，在RCPE任务上落后于几何基线和人类水平。

Abstract: Vision-Language Models (VLMs) perform well in 2D perception and semantic reasoning compared to their limited understanding of 3D spatial structure. We investigate this gap using relative camera pose estimation (RCPE), a fundamental vision task that requires inferring relative camera translation and rotation from a pair of images. We introduce VRRPI-Bench, a benchmark derived from unlabeled egocentric videos with verbalized annotations of relative camera motion, reflecting realistic scenarios with simultaneous translation and rotation around a shared object. We further propose VRRPI-Diag, a diagnostic benchmark that isolates individual motion degrees of freedom. Despite the simplicity of RCPE, most VLMs fail to generalize beyond shallow 2D heuristics, particularly for depth changes and roll transformations along the optical axis. Even state-of-the-art models such as GPT-5 ($0.64$) fall short of classic geometric baselines ($0.97$) and human performance ($0.92$). Moreover, VLMs exhibit difficulty in multi-image reasoning, with inconsistent performance (best $59.7\%$) when integrating spatial cues across frames. Our findings reveal limitations in grounding VLMs in 3D and multi-view spatial reasoning.

</details>


### [4] [Geometry without Position? When Positional Embeddings Help and Hurt Spatial Reasoning](https://arxiv.org/abs/2601.22231)
*Jian Shi,Michael Birsak,Wenqing Cui,Zhenyu Li,Peter Wonka*

Main category: cs.CV

TL;DR: 本文从几何视角重新审视ViT的位置信息，提出token级诊断指标并在14个ViT模型上验证：位置嵌入是塑造空间表示的几何先验，会因其一致性与形式影响多视图几何一致性与空间推理。


<details>
  <summary>Details</summary>
Motivation: 澄清PEs在ViT中真正的角色：是否仅为索引或具有更强的几何结构作用；理解PEs如何影响模型在空间理解与多视图一致性方面的表现。

Method: 提出基于token的诊断指标，量化多视图几何一致性对PEs一致性的依赖；在14个基础ViT模型上进行广泛实验，比较带/不带PE或替换PE的情况下表示的多视图几何特性与空间任务表现。

Result: 实验显示一致的PEs显著提高多视图几何一致性与空间推理能力；PEs的类型和一致性模式会改变表示的几何结构；消融与控制实验表明PEs为因果机制而非仅是装饰性信息。

Conclusion: PEs在ViT中不仅是位置索引，而是作为几何先验，塑造了表示的空间结构；一致的PEs促进多视图几何一致性，影响空间推理；实验在14个基础ViT模型上验证了PEs的因果作用。

Abstract: This paper revisits the role of positional embeddings (PEs) within vision transformers (ViTs) from a geometric perspective. We show that PEs are not mere token indices but effectively function as geometric priors that shape the spatial structure of the representation. We introduce token-level diagnostics that measure how multi-view geometric consistency in ViT representation depends on consitent PEs. Through extensive experiments on 14 foundation ViT models, we reveal how PEs influence multi-view geometry and spatial reasoning. Our findings clarify the role of PEs as a causal mechanism that governs spatial structure in ViT representations. Our code is provided in https://github.com/shijianjian/vit-geometry-probes

</details>


### [5] [Is Hierarchical Quantization Essential for Optimal Reconstruction?](https://arxiv.org/abs/2601.22244)
*Shirin Reyhanian,Laurenz Wiskott*

Main category: cs.CV

TL;DR: 在控制码本利用与表示容量后，单层VQ-VAE可与分层VQ-VAE在重建质量上匹配，挑战了分层在重建上固有优越性的假设。


<details>
  <summary>Details</summary>
Motivation: 质疑常见观点：分层VQ-VAE因将全局与局部特征分开而在重建上更好。观察到高层从低层获取信息，作者怀疑若表示预算匹配且无码本坍塌，单层能否达到相同重建质量。

Method: 在高分辨率ImageNet上，将两级VQ-VAE与容量匹配的单级模型比较；通过数据初始化、周期性重置不活跃码本向量及系统调参以减少码本坍塌；控制表示预算与量化稳定性进行消融实验。

Result: 确认单层VQ-VAE受限于码本利用不足与高维嵌入导致的不稳定性；提出轻量干预显著减少坍塌；在匹配预算与减少坍塌条件下，单层模型达到与两级模型相当的重建保真度。

Conclusion: 当匹配表示预算并有效防止码本坍塌时，单层VQ-VAE可以在重建保真度上与分层变体相当，表明分层结构并非在重建准确性上天然优越。

Abstract: Vector-quantized variational autoencoders (VQ-VAEs) are central to models that rely on high reconstruction fidelity, from neural compression to generative pipelines. Hierarchical extensions, such as VQ-VAE2, are often credited with superior reconstruction performance because they split global and local features across multiple levels. However, since higher levels derive all their information from lower levels, they should not carry additional reconstructive content beyond what the lower-level already encodes. Combined with recent advances in training objectives and quantization mechanisms, this leads us to ask whether a single-level VQ-VAE, with matched representational budget and no codebook collapse, can equal the reconstruction fidelity of its hierarchical counterpart. Although the multi-scale structure of hierarchical models may improve perceptual quality in downstream tasks, the effect of hierarchy on reconstruction accuracy, isolated from codebook utilization and overall representational capacity, remains empirically underexamined. We revisit this question by comparing a two-level VQ-VAE and a capacity-matched single-level model on high-resolution ImageNet images. Consistent with prior observations, we confirm that inadequate codebook utilization limits single-level VQ-VAEs and that overly high-dimensional embeddings destabilize quantization and increase codebook collapse. We show that lightweight interventions such as initialization from data, periodic reset of inactive codebook vectors, and systematic tuning of codebook hyperparameters significantly reduce collapse. Our results demonstrate that when representational budgets are matched, and codebook collapse is mitigated, single-level VQ-VAEs can match the reconstruction fidelity of hierarchical variants, challenging the assumption that hierarchical quantization is inherently superior for high-quality reconstructions.

</details>


### [6] [VMonarch: Efficient Video Diffusion Transformers with Structured Attention](https://arxiv.org/abs/2601.22275)
*Cheng Liang,Haoxian Chen,Liang Hou,Qi Fan,Gangshan Wu,Xin Tao,Limin Wang*

Main category: cs.CV

TL;DR: 通过Monarch结构化稀疏矩阵和算法工程（交替最小化+重算+在线熵更新），VMonarch在保持或提升生成质量的前提下，显著降低并加速Video DiTs的注意力计算。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制的二次复杂度限制了Video DiTs的上下文扩展能力，而视频注意力模式高度稀疏且结构化，适合用受控稀疏的结构化矩阵表示以实现子二次复杂度。

Method: 提出基于Monarch矩阵的稀疏时空注意力机制：1) 设计时空Monarch分解以分别建模帧内和帧间相关性；2) 使用交替最小化算法求解分解并引入重算以缓解数值不稳定导致的伪影；3) 将在线熵算法融合进FlashAttention以快速更新Monarch矩阵实现长序列高效计算。

Result: 在VBench上经过少量调参后，VMonarch在生成质量上与全注意力相当或更好；注意力FLOPs降低约17.5倍，长视频注意力计算速度提升超过5倍，在90%稀疏度下优于现有稀疏注意力方法。

Conclusion: VMonarch通过将视频的时空稀疏注意力模式映射到可控稀疏的Monarch矩阵，并结合交替最小化、重算策略和在线熵更新，成功在保证或提升生成质量的同时显著降低注意力计算复杂度与运行时间。

Abstract: The quadratic complexity of the attention mechanism severely limits the context scalability of Video Diffusion Transformers (DiTs). We find that the highly sparse spatio-temporal attention patterns exhibited in Video DiTs can be naturally represented by the Monarch matrix. It is a class of structured matrices with flexible sparsity, enabling sub-quadratic attention via an alternating minimization algorithm. Accordingly, we propose VMonarch, a novel attention mechanism for Video DiTs that enables efficient computation over the dynamic sparse patterns with structured Monarch matrices. First, we adapt spatio-temporal Monarch factorization to explicitly capture the intra-frame and inter-frame correlations of the video data. Second, we introduce a recomputation strategy to mitigate artifacts arising from instabilities during alternating minimization of Monarch matrices. Third, we propose a novel online entropy algorithm fused into FlashAttention, enabling fast Monarch matrix updates for long sequences. Extensive experiments demonstrate that VMonarch achieves comparable or superior generation quality to full attention on VBench after minimal tuning. It overcomes the attention bottleneck in Video DiTs, reduces attention FLOPs by a factor of 17.5, and achieves a speedup of over 5x in attention computation for long videos, surpassing state-of-the-art sparse attention methods at 90% sparsity.

</details>


### [7] [Coarse-to-Real: Generative Rendering for Populated Dynamic Scenes](https://arxiv.org/abs/2601.22301)
*Gonzalo Gomez-Nogales,Yicong Hong,Chongjian Ge,Marc Comino-Trinidad,Dan Casas,Yi Zhou*

Main category: cs.CV

TL;DR: C2R用混合CG-真实训练将粗糙3D仿真通过神经渲染器转换为可控、时序一致的真实风格城市人群视频，兼具可控性与高真实感。


<details>
  <summary>Details</summary>
Motivation: 传统渲染在动态拥挤场景中对资源、材质与光照的依赖使得可扩展性与真实感受限，且难以在大规模场景与控制需求间取得平衡。作者希望用生成模型把粗略模拟转为真实感视频，同时保留可控性。

Method: 使用两阶段混合CG-真实训练：先从大规模真实视频学习强生成先验，再通过共享隐式时空特征跨域引入可控性。输入为粗渲染（控制布局、相机、轨迹）和文本提示，引导神经渲染器合成外观、光照及细尺度动态。

Result: 系统支持从粗到细的控制、跨多种CG与游戏输入的泛化，并能以最小3D输入生成时序一致、可控且真实的城市场景视频。作者计划公开模型与项目页面。

Conclusion: C2R提出从粗糙3D仿真生成真实风格城市人群视频的可控生成渲染框架，能够在保持场景布局、相机运动和行人轨迹的显式控制下生成时序一致、细节丰富的真实感影像。

Abstract: Traditional rendering pipelines rely on complex assets, accurate materials and lighting, and substantial computational resources to produce realistic imagery, yet they still face challenges in scalability and realism for populated dynamic scenes. We present C2R (Coarse-to-Real), a generative rendering framework that synthesizes real-style urban crowd videos from coarse 3D simulations. Our approach uses coarse 3D renderings to explicitly control scene layout, camera motion, and human trajectories, while a learned neural renderer generates realistic appearance, lighting, and fine-scale dynamics guided by text prompts. To overcome the lack of paired training data between coarse simulations and real videos, we adopt a two-phase mixed CG-real training strategy that learns a strong generative prior from large-scale real footage and introduces controllability through shared implicit spatio-temporal features across domains. The resulting system supports coarse-to-fine control, generalizes across diverse CG and game inputs, and produces temporally consistent, controllable, and realistic urban scene videos from minimal 3D input. We will release the model and project webpage at https://gonzalognogales.github.io/coarse2real/.

</details>


### [8] [FlexMap: Generalized HD Map Construction from Flexible Camera Configurations](https://arxiv.org/abs/2601.22376)
*Run Wang,Chaoyi Zhou,Amir Salarpour,Xi Liu,Zhi-Qi Cheng,Feng Luo,Mert D. Pesé,Siyu Huang*

Main category: cs.CV

TL;DR: FlexMap通过跨帧注意力与相机token机制，在无需投影矩阵或重训练的情况下，适配可变相机配置，提升HD地图构建的鲁棒性和实用性。


<details>
  <summary>Details</summary>
Motivation: 现有HD地图构建方法依赖标定的多摄像头布局和显式2D到BEV变换，在传感器故障或车队中摄像头配置变化时鲁棒性差，限制实际部署。

Method: 引入两大模块：1）空间-时间增强模块（将跨视图的空间推理与时间动态分开处理）；2）带潜在相机token的相机感知解码器（通过相机token实现视图自适应注意力，免去投影矩阵）。整体上用跨帧注意力在特征空间内隐式编码三维场景理解，省去2D-to-BEV显式变换与标定依赖。

Result: 在多种摄像头配置下，FlexMap在精度与鲁棒性上均优于现有方法，并对缺失视角与传感器变化表现更稳健，适合实际部署场景。

Conclusion: FlexMap通过消除显式几何投影并使用具备跨帧注意力的几何感知基础模型，成功实现对可变摄像机配置的自适应HD地图构建，避免了为不同相机阵列的重训练与架构修改。

Abstract: High-definition (HD) maps provide essential semantic information of road structures for autonomous driving systems, yet current HD map construction methods require calibrated multi-camera setups and either implicit or explicit 2D-to-BEV transformations, making them fragile when sensors fail or camera configurations vary across vehicle fleets. We introduce FlexMap, unlike prior methods that are fixed to a specific N-camera rig, our approach adapts to variable camera configurations without any architectural changes or per-configuration retraining. Our key innovation eliminates explicit geometric projections by using a geometry-aware foundation model with cross-frame attention to implicitly encode 3D scene understanding in feature space. FlexMap features two core components: a spatial-temporal enhancement module that separates cross-view spatial reasoning from temporal dynamics, and a camera-aware decoder with latent camera tokens, enabling view-adaptive attention without the need for projection matrices. Experiments demonstrate that FlexMap outperforms existing methods across multiple configurations while maintaining robustness to missing views and sensor variations, enabling more practical real-world deployment.

</details>


### [9] [Jailbreaks on Vision Language Model via Multimodal Reasoning](https://arxiv.org/abs/2601.22398)
*Aarush Noheria,Yuguang Yao*

Main category: cs.CV

TL;DR: 提出一种结合后训练CoT提示和ReAct驱动自适应图像噪声的VLM越狱框架，通过提示与视觉扰动协同提高攻击成功率，同时保持输入自然性，揭示VLM安全对齐的脆弱性。


<details>
  <summary>Details</summary>
Motivation: VLM对提示和图像输入高度敏感，提示扰动与视觉扰动可以联合利用来突破安全对齐。作者希望展示一种能在不改动模型参数的前提下，利用提示设计和输入扰动提升越狱成功率的实用攻击框架，从而揭示当前VLM安全策略的弱点并推动更稳健的防御研究。

Method: 两部分策略：1) 后训练CoT提示工程：构造隐蔽的提示（jailbreak prompts），在推理阶段引导模型生成链式思维以规避安全策略；2) ReAct驱动的自适应噪声：利用ReAct范式从模型反馈中识别触发安全防护的区域，迭代地在这些区域加入扰动以增强攻击隐蔽性和有效性。实验通过比较有无两种策略的攻击成功率和自然性指标来评估方法。

Result: 实验结果表明，该双重策略在多个测试集和模型上显著提高了攻击成功率（ASR），并在文本与图像自然性评估上保持较高分数，说明扰动在感知上较为隐蔽。作者还可能提供消融研究显示CoT提示和自适应噪声各自对ASR的贡献，以及参数敏感性分析。

Conclusion: 该论文提出了结合后训练Chain-of-Thought提示和ReAct引导的自适应图像扰动机制的越狱框架，旨在规避视觉-语言模型的安全过滤。作者认为通过在提示中嵌入CoT并基于模型反馈迭代调整图像噪声，可以在保持文本和图像自然性的同时显著提高攻击成功率，从而暴露VLM在提示和视觉输入层面的安全脆弱性。

Abstract: Vision-language models (VLMs) have become central to tasks such as visual question answering, image captioning, and text-to-image generation. However, their outputs are highly sensitive to prompt variations, which can reveal vulnerabilities in safety alignment. In this work, we present a jailbreak framework that exploits post-training Chain-of-Thought (CoT) prompting to construct stealthy prompts capable of bypassing safety filters. To further increase attack success rates (ASR), we propose a ReAct-driven adaptive noising mechanism that iteratively perturbs input images based on model feedback. This approach leverages the ReAct paradigm to refine adversarial noise in regions most likely to activate safety defenses, thereby enhancing stealth and evasion. Experimental results demonstrate that the proposed dual-strategy significantly improves ASR while maintaining naturalness in both text and visual domains.

</details>


### [10] [EMBC Special Issue: Calibrated Uncertainty for Trustworthy Clinical Gait Analysis Using Probabilistic Multiview Markerless Motion Capture](https://arxiv.org/abs/2601.22412)
*Seth Donahue,Irina Djuraskovic,Kunal Shah,Fabian Sinz,Ross Chafetz,R. James Cotton*

Main category: cs.CV

TL;DR: 该研究验证了基于变分推断的概率化MMMC在真实数据上的校准性与可靠性：误差低且置信区间ECE<0.1，预测不确定性能有效指示重建不可靠性，支持临床部署潜力。


<details>
  <summary>Details</summary>
Motivation: 推动MMMC在临床和研究中的实际应用，关键在于不仅要有准确性，还需为每个个体给出可靠的置信区间以建立信任；因此需要评估概率化方法的校准性与不确定性指示能力。

Method: 在先前采用变分推断估计关节角后验分布工作的基础上，本文构建并评估了一个概率化的多视角无标记运动捕捉（MMMC）方法。利用来自两个机构的68名受试者数据，模型输出与带传感步道和传统基于标记的动作捕捉进行验证，使用期望校准误差（ECE）衡量置信区间的校准性，并计算步长、跨步长与下肢关节角的误差及不确定性相关性。

Result: 模型在步长与跨步长及偏差校正后的下肢运动学上表现出良好校准，ECE值通常低于0.1；中位步长误差约16 mm，跨步长约12 mm；偏差校正的关节角误差中位数在1.5–3.8度之间。预测不确定性的大小与观测误差强相关，表明模型主要量化了认识性（epistemic）不确定性。

Conclusion: 该论文证明了基于多视角无标记视频的概率化人体运动捕捉模型能够提供经过校准的不确定性估计，从而在无地面真值辅助下识别不可靠输出，提升临床和研究中的可置信度。

Abstract: Video-based human movement analysis holds potential for movement assessment in clinical practice and research. However, the clinical implementation and trust of multi-view markerless motion capture (MMMC) require that, in addition to being accurate, these systems produce reliable confidence intervals to indicate how accurate they are for any individual. Building on our prior work utilizing variational inference to estimate joint angle posterior distributions, this study evaluates the calibration and reliability of a probabilistic MMMC method. We analyzed data from 68 participants across two institutions, validating the model against an instrumented walkway and standard marker-based motion capture. We measured the calibration of the confidence intervals using the Expected Calibration Error (ECE). The model demonstrated reliable calibration, yielding ECE values generally < 0.1 for both step and stride length and bias-corrected gait kinematics. We observed a median step and stride length error of ~16 mm and ~12 mm respectively, with median bias-corrected kinematic errors ranging from 1.5 to 3.8 degrees across lower extremity joints. Consistent with the calibrated ECE, the magnitude of the model's predicted uncertainty correlated strongly with observed error measures. These findings indicate that, as designed, the probabilistic model reconstruction quantifies epistemic uncertainty, allowing it to identify unreliable outputs without the need for concurrent ground-truth instrumentation.

</details>


### [11] [Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework](https://arxiv.org/abs/2601.22451)
*Shiyu Liu,Xinyi Wen,Zhibin Lan,Ante Wang,Jinsong Su*

Main category: cs.CV

TL;DR: 论文发现生成越长LVLM越依赖语言先验导致对象幻觉，提出语言先验无关的自验证框架，在无需训练的情况下通过对候选描述验证并选择/聚合显著减少幻觉，效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM在图像描述中存在严重对象幻觉，归因于对语言先验的过度依赖，但现有工作缺乏对这一机制的深入分析和有效解决方案。作者旨在深入理解并抑制这种过度依赖，从而提高描述的可靠性。

Method: 首先通过实验证明生成长度增加会加剧对语言先验的依赖并提升虚假对象token的概率；随后提出Language-Prior-Free Verification用于真实评估对象存在性；在此基础上构建训练免疫的Self-Validation Framework：对采样得到的候选描述进行对象存在性验证，并通过选择或聚合策略减少幻觉。

Result: 在图像描述任务中显著降低对象幻觉，例如在LLaVA-v1.5-7B上CHAIRI指标提升65.6%，超过此前SOTA方法。

Conclusion: 该论文提出了一种无训练的自验证框架，通过语言先验无关的验证来缓解LVLM在图像描述任务中的对象幻觉问题。

Abstract: Despite progress in Large Vision Language Models (LVLMs), object hallucination remains a critical issue in image captioning task, where models generate descriptions of non-existent objects, compromising their reliability. Previous work attributes this to LVLMs' over-reliance on language priors and attempts to mitigate it through logits calibration. However, they still lack a thorough analysis of the over-reliance. To gain a deeper understanding of over-reliance, we conduct a series of preliminary experiments, indicating that as the generation length increases, LVLMs' over-reliance on language priors leads to inflated probability of hallucinated object tokens, consequently exacerbating object hallucination. To circumvent this issue, we propose Language-Prior-Free Verification to enable LVLMs to faithfully verify the confidence of object existence. Based on this, we propose a novel training-free Self-Validation Framework to counter the over-reliance trap. It first validates objects' existence in sampled candidate captions and further mitigates object hallucination via caption selection or aggregation. Experiment results demonstrate that our framework mitigates object hallucination significantly in image captioning task (e.g., 65.6% improvement on CHAIRI metric with LLaVA-v1.5-7B), surpassing the previous SOTA methods. This result highlights a novel path towards mitigating hallucination by unlocking the inherent potential within LVLMs themselves.

</details>


### [12] [ScribbleSense: Generative Scribble-Based Texture Editing with Intent Prediction](https://arxiv.org/abs/2601.22455)
*Yudi Zhang,Yeming Geng,Lei Zhang*

Main category: cs.CV

TL;DR: ScribbleSense结合MLLM和图像生成，通过识别涂鸦意图并用全局生成图像提取局部纹理细节，有效解决了涂鸦编辑的语义模糊与位置不确定问题，达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D纹理编辑多支持草图式轮廓交互，对粗粒度涂鸦交互支持不足，且涂鸦指令抽象导致意图与目标位置不明确。需要方法能从模糊涂鸦中恢复语义并定位目标。

Method: 用MLLM的视觉理解能力预测涂鸦的语义编辑意图，随后通过生成全局图像并提取局部纹理细节来锚定目标位置与语义，最终将细节映射到3D纹理上实现编辑。

Result: 实验表明ScribbleSense在涂鸦驱动的纹理编辑上性能优于现有方法，能更准确地推断意图并生成语义对齐的局部纹理，从而实现更自然和可控的编辑效果。

Conclusion: ScribbleSense有效利用多模态大模型与图像生成模型，能从涂鸦中识别意图并结合全局图像生成局部纹理细节，从而提升3D模型纹理涂鸦编辑的准确性与可控性。

Abstract: Interactive 3D model texture editing presents enhanced opportunities for creating 3D assets, with freehand drawing style offering the most intuitive experience. However, existing methods primarily support sketch-based interactions for outlining, while the utilization of coarse-grained scribble-based interaction remains limited. Furthermore, current methodologies often encounter challenges due to the abstract nature of scribble instructions, which can result in ambiguous editing intentions and unclear target semantic locations. To address these issues, we propose ScribbleSense, an editing method that combines multimodal large language models (MLLMs) and image generation models to effectively resolve these challenges. We leverage the visual capabilities of MLLMs to predict the editing intent behind the scribbles. Once the semantic intent of the scribble is discerned, we employ globally generated images to extract local texture details, thereby anchoring local semantics and alleviating ambiguities concerning the target semantic locations. Experimental results indicate that our method effectively leverages the strengths of MLLMs, achieving state-of-the-art interactive editing performance for scribble-based texture editing.

</details>


### [13] [Training-Free Representation Guidance for Diffusion Models with a Representation Alignment Projector](https://arxiv.org/abs/2601.22468)
*Wenqiang Zu,Shenghao Xie,Bo Lei,Lei Ma*

Main category: cs.CV

TL;DR: 在扩散采样中引入表示对齐投影器作为语义锚点，能显著减少早期去噪的语义漂移，提升图像一致性和质量，且兼容现有引导方法。


<details>
  <summary>Details</summary>
Motivation: 尽管无监督视觉表征蕴含丰富语义信息，但在推理阶段缺乏参考图像，导致生成过程不能充分利用这些表征；观察到扩散变换器早期去噪存在随机性引起的语义漂移，需要一种在推理时利用表征的方法。

Method: 设计了一个表示对齐投影器，在采样的中间步骤预测并注入语义表征到扩散采样过程中，无需修改原模型架构；可与classifier-free guidance联合使用。

Result: 在SiTs和REPAs基准上，提出的方法显著提升了ImageNet类条件合成的FID，例如REPA-XL/2从5.9降至3.3；对SiT模型优于代表性引导，并在与classifier-free guidance结合时进一步提升视觉保真度和语义一致性。

Conclusion: 本文提出在扩散变换器的去噪早期阶段存在语义漂移，通过在采样中间注入来自表示投影器的表征来作为语义锚点，以降低漂移并增强语义一致性。

Abstract: Recent progress in generative modeling has enabled high-quality visual synthesis with diffusion-based frameworks, supporting controllable sampling and large-scale training. Inference-time guidance methods such as classifier-free and representative guidance enhance semantic alignment by modifying sampling dynamics; however, they do not fully exploit unsupervised feature representations. Although such visual representations contain rich semantic structure, their integration during generation is constrained by the absence of ground-truth reference images at inference. This work reveals semantic drift in the early denoising stages of diffusion transformers, where stochasticity results in inconsistent alignment even under identical conditioning. To mitigate this issue, we introduce a guidance scheme using a representation alignment projector that injects representations predicted by a projector into intermediate sampling steps, providing an effective semantic anchor without modifying the model architecture. Experiments on SiTs and REPAs show notable improvements in class-conditional ImageNet synthesis, achieving substantially lower FID scores; for example, REPA-XL/2 improves from 5.9 to 3.3, and the proposed method outperforms representative guidance when applied to SiT models. The approach further yields complementary gains when combined with classifier-free guidance, demonstrating enhanced semantic coherence and visual fidelity. These results establish representation-informed diffusion sampling as a practical strategy for reinforcing semantic preservation and image consistency.

</details>


### [14] [Head-Aware Visual Cropping: Enhancing Fine-Grained VQA with Attention-Guided Subimage](https://arxiv.org/abs/2601.22483)
*Junfei Xie,Peng Pan,Xulong Zhang*

Main category: cs.CV

TL;DR: HAVC 利用经过 OCR 诊断筛选的注意力头，并在推理时用空间熵与梯度敏感度细化它们，融合后生成裁剪引导图来裁剪子图以增强 MLLM 在细粒度 VQA 的定位与回答精度（无需训练）。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在细粒度推理受限于低分辨率输入和噪声注意力聚合，亟需无训练代价且通用的视觉聚焦策略以改善视觉定位和答案精确度。

Method: 通过三步头部筛选与细化：先使用基于 OCR 的诊断任务筛除无效头；推理时用空间熵筛选增强空间集中性；用梯度敏感度评估预测贡献；融合信号生成裁剪引导图并裁剪子图与原图-问题对一起输入 MLLM。

Result: 在多个细粒度 VQA 基准上，HAVC 在裁剪策略上优于 SOTA，达到更精确的定位和更强的视觉对齐，提升了 MLLM 的回答精度。

Conclusion: HAVC 是一种训练-free 方法，通过选择性保留并细化部分注意力头，生成可信的视觉裁剪引导图，从而提升 MLLMs 在细粒度 VQA 中的视觉定位和推理性能。

Abstract: Multimodal Large Language Models (MLLMs) show strong performance in Visual Question Answering (VQA) but remain limited in fine-grained reasoning due to low-resolution inputs and noisy attention aggregation. We propose \textbf{Head Aware Visual Cropping (HAVC)}, a training-free method that improves visual grounding by leveraging a selectively refined subset of attention heads. HAVC first filters heads through an OCR-based diagnostic task, ensuring that only those with genuine grounding ability are retained. At inference, these heads are further refined using spatial entropy for stronger spatial concentration and gradient sensitivity for predictive contribution. The fused signals produce a reliable Visual Cropping Guidance Map, which highlights the most task-relevant region and guides the cropping of a subimage subsequently provided to the MLLM together with the image-question pair. Extensive experiments on multiple fine-grained VQA benchmarks demonstrate that HAVC consistently outperforms state-of-the-art cropping strategies, achieving more precise localization, stronger visual grounding, providing a simple yet effective strategy for enhancing precision in MLLMs.

</details>


### [15] [PromptMAD: Cross-Modal Prompting for Multi-Class Visual Anomaly Localization](https://arxiv.org/abs/2601.22492)
*Duncan McCain,Hossein Kashiani,Fatemeh Afghah*

Main category: cs.CV

TL;DR: PromptMAD结合CLIP文本提示、Focal loss与多尺度Transformer+扩散式分割器，通过语义增强的视觉重建实现了SOTA级的无监督像素级异常检测与定位。


<details>
  <summary>Details</summary>
Motivation: 多类别视觉异常检测存在类别多样性、异常样本稀缺和伪装缺陷等挑战，传统仅依赖像素重建或特征距离的方法难以捕捉细微/纹理异常。引入语义提示能为重建提供类别级先验，提升对微小异常的敏感性。

Method: 使用CLIP编码的文本提示描述每个类别的正常与异常特征，将这些语义向量注入视觉重建过程；在像素级不平衡问题上采用Focal loss以强调难检测区域；网络包含一个监督分割器，融合多尺度卷积特征、Transformer空间注意力，并结合扩散式迭代精修以生成高分辨率异常图。

Result: 在MVTec-AD数据集上达到像素级平均AUC 98.35%和AP 66.54%，在多类与伪装缺陷场景下表现稳健，同时保持计算效率。

Conclusion: PromptMAD通过引入跨模态（视觉-语言）提示与语义引导，有效提升了无监督多类别视觉异常检测与定位的性能，在MVTec-AD上达到领先的像素级指标。

Abstract: Visual anomaly detection in multi-class settings poses significant challenges due to the diversity of object categories, the scarcity of anomalous examples, and the presence of camouflaged defects. In this paper, we propose PromptMAD, a cross-modal prompting framework for unsupervised visual anomaly detection and localization that integrates semantic guidance through vision-language alignment. By leveraging CLIP-encoded text prompts describing both normal and anomalous class-specific characteristics, our method enriches visual reconstruction with semantic context, improving the detection of subtle and textural anomalies. To further address the challenge of class imbalance at the pixel level, we incorporate Focal loss function, which emphasizes hard-to-detect anomalous regions during training. Our architecture also includes a supervised segmentor that fuses multi-scale convolutional features with Transformer-based spatial attention and diffusion iterative refinement, yielding precise and high-resolution anomaly maps. Extensive experiments on the MVTec-AD dataset demonstrate that our method achieves state-of-the-art pixel-level performance, improving mean AUC to 98.35% and AP to 66.54%, while maintaining efficiency across diverse categories.

</details>


### [16] [MIRRORTALK: Forging Personalized Avatars Via Disentangled Style and Hierarchical Motion Control](https://arxiv.org/abs/2601.22501)
*Renjie Lu,Xulong Zhang,Xiaoyang Qu,Jianzong Wang,Shangfei Wang*

Main category: cs.CV

TL;DR: MirrorTalk使用语义解耦风格编码器与层次调制的条件扩散模型，从短视频中提取纯风格并在不同面部区域动态融合音频和风格，显著提升了lip-sync和个性化迁移效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法中说话者的个性化风格与语义内容在面部运动中混淆，导致难以将个性迁移到任意语音上。

Method: 提出SDSE从短视频中提取纯粹的风格表示，并在条件扩散模型中采用层次化调制策略，按面部区域动态融合音频与风格特征。

Result: 大量实验证明MirrorTalk在lip-sync准确性和个性化保持方面显著优于现有最先进方法。

Conclusion: MirrorTalk通过语义解耦的风格编码器和层次化调制扩散过程，实现了在保证lip-sync精度的同时有效迁移说话者个性化风格。

Abstract: Synthesizing personalized talking faces that uphold and highlight a speaker's unique style while maintaining lip-sync accuracy remains a significant challenge. A primary limitation of existing approaches is the intrinsic confounding of speaker-specific talking style and semantic content within facial motions, which prevents the faithful transfer of a speaker's unique persona to arbitrary speech. In this paper, we propose MirrorTalk, a generative framework based on a conditional diffusion model, combined with a Semantically-Disentangled Style Encoder (SDSE) that can distill pure style representations from a brief reference video. To effectively utilize this representation, we further introduce a hierarchical modulation strategy within the diffusion process. This mechanism guides the synthesis by dynamically balancing the contributions of audio and style features across distinct facial regions, ensuring both precise lip-sync accuracy and expressive full-face dynamics. Extensive experiments demonstrate that MirrorTalk achieves significant improvements over state-of-the-art methods in terms of lip-sync accuracy and personalization preservation.

</details>


### [17] [DreamVAR: Taming Reinforced Visual Autoregressive Model for High-Fidelity Subject-Driven Image Generation](https://arxiv.org/abs/2601.22507)
*Xin Jiang,Jingwen Chen,Yehao Li,Yingwei Pan,Kezhou Chen,Zechao Li,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: DreamVAR提出在VAR自回归生成中预填充多尺度参考主体特征并结合下一尺度预测与强化学习，显著提高主体外观保留与语义一致性，优于多款扩散模型基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的方法在主体保真上虽表现出色，但VAR模型的潜力尚未充分挖掘；作者希望利用VAR的统一架构和高效推理优势，通过设计针对多尺度条件的新策略来提升主体驱动合成效果。

Method: 框架流程包括：1) 用视觉tokenizer提取参考主体的多尺度特征；2) 在自回归生成时先预填充所有尺度的主体特征序列，再预测目标图像tokens（即下一尺度预测机制）；3) 引入强化学习优化语义对齐与主体一致性，联合训练以提升保真度与一致性。

Result: 大量实验表明，DreamVAR在主体外观保留方面优于主流扩散基线方法，且在多尺度条件下减少训练-测试不一致，获得更好的语义对齐和主体一致性。

Conclusion: DreamVAR在VAR模型基础上通过先填充完整的多尺度参考主体特征序列并采用下一尺度预测，简化了自回归依赖并减少多尺度条件下训练-测试差异，从而在主体驱动图像合成任务中实现更好的外观保留。

Abstract: Recent advances in subject-driven image generation using diffusion models have attracted considerable attention for their remarkable capabilities in producing high-quality images. Nevertheless, the potential of Visual Autoregressive (VAR) models, despite their unified architecture and efficient inference, remains underexplored. In this work, we present DreamVAR, a novel framework for subject-driven image synthesis built upon a VAR model that employs next-scale prediction. Technically, multi-scale features of the reference subject are first extracted by a visual tokenizer. Instead of interleaving these conditional features with target image tokens across scales, our DreamVAR pre-fills the full subject feature sequence prior to predicting target image tokens. This design simplifies autoregressive dependencies and mitigates the train-test discrepancy in multi-scale conditioning scenario within the VAR paradigm. DreamVAR further incorporates reinforcement learning to jointly enhance semantic alignment and subject consistency. Extensive experiments demonstrate that DreamVAR achieves superior appearance preservation compared to leading diffusion-based methods.

</details>


### [18] [CoVA: Text-Guided Composed Video Retrieval for Audio-Visual Content](https://arxiv.org/abs/2601.22508)
*Gyuwon Han,Young Kyun Jang,Chanho Eom*

Main category: cs.CV

TL;DR: 论文引入CoVA任务与AV-Comp数据集，将音频纳入复合视频检索，并提出AVT模型通过选择性模态对齐融合视听文本特征，显著优于单纯视觉融合基线。


<details>
  <summary>Details</summary>
Motivation: 现有复合视频检索基准仅关注视觉修改，忽视视觉相似但音频不同的情形；现实应用中音频信息常带有语义差异，因而需要同时建模视听变化以提高检索准确性。

Method: 构建了包含跨模态（视听）变化的视频对及文本差异描述的数据集AV-Comp；提出AVT Compositional Fusion (AVT) 模型，通过选择性地将查询与最相关模态（视频或音频）对齐，实现视频、音频与文本特征的融合，并与传统的单模态融合方法比较。

Result: 提出的数据集AV-Comp包含带有视听差异的视频对及相应文本查询；AVT模型在CoVA任务上优于传统的单模态融合方法，成为强基线。

Conclusion: 该论文提出了将音频纳入复合视频检索任务，扩展了现有仅关注视觉变化的CoVR设定，命名为CoVA，强调检索应同时考虑视觉与听觉差异。

Abstract: Composed Video Retrieval (CoVR) aims to retrieve a target video from a large gallery using a reference video and a textual query specifying visual modifications. However, existing benchmarks consider only visual changes, ignoring videos that differ in audio despite visual similarity. To address this limitation, we introduce Composed retrieval for Video with its Audio CoVA, a new retrieval task that accounts for both visual and auditory variations. To support this, we construct AV-Comp, a benchmark consisting of video pairs with cross-modal changes and corresponding textual queries that describe the differences. We also propose AVT Compositional Fusion (AVT), which integrates video, audio, and text features by selectively aligning the query to the most relevant modality. AVT outperforms traditional unimodal fusion and serves as a strong baseline for CoVA. Examples from the proposed dataset, including both visual and auditory information, are available at https://perceptualai-lab.github.io/CoVA/.

</details>


### [19] [DNA: Uncovering Universal Latent Forgery Knowledge](https://arxiv.org/abs/2601.22515)
*Jingtong Dou,Chuancheng Shi,Yemin Wang,Shiming Guo,Anqi Yi,Wenhua Wu,Li Zhang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 论文提出无需微调、通过挖掘预训练模型内部伪造敏感单元（FDUs）的DNA方法，并发布HIFI-Gen基准，展示在少样本及跨模型情形下的优越鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI逼真度提升，表面伪影检测失效。现有方法依赖大量资源微调黑盒骨干，作者假设预训练模型本身已编码伪造检测能力，可通过激活特定神经元唤醒。

Method: 提出DNA（discriminative neural anchors）框架：通过特征解耦与注意力分布迁移分析定位关键中间层，使用三元融合评分与曲率截断策略去除语义冗余，精确提取伪造-判别单元（FDUs）。

Result: 建立了基于最新生成模型的高保真合成基准HIFI-Gen，实验证明仅基于这些锚点的DNA在少样本条件下仍能取得优越检测性能，并对不同架构和未知生成器表现出显著鲁棒性，优于大规模微调方案。

Conclusion: 该论文认为无需端到端微调即可唤醒预训练模型中已存在的伪造检测能力，通过挖掘关键中间层的“伪造敏感”单元实现高效稳健的检测方法。

Abstract: As generative AI achieves hyper-realism, superficial artifact detection has become obsolete. While prevailing methods rely on resource-intensive fine-tuning of black-box backbones, we propose that forgery detection capability is already encoded within pre-trained models rather than requiring end-to-end retraining. To elicit this intrinsic capability, we propose the discriminative neural anchors (DNA) framework, which employs a coarse-to-fine excavation mechanism. First, by analyzing feature decoupling and attention distribution shifts, we pinpoint critical intermediate layers where the focus of the model logically transitions from global semantics to local anomalies. Subsequently, we introduce a triadic fusion scoring metric paired with a curvature-truncation strategy to strip away semantic redundancy, precisely isolating the forgery-discriminative units (FDUs) inherently imprinted with sensitivity to forgery traces. Moreover, we introduce HIFI-Gen, a high-fidelity synthetic benchmark built upon the very latest models, to address the lag in existing datasets. Experiments demonstrate that by solely relying on these anchors, DNA achieves superior detection performance even under few-shot conditions. Furthermore, it exhibits remarkable robustness across diverse architectures and against unseen generative models, validating that waking up latent neurons is more effective than extensive fine-tuning.

</details>


### [20] [Can 3D point cloud data improve automated body condition score prediction in dairy cattle?](https://arxiv.org/abs/2601.22522)
*Zhou Tang,Jin Wang,Angelo De Castro,Yuxi Zhang,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Xu Wang,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: 在此数据与设定下，深度图像比点云更稳健，点云未显示出一致优势，且更易受噪声与模型选择影响。


<details>
  <summary>Details</summary>
Motivation: 传统视觉评分主观且费时，深度图像与点云能提供与毛色无关的几何信息，作者欲评估点云是否能在BCS预测上优于已有的深度图像方法。

Method: 比较顶视深度图像与三维点云在四种数据处理设定（原始未分割、全身分割、后躯分割、手工特征）下的BCS预测性能，使用来自1020头奶牛的商业农场数据，并采用以牛为单位的交叉验证以防数据泄漏。

Result: 深度图像在未经分割与全身分割时均优于点云；在仅后躯分割时两者性能相当；使用手工特征时两者准确率下降。点云预测对噪声和模型架构更为敏感。

Conclusion: 在该研究条件下，三维点云并未在BCS预测上普遍优于深度图像；深度图像在未经分割与全身分割情形下表现更稳定、准确，点云对噪声与模型结构更敏感。

Abstract: Body condition score (BCS) is a widely used indicator of body energy status and is closely associated with metabolic status, reproductive performance, and health in dairy cattle; however, conventional visual scoring is subjective and labor-intensive. Computer vision approaches have been applied to BCS prediction, with depth images widely used because they capture geometric information independent of coat color and texture. More recently, three-dimensional point cloud data have attracted increasing interest due to their ability to represent richer geometric characteristics of animal morphology, but direct head-to-head comparisons with depth image-based approaches remain limited. In this study, we compared top-view depth image and point cloud data for BCS prediction under four settings: 1) unsegmented raw data, 2) segmented full-body data, 3) segmented hindquarter data, and 4) handcrafted feature data. Prediction models were evaluated using data from 1,020 dairy cows collected on a commercial farm, with cow-level cross-validation to prevent data leakage. Depth image-based models consistently achieved higher accuracy than point cloud-based models when unsegmented raw data and segmented full-body data were used, whereas comparable performance was observed when segmented hindquarter data were used. Both depth image and point cloud approaches showed reduced accuracy when handcrafted feature data were employed compared with the other settings. Overall, point cloud-based predictions were more sensitive to noise and model architecture than depth image-based predictions. Taken together, these results indicate that three-dimensional point clouds do not provide a consistent advantage over depth images for BCS prediction in dairy cattle under the evaluated conditions.

</details>


### [21] [SHED Light on Segmentation for Dense Prediction](https://arxiv.org/abs/2601.22529)
*Seung Hyun Lee,Sangwoo Mo,Stella X. Yu*

Main category: cs.CV

TL;DR: SHED injects emergent hierarchical segmentation into an encoder-decoder for dense prediction, producing structurally coherent depth, segmentation, and 3D reconstruction with strong generalization.


<details>
  <summary>Details</summary>
Motivation: To address structural inconsistencies of per-pixel dense prediction by leveraging scene structure via segmentation priors, enabling more coherent and geometrically consistent depth, segmentation, and 3D reconstruction.

Method: SHED uses an encoder that hierarchically pools segment tokens and a decoder that unpools them via bidirectional hierarchical reasoning; supervision is applied only to final outputs so segment hierarchies emerge without explicit segmentation labels.

Result: SHED improves depth boundary sharpness, segment coherence, cross-domain generalization from synthetic to real data, semantic segmentation accuracy through hierarchy-aware decoding, and 3D reconstruction quality with interpretable part-level structures.

Conclusion: The paper introduces SHED, an encoder-decoder that enforces geometric priors by integrating hierarchical, bidirectional segmentation tokens into dense prediction, yielding improved structural consistency, sharper depth boundaries, better cross-domain generalization, enhanced semantic segmentation and 3D reconstruction, and interpretable part-level structures.

Abstract: Dense prediction infers per-pixel values from a single image and is fundamental to 3D perception and robotics. Although real-world scenes exhibit strong structure, existing methods treat it as an independent pixel-wise prediction, often resulting in structural inconsistencies. We propose SHED, a novel encoder-decoder architecture that enforces geometric prior explicitly by incorporating segmentation into dense prediction. By bidirectional hierarchical reasoning, segment tokens are hierarchically pooled in the encoder and unpooled in the decoder to reverse the hierarchy. The model is supervised only at the final output, allowing the segment hierarchy to emerge without explicit segmentation supervision. SHED improves depth boundary sharpness and segment coherence, while demonstrating strong cross-domain generalization from synthetic to the real-world environments. Its hierarchy-aware decoder better captures global 3D scene layouts, leading to improved semantic segmentation performance. Moreover, SHED enhances 3D reconstruction quality and reveals interpretable part-level structures that are often missed by conventional pixel-wise methods.

</details>


### [22] [Hybrid Cross-Device Localization via Neural Metric Learning and Feature Fusion](https://arxiv.org/abs/2601.22551)
*Meixia Lin,Mingkai Liu,Shuxue Peng,Dikai Fan,Shengyu Gu,Xianliang Huang,Haoyang Ye,Xiao Liu*

Main category: cs.CV

TL;DR: 提出混合跨设备定位框架：共享检索编码器+几何PnP分支+神经前馈分支+神经筛选+深度条件化定位，挑战成绩92.62。


<details>
  <summary>Details</summary>
Motivation: 解决跨设备定位中检索与度量定位之间的差异与尺度不一致问题，提高召回率和位姿精度，兼顾鲁棒性与精度。

Method: 采用共享检索编码器生成候选地图帧；经典几何分支通过特征融合和PnP进行位姿估计；神经前馈分支（MapAnything）以几何输入为条件直接回归度量位姿；使用神经引导的候选剪枝基于平移一致性过滤不可靠的地图帧；在Spot场景上引入深度条件化的定位以校正尺度并精化平移。

Result: 在HYDRO与SUCCU基准上显著提升召回率和精度，最终在挑战中达到92.62（R@0.5m,5°）。

Conclusion: 该方法通过结合经典几何分支与神经网络前馈分支，以及检索编码器与神经引导的候选筛选与深度条件化定位，在CroCoDL 2025 Challenge中实现了高精度跨设备定位，最终在R@0.5m,5°上取得92.62的成绩。

Abstract: We present a hybrid cross-device localization pipeline developed for the CroCoDL 2025 Challenge. Our approach integrates a shared retrieval encoder and two complementary localization branches: a classical geometric branch using feature fusion and PnP, and a neural feed-forward branch (MapAnything) for metric localization conditioned on geometric inputs. A neural-guided candidate pruning strategy further filters unreliable map frames based on translation consistency, while depth-conditioned localization refines metric scale and translation precision on Spot scenes. These components jointly lead to significant improvements in recall and accuracy across both HYDRO and SUCCU benchmarks. Our method achieved a final score of 92.62 (R@0.5m, 5°) during the challenge.

</details>


### [23] [Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction](https://arxiv.org/abs/2601.22570)
*Aditya Sarkar,Yi Li,Jiacheng Cheng,Shlok Mishra,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 提出一种基于检索记忆与对比归一化的训练免费方法MA-PaPSP，解决视觉-语言嵌入方差大与相似度校准差的问题，从而提高基础模型的选择性预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有选择性预测主要针对封闭集任务，难以直接应用于具有开放集或无限词汇表的视觉-语言基础模型（如图像描述）。且基于CLIP等的视觉语言嵌入存在不稳定性与相似度校准不良的问题，影响拒绝选项的可靠性。

Method: 在基础的PaPSP框架上，构建一个检索数据库（图像-文本对），对输入图像基于视觉-语言嵌入检索K近邻并对其文本或相似度进行平均以降低嵌入不稳定性，同时对相似度分数施加对比归一化（contrastive normalization）以改善校准，整个流程无需对模型重训练。

Result: 在多个数据集上进行的大规模实验表明，MA-PaPSP在选择性图像描述、图文匹配与细粒度分类任务中，相比原始PaPSP和其他选择性预测基线方法，在拒绝-性能权衡（例如保留率与准确率/质量）上取得显著提升。作者已公开代码。

Conclusion: 本文提出了Memory Augmented Plug-and-Play Selective Prediction（MA-PaPSP），在无需训练的前提下增强视觉-语言基础模型的选择性预测能力，通过检索记忆集和对比归一化来降低嵌入方差并改善相似度分数校准，从而在多任务（选择性图像描述、图文匹配、细粒度分类）上优于基线。

Abstract: Selective prediction aims to endow predictors with a reject option, to avoid low confidence predictions. However, existing literature has primarily focused on closed-set tasks, such as visual question answering with predefined options or fixed-category classification. This paper considers selective prediction for visual language foundation models, addressing a taxonomy of tasks ranging from closed to open set and from finite to unbounded vocabularies, as in image captioning. We seek training-free approaches of low-complexity, applicable to any foundation model and consider methods based on external vision-language model embeddings, like CLIP. This is denoted as Plug-and-Play Selective Prediction (PaPSP). We identify two key challenges: (1) instability of the visual-language representations, leading to high variance in image-text embeddings, and (2) poor calibration of similarity scores. To address these issues, we propose a memory augmented PaPSP (MA-PaPSP) model, which augments PaPSP with a retrieval dataset of image-text pairs. This is leveraged to reduce embedding variance by averaging retrieved nearest-neighbor pairs and is complemented by the use of contrastive normalization to improve score calibration. Through extensive experiments on multiple datasets, we show that MA-PaPSP outperforms PaPSP and other selective prediction baselines for selective captioning, image-text matching, and fine-grained classification. Code is publicly available at https://github.com/kingston-aditya/MA-PaPSP.

</details>


### [24] [DELNet: Continuous All-in-One Weather Removal via Dynamic Expert Library](https://arxiv.org/abs/2601.22573)
*Shihong Liu,Kun Zuo,Hanguang Xiao*

Main category: cs.CV

TL;DR: 提出DELNet：通过任务相似度判断与动态专家库实现天气图像恢复的持续学习，减少重训练并显著提升PSNR。


<details>
  <summary>Details</summary>
Motivation: 现有全能天气图像恢复方法依赖大量预先收集的数据，且遇到未见降解需重训练，成本高且不适合实际部署。DELNet旨在降低重训练成本并实现在线/持续部署。

Method: DELNet包含判断阀（measure task similarity）和动态专家库（dynamic expert library）。对于新任务，阀选取top-k专家进行知识迁移并新增专家以学习任务特有特征；对于已知任务，直接复用对应专家。该机制允许在不影响既有专家的前提下持续优化。

Result: 在OTS、Rain100H和Snow100K上，DELNet优于现有持续学习方法，分别带来PSNR提升约16%、11%和12%，显示出在效果、鲁棒性与效率上的优势。

Conclusion: DELNet提出了一种可持续增量学习框架，用于天气图像恢复，通过判断阀衡量任务相似度并动态管理专家库，实现对新/已知降解的高效处理，从而避免对整个模型的重训练。

Abstract: All-in-one weather image restoration methods are valuable in practice but depend on pre-collected data and require retraining for unseen degradations, leading to high cost. We propose DELNet, a continual learning framework for weather image restoration. DELNet integrates a judging valve that measures task similarity to distinguish new from known tasks, and a dynamic expert library that stores experts trained on different degradations. For new tasks, the valve selects top-k experts for knowledge transfer while adding new experts to capture task-specific features; for known tasks, the corresponding experts are directly reused. This design enables continuous optimization without retraining existing models. Experiments on OTS, Rain100H, and Snow100K demonstrate that DELNet surpasses state-of-the-art continual learning methods, achieving PSNR gains of 16\%, 11\%, and 12\%, respectively. These results highlight the effectiveness, robustness, and efficiency of DELNet, which reduces retraining cost and enables practical deployment in real-world scenarios.

</details>


### [25] [Mitigating Hallucinations in Video Large Language Models via Spatiotemporal-Semantic Contrastive Decoding](https://arxiv.org/abs/2601.22574)
*Yuansheng Gao,Jinman Zhao,Tong Zhang,Xingguo Xu,Han Bao,Zonghui Wang,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出时空语义对比解码，通过构造扰动的负特征并在推理时进行对比抑制，有效减少视频大模型幻觉且保持理解与推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频解码方法多依赖启发式设计，难以精确捕捉幻觉根源及其细粒度的时序与语义关联，导致在复杂场景下鲁棒性与泛化性不足。

Method: 通过构造负样本（扰动时空一致性与语义关联的特征），在推理阶段对原始视频特征进行对比解码，抑制与负样本相似的生成，减少幻觉产生。

Result: 大量实验表明，该方法在显著降低幻觉发生率的同时，保持了模型的视频理解与推理能力。

Conclusion: 本文提出了一种基于对比解码的新策略——时空语义对比解码，以抑制视频大模型生成与视频内容不一致的幻觉。

Abstract: Although Video Large Language Models perform remarkably well across tasks such as video understanding, question answering, and reasoning, they still suffer from the problem of hallucination, which refers to generating outputs that are inconsistent with explicit video content or factual evidence. However, existing decoding methods for mitigating video hallucinations, while considering the spatiotemporal characteristics of videos, mostly rely on heuristic designs. As a result, they fail to precisely capture the root causes of hallucinations and their fine-grained temporal and semantic correlations, leading to limited robustness and generalization in complex scenarios. To more effectively mitigate video hallucinations, we propose a novel decoding strategy termed Spatiotemporal-Semantic Contrastive Decoding. This strategy constructs negative features by deliberately disrupting the spatiotemporal consistency and semantic associations of video features, and suppresses video hallucinations through contrastive decoding against the original video features during inference. Extensive experiments demonstrate that our method not only effectively mitigates the occurrence of hallucinations, but also preserves the general video understanding and reasoning capabilities of the model.

</details>


### [26] [PhoStream: Benchmarking Real-World Streaming for Omnimodal Assistants in Mobile Scenarios](https://arxiv.org/abs/2601.22575)
*Xudong Lu,Huankang Guan,Yang Bo,Jinpeng Chen,Xintong Guo,Shuhan Li,Fang Liu,Peiwen Sun,Xueying Li,Wei Zhang,Xue Yang,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: PhoStream为移动流式多模态理解提出了首个基准，证明当前MMLM更擅长内容而非发言时机，需重视何时发言的能力改进。


<details>
  <summary>Details</summary>
Motivation: 移动助理需要在连续的音视频流中实时判断并回应，但现有基准多为选择题或短视频，无法考察在线流式理解与时机判断能力。

Method: 构建了包含5,572条开放式问答、578个视频、4类场景和10项能力的数据集；采用自动生成流水线并辅以人工验证；评估时使用在线推理流水线并以LLM作为判分器对开放式回答打分。

Result: 实验显示在LLM判分（0-100）下，模型在瞬时与回顾任务上表现良好（如Gemini 3 Pro>80），但在前瞻任务上表现严重下降（如16.40），主要因模型倾向于在视觉/音频线索出现前提前回答。

Conclusion: 本文提出了PhoStream，一个面向移动场景的流式多模态理解基准，揭示了现有多模态大模型在“何时说话”上的显著不足。

Abstract: Multimodal Large Language Models excel at offline audio-visual understanding, but their ability to serve as mobile assistants in continuous real-world streams remains underexplored. In daily phone use, mobile assistants must track streaming audio-visual inputs and respond at the right time, yet existing benchmarks are often restricted to multiple-choice questions or use shorter videos. In this paper, we introduce PhoStream, the first mobile-centric streaming benchmark that unifies on-screen and off-screen scenarios to evaluate video, audio, and temporal reasoning. PhoStream contains 5,572 open-ended QA pairs from 578 videos across 4 scenarios and 10 capabilities. We build it with an Automated Generative Pipeline backed by rigorous human verification, and evaluate models using a realistic Online Inference Pipeline and LLM-as-a-Judge evaluation for open-ended responses. Experiments reveal a temporal asymmetry in LLM-judged scores (0-100): models perform well on Instant and Backward tasks (Gemini 3 Pro exceeds 80), but drop sharply on Forward tasks (16.40), largely due to early responses before the required visual and audio cues appear. This highlights a fundamental limitation: current MLLMs struggle to decide when to speak, not just what to say. Code and datasets used in this work will be made publicly accessible at https://github.com/Lucky-Lance/PhoStream.

</details>


### [27] [Cross-Domain Few-Shot Learning for Hyperspectral Image Classification Based on Mixup Foundation Model](https://arxiv.org/abs/2601.22581)
*Naeem Paeedeh,Mahardhika Pratama,Ary Shiddiqi,Zehong Cao,Mukesh Prasad,Wisnu Jatmiko*

Main category: cs.CV

TL;DR: 利用预训练遥感基础模型并结合共聚投影、mixup域自适应与标签平滑，MIFOMO在跨域少样本高光谱分类中显著优于先前方法（最高提升14%）。


<details>
  <summary>Details</summary>
Motivation: 现有跨域少样本高光谱分类研究常依赖外部噪声增强样本、并需要大量参数更新，易过拟合，且未充分利用具备强泛化能力的遥感基础模型。为解决样本稀缺、域差异大和伪标签噪声问题，提出基于基础模型的快速适配与域混合策略。

Method: 方法包括：1) 使用大规模遥感任务预训练的基础模型作为特征骨干，保留其通用特征表示；2) 引入共聚投影（coalescent projection, CP）模块，在保持骨干冻结的前提下，通过轻量投影层快速适配下游任务；3) 提出mixup域自适应（MDM），在域间混合样本与标签以缓解极端域差异；4) 引入标签平滑以减少伪标签噪声影响。

Result: 作者在严格实验中展示，MIFOMO较现有方法最高提升约14%的性能，并公开了代码以便复现与后续研究。

Conclusion: 本文提出的MIFOMO模型在跨域少样本高光谱图像分类上表现出显著优势，通过利用预训练的遥感基础模型并结合配对投影、mixup域自适应和标签平滑策略，实现了在冻结主干网络情况下的快速迁移与鲁棒性提升。

Abstract: Although cross-domain few-shot learning (CDFSL) for hyper-spectral image (HSI) classification has attracted significant research interest, existing works often rely on an unrealistic data augmentation procedure in the form of external noise to enlarge the sample size, thus greatly simplifying the issue of data scarcity. They involve a large number of parameters for model updates, being prone to the overfitting problem. To the best of our knowledge, none has explored the strength of the foundation model, having strong generalization power to be quickly adapted to downstream tasks. This paper proposes the MIxup FOundation MOdel (MIFOMO) for CDFSL of HSI classifications. MIFOMO is built upon the concept of a remote sensing (RS) foundation model, pre-trained across a large scale of RS problems, thus featuring generalizable features. The notion of coalescent projection (CP) is introduced to quickly adapt the foundation model to downstream tasks while freezing the backbone network. The concept of mixup domain adaptation (MDM) is proposed to address the extreme domain discrepancy problem. Last but not least, the label smoothing concept is implemented to cope with noisy pseudo-label problems. Our rigorous experiments demonstrate the advantage of MIFOMO, where it beats prior arts with up to 14% margin. The source code of MIFOMO is open-sourced in https://github.com/Naeem- Paeedeh/MIFOMO for reproducibility and convenient further study.

</details>


### [28] [FOTBCD: A Large-Scale Building Change Detection Benchmark from French Orthophotos and Topographic Data](https://arxiv.org/abs/2601.22596)
*Abdelrrahman Moubane*

Main category: cs.CV

TL;DR: FOTBCD是一个覆盖28个法国产省、分辨率0.2m/像素的大规模建筑变化检测数据集，包含约28k对带像素级二元掩码的图像对及实例级子集，专注评估地理域外泛化，实验显示地理多样性有助于跨域性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有建筑变化检测基准数据集通常局限于单一城市或区域，缺乏地理多样性；作者希望通过更广泛的地理覆盖以改善训练模型在新的地理域上的泛化能力并提供大规模公开基准。

Method: 基于IGN法国的正射影像和地形建筑要素构建数据集，按省划分训练/验证/测试集（25省训练，3省持出评估），生成约28,000对前后图像的像素级二元变化掩码（FOTBCD-Binary）及若干千对图像的实例级注释子集（FOTBCD-Instances），并使用固定基线在LEVER-CD+和WHU-CD上进行基准对比评估跨域泛化性能。

Result: 发布了FOTBCD-Binary（~28k对图像）和FOTBCD-Instances子集，验证集和测试集来自持出省且人为校验标签质量；基准实验表明，数据集的地理多样性与跨域泛化性能提升相关。

Conclusion: 该论文提出了FOTBCD，一个覆盖法国内28个省、精度为0.2m/像素的大规模建筑物变化检测数据集，公开发布了二元和实例级子集，并在地域域外转移上表现出更好的泛化能力。

Abstract: We introduce FOTBCD, a large-scale building change detection dataset derived from authoritative French orthophotos and topographic building data provided by IGN France. Unlike existing benchmarks that are geographically constrained to single cities or limited regions, FOTBCD spans 28 departments across mainland France, with 25 used for training and three geographically disjoint departments held out for evaluation. The dataset covers diverse urban, suburban, and rural environments at 0.2m/pixel resolution. We publicly release FOTBCD-Binary, a dataset comprising approximately 28,000 before/after image pairs with pixel-wise binary building change masks, each associated with patch-level spatial metadata. The dataset is designed for large-scale benchmarking and evaluation under geographic domain shift, with validation and test samples drawn from held-out departments and manually verified to ensure label quality. In addition, we publicly release FOTBCD-Instances, a publicly available instance-level annotated subset comprising several thousand image pairs, which illustrates the complete annotation schema used in the full instance-level version of FOTBCD. Using a fixed reference baseline, we benchmark FOTBCD-Binary against LEVIR-CD+ and WHU-CD, providing strong empirical evidence that geographic diversity at the dataset level is associated with improved cross-domain generalization in building change detection.

</details>


### [29] [TTSA3R: Training-Free Temporal-Spatial Adaptive Persistent State for Streaming 3D Reconstruction](https://arxiv.org/abs/2601.22615)
*Zhijie Zheng,Xinhao Xiang,Jiawei Zhang*

Main category: cs.CV

TL;DR: 提出TTSA3R，通过时间和空间双信号无训练自适应状态更新，显著缓解长期序列的记忆遗忘问题，在延长序列上表现稳健，误差增长仅15%对比基线的200%+


<details>
  <summary>Details</summary>
Motivation: 流式递归3D重建在长序列下易发生灾难性遗忘，现有基于注意力的自适应方法通常只在单一维度上工作，缺乏时间与空间一致性的联合考量。

Method: 设计了两个模块：Temporal Adaptive Update Module通过分析时间上状态演化模式来调节更新幅度；Spatial Contextual Update Module通过观测-状态对齐与场景动态定位需要更新的空间区域。两个模块的输出融合以决定最终更新策略。

Result: 在多种3D任务上，TTSA3R表现优越：在延长序列时，仅导致约15%的误差增长，而基线模型误差增长超过200%，表明显著提升长期重建稳定性。

Conclusion: 本文提出了TTSA3R，一个无训练框架，通过同时利用时间状态演化和空间观测质量来实现自适应状态更新，显著减缓长期序列中的记忆遗忘问题。

Abstract: Streaming recurrent models enable efficient 3D reconstruction by maintaining persistent state representations. However, they suffer from catastrophic memory forgetting over long sequences due to balancing historical information with new observations. Recent methods alleviate this by deriving adaptive signals from attention perspective, but they operate on single dimensions without considering temporal and spatial consistency. To this end, we propose a training-free framework termed TTSA3R that leverages both temporal state evolution and spatial observation quality for adaptive state updates in 3D reconstruction. In particular, we devise a Temporal Adaptive Update Module that regulates update magnitude by analyzing temporal state evolution patterns. Then, a Spatial Contextual Update Module is introduced to localize spatial regions that require updates through observation-state alignment and scene dynamics. These complementary signals are finally fused to determine the state updating strategies. Extensive experiments demonstrate the effectiveness of TTSA3R in diverse 3D tasks. Moreover, our method exhibits only 15% error increase compared to over 200% degradation in baseline models on extended sequences, significantly improving long-term reconstruction stability. Our codes will be available soon.

</details>


### [30] [UniGeo: A Unified 3D Indoor Object Detection Framework Integrating Geometry-Aware Learning and Dynamic Channel Gating](https://arxiv.org/abs/2601.22616)
*Xing Yi,Jinyang Huang,Feng-Qi Cui,Anyang Tong,Ruimin Wang,Liu Liu,Dan Guo*

Main category: cs.CV

TL;DR: UniGeo通过几何感知权重映射和动态通道门控增强稀疏点云特征，在六个室内数据集上显著提升3D目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨数据集统一训练方法未充分建模稀疏点云场景中的几何关系且忽略重要区域的特征分布，导致检测性能受限。

Method: 设计几何感知学习模块将空间关系映射为可学习的特征权重以增强几何特征；设计动态通道门控对稀疏3D U-Net输出的通道进行自适应加权，突出关键几何信息；将两者整合到统一的3D室内检测框架并进行训练与评估。

Result: 在六个不同室内场景数据集上的大量实验证明UniGeo在性能上优于对比方法，表明几何增强与通道门控能显著提升检测效果。

Conclusion: 提出的UniGeo通过几何感知学习模块与动态通道门控机制，增强稀疏点云特征，改善了多数据集统一训练下的性能限制。

Abstract: The growing adoption of robotics and augmented reality in real-world applications has driven considerable research interest in 3D object detection based on point clouds. While previous methods address unified training across multiple datasets, they fail to model geometric relationships in sparse point cloud scenes and ignore the feature distribution in significant areas, which ultimately restricts their performance. To deal with this issue, a unified 3D indoor detection framework, called UniGeo, is proposed. To model geometric relations in scenes, we first propose a geometry-aware learning module that establishes a learnable mapping from spatial relationships to feature weights, which enabes explicit geometric feature enhancement. Then, to further enhance point cloud feature representation, we propose a dynamic channel gating mechanism that leverages learnable channel-wise weighting. This mechanism adaptively optimizes features generated by the sparse 3D U-Net network, significantly enhancing key geometric information. Extensive experiments on six different indoor scene datasets clearly validate the superior performance of our method.

</details>


### [31] [LINA: Linear Autoregressive Image Generative Models with Continuous Tokens](https://arxiv.org/abs/2601.22630)
*Jiahao Wang,Ting Pan,Haoge Deng,Dongchen Han,Taiqiang Wu,Xinlong Wang,Ping Luo*

Main category: cs.CV

TL;DR: TL;DR：通过系统实验和设计改进（division-based归一化、局部卷积、KV门控），作者构建了LINA——一个全线性注意力的高效T2I模型，在大分辨率图像生成上实现了与softmax注意力模型竞品相当的质量同时显著降低计算量。


<details>
  <summary>Details</summary>
Motivation: 动机是降低自回归视觉生成中高开销的softmax注意力计算成本，探索在保持生成质量前提下更高效的线性注意力设计，明确不同归一化方式和局部性增强手段对可扩展性和生成质量的影响，并提升线性注意力的记忆管理能力以适应双向建模需求。

Method: 方法上，作者对线性自回归Transformer在参数规模下的扩展性进行了系统性实验，比较了两类归一化方案（division-based vs subtraction-based）和在注意力中加入深度可分卷积以增强局部性；将常用于因果线性注意力的门控机制推广到双向情形，提出KV gate，通过向key和value状态引入与数据无关的可学习参数为每个token分配记忆权重；在此基础上构建LINA模型并在ImageNet和GenEval等T2I基准上评估。

Result: 结果包括：1) 在生成任务中division-based归一化随参数规模扩展优于subtraction-based；2) 深度可分卷积显著提升自回归生成质量；3) KV gate在双向线性注意力中有效分配记忆权重，提升模型表现；4) 提出LINA能在约1.4–1.5B参数规模下取得ImageNet FID=2.18与GenEval=0.74；单个线性注意力模块将FLOPs相比softmax注意力降低约61%。

Conclusion: 本论文得出结论：在自回归视觉生成中，基于除法的归一化（division-based）较减法归一化（subtraction-based）对线性注意力的扩展性更好；深度可分卷积用于增强局部性对生成任务至关重要；并通过引入KV gate提高双向线性注意力的记忆管理能力，从而在纯线性注意力架构上实现高效高质量的文本到图像（T2I）生成。基于这些发现，提出了LINA，一种完全由线性注意力构成的计算高效T2I模型，能生成1024x1024高分辨率图像，并在多个基准上取得竞争性能。

Abstract: Autoregressive models with continuous tokens form a promising paradigm for visual generation, especially for text-to-image (T2I) synthesis, but they suffer from high computational cost. We study how to design compute-efficient linear attention within this framework. Specifically, we conduct a systematic empirical analysis of scaling behavior with respect to parameter counts under different design choices, focusing on (1) normalization paradigms in linear attention (division-based vs. subtraction-based) and (2) depthwise convolution for locality augmentation.
  Our results show that although subtraction-based normalization is effective for image classification, division-based normalization scales better for linear generative transformers. In addition, incorporating convolution for locality modeling plays a crucial role in autoregressive generation, consistent with findings in diffusion models.
  We further extend gating mechanisms, commonly used in causal linear attention, to the bidirectional setting and propose a KV gate. By introducing data-independent learnable parameters to the key and value states, the KV gate assigns token-wise memory weights, enabling flexible memory management similar to forget gates in language models.
  Based on these findings, we present LINA, a simple and compute-efficient T2I model built entirely on linear attention, capable of generating high-fidelity 1024x1024 images from user instructions. LINA achieves competitive performance on both class-conditional and T2I benchmarks, obtaining 2.18 FID on ImageNet (about 1.4B parameters) and 0.74 on GenEval (about 1.5B parameters). A single linear attention module reduces FLOPs by about 61 percent compared to softmax attention. Code and models are available at: https://github.com/techmonsterwang/LINA.

</details>


### [32] [What can Computer Vision learn from Ranganathan?](https://arxiv.org/abs/2601.22634)
*Mayukh Bagchi,Fausto Giunchiglia*

Main category: cs.CV

TL;DR: 将Ranganathan分类原则改编用于计算机视觉注释，提出vTelos方法并通过实验验证其能改善注释质量与模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有CV数据集受视觉与语言语义错配影响，导致标签模糊、重叠和基准测试失真，亟需有原则性的方法学来指导高质量标签体系与注释实践。

Method: 提出将五大Ranganathan原则（如穷尽、互斥、简洁等）适配到视觉语义标签体系，形成vTelos注释流程；在数据集构建阶段对标签定义、层级组织和实例选择进行规则化约束，并在标注过程中结合人工-机器协同验证。

Result: 通过小规模实验（包括注释一致性评估和基准分类任务），vTelos显示出提高标注一致性、减少标签冲突并小幅提升下游模型准确率的效果，从而初步验证了方法有效性。

Conclusion: 本文将Ranganathan的分类原则应用于计算机视觉数据集设计，以解决视觉语义与词汇语义不一致的语义鸿沟问题，提出了vTelos注释方法并验证其能提升标注质量和模型准确性。

Abstract: The Semantic Gap Problem (SGP) in Computer Vision (CV) arises from the misalignment between visual and lexical semantics leading to flawed CV dataset design and CV benchmarks. This paper proposes that classification principles of S.R. Ranganathan can offer a principled starting point to address SGP and design high-quality CV datasets. We elucidate how these principles, suitably adapted, underpin the vTelos CV annotation methodology. The paper also briefly presents experimental evidence showing improvements in CV annotation and accuracy, thereby, validating vTelos.

</details>


### [33] [Unsupervised Synthetic Image Attribution: Alignment and Disentanglement](https://arxiv.org/abs/2601.22663)
*Zongfang Liu,Guangyi Chen,Boyang Sun,Tongliang Liu,Kun Zhang*

Main category: cs.CV

TL;DR: 提出以对比自监督对齐加上Infomax去纠缠的无监督合成图像归属方法，并通过理论解析与实验证明其可在无标注情况下超越监督方法。


<details>
  <summary>Details</summary>
Motivation: 在标注配对的合成图像与真实源图像代价高昂或不可行的情况下，探索是否能通过无监督方法实现合成图像归属，以便保护版权并提升模型透明性。

Method: 先用对比自监督学习（如MoCo、DINO）实现跨域的基础概念对齐，再通过Infomax损失促进表示的去纠缠，理论上把该过程等价或近似为对典型相关分析目标的分解，并在真实基准（AbC）上进行验证。

Result: 在AbC等真实世界基准上，提出的方法在无监督情形下竟然超过了现有监督方法，表明对齐+去纠缠策略在概念匹配上有效。

Conclusion: 该论文的无监督方法（Alignment and Disentanglement，AbC）通过对比自监督学习进行概念对齐并结合信息最大化损失实现表示去纠缠，从而在合成图像归属任务中取得了优于监督方法的结果。

Abstract: As the quality of synthetic images improves, identifying the underlying concepts of model-generated images is becoming increasingly crucial for copyright protection and ensuring model transparency. Existing methods achieve this attribution goal by training models using annotated pairs of synthetic images and their original training sources. However, obtaining such paired supervision is challenging, as it requires either well-designed synthetic concepts or precise annotations from millions of training sources. To eliminate the need for costly paired annotations, in this paper, we explore the possibility of unsupervised synthetic image attribution. We propose a simple yet effective unsupervised method called Alignment and Disentanglement. Specifically, we begin by performing basic concept alignment using contrastive self-supervised learning. Next, we enhance the model's attribution ability by promoting representation disentanglement with the Infomax loss. This approach is motivated by an interesting observation: contrastive self-supervised models, such as MoCo and DINO, inherently exhibit the ability to perform simple cross-domain alignment. By formulating this observation as a theoretical assumption on cross-covariance, we provide a theoretical explanation of how alignment and disentanglement can approximate the concept-matching process through a decomposition of the canonical correlation analysis objective. On the real-world benchmarks, AbC, we show that our unsupervised method surprisingly outperforms the supervised methods. As a starting point, we expect our intuitive insights and experimental findings to provide a fresh perspective on this challenging task.

</details>


### [34] [ExpAlign: Expectation-Guided Vision-Language Alignment for Open-Vocabulary Grounding](https://arxiv.org/abs/2601.22666)
*Junyi Hu,Tian Bai,Fengyi Wu,Wenyan Li,Zhenming Peng,Yi Zhang*

Main category: cs.CV

TL;DR: 提出基于软MIL池化的Expectation Alignment Head与能量一致性正则化的ExpAlign，能在弱监督下实现隐式token-region对齐，提升开放词汇检测与零-shot实例分割性能，尤其在长尾类别上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么使用全局句子嵌入，缺乏细粒度表达；要么依赖显式的token级监督或沉重的交叉注意力结构。论文旨在在弱监督条件下实现精确的视觉-语言细粒度对齐，并保持轻量和高效推理。

Method: ExpAlign基于多实例学习构造，设计了Expectation Alignment Head：对token-region相似度施加注意力式的软MIL池化，从而在没有额外标注下隐式完成token与实例选择；同时提出多尺度的能量一致性正则化，包括Top-K多正样本对比目标和基于拉格朗日约束自由能最小化导出的几何感知一致性目标，以稳定对齐学习。

Result: 在大量实验中，ExpAlign在开放词汇检测与零样本实例分割上持续提升，尤其对长尾类别改善显著。在LVIS minival上取得36.2 AP_r，优于同等规模的其他最先进方法，同时模型保持轻量与推理高效。

Conclusion: 该论文提出的ExpAlign为无监督或弱监督的开放词汇目标定位提供了一种理论扎实且高效的对齐框架，通过软的多实例学习（MIL）池化和能量基的一致性正则化，实现对细粒度token-region对齐的隐式选择，改善了长尾类别表现并提升了开放词汇检测与零样本实例分割性能。

Abstract: Open-vocabulary grounding requires accurate vision-language alignment under weak supervision, yet existing methods either rely on global sentence embeddings that lack fine-grained expressiveness or introduce token-level alignment with explicit supervision or heavy cross-attention designs. We propose ExpAlign, a theoretically grounded vision-language alignment framework built on a principled multiple instance learning formulation. ExpAlign introduces an Expectation Alignment Head that performs attention-based soft MIL pooling over token-region similarities, enabling implicit token and instance selection without additional annotations. To further stabilize alignment learning, we develop an energy-based multi-scale consistency regularization scheme, including a Top-K multi-positive contrastive objective and a Geometry-Aware Consistency Objective derived from a Lagrangian-constrained free-energy minimization. Extensive experiments show that ExpAlign consistently improves open-vocabulary detection and zero-shot instance segmentation, particularly on long-tail categories. Most notably, it achieves 36.2 AP$_r$ on the LVIS minival split, outperforming other state-of-the-art methods at comparable model scale, while remaining lightweight and inference-efficient.

</details>


### [35] [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/abs/2601.22674)
*Hanxun Yu,Wentong Li,Xuan Qu,Song Wang,Junbo Chen,Jianke Zhu*

Main category: cs.CV

TL;DR: VisionTrim提出DVTS和TGVC两个训练免费模块，通过保留重要视觉token和文本引导的合并，显著加速MLLM在高分辨率与视频场景中的推理，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: MLLM在高分辨率与视频场景中由于过多视觉token导致计算量巨大，现有token压缩方法忽视文本与视觉对齐，从而损失性能；因此需一个兼顾效率与文本对齐的训练免费加速方案。

Method: 提出两模块的统一框架：Dominant Vision Token Selection(DVTS)利用全局-局部视角保留重要视觉token；Text-Guided Vision Complement(TGVC)用文本提示指导上下文感知的token合并；两模块均为即插即用且无需训练。

Result: 在多种图像和视频多模态基准上，VisionTrim优于现有方法，展示了更好的性能-效率权衡，并提供代码实现。

Conclusion: VisionTrim在训练免费条件下，通过选择关键视觉token及文本引导的补充合并，实现了在图像与视频任务中减小计算开销同时保持或提升性能，适合实际部署。

Abstract: Multimodal large language models (MLLMs) suffer from high computational costs due to excessive visual tokens, particularly in high-resolution and video-based scenarios. Existing token reduction methods typically focus on isolated pipeline components and often neglect textual alignment, leading to performance degradation. In this paper, we propose VisionTrim, a unified framework for training-free MLLM acceleration, integrating two effective plug-and-play modules: 1) the Dominant Vision Token Selection (DVTS) module, which preserves essential visual tokens via a global-local view, and 2) the Text-Guided Vision Complement (TGVC) module, which facilitates context-aware token merging guided by textual cues. Extensive experiments across diverse image and video multimodal benchmarks demonstrate the performance superiority of our VisionTrim, advancing practical MLLM deployment in real-world applications. The code is available at: https://github.com/hanxunyu/VisionTrim.

</details>


### [36] [Fire on Motion: Optimizing Video Pass-bands for Efficient Spiking Action Recognition](https://arxiv.org/abs/2601.22675)
*Shuhan Ye,Yuanbin Qian,Yi Yu,Chong Wang,Yuqi Xie,Jiazhen Xu,Kun Wang,Xudong Jiang*

Main category: cs.CV

TL;DR: 论文发现SNN时域响应为低通，抑制运动信息，提出仅含两参数的PBO模块以高通过滤运动频段，显著提升SNN在各类视频任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN具备天然的时间处理能力，但在动态任务上表现不如ANN，作者怀疑原因是脉冲神经的时间响应偏向低频，导致运动信息被衰减。目标是修正通带以聚焦任务相关的运动频段，从而提升SNN在视频任务上的表现。

Method: 提出Pass-Bands Optimizer (PBO)，一个插拔式模块，仅含两个可学习参数和一个轻量级一致性约束，用于调整SNN的时间通带；实现方式为增强高频（运动）成分、抑制低频（静态）成分，保持语义与边界信息，且不改变网络结构与显著增加计算开销。

Result: 在UCF101上PBO提升超过10个百分点；在复杂的多模态动作识别和弱监督视频异常检测上也带来稳定且显著的性能提升，表明方法具有广泛适用性且计算开销极小。

Conclusion: 该论文确认SNN在视频任务中性能落后的根因是时间通带不匹配：标准脉冲动力学作为低通滤波器，压制运动相关频段，从而弱化动态信息的表征。提出的PBO通过可学习的两参数和一致性约束，提升动作频带响应、抑制静态成分，显著改善视频理解性能。

Abstract: Spiking neural networks (SNNs) have gained traction in vision due to their energy efficiency, bio-plausibility, and inherent temporal processing. Yet, despite this temporal capacity, most progress concentrates on static image benchmarks, and SNNs still underperform on dynamic video tasks compared to artificial neural networks (ANNs). In this work, we diagnose a fundamental pass-band mismatch: Standard spiking dynamics behave as a temporal low pass that emphasizes static content while attenuating motion bearing bands, where task relevant information concentrates in dynamic tasks. This phenomenon explains why SNNs can approach ANNs on static tasks yet fall behind on tasks that demand richer temporal understanding.To remedy this, we propose the Pass-Bands Optimizer (PBO), a plug-and-play module that optimizes the temporal pass-band toward task-relevant motion bands. PBO introduces only two learnable parameters, and a lightweight consistency constraint that preserves semantics and boundaries, incurring negligible computational overhead and requires no architectural changes. PBO deliberately suppresses static components that contribute little to discrimination, effectively high passing the stream so that spiking activity concentrates on motion bearing content. On UCF101, PBO yields over ten percentage points improvement. On more complex multi-modal action recognition and weakly supervised video anomaly detection, PBO delivers consistent and significant gains, offering a new perspective for SNN based video processing and understanding.

</details>


### [37] [Visual Personalization Turing Test](https://arxiv.org/abs/2601.22680)
*Rameen Abdal,James Burgess,Sergey Tulyakov,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 提出以“感知不可区分性”为核心的VPTT范式及框架（VPTT-Bench、VPRAG、VPTT Score），在隐私安全和个性化质量上取得可扩展的效果。


<details>
  <summary>Details</summary>
Motivation: 传统的视觉个性化过度关注身份复制（隐私问题）或难以量化創意与一致性之间的权衡；因此提出基于感知不可区分性的评价标准以兼顾隐私、安全与个性化效果。

Method: 提出VPTT框架：包括10k个人设基准VPTT-Bench、基于视觉检索增强的生成器VPRAG，以及与人类和VLM判断校准的仅文本度量VPTT Score。并通过人类评估与VLM评估比较验证VPTT Score的可靠性。

Result: 实验证明VPRAG在保持与个性化一致性（alignment）与原创性（originality）之间取得最佳平衡；VPTT Score与人类及VLM判断高度相关，表明其可作为可扩展且隐私安全的个性化生成AI评估基础。

Conclusion: 本文提出了Visual Personalization Turing Test (VPTT)，用感知不可区分性替代身份复制作为视觉个性化评估范式，强调输出是否与某人可能创作或分享的内容在感知上不可区分。

Abstract: We introduce the Visual Personalization Turing Test (VPTT), a new paradigm for evaluating contextual visual personalization based on perceptual indistinguishability, rather than identity replication. A model passes the VPTT if its output (image, video, 3D asset, etc.) is indistinguishable to a human or calibrated VLM judge from content a given person might plausibly create or share. To operationalize VPTT, we present the VPTT Framework, integrating a 10k-persona benchmark (VPTT-Bench), a visual retrieval-augmented generator (VPRAG), and the VPTT Score, a text-only metric calibrated against human and VLM judgments. We show high correlation across human, VLM, and VPTT evaluations, validating the VPTT Score as a reliable perceptual proxy. Experiments demonstrate that VPRAG achieves the best alignment-originality balance, offering a scalable and privacy-safe foundation for personalized generative AI.

</details>


### [38] [OOVDet: Low-Density Prior Learning for Zero-Shot Out-of-Vocabulary Object Detection](https://arxiv.org/abs/2601.22685)
*Binyi Su,Chenghao Huang,Haiyong Chen*

Main category: cs.CV

TL;DR: 提出OOVDet：通过隐空间低密度采样合成OOV提示并用Dirichlet梯度选取伪OOV图像，结合核密度的低密度先验构建判别边界，显著提升零样本OOV检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-OOVD方法易对IV类过拟合，导致OOV样本被高置信度误判为IV，需在零样本场景下可靠拒绝未定义类别。

Method: 在隐空间对每个IV类拟合条件高斯分布，从低似然区域采样合成区域级OOV提示；对无标签图像用Dirichlet证据解释归因梯度以估计不确定性，选择高不确定性样本作为伪OOV；利用高斯核密度估计与低密度先验约束共同优化OOV类别决策边界。

Result: 在零样本OOV检测任务上，OOVDet显著提升了OOV检测性能（论文报告的实验结果优于基线），并公开了代码。

Conclusion: OOVDet通过在隐空间低密度区域合成区域级OOV提示并利用Dirichlet梯度归因挖掘伪OOV图像，构建基于低密度先验的决策边界，从而在零样本场景下有效检测OOV并减少对IV类的过拟合。

Abstract: Zero-shot out-of-vocabulary detection (ZS-OOVD) aims to accurately recognize objects of in-vocabulary (IV) categories provided at zero-shot inference, while simultaneously rejecting undefined ones (out-of-vocabulary, OOV) that lack corresponding category prompts. However, previous methods are prone to overfitting the IV classes, leading to the OOV or undefined classes being misclassified as IV ones with a high confidence score. To address this issue, this paper proposes a zero-shot OOV detector (OOVDet), a novel framework that effectively detects predefined classes while reliably rejecting undefined ones in zero-shot scenes. Specifically, due to the model's lack of prior knowledge about the distribution of OOV data, we synthesize region-level OOV prompts by sampling from the low-likelihood regions of the class-conditional Gaussian distributions in the hidden space, motivated by the assumption that unknown semantics are more likely to emerge in low-density areas of the latent space. For OOV images, we further propose a Dirichlet-based gradient attribution mechanism to mine pseudo-OOV image samples, where the attribution gradients are interpreted as Dirichlet evidence to estimate prediction uncertainty, and samples with high uncertainty are selected as pseudo-OOV images. Building on these synthesized OOV prompts and pseudo-OOV images, we construct the OOV decision boundary through a low-density prior constraint, which regularizes the optimization of OOV classes using Gaussian kernel density estimation in accordance with the above assumption.
  Experimental results show that our method significantly improves the OOV detection performance in zero-shot scenes. The code is available at https://github.com/binyisu/OOV-detector.

</details>


### [39] [PEAR: Pixel-aligned Expressive humAn mesh Recovery](https://arxiv.org/abs/2601.22693)
*Jiahao Wu,Yunfei Liu,Lijian Lin,Ye Zhu,Lei Zhu,Jingyi Li,Yu Li*

Main category: cs.CV

TL;DR: PEAR使用轻量统一的ViT回归SMPLX参数，辅以像素级监督和模块化数据增强，实现实时（>100FPS）且更精细的三维人体与面部/手部重建。


<details>
  <summary>Details</summary>
Motivation: 现有SMPLX方法推理慢、对细节（面部/手部）定位不准且存在不自然伪影，限制下游应用；需要一个实时且能恢复细粒度表情与姿态的解决方案。

Method: 采用单分支ViT作为主干以高效回归粗略SMPLX参数，结合像素级监督（pixel-aligned losses）优化几何细节，并通过模块化数据注释扩充训练集以增强鲁棒性；同时支持同时输出SMPLX与缩放FLAME参数。

Result: 在多个基准数据集上，PEAR在姿态估计精度上相比先前SMPLX-based方法有显著提升，并能在无预处理条件下实现超100 FPS的推理速度，细粒度面部与手部重建效果改善。

Conclusion: PEAR提出了一个实时、统一的ViT-based框架，用于从单张图像重建像素对齐的高表达力人体网格（SMPLX/FLAME），通过像素级监督和模块化数据标注策略弥补简化模型丢失的细节，达到>100 FPS并在姿态与面部细节上超越现有方法。

Abstract: Reconstructing detailed 3D human meshes from a single in-the-wild image remains a fundamental challenge in computer vision. Existing SMPLX-based methods often suffer from slow inference, produce only coarse body poses, and exhibit misalignments or unnatural artifacts in fine-grained regions such as the face and hands. These issues make current approaches difficult to apply to downstream tasks. To address these challenges, we propose PEAR-a fast and robust framework for pixel-aligned expressive human mesh recovery. PEAR explicitly tackles three major limitations of existing methods: slow inference, inaccurate localization of fine-grained human pose details, and insufficient facial expression capture. Specifically, to enable real-time SMPLX parameter inference, we depart from prior designs that rely on high resolution inputs or multi-branch architectures. Instead, we adopt a clean and unified ViT-based model capable of recovering coarse 3D human geometry. To compensate for the loss of fine-grained details caused by this simplified architecture, we introduce pixel-level supervision to optimize the geometry, significantly improving the reconstruction accuracy of fine-grained human details. To make this approach practical, we further propose a modular data annotation strategy that enriches the training data and enhances the robustness of the model. Overall, PEAR is a preprocessing-free framework that can simultaneously infer EHM-s (SMPLX and scaled-FLAME) parameters at over 100 FPS. Extensive experiments on multiple benchmark datasets demonstrate that our method achieves substantial improvements in pose estimation accuracy compared to previous SMPLX-based approaches. Project page: https://wujh2001.github.io/PEAR

</details>


### [40] [Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding](https://arxiv.org/abs/2601.22696)
*Tae Hun Kim,Hyun Gyu Lee*

Main category: cs.CV

TL;DR: 将微调目标从全局相似性最大化改为条件语义比较的Bi-MCQ，有效提升医学VLM对否定语句的理解和多标签平衡性。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习（InfoNCE）的VLM对否定语句鲁棒性差，且在多标签设置下易强化积极对应，导致无法有效学习疾病缺失信息。需引入能区分肯定/否定语义的目标。

Method: 提出双向多项选择学习框架（Bi-MCQ），联合训练Image-to-Text和Text-to-Image MCQ任务，使用肯定、否定和混合提示，使用面向方向的Cross-Attention融合模块以减少对齐干扰并支持不对称线索。

Result: 在ChestXray14、Open-I、CheXpert和PadChest上，Bi-MCQ相比CARZero零样本在否定理解上最多提升0.47 AUC，在PNC评价上提升最多0.08，且平均缩小肯定-否定AUC差0.12，比InfoNCE微调更平衡。

Conclusion: 本文提出的Bi-MCQ通过将视觉-语言对齐重构为条件语义比较，有效提升了医学VLM对否定语句的理解能力。实验证明在多个胸片数据集上显著改善了否定检测的AUC并缩小肯定/否定性能差距。

Abstract: Recent vision-language models (VLMs) achieve strong zero-shot performance via large-scale image-text pretraining and have been widely adopted in medical image analysis. However, existing VLMs remain notably weak at understanding negated clinical statements, largely due to contrastive alignment objectives that treat negation as a minor linguistic variation rather than a meaning-inverting operator. In multi-label settings, prompt-based InfoNCE fine-tuning further reinforces easy-positive image-prompt alignments, limiting effective learning of disease absence. To overcome these limitations, we reformulate vision-language alignment as a conditional semantic comparison problem, which is instantiated through a bi-directional multiple-choice learning framework(Bi-MCQ). By jointly training Image-to-Text and Text-to-Image MCQ tasks with affirmative, negative, and mixed prompts, our method implements fine-tuning as conditional semantic comparison instead of global similarity maximization. We further introduce direction-specific Cross-Attention fusion modules to address asymmetric cues required by bi-directional reasoning and reduce alignment interference. Experiments on ChestXray14, Open-I, CheXpert, and PadChest show that Bi-MCQ improves negation understanding by up to 0.47 AUC over the zero-shot performance of the state-of-the-art CARZero model, while achieving up to a 0.08 absolute gain on positive-negative combined (PNC) evaluation. Additionally, Bi-MCQ reduces the affirmative-negative AUC gap by an average of 0.12 compared to InfoNCE-based fine-tuning, demonstrating that objective reformulation can substantially enhance negation understanding in medical VLMs.

</details>


### [41] [DAVIS: OOD Detection via Dominant Activations and Variance for Increased Separation](https://arxiv.org/abs/2601.22703)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

TL;DR: 通过在GAP后补充通道方差与最大激活，DAVIS丰富了特征表征，显著提升了后验OOD检测器的性能。


<details>
  <summary>Details</summary>
Motivation: GAP丢弃了激活图中的分布信息（如方差和极值），而这些信息对区分ID与OOD样本具有判别力。作者认为仅使用均值不足以获得最佳的OOD检测效果。

Method: 在原始网络的penultimate层激活图上计算额外统计量（通道方差与每通道最大值），将这些统计量与全局平均池化得到的均值拼接形成丰富的特征向量，作为现有后处理检测器（如基于能量或距离的方法）的输入。

Result: 在多种架构（ResNet、DenseNet、EfficientNet、MobileNet）和数据集（CIFAR-10/100、ImageNet-1k）上，DAVIS显著降低FPR95，报告的改进幅度例如ResNet-18上的CIFAR-10下降48.26%、ResNet-34上的CIFAR-100下降38.13%、MobileNet-v2上的ImageNet-1k下降26.83%。

Conclusion: 本文提出DAVIS，通过在GAP后补充通道方差与最大激活等统计量来缓解信息丢失，从而显著提升OOD检测性能。

Abstract: Detecting out-of-distribution (OOD) inputs is a critical safeguard for deploying machine learning models in the real world. However, most post-hoc detection methods operate on penultimate feature representations derived from global average pooling (GAP) -- a lossy operation that discards valuable distributional statistics from activation maps prior to global average pooling. We contend that these overlooked statistics, particularly channel-wise variance and dominant (maximum) activations, are highly discriminative for OOD detection. We introduce DAVIS, a simple and broadly applicable post-hoc technique that enriches feature vectors by incorporating these crucial statistics, directly addressing the information loss from GAP. Extensive evaluations show DAVIS sets a new benchmark across diverse architectures, including ResNet, DenseNet, and EfficientNet. It achieves significant reductions in the false positive rate (FPR95), with improvements of 48.26\% on CIFAR-10 using ResNet-18, 38.13\% on CIFAR-100 using ResNet-34, and 26.83\% on ImageNet-1k benchmarks using MobileNet-v2. Our analysis reveals the underlying mechanism for this improvement, providing a principled basis for moving beyond the mean in OOD detection.

</details>


### [42] [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/abs/2601.22709)
*Yanlong Chen,Amirhossein Habibian,Luca Benini,Yawei Li*

Main category: cs.CV

TL;DR: GRACE在信息瓶颈框架下把QAT与蒸馏结合，通过置信门控蒸馏、关系中心核对齐与拉格朗日自适应控制，在INT4下实现接近甚至超越FP16教师的VLM性能，同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 部署VLMs代价高昂，后训练量化会带来显著精度损失。量化感知训练结合知识蒸馏在VLM上尚未被充分研究，需一种理论指导的方法在低比特环境下保留任务相关信息。

Method: 将教师视为任务相关信息的代理，提出（1）置信门控解耦蒸馏，用于过滤不可靠监督；（2）关系中心核对齐，转移视觉token结构；（3）通过拉格朗日松弛的自适应控制器，在保真度与信息容量约束之间动态平衡。实验在LLaVA与Qwen系列的广泛基准上验证，并使用真实INT4卷积核实现显著加速与内存节省。

Result: 在多个基准上，INT4模型超越FP16基线（示例：LLaVA-1.5-7B在SQA上70.1 vs 66.8；Qwen2-VL-2B在MMBench上76.9 vs 72.6），实现3×吞吐与54%内存减少，接近教师性能并优于现有量化方法。

Conclusion: GRACE通过把知识蒸馏与量化感知训练(QAT)在信息瓶颈原理下统一，成功在INT4量化下保持并提升VLMs性能，几乎达到FP16教师模型表现，同时显著提高吞吐与减少内存占用。

Abstract: Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.

</details>


### [43] [OpenVTON-Bench: A Large-Scale High-Resolution Benchmark for Controllable Virtual Try-On Evaluation](https://arxiv.org/abs/2601.22725)
*Jin Li,Tao Chen,Shuai Jiang,Weijie Wang,Jingwen Luo,Chenhui Wu*

Main category: cs.CV

TL;DR: 作者构建了大规模高分辨率VTON基准数据集并提出多模态、可解释的五维评估协议，通过VLM与SAM3相关技术分离边界与纹理问题，显著提升对VTON效果的评估可靠性，与人工判断高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型提升了VTON视觉真实感，但缺乏可靠的、能衡量细粒度纹理和语义一致性的评估方法；现有数据集规模和多样性不足以支持商业级评估。

Method: 构建约10万对高分辨率图像数据集（最高1536×1536），采用DINOv3层级聚类实现语义均衡采样，并用Gemini进行密集图像描述，覆盖20个细粒度服装类别；提出多模态评估协议，结合视觉语言模型（VLM）语义推理与基于SAM3分割和形态学腐蚀的多尺度表示度量，分离边界对齐误差与内部纹理伪影。

Result: OpenVTON-Bench提供了约100K高分辨率图像对和五维可解释评估（背景一致性、身份保真、纹理保真、形状合理性、整体真实感）；提出的方法与人工判断的Kendall's τ达0.833，显著优于SSIM的0.611，表明较高的相关性和可信度。

Conclusion: 本文提出了OpenVTON-Bench，通过大规模、高分辨率数据集与多模态评估协议，显著提升了虚拟试衣（VTON）系统的可靠性评估能力，证明了其评估结果与人工判断高度一致，优于传统指标。

Abstract: Recent advances in diffusion models have significantly elevated the visual fidelity of Virtual Try-On (VTON) systems, yet reliable evaluation remains a persistent bottleneck. Traditional metrics struggle to quantify fine-grained texture details and semantic consistency, while existing datasets fail to meet commercial standards in scale and diversity. We present OpenVTON-Bench, a large-scale benchmark comprising approximately 100K high-resolution image pairs (up to $1536 \times 1536$). The dataset is constructed using DINOv3-based hierarchical clustering for semantically balanced sampling and Gemini-powered dense captioning, ensuring a uniform distribution across 20 fine-grained garment categories. To support reliable evaluation, we propose a multi-modal protocol that measures VTON quality along five interpretable dimensions: background consistency, identity fidelity, texture fidelity, shape plausibility, and overall realism. The protocol integrates VLM-based semantic reasoning with a novel Multi-Scale Representation Metric based on SAM3 segmentation and morphological erosion, enabling the separation of boundary alignment errors from internal texture artifacts. Experimental results show strong agreement with human judgments (Kendall's $τ$ of 0.833 vs. 0.611 for SSIM), establishing a robust benchmark for VTON evaluation.

</details>


### [44] [GaussianOcc3D: A Gaussian-Based Adaptive Multi-modal 3D Occupancy Prediction](https://arxiv.org/abs/2601.22729)
*A. Enes Doruk,Hasan F. Ates*

Main category: cs.CV

TL;DR: 提出GaussianOcc3D，使用连续高斯表示与四模块设计高效融合相机与LiDAR，兼顾语义与几何，实现SOTA占据预测及更好鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 单模态方法在语义（相机）与几何（LiDAR）之间存在权衡；传统多模态方法受模态差异、空间对齐困难和体素计算开销或BEV信息丢失等问题限制。

Method: 提出基于四大模块的框架：LDFA利用深度引导的可变形采样将稀疏LiDAR特征提升到高斯原语；EBFS基于熵的平滑抑制域噪声；ACLF采用不确定性感知的加权策略融合相机与LiDAR；Gauss-Mamba Head使用选择性状态空间模型以线性复杂度捕获全局上下文。

Result: 在Occ3D、SurroundOcc和SemanticKITTI基准上分别达成49.4%、28.9%和25.2% mIoU，并在雨天与夜间场景表现出更强的鲁棒性。

Conclusion: GaussianOcc3D通过连续高斯体素表示在多模态3D语义占据预测上取得有效平衡，解决了模态异构、空间错位与表示危机问题。

Abstract: 3D semantic occupancy prediction is a pivotal task in autonomous driving, providing a dense and fine-grained understanding of the surrounding environment, yet single-modality methods face trade-offs between camera semantics and LiDAR geometry. Existing multi-modal frameworks often struggle with modality heterogeneity, spatial misalignment, and the representation crisis--where voxels are computationally heavy and BEV alternatives are lossy. We present GaussianOcc3D, a multi-modal framework bridging camera and LiDAR through a memory-efficient, continuous 3D Gaussian representation. We introduce four modules: (1) LiDAR Depth Feature Aggregation (LDFA), using depth-wise deformable sampling to lift sparse signals onto Gaussian primitives; (2) Entropy-Based Feature Smoothing (EBFS) to mitigate domain noise; (3) Adaptive Camera-LiDAR Fusion (ACLF) with uncertainty-aware reweighting for sensor reliability; and (4) a Gauss-Mamba Head leveraging Selective State Space Models for global context with linear complexity. Evaluations on Occ3D, SurroundOcc, and SemanticKITTI benchmarks demonstrate state-of-the-art performance, achieving mIoU scores of 49.4%, 28.9%, and 25.2% respectively. GaussianOcc3D exhibits superior robustness across challenging rainy and nighttime conditions.

</details>


### [45] [ImgCoT: Compressing Long Chain of Thought into Compact Visual Tokens for Efficient Reasoning of Large Language Model](https://arxiv.org/abs/2601.22730)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Taichun Zhou,Xinwang Liu*

Main category: cs.CV

TL;DR: 将CoT渲染为图像作为重建目标，利用空间布局偏置让潜在令牌更关注全局推理结构；并用少量关键文本步骤弥补细节，提升长CoT压缩与推理效率。


<details>
  <summary>Details</summary>
Motivation: 文本重建将潜在表示强制保留词汇和句法等语言表面特征，导致对推理结构的抽象能力受限；通过将目标改为视觉表征，希望替换语言偏置为空间偏置，从而更好地编码全局逻辑结构并提高压缩效率。

Method: 使用自动编码器将长CoT压缩为紧凑的潜在令牌，但将重建目标改为图像化的CoT（视觉CoT），引入空间归纳偏置以捕捉推理步骤的空间布局；另有Loose版本在复原时结合若干基于低token对数似然挑选的关键文本步骤，形成混合输入供LLM推理。

Result: 实验证明ImgCoT和Loose ImgCoT在多种数据集和LLM上均能在更少令牌下保留推理结构并提高推理性能，其中Loose版本在保留细节方面效果更好。

Conclusion: 本文提出ImgCoT，通过将重建目标从文本CoT替换为渲染为图像的视觉CoT，从而减弱文本表层语言偏置，鼓励潜在令牌编码全局推理结构；并提出Loose ImgCoT，通过补充若干低似然的关键文本推理步骤来恢复细节。两种方法在多数据集和多种LLM上均提升了压缩CoT的效果。

Abstract: Compressing long chains of thought (CoT) into compact latent tokens is crucial for efficient reasoning with large language models (LLMs). Recent studies employ autoencoders to achieve this by reconstructing textual CoT from latent tokens, thus encoding CoT semantics. However, treating textual CoT as the reconstruction target forces latent tokens to preserve surface-level linguistic features (e.g., word choice and syntax), introducing a strong linguistic inductive bias that prioritizes linguistic form over reasoning structure and limits logical abstraction. Thus, we propose ImgCoT that replaces the reconstruction target from textual CoT to the visual CoT obtained by rendering CoT into images. This substitutes linguistic bias with spatial inductive bias, i.e., a tendency to model spatial layouts of the reasoning steps in visual CoT, enabling latent tokens to better capture global reasoning structure. Moreover, although visual latent tokens encode abstract reasoning structure, they may blur reasoning details. We thus propose a loose ImgCoT, a hybrid reasoning that augments visual latent tokens with a few key textual reasoning steps, selected based on low token log-likelihood. This design allows LLMs to retain both global reasoning structure and fine-grained reasoning details with fewer tokens than the complete CoT. Extensive experiments across multiple datasets and LLMs demonstrate the effectiveness of the two versions of ImgCoT.

</details>


### [46] [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/abs/2601.22737)
*Enyi Shi,Pengyang Shao,Yanxin Zhang,Chenhang Cui,Jiayi Lyu,Xu Xie,Xiaobo Xia,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出Lingua-SafetyBench：一个跨10语种的多模态安全基准（100,440条有害图文对），揭示了图像/文本主导风险在高低资源语言上的不对称影响，并指出模型扩展虽降低整体风险却加剧了HRL与Non-HRL间的差距，强调需要语言与模态感知的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基准要么是多语种但仅限文本，要么是多模态但仅限单语，缺乏真实语义的图文配对来覆盖跨模态交互的风险。为此需要一个大规模、跨语言且语义关联的多模态安全基准。

Method: 构建了一个包含10种语言、100,440条有害图文对的数据集，并显式划分为图像主导与文本主导子集；在11个开源VLLMs上进行评估；对Qwen系列进行了可控研究以分析模型规模与版本升级对安全性的影响。

Result: 评估结果表明：(1) 图像主导风险在高资源语言中导致更高ASR；(2) 文本主导风险在非高资源语言中更严重；(3) 模型扩展与升级降低整体ASR，但偏向惠及高资源语言，从而在文本主导场景下扩大语言间差距。作者将公开数据集与代码以便复现。

Conclusion: 该论文提出并公开了Lingua-SafetyBench，一个包含100,440条跨10种语言的有害图文对的基准，旨在评估视觉-语言大模型（VLLMs）在多语种多模态输入下的鲁棒安全性。评估显示在图像主导风险下，高资源语言（HRL）具有更高的攻击成功率（ASR），而在文本主导风险下，非高资源语言（Non-HRL）更脆弱。模型规模和版本升级总体降低ASR，但对HRL的收益更大，扩大了HRL与Non-HRL之间在文本主导风险上的差距。作者强调需要语言与模态感知的安全对齐策略。

Abstract: Robust safety of vision-language large models (VLLMs) under joint multilingual and multimodal inputs remains underexplored. Existing benchmarks are typically multilingual but text-only, or multimodal but monolingual. Recent multilingual multimodal red-teaming efforts render harmful prompts into images, yet rely heavily on typography-style visuals and lack semantically grounded image-text pairs, limiting coverage of realistic cross-modal interactions. We introduce Lingua-SafetyBench, a benchmark of 100,440 harmful image-text pairs across 10 languages, explicitly partitioned into image-dominant and text-dominant subsets to disentangle risk sources. Evaluating 11 open-source VLLMs reveals a consistent asymmetry: image-dominant risks yield higher ASR in high-resource languages, while text-dominant risks are more severe in non-high-resource languages. A controlled study on the Qwen series shows that scaling and version upgrades reduce Attack Success Rate (ASR) overall but disproportionately benefit HRLs, widening the gap between HRLs and Non-HRLs under text-dominant risks. This underscores the necessity of language- and modality-aware safety alignment beyond mere scaling.To facilitate reproducibility and future research, we will publicly release our benchmark, model checkpoints, and source code.The code and dataset will be available at https://github.com/zsxr15/Lingua-SafetyBench.Warning: this paper contains examples with unsafe content.

</details>


### [47] [StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing](https://arxiv.org/abs/2601.22738)
*Han Wang,Deyi Ji,Lanyun Zhu,Jiebo Luo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 提出通过轻量流式编码器+按需升级/延期策略来高效检测直播社交信号，训练引入跨模态对比与IoU加权损失，实验证明在准确性和计算/延迟上优于VLM-only方法。


<details>
  <summary>Details</summary>
Motivation: 实时直播场景下信号稀疏、证据异步且计算受限，需要在低延迟与高准确性间取得权衡，且避免对昂贵VLM的频繁调用。

Method: 设计轻量流式编码器处理大多数时间点；在不确定或困难样本上选择性升级到VLM；支持延期决策以等待更多上下文。训练上结合跨模态对比项使视/音与文本对齐，及IoU加权损失以弱化跨段标注干扰。

Result: 在多项社交流媒体检测任务（如情感分类、仇恨内容检测）上，StreamSense较纯VLM流式方法取得更高准确率，且仅在少数情况下调用VLM，从而降低了平均延迟和计算资源消耗。

Conclusion: StreamSense在流式社交信号检测中通过轻量流式编码器与按需路由到视觉-语言模型(VLM)专家相结合，实现了效率与准确性的良好平衡。

Abstract: Live streaming platforms require real-time monitoring and reaction to social signals, utilizing partial and asynchronous evidence from video, text, and audio. We propose StreamSense, a streaming detector that couples a lightweight streaming encoder with selective routing to a Vision-Language Model (VLM) expert. StreamSense handles most timestamps with the lightweight streaming encoder, escalates hard/ambiguous cases to the VLM, and defers decisions when context is insufficient. The encoder is trained using (i) a cross-modal contrastive term to align visual/audio cues with textual signals, and (ii) an IoU-weighted loss that down-weights poorly overlapping target segments, mitigating label interference across segment boundaries. We evaluate StreamSense on multiple social streaming detection tasks (e.g., sentiment classification and hate content moderation), and the results show that StreamSense achieves higher accuracy than VLM-only streaming while only occasionally invoking the VLM, thereby reducing average latency and compute. Our results indicate that selective escalation and deferral are effective primitives for understanding streaming social tasks. Code is publicly available on GitHub.

</details>


### [48] [Beauty and the Beast: Imperceptible Perturbations Against Diffusion-Based Face Swapping via Directional Attribute Editing](https://arxiv.org/abs/2601.22744)
*Yilong Huang,Songze Li*

Main category: cs.CV

TL;DR: 提出FaceDefense：通过扩散损失与定向属性编辑的两阶段交替优化，显著提升扩散式换脸的主动防护效果且保持高视觉隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 扩散式换脸能力强、滥用风险高，现有主动防护在扰动大小与视觉失真之间存在权衡，需要一种既能强效防护又保持视觉自然的方案。

Method: 在生成对抗样本时，新增扩散损失（针对扩散模型攻击路径优化）增强防护能力；利用定向面部属性编辑恢复因扰动引起的面部结构失真；采用两阶段交替优化策略先求防护强度再修复视觉质量，从而得到最终扰动图像。

Result: 实验表明FaceDefense在隐蔽性和防护有效性上均显著优于现有方法，实现更优的权衡。

Conclusion: FaceDefense在扩散式换脸防护中通过引入扩散损失和定向面部属性编辑，结合两阶段交替优化，在保持感知质量的同时显著提高对抗扩散换脸模型的效果，达到了更好的折中。

Abstract: Diffusion-based face swapping achieves state-of-the-art performance, yet it also exacerbates the potential harm of malicious face swapping to violate portraiture right or undermine personal reputation. This has spurred the development of proactive defense methods. However, existing approaches face a core trade-off: large perturbations distort facial structures, while small ones weaken protection effectiveness. To address these issues, we propose FaceDefense, an enhanced proactive defense framework against diffusion-based face swapping. Our method introduces a new diffusion loss to strengthen the defensive efficacy of adversarial examples, and employs a directional facial attribute editing to restore perturbation-induced distortions, thereby enhancing visual imperceptibility. A two-phase alternating optimization strategy is designed to generate final perturbed face images. Extensive experiments show that FaceDefense significantly outperforms existing methods in both imperceptibility and defense effectiveness, achieving a superior trade-off.

</details>


### [49] [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/abs/2601.22754)
*Guillermo Gil de Avalle,Laura Maruster,Christos Emmanouilidis*

Main category: cs.CV

TL;DR: 对工业故障排查图，布局提示能改善VLM的结构化提取但不同模型在布局敏感性与语义稳健性上存在权衡，实际应用应根据模型特性选择或组合提示策略。


<details>
  <summary>Details</summary>
Motivation: 工业故障排查指南以流程图的形式同时承载视觉布局与术语语义，手动将其转为机器可用结构化知识消耗人力且易出错。评估VLM自动提取能力有助于将这些指南快速集成到操作员支持系统中，提高诊断效率并降低错误率。

Method: 选取两种代表性视觉-语言模型，设计两类提示策略：标准指令式提示和增强布局提示（显式提示图表节点、流程连线和文本位置关系）。在相同数据集上对比模型在结构化输出（如节点、边、动作与条件）的精确率、召回率和F1。并分析错误类型以确定布局敏感性与语义稳健性的权衡。

Result: 增强布局提示可显著提升某些模型对节点和连线的识别召回（相对提高20%左右），但对涉及复杂技术术语的动作/条件识别影响有限。另一模型在语义标注（动作类别、部件名）上表现更好，但对布局提示依赖不足导致流程连线识别精度低。总体上需要按模型特性选择提示策略或融合多模型结果以获得最稳健的结构化输出。

Conclusion: 两种VLM在提取故障排查图的结构化知识时各有优劣：一种更敏感于布局提示但语义稳健性较弱，另一种语义更稳健但对图表布局信息利用不足。结合布局提示的增强提示策略能改善布局相关提取，但效果依模型而异。

Abstract: Industrial troubleshooting guides encode diagnostic procedures in flowchart-like diagrams where spatial layout and technical language jointly convey meaning. To integrate this knowledge into operator support systems, which assist shop-floor personnel in diagnosing and resolving equipment issues, the information must first be extracted and structured for machine interpretation. However, when performed manually, this extraction is labor-intensive and error-prone. Vision Language Models offer potential to automate this process by jointly interpreting visual and textual meaning, yet their performance on such guides remains underexplored. This paper evaluates two VLMs on extracting structured knowledge, comparing two prompting strategies: standard instruction-guided versus an augmented approach that cues troubleshooting layout patterns. Results reveal model-specific trade-offs between layout sensitivity and semantic robustness, informing practical deployment decisions.

</details>


### [50] [Is Training Necessary for Anomaly Detection?](https://arxiv.org/abs/2601.22763)
*Xingwu Zhang,Guanxuan Li,Paul Henderson,Gerardo Aragon-Camarasa,Zijun Long*

Main category: cs.CV

TL;DR: 本文提出了无需训练的检索式异常检测（RAD），通过存储正常样本特征并进行多层次匹配，解决了基于重构方法的保真度-稳定性两难，在多数据集和少样本情形下均达到了SOTA性能，并给出理论上检索得分优于重构残差的证明。


<details>
  <summary>Details</summary>
Motivation: 作者观察到基于重构的方法面临保真度-稳定性两难：高保真重构会重建异常导致漏检，而追求不重构会带来训练不稳定和低性能。为摆脱此困境，提出直接利用异常-free特征库进行检索，从而避免重构范式的本质缺陷。

Method: 放弃重构范式，采用训练自由的内存检索框架。具体做法为：提取异常-free图像的多层特征并存入记忆库；测试时对每个patch在多层特征空间进行检索，计算匹配距离作为异常得分；结合多层级融合和少样本插值以提升泛化。

Result: 在四个基准（MVTec-AD, VisA, Real-IAD, 3D-ADAM）及标准与少样本设置下，RAD取得了最先进的结果。例如在MVTec-AD上：单张无异常图像即可达到96.7%像素AUROC，完整数据下达到98.5%。作者还证明检索得分在理论上为重构残差得分的上界。

Conclusion: 本文否定了基于重构的多类无监督异常检测（MUAD）范式，提出了无需训练的检索式异常检测方法RAD，利用内存存储无异常特征并通过多层次检索匹配来检测异常，实验表明在多个基准上取得最优或接近最优性能，并证明了检索得分在理论上上界重构残差得分。

Abstract: Current state-of-the-art multi-class unsupervised anomaly detection (MUAD) methods rely on training encoder-decoder models to reconstruct anomaly-free features. We first show these approaches have an inherent fidelity-stability dilemma in how they detect anomalies via reconstruction residuals. We then abandon the reconstruction paradigm entirely and propose Retrieval-based Anomaly Detection (RAD). RAD is a training-free approach that stores anomaly-free features in a memory and detects anomalies through multi-level retrieval, matching test patches against the memory. Experiments demonstrate that RAD achieves state-of-the-art performance across four established benchmarks (MVTec-AD, VisA, Real-IAD, 3D-ADAM) under both standard and few-shot settings. On MVTec-AD, RAD reaches 96.7\% Pixel AUROC with just a single anomaly-free image compared to 98.5\% of RAD's full-data performance. We further prove that retrieval-based scores theoretically upper-bound reconstruction-residual scores. Collectively, these findings overturn the assumption that MUAD requires task-specific training, showing that state-of-the-art anomaly detection is feasible with memory-based retrieval. Our code is available at https://github.com/longkukuhi/RAD.

</details>


### [51] [Color Matters: Demosaicing-Guided Color Correlation Training for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2601.22778)
*Nan Zhong,Yiran Xu,Mian Zou*

Main category: cs.CV

TL;DR: 通过模拟相机CFA与去马赛克过程，用自监督U-Net预测缺失颜色通道并提取颜色相关性差异，DCCT在未见生成器上实现了对AI生成图像检测的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成伪影的检测方法在遇到新的、未见的生成模型时泛化失败。论文动机是利用相机物理过程（CFA采样与去马赛克）产生的真实照片内部颜色相关性，这些相关性在AI生成图像中难以被真实还原，从而作为更稳健的判别线索。

Method: 通过模拟CFA采样，将每张彩色图像分解为一个单通道条件输入和其余两个通道作为预测目标，训练一个采用混合对数（mixture of logistics）参数化的自监督U-Net来建模缺失颜色通道的条件分布；从U-Net的预测或重建误差中提取颜色相关性特征并用于二分类检测。

Result: 理论分析表明DCCT能在颜色相关特征上放大真实照片与AI图像的分布差异；在实证上，其在超过20个未见生成器上显著优于先前方法，取得了最先进的泛化性与鲁棒性。

Conclusion: 该论文提出了基于去马赛克（demosaicing）引导的颜色相关性训练框架（DCCT），利用相机成像流程中CFA与去马赛克造成的颜色相关性差异，提升了AI生成图像检测器对未见生成器的泛化能力。

Abstract: As realistic AI-generated images threaten digital authenticity, we address the generalization failure of generative artifact-based detectors by exploiting the intrinsic properties of the camera imaging pipeline. Concretely, we investigate color correlations induced by the color filter array (CFA) and demosaicing, and propose a Demosaicing-guided Color Correlation Training (DCCT) framework for AI-generated image detection. By simulating the CFA sampling pattern, we decompose each color image into a single-channel input (as the condition) and the remaining two channels as the ground-truth targets (for prediction). A self-supervised U-Net is trained to model the conditional distribution of the missing channels from the given one, parameterized via a mixture of logistic functions. Our theoretical analysis reveals that DCCT targets a provable distributional difference in color-correlation features between photographic and AI-generated images. By leveraging these distinct features to construct a binary classifier, DCCT achieves state-of-the-art generalization and robustness, significantly outperforming prior methods across over 20 unseen generators.

</details>


### [52] [Diachronic Stereo Matching for Multi-Date Satellite Imagery](https://arxiv.org/abs/2601.22808)
*Elías Masquil,Luca Savant Aira,Roger Marí,Thibaud Ehret,Pablo Musé,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 通过在含时序差异的卫星立体对数据上微调携带单目先验的深度立体网络，本文实现了对长期时间差影像对的可靠三维重建，显著优于零样本或传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统立体匹配在季节、光照、阴影等大幅变化下失效，而多日期NeRF/高斯散点方法虽精确但不适合同步双目管线的规模性与鲁棒性，需一种能处理长时间差异的立体匹配方法。

Method: 基于预训练MonSter深度立体网络（结合单目深度先验）进行微调，并在专门构建、包含多样时序差异的立体对数据集上训练；数据来源于DFC2019，含同步与时序差异对。

Result: 在WorldView-3多日期数据集上，方法在同步与时序差异设置下均优于经典管线和未适配的深度立体模型；实例中冬—秋对比表明平均高度误差从3.99m降至1.23m。

Conclusion: 本文提出首个面向卫星影像的时序差异立体匹配方法，能从时间间隔较大的图像对恢复可靠3D几何。

Abstract: Recent advances in image-based satellite 3D reconstruction have progressed along two complementary directions. On one hand, multi-date approaches using NeRF or Gaussian-splatting jointly model appearance and geometry across many acquisitions, achieving accurate reconstructions on opportunistic imagery with numerous observations. On the other hand, classical stereoscopic reconstruction pipelines deliver robust and scalable results for simultaneous or quasi-simultaneous image pairs. However, when the two images are captured months apart, strong seasonal, illumination, and shadow changes violate standard stereoscopic assumptions, causing existing pipelines to fail. This work presents the first Diachronic Stereo Matching method for satellite imagery, enabling reliable 3D reconstruction from temporally distant pairs. Two advances make this possible: (1) fine-tuning a state-of-the-art deep stereo network that leverages monocular depth priors, and (2) exposing it to a dataset specifically curated to include a diverse set of diachronic image pairs. In particular, we start from a pretrained MonSter model, trained initially on a mix of synthetic and real datasets such as SceneFlow and KITTI, and fine-tune it on a set of stereo pairs derived from the DFC2019 remote sensing challenge. This dataset contains both synchronic and diachronic pairs under diverse seasonal and illumination conditions. Experiments on multi-date WorldView-3 imagery demonstrate that our approach consistently surpasses classical pipelines and unadapted deep stereo models on both synchronic and diachronic settings. Fine-tuning on temporally diverse images, together with monocular priors, proves essential for enabling 3D reconstruction from previously incompatible acquisition dates. Left image (winter) Right image (autumn) DSM geometry Ours (1.23 m) Zero-shot (3.99 m) LiDAR GT Figure 1. Output geometry for a winter-autumn image pair from Omaha (OMA 331 test scene). Our method recovers accurate geometry despite the diachronic nature of the pair, exhibiting strong appearance changes, which cause existing zero-shot methods to fail. Missing values due to perspective shown in black.  Mean altitude error in parentheses; lower is better.

</details>


### [53] [FarmMind: Reasoning-Query-Driven Dynamic Segmentation for Farmland Remote Sensing Images](https://arxiv.org/abs/2601.22809)
*Haiyang Wu,Weiliang Mu,Jipeng Zhang,Zhong Dandan,Zhuofei Du,Haifeng Li,Tao Chao*

Main category: cs.CV

TL;DR: 针对单图分割信息不足问题，FarmMind通过先推理歧义来源再按需查询辅助影像的动态分割框架，有效提升了遥感农田分割性能与泛化。


<details>
  <summary>Details</summary>
Motivation: 现有FRSI分割方法受限于静态分割范式，仅依赖单一图像信息，导致在复杂、模糊场景下推理能力不足；人类专家在遇到歧义时会主动查询辅助影像以交叉验证和补充信息，启发提出动态查询机制。

Method: 设计推理-查询机制：先分析单张输入产生歧义的原因，再基于推理结果决定需要何种类型的辅助影像（高分辨率、尺度更大或时间相邻影像等）进行按需查询并融合到分割流程中，从而实现动态补偿信息不足。实现了以模拟专家思考过程的查询策略和相应的数据融合模块。

Result: 在大量实验中，FarmMind在分割精度和泛化能力上优于现有方法。作者并公开了代码和数据集以利复现。

Conclusion: FarmMind提出了以推理-查询驱动的动态分割框架，突破静态分割范式，通过依据分割歧义的根因进行推理并按需查询外部辅助影像，提升了遥感农田图像分割的性能与泛化能力。

Abstract: Existing methods for farmland remote sensing image (FRSI) segmentation generally follow a static segmentation paradigm, where analysis relies solely on the limited information contained within a single input patch. Consequently, their reasoning capability is limited when dealing with complex scenes characterized by ambiguity and visual uncertainty. In contrast, human experts, when interpreting remote sensing images in such ambiguous cases, tend to actively query auxiliary images (such as higher-resolution, larger-scale, or temporally adjacent data) to conduct cross-verification and achieve more comprehensive reasoning. Inspired by this, we propose a reasoning-query-driven dynamic segmentation framework for FRSIs, named FarmMind. This framework breaks through the limitations of the static segmentation paradigm by introducing a reasoning-query mechanism, which dynamically and on-demand queries external auxiliary images to compensate for the insufficient information in a single input image. Unlike direct queries, this mechanism simulates the thinking process of human experts when faced with segmentation ambiguity: it first analyzes the root causes of segmentation ambiguities through reasoning, and then determines what type of auxiliary image needs to be queried based on this analysis. Extensive experiments demonstrate that FarmMind achieves superior segmentation performance and stronger generalization ability compared with existing methods. The source code and dataset used in this work are publicly available at: https://github.com/WithoutOcean/FarmMind.

</details>


### [54] [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/abs/2601.22830)
*Ji Zhou,Yilin Ding,Yongqi Zhao,Jiachen Xu,Arno Eichberger*

Main category: cs.CV

TL;DR: LVLMs在长尾和退化环境下显著提升召回，YOLO在几何精度上更稳健，建议将LVLMs作为自动驾驶的高阶安全验证器。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶环境感知在恶劣条件下的不足，评估LVLMs在安全关键2D目标检测中的有效性，以支持SOTIF问题。

Method: 系统性评估了十款代表性LVLMs（如Gemini 3、Doubao等）在PeSOTIF数据集上的性能，并与基于YOLO的经典检测方法进行定量比较。使用召回率、几何精度等指标，在长尾交通场景和环境退化条件下进行对比实验。

Result: 实验表明：在复杂自然场景中，表现最好的LVLMs在召回率上比YOLO高出超过25%，对视觉退化更鲁棒；而YOLO在合成扰动导致的几何精度指标上占优。总体显示语义推理与几何回归的互补性。

Conclusion: 该论文结论指出，LVLMs在复杂自然场景和视觉退化下的目标召回率显著优于传统YOLO检测器，而YOLO在几何精度（如合成扰动下）上仍占优势。两者具有互补性，建议将LVLMs用作SOTIF导向自动驾驶系统的高层安全验证器。

Abstract: Reliable environmental perception remains one of the main obstacles for safe operation of automated vehicles. Safety of the Intended Functionality (SOTIF) concerns safety risks from perception insufficiencies, particularly under adverse conditions where conventional detectors often falter. While Large Vision-Language Models (LVLMs) demonstrate promising semantic reasoning, their quantitative effectiveness for safety-critical 2D object detection is underexplored. This paper presents a systematic evaluation of ten representative LVLMs using the PeSOTIF dataset, a benchmark specifically curated for long-tail traffic scenarios and environmental degradations. Performance is quantitatively compared against the classical perception approach, a YOLO-based detector. Experimental results reveal a critical trade-off: top-performing LVLMs (e.g., Gemini 3, Doubao) surpass the YOLO baseline in recall by over 25% in complex natural scenarios, exhibiting superior robustness to visual degradation. Conversely, the baseline retains an advantage in geometric precision for synthetic perturbations. These findings highlight the complementary strengths of semantic reasoning versus geometric regression, supporting the use of LVLMs as high-level safety validators in SOTIF-oriented automated driving systems.

</details>


### [55] [NativeTok: Native Visual Tokenization for Improved Image Generation](https://arxiv.org/abs/2601.22837)
*Bin Wu,Mengqi Huang,Weinan Jia,Zhendong Mao*

Main category: cs.CV

TL;DR: NativeTok通过在token化阶段强制因果依赖，并采用MIT与MoCET架构及分层训练策略，提高VQ图像生成的重建质量与生成一致性，同时保持训练与推理效率。


<details>
  <summary>Details</summary>
Motivation: 观察到现有VQ方法尽管改进了token化，但未约束token间依赖，导致生成模型从无序分布学习，产生偏差与一致性弱的问题，因此需要在token化阶段引入因果依赖以与第二阶段目标一致。

Method: 提出NativeTok框架，包括Meta Image Transformer (MIT)用于图像潜变量建模，和Mixture of Causal Expert Transformer (MoCET)，其中每个轻量专家块在先前tokens和潜变量特征条件下生成单个token；并设计Hierarchical Native Training，只更新新增专家块以提高训练效率。

Result: 在大量实验中，NativeTok在重建效率和token序列一致性方面优于基线方法，显示出更好的生成一致性和更低的重建误差（摘要中无具体数值）。

Conclusion: 本文提出的NativeTok通过在视觉标记化阶段引入因果依赖来缩小与生成模型的任务失配，从而提升第二阶段生成的一致性和重建质量。实验表明，该方法能高效重建图像并改善token序列的关系约束。

Abstract: VQ-based image generation typically follows a two-stage pipeline: a tokenizer encodes images into discrete tokens, and a generative model learns their dependencies for reconstruction. However, improved tokenization in the first stage does not necessarily enhance the second-stage generation, as existing methods fail to constrain token dependencies. This mismatch forces the generative model to learn from unordered distributions, leading to bias and weak coherence. To address this, we propose native visual tokenization, which enforces causal dependencies during tokenization. Building on this idea, we introduce NativeTok, a framework that achieves efficient reconstruction while embedding relational constraints within token sequences. NativeTok consists of: (1) a Meta Image Transformer (MIT) for latent image modeling, and (2) a Mixture of Causal Expert Transformer (MoCET), where each lightweight expert block generates a single token conditioned on prior tokens and latent features. We further design a Hierarchical Native Training strategy that updates only new expert blocks, ensuring training efficiency. Extensive experiments demonstrate the effectiveness of NativeTok.

</details>


### [56] [Neural Clothing Tryer: Customized Virtual Try-On via Semantic Enhancement and Controlling Diffusion Model](https://arxiv.org/abs/2601.22838)
*Zhijing Yang,Weiwei Zhang,Mingliang Yang,Siyuan Peng,Yukai Shi,Junpeng Tan,Tianshui Chen,Liruo Zhong*

Main category: cs.CV

TL;DR: 提出一种基于语义增强与控制的扩散模型框架NCT，用于可自定义的虚拟试穿，在语义保真与可控编辑上表现出色。


<details>
  <summary>Details</summary>
Motivation: 提出Cu-VTON以扩展传统VTON，允许用户按喜好自定义数字化人像的外观、姿态和其他属性，从而提高虚拟试穿的灵活性与参与感。

Method: 基于扩散模型，NCT包含语义增强模块（使用视觉-语言编码器将服装语义与图像特征对齐，作为扩散模型的条件输入）和语义控制模块（输入服装图、定制姿态图与语义描述，以同时保持服装细节并编辑模特姿态/表情/属性）。

Result: 在公开基准上的大量实验表明，NCT优于现有方法，能更好地保留服装语义与纹理并实现多样化的模特编辑。

Conclusion: 本文提出的NCT框架在自定义虚拟试穿（Cu-VTON）任务上展示了有效性，能在保持服装语义与纹理细节的同时实现对模特外观、姿态和属性的灵活编辑。

Abstract: This work aims to address a novel Customized Virtual Try-ON (Cu-VTON) task, enabling the superimposition of a specified garment onto a model that can be customized in terms of appearance, posture, and additional attributes. Compared with traditional VTON task, it enables users to tailor digital avatars to their individual preferences, thereby enhancing the virtual fitting experience with greater flexibility and engagement. To address this task, we introduce a Neural Clothing Tryer (NCT) framework, which exploits the advanced diffusion models equipped with semantic enhancement and controlling modules to better preserve semantic characterization and textural details of the garment and meanwhile facilitating the flexible editing of the model's postures and appearances. Specifically, NCT introduces a semantic-enhanced module to take semantic descriptions of garments and utilizes a visual-language encoder to learn aligned features across modalities. The aligned features are served as condition input to the diffusion model to enhance the preservation of the garment's semantics. Then, a semantic controlling module is designed to take the garment image, tailored posture image, and semantic description as input to maintain garment details while simultaneously editing model postures, expressions, and various attributes. Extensive experiments on the open available benchmark demonstrate the superior performance of the proposed NCT framework.

</details>


### [57] [How Much of a Model Do We Need? Redundancy and Slimmability in Remote Sensing Foundation Models](https://arxiv.org/abs/2601.22841)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: Remote sensing FMs show high representational redundancy and can be heavily slimmed with little performance loss, challenging CV-derived scaling assumptions and enabling lightweight deployment.


<details>
  <summary>Details</summary>
Motivation: Question whether scaling assumptions from CV apply to remote sensing; hypothesize RS models reach redundancy earlier, affecting deployment and design.

Method: Use post-hoc slimming (uniformly reduce encoder width) on six RS FMs across four downstream classification tasks; compare with ImageNet MAE; apply learned slimmable training; analyze explained variance ratio and feature correlation.

Result: RS FMs retain >71% relative accuracy at 1% FLOPs vs <10% for ImageNet MAE; learned slimmable training improves MoCo and MAE models; feature analyses show high redundancy in task-relevant info.

Conclusion: RS foundation models become overparameterized at much smaller scales than CV models; increasing parameters yields redundant representations rather than new abstractions.

Abstract: Large-scale foundation models (FMs) in remote sensing (RS) are developed based on the paradigms established in computer vision (CV) and have shown promise for various Earth observation applications. However, the direct transfer of scaling assumptions from CV to RS has not been adequately examined. We hypothesize that RS FMs enter an overparameterized regime at substantially smaller scales than their CV counterparts, where increasing parameter count primarily induces redundant representations rather than qualitatively new abstractions. To test this hypothesis, we use post-hoc slimming, where we uniformly reduce the width of pretrained encoder, as a tool to measure representational redundancy across six state-of-the-art RS FMs on four downstream classification tasks. Our findings reveal a significant contrast with those in the CV domain: while a post-hoc slimmed masked autoencoder (MAE) trained on ImageNet retains less than 10% accuracy at 1% FLOPs, RS FMs maintain over 71% relative accuracy at the same budget. This sevenfold difference provides strong empirical support for our hypothesis. We further demonstrate that learned slimmable training can improve both Momentum Contrast (MoCo)- and MAE- based models. In addition, through the explained variance ratio and the feature correlation analysis, we provide mechanistic explanations showing that RS FMs distribute task-relevant information with high redundancy. Our findings establish post-hoc slimmability as both a practical deployment strategy for resource-constrained environments and a diagnostic tool that challenges the prevailing scaling paradigm in RS. Upon acceptance, we will publish all code.

</details>


### [58] [Inference-Time Dynamic Modality Selection for Incomplete Multimodal Classification](https://arxiv.org/abs/2601.22853)
*Siyi Du,Xinzhe Luo,Declan P. O'Regan,Chen Qin*

Main category: cs.CV

TL;DR: DyMo在推理阶段动态选择可靠的恢复模态，通过用任务损失作为信息代理并设计奖励函数，有效整合有用模态信息，显著提升不完整多模态学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有不完整MDL方法在丢弃和插补之间两难：丢弃可能损失有用信息，而插补可能引入噪声。为突破这一困境，作者提出在推理时动态地选择那些能为当前样本提供有用信息的恢复模态。

Method: DyMo在推理时针对每个测试样本自适应选择可靠的恢复模态，提出了基于任务损失的可计算代理来估计模态的任务相关信息，并据此设计了奖励函数引导选择。为支持任意模态组合，论文还设计了灵活的多模态网络结构与专门的训练策略。

Result: 在多个人类和医疗图像数据集上的大量实验表明，DyMo在各种缺失数据场景下显著优于现有不完整/动态多模态方法。

Conclusion: 该论文提出了DyMo，一种在推理时动态选择恢复模态的框架，通过最大化对任务有用的信息来解决丢弃-插补困境，从而在不依赖完整模态的情况下提升多模态任务性能。

Abstract: Multimodal deep learning (MDL) has achieved remarkable success across various domains, yet its practical deployment is often hindered by incomplete multimodal data. Existing incomplete MDL methods either discard missing modalities, risking the loss of valuable task-relevant information, or recover them, potentially introducing irrelevant noise, leading to the discarding-imputation dilemma. To address this dilemma, in this paper, we propose DyMo, a new inference-time dynamic modality selection framework that adaptively identifies and integrates reliable recovered modalities, fully exploring task-relevant information beyond the conventional discard-or-impute paradigm. Central to DyMo is a novel selection algorithm that maximizes multimodal task-relevant information for each test sample. Since direct estimation of such information at test time is intractable due to the unknown data distribution, we theoretically establish a connection between information and the task loss, which we compute at inference time as a tractable proxy. Building on this, a novel principled reward function is proposed to guide modality selection. In addition, we design a flexible multimodal network architecture compatible with arbitrary modality combinations, alongside a tailored training strategy for robust representation learning. Extensive experiments on diverse natural and medical image datasets show that DyMo significantly outperforms state-of-the-art incomplete/dynamic MDL methods across various missing-data scenarios. Our code is available at https://github.com//siyi-wind/DyMo.

</details>


### [59] [Under-Canopy Terrain Reconstruction in Dense Forests Using RGB Imaging and Neural 3D Reconstruction](https://arxiv.org/abs/2601.22861)
*Refael Sheffer,Chen Pinchover,Haim Zisman,Dror Ozeri,Roee Litman*

Main category: cs.CV

TL;DR: 用普通RGB图像和改进的NeRF技术，在采集时辅以适当照明与低光损失，以及两种射线级遮挡去除策略，实现了去冠层的真实感地面视图重建，可用于人员搜索与树木计数，作为廉价高分辨率替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵或专用传感器（机载LiDAR或热成像AOS）来揭示森林下方地形与植被，成本高且设备重。作者希望用廉价普通RGB相机实现同样功能。

Method: 基于NeRF进行3D重建，加入低光损失以改善暗下部场景表现；提出两种基于按射线积分控制的遮挡去除方法；并给出采集时的照明建议以揭示林下细节。

Result: 在SAR中，使用本方法的可见光重建能检测到人员，性能可与热AOS相媲美；在森林清点任务（如树木计数）上也展示了有效性，证明了方法的实用价值。

Conclusion: 本文提出了仅使用常规RGB图像、基于NeRF的管层去除与地面重建方法，展示了在SAR与森林清点任务上的潜力，成为比专业传感器更廉价的高分辨率替代方案。

Abstract: Mapping the terrain and understory hidden beneath dense forest canopies is of great interest for numerous applications such as search and rescue, trail mapping, forest inventory tasks, and more. Existing solutions rely on specialized sensors: either heavy, costly airborne LiDAR, or Airborne Optical Sectioning (AOS), which uses thermal synthetic aperture photography and is tailored for person detection.
  We introduce a novel approach for the reconstruction of canopy-free, photorealistic ground views using only conventional RGB images. Our solution is based on the celebrated Neural Radiance Fields (NeRF), a recent 3D reconstruction method. Additionally, we include specific image capture considerations, which dictate the needed illumination to successfully expose the scene beneath the canopy. To better cope with the poorly lit understory, we employ a low light loss. Finally, we propose two complementary approaches to remove occluding canopy elements by controlling per-ray integration procedure.
  To validate the value of our approach, we present two possible downstream tasks. For the task of search and rescue (SAR), we demonstrate that our method enables person detection which achieves promising results compared to thermal AOS (using only RGB images). Additionally, we show the potential of our approach for forest inventory tasks like tree counting. These results position our approach as a cost-effective, high-resolution alternative to specialized sensors for SAR, trail mapping, and forest-inventory tasks.

</details>


### [60] [When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection](https://arxiv.org/abs/2601.22868)
*Shashank Mishra,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出CAAD-3K和基于视觉-语言的条件兼容性学习，针对情境性异常检测取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测假设异常为观察本身的固有属性，但在现实中同一对象/动作在不同情境下可为正常或异常，需重新定义并研究情境依赖的异常检测。

Method: 构建CAAD-3K基准数据集，通过固定主体身份并改变背景来孤立情境异常；提出基于视觉-语言表征的条件兼容性学习框架，在有限监督下建模主体-情境关系。

Result: 方法在CAAD-3K上显著优于既有方法，并在MVTec-AD和VisA上达到最先进表现，表明建模情境依赖可补充结构性异常检测。

Conclusion: 本文提出在视觉领域系统化研究“情境性异常检测（contextual anomaly detection）”，认为异常性依赖于主体-情境兼容性而非仅外观。

Abstract: Anomaly detection is often formulated under the assumption that abnormality is an intrinsic property of an observation, independent of context. This assumption breaks down in many real-world settings, where the same object or action may be normal or anomalous depending on latent contextual factors (e.g., running on a track versus on a highway). We revisit \emph{contextual anomaly detection}, classically defined as context-dependent abnormality, and operationalize it in the visual domain, where anomaly labels depend on subject--context compatibility rather than intrinsic appearance. To enable systematic study of this setting, we introduce CAAD-3K, a benchmark that isolates contextual anomalies by controlling subject identity while varying context. We further propose a conditional compatibility learning framework that leverages vision--language representations to model subject--context relationships under limited supervision. Our method substantially outperforms existing approaches on CAAD-3K and achieves state-of-the-art performance on MVTec-AD and VisA, demonstrating that modeling context dependence complements traditional structural anomaly detection. Our code and dataset will be publicly released.

</details>


### [61] [DINO-SAE: DINO Spherical Autoencoder for High-Fidelity Image Reconstruction and Generation](https://arxiv.org/abs/2601.22904)
*Hun Chang,Byunghee Cha,Jong Chul Ye*

Main category: cs.CV

TL;DR: 在超球面潜空间上，用余弦对齐和分层卷积补丁嵌入保留细节，并用黎曼流匹配训练DiT，从而同时实现高保真重建与语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有以VFM为基础的生成自编码器在重建时丢失高频细节，因为对特征幅值的严格匹配抑制了编码器保留细粒度信息的能力。作者观察到对比表示的语义主要编码在特征方向上且表征位于超球面，因此通过放宽幅值约束和在球面上建模可以兼顾语义与细节。

Method: 引入分层卷积补丁嵌入（Hierarchical Convolutional Patch Embedding）以增强局部纹理保留，采用余弦相似性对齐目标（Cosine Similarity Alignment）保持语义一致性但放宽幅值匹配，并在球面潜空间上用黎曼流匹配（Riemannian Flow Matching）训练Diffusion Transformer（DiT）。

Result: 在ImageNet-1K上，DINO-SAE取得了0.37 rFID和26.2 dB PSNR的最优重建质量，并在80个epoch内用黎曼流匹配的DiT达到3.47 gFID，显示收敛高效且保持与预训练VFM的语义对齐。

Conclusion: 本文提出DINO-SAE，通过在球面潜空间上对比语义一致性与保留细节之间的权衡，显著提升了重建质量与语义对齐。

Abstract: Recent studies have explored using pretrained Vision Foundation Models (VFMs) such as DINO for generative autoencoders, showing strong generative performance. Unfortunately, existing approaches often suffer from limited reconstruction fidelity due to the loss of high-frequency details. In this work, we present the DINO Spherical Autoencoder (DINO-SAE), a framework that bridges semantic representation and pixel-level reconstruction. Our key insight is that semantic information in contrastive representations is primarily encoded in the direction of feature vectors, while forcing strict magnitude matching can hinder the encoder from preserving fine-grained details. To address this, we introduce Hierarchical Convolutional Patch Embedding module that enhances local structure and texture preservation, and Cosine Similarity Alignment objective that enforces semantic consistency while allowing flexible feature magnitudes for detail retention. Furthermore, leveraging the observation that SSL-based foundation model representations intrinsically lie on a hypersphere, we employ Riemannian Flow Matching to train a Diffusion Transformer (DiT) directly on this spherical latent manifold. Experiments on ImageNet-1K demonstrate that our approach achieves state-of-the-art reconstruction quality, reaching 0.37 rFID and 26.2 dB PSNR, while maintaining strong semantic alignment to the pretrained VFM. Notably, our Riemannian Flow Matching-based DiT exhibits efficient convergence, achieving a gFID of 3.47 at 80 epochs.

</details>


### [62] [Multi-Cue Anomaly Detection and Localization under Data Contamination](https://arxiv.org/abs/2601.22913)
*Anindya Sundar Das,Monowar Bhuyan*

Main category: cs.CV

TL;DR: 结合少量标注异常与自适应实例加权的复合异常评分方法，在有污染训练数据下实现更鲁棒且可解释的视觉异常检测与定位。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中训练数据常被异常样本污染，且可用标注异常样本稀少。现有仅用纯正常或默认未污染无标签数据的方法在有污染时性能下降，且无法学习异常的判别特征。为此需要在有限异常监督下提升鲁棒性与定位能力。

Method: 在自适应偏差学习基础上引入少量异常监督，设计一个由三部分构成的复合异常评分：偏差分数（统计不规则性）、熵不确定性分数（预测不一致性）和分割分数（空间异常提示）。训练时使用自适应实例加权缓解被污染样本的影响，并利用复合分数支持基于梯度的异常定位。

Result: 在MVTec与VisA基准上的广泛实验显示，所提框架在检测与定位上均超过最先进基线，并在不同污染程度下保持强鲁棒性和良好可解释性。

Conclusion: 该论文提出一个结合少量标注异常样本与自适应偏差学习的鲁棒视觉异常检测框架，通过复合异常评分整合统计偏差、熵不确定性与分割空间异常三方面证据，实现检测与基于梯度的定位，并通过自适应样本加权减轻训练数据污染影响。实验证明在MVTec与VisA上优于现有方法，在不同污染水平下保持稳健性与可解释性。

Abstract: Visual anomaly detection in real-world industrial settings faces two major limitations. First, most existing methods are trained on purely normal data or on unlabeled datasets assumed to be predominantly normal, presuming the absence of contamination, an assumption that is rarely satisfied in practice. Second, they assume no access to labeled anomaly samples, limiting the model from learning discriminative characteristics of true anomalies. Therefore, these approaches often struggle to distinguish anomalies from normal instances, resulting in reduced detection and weak localization performance. In real-world applications, where training data are frequently contaminated with anomalies, such methods fail to deliver reliable performance. In this work, we propose a robust anomaly detection framework that integrates limited anomaly supervision into the adaptive deviation learning paradigm. We introduce a composite anomaly score that combines three complementary components: a deviation score capturing statistical irregularity, an entropy-based uncertainty score reflecting predictive inconsistency, and a segmentation-based score highlighting spatial abnormality. This unified scoring mechanism enables accurate detection and supports gradient-based localization, providing intuitive and explainable visual evidence of anomalous regions. Following the few-anomaly paradigm, we incorporate a small set of labeled anomalies during training while simultaneously mitigating the influence of contaminated samples through adaptive instance weighting. Extensive experiments on the MVTec and VisA benchmarks demonstrate that our framework outperforms state-of-the-art baselines and achieves strong detection and localization performance, interpretability, and robustness under various levels of data contamination.

</details>


### [63] [Deep in the Jungle: Towards Automating Chimpanzee Population Estimation](https://arxiv.org/abs/2601.22917)
*Tom Raynes,Otto Brookes,Timm Haucke,Lukas Bösch,Anne-Sophie Crunchant,Hjalmar Kühl,Sara Beery,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: 在220段黑猩猩相机视频上，校准的DPT比Depth Anything更准确，但两者在森林环境中倾向高估距离并低估种群，整体估计与人工方法差异约22%，表明MDE可作为实用替代。


<details>
  <summary>Details</summary>
Motivation: 传统估计未标记种群密度依赖于动物到相机的距离，但这些距离通常需要人工在大量视频中逐条标注，费时费力；研究探索自动化、可扩展的解决方案以提升效率并支持保护决策。

Method: 使用220段野外黑猩猩相机陷阱视频，比较两种MDE模型（Dense Prediction Transformers，DPT；Depth Anything）与多种距离抽样策略，生成检测距离并推断密度与丰度；并将结果与人工测距的真实值比较，评估误差与偏差来源。

Result: 校准后的DPT在距离估计与下游密度、丰度推断上均优于Depth Anything；两模型在复杂林地环境中普遍高估检测距离，导致低估密度/丰度；检测失败（不同距离范围）是限制精度的主要因素；总体上，MDE方法的种群估计与传统方法相差约22%。

Conclusion: 本文表明将单目深度估计（MDE）整合到相机陷阱工作流程，用于估算未标记大型猿类（黑猩猩）种群密度和丰度是可行的，且在校准后可接近人工方法的结果。

Abstract: The estimation of abundance and density in unmarked populations of great apes relies on statistical frameworks that require animal-to-camera distance measurements. In practice, acquiring these distances depends on labour-intensive manual interpretation of animal observations across large camera trap video corpora. This study introduces and evaluates an only sparsely explored alternative: the integration of computer vision-based monocular depth estimation (MDE) pipelines directly into ecological camera trap workflows for great ape conservation. Using a real-world dataset of 220 camera trap videos documenting a wild chimpanzee population, we combine two MDE models, Dense Prediction Transformers and Depth Anything, with multiple distance sampling strategies. These components are used to generate detection distance estimates, from which population density and abundance are inferred. Comparative analysis against manually derived ground-truth distances shows that calibrated DPT consistently outperforms Depth Anything. This advantage is observed in both distance estimation accuracy and downstream density and abundance inference. Nevertheless, both models exhibit systematic biases. We show that, given complex forest environments, they tend to overestimate detection distances and consequently underestimate density and abundance relative to conventional manual approaches. We further find that failures in animal detection across distance ranges are a primary factor limiting estimation accuracy. Overall, this work provides a case study that shows MDE-driven camera trap distance sampling is a viable and practical alternative to manual distance estimation. The proposed approach yields population estimates within 22% of those obtained using traditional methods.

</details>


### [64] [Q-Hawkeye: Reliable Visual Policy Optimization for Image Quality Assessment](https://arxiv.org/abs/2601.22920)
*Wulin Xie,Rui Dai,Ruidong Ding,Kaikui Liu,Xiangxiang Chu,Xinwen Hou,Jie Wen*

Main category: cs.CV

TL;DR: 提出Q-Hawkeye：结合不确定性感知动态重加权和基于原-退化图像对的隐式感知损失，提升RL驱动的MLLM图像质量评估模型的稳定性与视觉感知可靠性，实验证明优于SOTA并具备更好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有基于GRPO的RL-IQA方法存在两大可靠性问题：一是对不同训练样本预测稳定性差异显著，但采用统一的优势（advantage）权重，导致不稳定样本放大噪声；二是过度强调文本化推理而忽视模型对图像内容的实际视觉感知能力。为此作者提出重新设计学习信号以提高稳健性与感知性。

Method: 方法包含两部分：1) Uncertainty-Aware Dynamic Optimization：对同一样本进行多次rollout，计算预测分数方差作为不确定性估计，并据此重加权每个样本的梯度更新，抑制不稳定样本的噪声信号；2) Perception-Aware Optimization：构造退化图像与原图的配对输入，引入Implicit Perception Loss，强制模型基于视觉证据给出质量判断，从而提升视觉感知能力。整体在基于MLLM的RL策略优化框架下实现。

Result: 在多数据集上的广泛实验表明，Q-Hawkeye在性能上优于现有最先进方法，并在跨数据集泛化能力上表现更好。作者表示将开源代码与模型。

Conclusion: 本论文提出Q-Hawkeye，通过不确定性感知的动态优化和感知感知优化，增强基于RL的图像质量评估模型的可靠性与感知归因能力。

Abstract: Image Quality Assessment (IQA) predicts perceptual quality scores consistent with human judgments. Recent RL-based IQA methods built on MLLMs focus on generating visual quality descriptions and scores, ignoring two key reliability limitations: (i) although the model's prediction stability varies significantly across training samples, existing GRPO-based methods apply uniform advantage weighting, thereby amplifying noisy signals from unstable samples in gradient updates; (ii) most works emphasize text-grounded reasoning over images while overlooking the model's visual perception ability of image content. In this paper, we propose Q-Hawkeye, an RL-based reliable visual policy optimization framework that redesigns the learning signal through unified Uncertainty-Aware Dynamic Optimization and Perception-Aware Optimization. Q-Hawkeye estimates predictive uncertainty using the variance of predicted scores across multiple rollouts and leverages this uncertainty to reweight each sample's update strength, stabilizing policy optimization. To strengthen perceptual reliability, we construct paired inputs of degraded images and their original images and introduce an Implicit Perception Loss that constrains the model to ground its quality judgments in genuine visual evidence. Extensive experiments demonstrate that Q-Hawkeye outperforms state-of-the-art methods and generalizes better across multiple datasets. The code and models will be made available.

</details>


### [65] [Semantic Leakage from Image Embeddings](https://arxiv.org/abs/2601.22929)
*Yiyi Chen,Qiongkai Xu,Desmond Eliott,Qiongxiu Li,Johannes Bjerva*

Main category: cs.CV

TL;DR: 作者提出并实证SLImE：通过保持局部语义邻域的嵌入对齐，能从压缩图像嵌入中恢复丰富语义，揭示图像嵌入的隐私漏洞。


<details>
  <summary>Details</summary>
Motivation: 挑战“图像嵌入隐私风险低”的常识性假设，形式化语义泄露概念，强调即便无精确重建，保留的局部语义邻域也能通过一系列有损映射传播语义信息。

Method: 提出SLImE框架：通过局部训练的语义检索器与现成模型（如GEMINI、COHERE、NOMIC、CLIP）结合，在无需训练专门解码器的情况下，从独立压缩嵌入中推断语义标签、符号表示及连贯描述；验证了对齐嵌入到标签检索再到文本生成的每一步。

Result: 在多种开/闭源嵌入模型上均能稳定恢复语义信息，证明了嵌入在对齐下的固有脆弱性，提示隐私保护难题。

Conclusion: 该论文表明压缩图像嵌入并非隐私安全；在保持局部语义邻域结构的对齐下，即使无法重建原始图像，也能泄露语义信息。

Abstract: Image embeddings are generally assumed to pose limited privacy risk. We challenge this assumption by formalizing semantic leakage as the ability to recover semantic structures from compressed image embeddings. Surprisingly, we show that semantic leakage does not require exact reconstruction of the original image. Preserving local semantic neighborhoods under embedding alignment is sufficient to expose the intrinsic vulnerability of image embeddings. Crucially, this preserved neighborhood structure allows semantic information to propagate through a sequence of lossy mappings. Based on this conjecture, we propose Semantic Leakage from Image Embeddings (SLImE), a lightweight inference framework that reveals semantic information from standalone compressed image embeddings, incorporating a locally trained semantic retriever with off-the-shelf models, without training task-specific decoders. We thoroughly validate each step of the framework empirically, from aligned embeddings to retrieved tags, symbolic representations, and grammatical and coherent descriptions. We evaluate SLImE across a range of open and closed embedding models, including GEMINI, COHERE, NOMIC, and CLIP, and demonstrate consistent recovery of semantic information across diverse inference tasks. Our results reveal a fundamental vulnerability in image embeddings, whereby the preservation of semantic neighborhoods under alignment enables semantic leakage, highlighting challenges for privacy preservation.1

</details>


### [66] [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/abs/2601.22959)
*Anmin Wang,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Triage是一种无需训练、即插即用的两阶段视觉预算框架：先选关键帧，再在令牌层用Core+MMR选取上下文令牌，从而在减少计算与内存的同时保持或提升视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 视频中的大量冗余帧和视觉信息导致视觉-语言模型在时间和空间维度上产生过长令牌序列，带来计算与内存瓶颈。Motivation是将视频推理视作资源分配问题，通过层级预算控制令牌数量以提高效率，同时尽量保持信息完整性。

Method: 方法包括：第一阶段帧级预算（Frame-Level Budgeting），基于视觉动态性与与任务相关性为帧打分并挑选关键帧；第二阶段令牌级预算（Token-Level Budgeting），先保留高相关性的核心令牌（Core Tokens），再用高效批量化的最大边际相关性（MMR）算法挑选多样化的上下文令牌（Context Tokens）。整个框架不需额外训练，可作为现有VLMs的插拔式模块。

Result: 实验表明Triage在多种视频推理数据集上显著提高推理速度并降低内存占用，在很多情况下与或优于现有基线方法与其他压缩/选择策略。

Conclusion: Triage通过两阶段的视觉预算策略，有效减少视频处理中冗余信息带来的计算与内存开销，同时在多项视频推理基准上保持或超越基线性能，证明了训练免费且即插即用的帧级与令牌级分配方案的实用性。

Abstract: Vision-Language Models (VLMs) face significant computational challenges in video processing due to massive data redundancy, which creates prohibitively long token sequences. To address this, we introduce Triage, a training-free, plug-and-play framework that reframes video reasoning as a resource allocation problem via hierarchical visual budgeting. Its first stage, Frame-Level Budgeting, identifies keyframes by evaluating their visual dynamics and relevance, generating a strategic prior based on their importance scores. Guided by this prior, the second stage, Token-Level Budgeting, allocates tokens in two phases: it first secures high-relevance Core Tokens, followed by diverse Context Tokens selected with an efficient batched Maximal Marginal Relevance (MMR) algorithm. Extensive experiments demonstrate that Triage improves inference speed and reduces memory footprint, while maintaining or surpassing the performance of baselines and other methods on various video reasoning benchmarks.

</details>


### [67] [Improving Supervised Machine Learning Performance in Optical Quality Control via Generative AI for Dataset Expansion](https://arxiv.org/abs/2601.22961)
*Dennis Sprute,Hanna Senke,Holger Flatt*

Main category: cs.CV

TL;DR: 针对缺陷样本稀少问题，作者用Stable Diffusion和CycleGAN生成热成像数据扩充训练集。结果显示Stable Diffusion能将分割Mean IoU提升到84.6%，比基线高4.6%。


<details>
  <summary>Details</summary>
Motivation: 生产线上缺陷样本稀少导致训练数据高度不平衡，传统方法（特定损失函数或常规数据增强）有局限，研究旨在探索生成式AI作为数据扩充手段能否改善检测性能。

Method: 作者比较了两类生成模型：Stable Diffusion和CycleGAN，用于生成热成像中联合收割机零部件的图像以扩充训练集，之后将扩充后的数据用于监督分割模型的训练并评估Mean IoU。

Result: 使用Stable Diffusion生成的数据扩充使分割性能提升4.6个百分点，Mean IoU达到84.6%，优于CycleGAN和未扩充基线。

Conclusion: 该研究表明在工业光学质量检测中，使用生成式AI扩充数据集能缓解数据不平衡问题，并显著提升分割模型性能，尤其是Stable Diffusion优于CycleGAN。

Abstract: Supervised machine learning algorithms play a crucial role in optical quality control within industrial production. These approaches require representative datasets for effective model training. However, while non-defective components are frequent, defective parts are rare in production, resulting in highly imbalanced datasets that adversely impact model performance. Existing strategies to address this challenge, such as specialized loss functions or traditional data augmentation techniques, have limitations, including the need for careful hyperparameter tuning or the alteration of only simple image features. Therefore, this work explores the potential of generative artificial intelligence (GenAI) as an alternative method for expanding limited datasets and enhancing supervised machine learning performance. Specifically, we investigate Stable Diffusion and CycleGAN as image generation models, focusing on the segmentation of combine harvester components in thermal images for subsequent defect detection. Our results demonstrate that dataset expansion using Stable Diffusion yields the most significant improvement, enhancing segmentation performance by 4.6 %, resulting in a Mean Intersection over Union (Mean IoU) of 84.6 %.

</details>


### [68] [About an Automating Annotation Method for Robot Markers](https://arxiv.org/abs/2601.22982)
*Wataru Uemura,Takeru Nagashima*

Main category: cs.CV

TL;DR: 利用ArUco内置检测自动生成标注数据，训练YOLO模型可在恶劣成像条件下提升ArUco识别性能并降低人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于OpenCV的图像处理方法在噪声、运动模糊、散焦或光照变化下识别失败率高，而深度学习方法鲁棒性更好但受限于人工标注成本，因此提出自动标注以降低成本并提升识别性能。

Method: 通过ArUco模块提取标记ID与位置信息，自动生成用于YOLO模型训练的标注数据集；训练YOLO模型并在多种模糊、散焦和光照变化条件下评估其性能。

Result: 实验表明，基于自动标注训练的YOLO模型在模糊或散焦图像上的识别性能优于传统图像处理方法；自动标注显著减少人工工作并保证标注一致性。

Conclusion: 本文提出了一种利用ArUco标记内置检测模块进行自动标注的方法，用以训练深度学习模型识别ArUco标记，从而免去人工标注。

Abstract: Factory automation has become increasingly important due to labor shortages, leading to the introduction of autonomous mobile robots for tasks such as material transportation. Markers are commonly used for robot self-localization and object identification. In the RoboCup Logistics League (RCLL), ArUco markers are employed both for robot localization and for identifying processing modules. Conventional recognition relies on OpenCV-based image processing, which detects black-and-white marker patterns. However, these methods often fail under noise, motion blur, defocus, or varying illumination conditions. Deep-learning-based recognition offers improved robustness under such conditions, but requires large amounts of annotated data. Annotation must typically be done manually, as the type and position of objects cannot be detected automatically, making dataset preparation a major bottleneck. In contrast, ArUco markers include built-in recognition modules that provide both ID and positional information, enabling automatic annotation. This paper proposes an automated annotation method for training deep-learning models on ArUco marker images. By leveraging marker detection results obtained from the ArUco module, the proposed approach eliminates the need for manual labeling. A YOLO-based model is trained using the automatically annotated dataset, and its performance is evaluated under various conditions. Experimental results demonstrate that the proposed method improves recognition performance compared with conventional image-processing techniques, particularly for images affected by blur or defocus. Automatic annotation also reduces human effort and ensures consistent labeling quality. Future work will investigate the relationship between confidence thresholds and recognition performance.

</details>


### [69] [Self-Supervised Slice-to-Volume Reconstruction with Gaussian Representations for Fetal MRI](https://arxiv.org/abs/2601.22990)
*Yinsong Wang,Thomas Fletcher,Xinzhe Luo,Aine Travers Dineen,Rhodri Cusack,Chen Qin*

Main category: cs.CV

TL;DR: GaussianSVR: a self-supervised SVR using 3D Gaussian volume representation and simulated slice acquisition with multi-resolution joint optimization; avoids ground-truth, faster and more accurate than baselines.


<details>
  <summary>Details</summary>
Motivation: Existing SVR methods are slow and need multiple orthogonal stacks; learning-based methods require inaccessible ground-truth volumes. A self-supervised method that models volume compactly and simulates slice acquisition can remove ground-truth dependence and speed up reconstruction.

Method: Represent target volume as a set of 3D Gaussians; employ a simulated forward slice acquisition model for self-supervision; jointly optimize Gaussian parameters and spatial transformations across multi-resolution levels during training.

Result: GaussianSVR outperforms baseline methods on fetal MR volumetric reconstruction; shows improved fidelity and efficiency. Code to be released upon acceptance.

Conclusion: This paper proposes GaussianSVR, a self-supervised slice-to-volume reconstruction framework using 3D Gaussian representations and simulated forward acquisition; it achieves higher-fidelity fetal MR reconstructions without ground truth and improves accuracy and efficiency via multi-resolution training.

Abstract: Reconstructing 3D fetal MR volumes from motion-corrupted stacks of 2D slices is a crucial and challenging task. Conventional slice-to-volume reconstruction (SVR) methods are time-consuming and require multiple orthogonal stacks for reconstruction. While learning-based SVR approaches have significantly reduced the time required at the inference stage, they heavily rely on ground truth information for training, which is inaccessible in practice. To address these challenges, we propose GaussianSVR, a self-supervised framework for slice-to-volume reconstruction. GaussianSVR represents the target volume using 3D Gaussian representations to achieve high-fidelity reconstruction. It leverages a simulated forward slice acquisition model to enable self-supervised training, alleviating the need for ground-truth volumes. Furthermore, to enhance both accuracy and efficiency, we introduce a multi-resolution training strategy that jointly optimizes Gaussian parameters and spatial transformations across different resolution levels. Experiments show that GaussianSVR outperforms the baseline methods on fetal MR volumetric reconstruction. Code will be available upon acceptance.

</details>


### [70] [Leveraging Multi-Rater Annotations to Calibrate Object Detectors in Microscopy Imaging](https://arxiv.org/abs/2601.23007)
*Francesco Campi,Lucrezia Tondo,Ekin Karabati,Johannes Betge,Marie Piraud*

Main category: cs.CV

TL;DR: 针对多标注者注释，分别训练标注者专属模型并聚合其预测，可以更好地捕捉标注差异并提高检测器置信度校准，而不损失检测性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习物体检测器在显微成像上性能优异，但置信度估计（calibration）不足，限制了在生物医学应用中的可信度。多标注者提供的注释包含有价值的分歧信息，可用于改进模型校准。

Method: 为每一位专家（标注者）单独训练检测模型（rater-specific models），在推理时对这些模型的预测进行聚合以生成最终置信度与检测结果；与将不同标注混合训练的标签采样策略相比，这种策略更好地反映标注差异并提高校准。

Result: 在一项由两位专家标注的大肠类器官（colorectal organoid）数据集上的实验证明，rater-specific ensemble策略相比标签混合训练在校准上有改进，同时检测准确性相当。

Conclusion: 通过对不同标注者分别训练模型并将其预测聚合，能更好地模拟多标注者共识，从而提高显著性目标检测器的置信度校准性，同时保持检测精度。

Abstract: Deep learning-based object detectors have achieved impressive performance in microscopy imaging, yet their confidence estimates often lack calibration, limiting their reliability for biomedical applications. In this work, we introduce a new approach to improve model calibration by leveraging multi-rater annotations. We propose to train separate models on the annotations from single experts and aggregate their predictions to emulate consensus. This improves upon label sampling strategies, where models are trained on mixed annotations, and offers a more principled way to capture inter-rater variability. Experiments on a colorectal organoid dataset annotated by two experts demonstrate that our rater-specific ensemble strategy improves calibration performance while maintaining comparable detection accuracy. These findings suggest that explicitly modelling rater disagreement can lead to more trustworthy object detectors in biomedical imaging.

</details>


### [71] [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/abs/2601.23041)
*Youxu Shi,Suorong Yang,Dong Liu*

Main category: cs.CV

TL;DR: 提出OSGA：用一个信息性样本和生成锚点正则化，只学一个可重用的steering向量，轻量级地提高VLM的可靠性与安全性。


<details>
  <summary>Details</summary>
Motivation: 现有VLM尽管规模化仍存在幻觉与安全失败，传统steering在效率与效果间需权衡。观察到当任务语义意图对齐时，steering向量可在输入间泛化，因此探索输入无关的单次优化方法以提高可靠性。

Method: OSGA先用基于方差的数据选择策略挑选一个信息量高的样本，然后用对比目标结合生成锚点正则化来学习一个单一的steering向量；该向量在推理阶段注入到模型特定层，无需修改模型参数。

Result: 跨多个基准实验证明，单个OSGA优化得到的steering向量能在不同输入上持续提升幻觉缓解与安全性，且计算开销极低。

Conclusion: 本文提出的OSGA通过学习单一输入无关的steering向量在视觉语言模型上进行一次性优化，可在推理时不改动模型参数即可应用，显著降低幻觉和安全相关失误，证明一-shot steering在效率与效果上具有实用性和可扩展性。

Abstract: Vision Language Models (VLMs) achieve strong performance on multimodal tasks but still suffer from hallucination and safety-related failures that persist even at scale. Steering offers a lightweight technique to improve model performance. However, steering, whether input-dependent or input-independent, achieves a meaningful trade-off between efficiency and effectiveness. In this work, we observe that steering vectors can generalize across inputs when tasks share aligned semantic intent. Based on this insight, we propose \textbf{OSGA} (\textbf{O}ne-shot \textbf{S}teering with \textbf{G}enerative \textbf{A}nchor), an input-independent framework that improves model performance with a single optimization instance. OSGA first selects an informative sample via a variance-based data selection strategy and learns a single steering vector with a contrastive objective with generative anchor regularization. The resulting vector can be universally applied at a certain layer during inference time without modifying model parameters. Experiments across multiple benchmarks show that a single OSGA-optimized steering vector consistently improves hallucination mitigation and safety enhancement with negligible overhead, highlighting one-shot steering as a practical and scalable solution for reliable VLMs.

</details>


### [72] [HierLoc: Hyperbolic Entity Embeddings for Hierarchical Visual Geolocation](https://arxiv.org/abs/2601.23064)
*Hari Krishna Gadi,Daniel Matos,Hongyi Luo,Lu Liu,Yongliang Wang,Yanfeng Zhang,Liqiu Meng*

Main category: cs.CV

TL;DR: 用双曲空间中的地理实体层次和将haversine距离纳入对比学习的几何加权方法，替代大规模图像检索，达到更高精度与更低存储需求。


<details>
  <summary>Details</summary>
Motivation: 视觉地理定位面临全球尺度、视觉歧义和地理层级结构挑战。现有方法要么依赖大规模检索需存储海量图像嵌入，要么采用格网分类忽略地理连续性，生成模型虽能覆盖空间但难以捕捉细节。为此提出基于地理实体层次与几何意识的替代方案。

Method: 提出实体中心的层次结构，将国家/地区/子区域/城市作为实体，嵌入到双曲空间；设计Geo-Weighted Hyperbolic对比学习，将图像与各层级实体直接对齐，并在对比损失中加入基于haversine距离的权重以保留地理连续性；推理时只需查询实体嵌入，显著减少存储与计算。使用OSV5M数据集进行训练与评估，比较了基线检索、格网分类与生成模型。

Result: 在OSV5M基准上达到新SOTA，用24万实体嵌入替代5M+图像嵌入，平均大地误差减少19.5%，子区域精确度提高43%，并在推理效率与可解释性上有显著优势。

Conclusion: 该论文提出了一种基于实体（国家、地区、子区域、城市）分层表示并嵌入到双曲空间的视觉地理定位方法，通过在对比学习目标中直接引入haversine距离实现Geo-Weighted Hyperbolic对比学习，替代了传统大规模图像检索与格网分类策略，从而在OSV5M基准上以仅24万实体嵌入替代500万+图像嵌入，显著提升了定位精度，平均大地误差降低19.5%，子区域精确度提升43%。

Abstract: Visual geolocalization, the task of predicting where an image was taken, remains challenging due to global scale, visual ambiguity, and the inherently hierarchical structure of geography. Existing paradigms rely on either large-scale retrieval, which requires storing a large number of image embeddings, grid-based classifiers that ignore geographic continuity, or generative models that diffuse over space but struggle with fine detail. We introduce an entity-centric formulation of geolocation that replaces image-to-image retrieval with a compact hierarchy of geographic entities embedded in Hyperbolic space. Images are aligned directly to country, region, subregion, and city entities through Geo-Weighted Hyperbolic contrastive learning by directly incorporating haversine distance into the contrastive objective. This hierarchical design enables interpretable predictions and efficient inference with 240k entity embeddings instead of over 5 million image embeddings on the OSV5M benchmark, on which our method establishes a new state-of-the-art performance. Compared to the current methods in the literature, it reduces mean geodesic error by 19.5\%, while improving the fine-grained subregion accuracy by 43%. These results demonstrate that geometry-aware hierarchical embeddings provide a scalable and conceptually new alternative for global image geolocation.

</details>


### [73] [Rethinking Transferable Adversarial Attacks on Point Clouds from a Compact Subspace Perspective](https://arxiv.org/abs/2601.23102)
*Keke Tang,Xianheng Liu,Weilong Peng,Xiaofei Wang,Daizong Liu,Peican Zhu,Can Lu,Zhihong Tian*

Main category: cs.CV

TL;DR: CoSA通过类特定原型和低秩语义子空间约束扰动，实现对点云的架构无关且语义一致的可迁移对抗攻击，在跨模型性能上优于现有方法且保持良好隐蔽性和防御鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有点云可迁移攻击依赖模型特定梯度或启发式，泛化到未知网络时效果差；因此希望通过共享的语义子空间来获得与模型无关的扰动方向，提高迁移性。

Method: 将每个点云表示为若干类特定原型的紧凑组合，构建共享低维语义空间；在该空间内对扰动进行低秩子空间优化，抑制模型相关噪声并限制扰动方向为语义有意义的变化。

Result: 在多个数据集与网络架构上广泛实验，CoSA在迁移攻击成功率上持续超过最先进方法，同时在不可察觉性和对常见防御策略的鲁棒性方面保持竞争力。

Conclusion: 本文提出的CoSA从紧凑子空间视角提高点云攻击的可迁移性，通过类特定原型表示和低秩子空间约束扰动，使攻击更具语义一致性与架构无关性，从而在跨模型迁移上优于现有方法。

Abstract: Transferable adversarial attacks on point clouds remain challenging, as existing methods often rely on model-specific gradients or heuristics that limit generalization to unseen architectures. In this paper, we rethink adversarial transferability from a compact subspace perspective and propose CoSA, a transferable attack framework that operates within a shared low-dimensional semantic space. Specifically, each point cloud is represented as a compact combination of class-specific prototypes that capture shared semantic structure, while adversarial perturbations are optimized within a low-rank subspace to induce coherent and architecture-agnostic variations. This design suppresses model-dependent noise and constrains perturbations to semantically meaningful directions, thereby improving cross-model transferability without relying on surrogate-specific artifacts. Extensive experiments on multiple datasets and network architectures demonstrate that CoSA consistently outperforms state-of-the-art transferable attacks, while maintaining competitive imperceptibility and robustness under common defense strategies. Codes will be made public upon paper acceptance.

</details>


### [74] [FlowCalib: LiDAR-to-Vehicle Miscalibration Detection using Scene Flows](https://arxiv.org/abs/2601.23107)
*Ilir Tahiraj,Peter Wittal,Markus Lienkamp*

Main category: cs.CV

TL;DR: FlowCalib 利用静态场景的场流系统性偏差，结合神经场流估计与双分支（学习+手工）检测网络，实现无需额外传感器的 LiDAR-车体角度错位检测，在 nuScenes 上表现稳健，建立了基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦传感器间校准而忽视单个传感器自身相对于车体的错位；角度错位会在自动驾驶中引起安全问题，因此需要单传感器到车体的错位检测方法。

Method: 提出将神经场流先验用于从连续 3D 点云估计场流，结合双分支检测网络：一分支学习全局流特征，另一分支使用手工几何描述符，两者融合后进行二元分类（全局是否错位）及按轴的二元分类（各旋转轴是否错位）。

Result: 在 nuScenes 数据集上，FlowCalib 能够稳健检测 LiDAR-车体的错位，成功判别全局错位与按轴错位，实验结果确立了该领域的检测基准。

Conclusion: FlowCalib 有效利用静态物体的场流系统性偏差来检测 LiDAR 与车体的角度错位，提出了首个无需额外传感器的检测框架，并在 nuScenes 上展示了鲁棒性，建立了检测基准。

Abstract: Accurate sensor-to-vehicle calibration is essential for safe autonomous driving. Angular misalignments of LiDAR sensors can lead to safety-critical issues during autonomous operation. However, current methods primarily focus on correcting sensor-to-sensor errors without considering the miscalibration of individual sensors that cause these errors in the first place. We introduce FlowCalib, the first framework that detects LiDAR-to-vehicle miscalibration using motion cues from the scene flow of static objects. Our approach leverages the systematic bias induced by rotational misalignment in the flow field generated from sequential 3D point clouds, eliminating the need for additional sensors. The architecture integrates a neural scene flow prior for flow estimation and incorporates a dual-branch detection network that fuses learned global flow features with handcrafted geometric descriptors. These combined representations allow the system to perform two complementary binary classification tasks: a global binary decision indicating whether misalignment is present and separate, axis-specific binary decisions indicating whether each rotational axis is misaligned. Experiments on the nuScenes dataset demonstrate FlowCalib's ability to robustly detect miscalibration, establishing a benchmark for sensor-to-vehicle miscalibration detection.

</details>


### [75] [Segment Any Events with Language](https://arxiv.org/abs/2601.23159)
*Seungjun Lee,Gim Hee Lee*

Main category: cs.CV

TL;DR: SEAL是首个面向事件传感器的开放词汇事件实例分割框架，支持多粒度实例与部件级分割，表现优于基线并提供无需提示的变体。


<details>
  <summary>Details</summary>
Motivation: 现有自由文本场景理解在图像、点云和LiDAR上有大量研究，但在事件传感器上研究稀缺且多集中于语义层面，缺乏面向实例与开放词汇的细粒度分割方法。

Method: 构建一个统一模型，以视觉提示（visual prompt）作为输入，执行事件分割与开放词汇掩码分类；设计参数高效的架构以提升推理速度与性能；并在附录中给出无需视觉提示的变体以实现通用时空OV-EIS。

Result: 通过构建四个基准覆盖从粗到细的标签粒度与从实例到部件的语义粒度，实验证明SEAL在性能和推理速度上明显优于所提基线，且模型参数高效。

Conclusion: 该论文提出了名为SEAL的首个面向事件传感器的语义感知分段任意事件框架，用于开放词汇事件实例分割（OV-EIS），并在多粒度（实例级和部件级）上支持分割与开放词汇掩码分类。

Abstract: Scene understanding with free-form language has been widely explored within diverse modalities such as images, point clouds, and LiDAR. However, related studies on event sensors are scarce or narrowly centered on semantic-level understanding. We introduce SEAL, the first Semantic-aware Segment Any Events framework that addresses Open-Vocabulary Event Instance Segmentation (OV-EIS). Given the visual prompt, our model presents a unified framework to support both event segmentation and open-vocabulary mask classification at multiple levels of granularity, including instance-level and part-level. To enable thorough evaluation on OV-EIS, we curate four benchmarks that cover label granularity from coarse to fine class configurations and semantic granularity from instance-level to part-level understanding. Extensive experiments show that our SEAL largely outperforms proposed baselines in terms of performance and inference speed with a parameter-efficient architecture. In the Appendix, we further present a simple variant of our SEAL achieving generic spatiotemporal OV-EIS that does not require any visual prompts from users in the inference. Check out our project page in https://0nandon.github.io/SEAL

</details>


### [76] [Hi-Light: A Path to high-fidelity, high-resolution video relighting with a Novel Evaluation Paradigm](https://arxiv.org/abs/2601.23167)
*Xiangrui Liu,Haoxiang Li,Yezhou Yang*

Main category: cs.CV

TL;DR: Hi-Light是一种无需训练的视频重光照框架，结合光度先验引导扩散、运动自适应光照平滑和LAB细节融合，并提出专门的光照稳定性指标，能生成稳定且高保真的重光照视频，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照存在缺乏专用评估指标、输出光照随时间闪烁严重以及重光照过程中细节丢失三大问题，且现有方法在高分辨率、高保真度和鲁棒性上不足，因而提出无需训练的Hi-Light来应对这些限制。

Method: 方法包含：1) 基于光度先验的引导扩散（lightness prior anchored guided relighting diffusion）用于稳定中间重光照视频；2) 混合运动自适应光照平滑滤波器（Hybrid Motion-Adaptive Lighting Smoothing Filter），利用光流保证时序稳定性且避免运动模糊；3) 基于LAB的细节融合模块（LAB-based Detail Fusion）用于从原始视频保留高频细节。并提出Light Stability Score作为专门衡量光照一致性的量化指标。

Result: 论文通过大量实验（定性与定量）验证，Hi-Light在细节保留、光照稳定性和视觉质量上显著优于现有最先进方法，并在Light Stability Score上取得更好表现，生成稳定且高细节的重光照视频。

Conclusion: 该论文提出了Hi-Light，一个无需训练的高保真、高分辨率且鲁棒的视频重光照框架，通过三项技术创新解决了评估指标缺失、光照闪烁严重和细节退化问题，实验表明优于现有方法。

Abstract: Video relighting offers immense creative potential and commercial value but is hindered by challenges, including the absence of an adequate evaluation metric, severe light flickering, and the degradation of fine-grained details during editing. To overcome these challenges, we introduce Hi-Light, a novel, training-free framework for high-fidelity, high-resolution, robust video relighting. Our approach introduces three technical innovations: lightness prior anchored guided relighting diffusion that stabilises intermediate relit video, a Hybrid Motion-Adaptive Lighting Smoothing Filter that leverages optical flow to ensure temporal stability without introducing motion blur, and a LAB-based Detail Fusion module that preserves high-frequency detail information from the original video. Furthermore, to address the critical gap in evaluation, we propose the Light Stability Score, the first quantitative metric designed to specifically measure lighting consistency. Extensive experiments demonstrate that Hi-Light significantly outperforms state-of-the-art methods in both qualitative and quantitative comparisons, producing stable, highly detailed relit videos.

</details>


### [77] [Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training](https://arxiv.org/abs/2601.23220)
*Anglin Liu,Ruichao Chen,Yi Lu,Hongxia Xu,Jintai Chen*

Main category: cs.CV

TL;DR: 通过三种无监督代理任务与强化学习，Med-Scout修复了医学多模态模型的几何盲点，大幅提升几何感知及医学诊断相关任务表现。


<details>
  <summary>Details</summary>
Motivation: 观察到现有MLLM在医学诊断中尽管语言能力强，但在几何感知上存在盲点，导致生成看似合理但与几何事实冲突的错误；希望通过利用影像内部几何信息来弥补这一缺陷，避免昂贵的专家标注。

Method: 不依赖专家标注，设计三个代理任务（层级尺度定位、拓扑拼图重构、异常一致性检测），通过强化学习利用未标注医学影像中固有的几何逻辑生成可验证的监督信号，从而校正模型的几何感知。

Result: 在新提出的Med-Scout-Bench几何感知评测上，Med-Scout较领先的专有和开源MLLM提升超过40%；同时在放射学和综合医学VQA任务上也表现更好，显示了几何感知提升的泛化性。

Conclusion: 本文提出的Med-Scout通过无监督代理任务和强化学习显著改善了多模态大模型在医学影像几何感知方面的缺陷，能减少基于几何约束的虚构输出并提升医学理解能力。

Abstract: Despite recent Multimodal Large Language Models (MLLMs)' linguistic prowess in medical diagnosis, we find even state-of-the-art MLLMs suffer from a critical perceptual deficit: geometric blindness. This failure to ground outputs in objective geometric constraints leads to plausible yet factually incorrect hallucinations, rooted in training paradigms that prioritize linguistic fluency over geometric fidelity. This paper introduces Med-Scout, a novel framework that "cures" this blindness via Reinforcement Learning (RL) that leverages the intrinsic geometric logic latent within unlabeled medical images. Instead of relying on costly expert annotations, Med-Scout derives verifiable supervision signals through three strategic proxy tasks: Hierarchical Scale Localization, Topological Jigsaw Reconstruction, and Anomaly Consistency Detection. To rigorously quantify this deficit, we present Med-Scout-Bench, a new benchmark specifically designed to evaluate geometric perception. Extensive evaluations show that Med-Scout significantly mitigates geometric blindness, outperforming leading proprietary and open-source MLLMs by over 40% on our benchmark. Furthermore, this enhanced geometric perception generalizes to broader medical understanding, achieving superior results on radiological and comprehensive medical VQA tasks.

</details>


### [78] [Region-Normalized DPO for Medical Image Segmentation under Noisy Judges](https://arxiv.org/abs/2601.23222)
*Hamza Kalisch,Constantin Seibold,Jens Kleesiek,Ken Herrmann,Frederic Jonske*

Main category: cs.CV

TL;DR: 提出针对噪声QC信号的分割偏好微调方法RN-DPO，通过按不一致区域大小归一化偏好更新，减少有害影响，稳定并提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 像素级标注昂贵但很多系统已有廉价自动QC信号（模型一致性、不确定性或学习到的掩码质量分数），希望利用这些信号进行无额外标注的分割微调，但噪声和偏差会导致有害偏好更新，需要更稳健的方法。

Method: 基于一个用少量有标签数据训练的监督分割器生成候选掩码，并使用DPO对这些候选方案按QC判断进行偏好微调；发现偏好对抽样策略敏感，提出RN-DPO对DPO目标进行修改：在计算偏好梯度时以两掩码不一致的像素区域大小进行归一化，从而减弱小区域差异造成的过大更新。

Result: 在两个医学数据集及多种设定下，RN-DPO较标准DPO和强基线方法表现更稳定、持续提升性能，能在不需额外像素标注的情况下获得更好分割效果。

Conclusion: 该论文提出在存在噪声和偏差的自动质量控制（QC）信号下，采用偏好学习进行医学图像分割可能带来有害更新；提出区域归一化直接偏好优化（RN-DPO），通过按掩码不一致区域大小归一化偏好更新，降低有害比较的影响并稳定优化。

Abstract: While dense pixel-wise annotations remain the gold standard for medical image segmentation, they are costly to obtain and limit scalability. In contrast, many deployed systems already produce inexpensive automatic quality-control (QC) signals like model agreement, uncertainty measures, or learned mask-quality scores which can be used for further model training without additional ground-truth annotation. However, these signals can be noisy and biased, making preference-based fine-tuning susceptible to harmful updates. We study Direct Preference Optimization (DPO) for segmentation from such noisy judges using proposals generated by a supervised base segmenter trained on a small labeled set. We find that outcomes depend strongly on how preference pairs are mined: selecting the judge's top-ranked proposal can improve peak performance when the judge is reliable, but can amplify harmful errors under weaker judges. We propose Region-Normalized DPO (RN-DPO), a segmentation-aware objective which normalizes preference updates by the size of the disagreement region between masks, reducing the leverage of harmful comparisons and improving optimization stability. Across two medical datasets and multiple regimes, RN-DPO improves sustained performance and stabilizes preference-based fine-tuning, outperforming standard DPO and strong baselines without requiring additional pixel annotations.

</details>


### [79] [Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning](https://arxiv.org/abs/2601.23224)
*Xiangyu Zeng,Zhiqiu Zhang,Yuhan Zhu,Xinhao Li,Zikang Wang,Changlian Ma,Qingyu Zhang,Zizheng Huang,Kun Ouyang,Tianxiang Jiang,Ziang Yan,Yi Wang,Hongjie Zhang,Yali Wang,Limin Wang*

Main category: cs.CV

TL;DR: Video-o3 通过按步注意力隔离与轨迹奖励，结合大规模交互式训练数据（Seeker-173K），显著改善长视频中稀疏证据发现与多轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型在长视频中依赖均匀采样和单轮推理，难以在冗余信息中找到稀疏但关键的证据；需要交互式工具调用与多轮检索策略。

Method: 引入 Task-Decoupled Attention Masking（隔离每一步关注、保留全局上下文）和 Verifiable Trajectory-Guided Reward（在探索覆盖与推理效率间权衡以控制上下文增长）；结合数据合成管线生成 Seeker-173K 用于监督与强化学习训练。

Result: 在基准上有显著提升：MLVU 准确率72.1%、Video-Holmes 46.5%，显示在多跳证据寻求与推理能力上的优势，并验证原生工具调用在长视频任务中的有效性。

Conclusion: Video-o3 提出一种能在长视频理解中迭代发现关键信息并自适应终止的框架，通过隔离步骤注意力与奖励引导轨迹，提升多跳证据检索能力。

Abstract: Existing multimodal large language models for long-video understanding predominantly rely on uniform sampling and single-turn inference, limiting their ability to identify sparse yet critical evidence amid extensive redundancy. We introduce Video-o3, a novel framework that supports iterative discovery of salient visual clues, fine-grained inspection of key segments, and adaptive termination once sufficient evidence is acquired. Technically, we address two core challenges in interleaved tool invocation. First, to mitigate attention dispersion induced by the heterogeneity of reasoning and tool-calling, we propose Task-Decoupled Attention Masking, which isolates per-step concentration while preserving shared global context. Second, to control context length growth in multi-turn interactions, we introduce a Verifiable Trajectory-Guided Reward that balances exploration coverage with reasoning efficiency. To support training at scale, we further develop a data synthesis pipeline and construct Seeker-173K, comprising 173K high-quality tool-interaction trajectories for effective supervised and reinforcement learning. Extensive experiments show that Video-o3 substantially outperforms state-of-the-art methods, achieving 72.1% accuracy on MLVU and 46.5% on Video-Holmes. These results demonstrate Video-o3's strong multi-hop evidence-seeking and reasoning capabilities, and validate the effectiveness of native tool invocation in long-video scenarios.

</details>


### [80] [ShotFinder: Imagination-Driven Open-Domain Video Shot Retrieval via Web Search](https://arxiv.org/abs/2601.23232)
*Tao Yu,Haopeng Jin,Hao Wang,Shenghua Chai,Yujia Yang,Junhao Gong,Jiaming Guo,Minghui Zhang,Xinlong Chen,Zhenghao Zhang,Yuxuan Zhou,Yanpei Gong,YuanCheng Liu,Yiming Ding,Kangwei Zeng,Pengfei Yang,Zhongtian Luo,Yufei Xiong,Shanbin Zhang,Shaoxiong Cheng,Huang Ruilin,Li Shuo,Yuxi Niu,Xinyuan Zhang,Yueya Xu,Jie Mao,Ruixuan Ji,Yaru Zhao,Mingchen Zhang,Jiabing Yang,Jiaqi Liu,YiFan Zhang,Hongzhu Yi,Xinming Wang,Cheng Zhong,Xiao Ma,Zhang Zhang,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 引入ShotFinder基准与三阶段检索管线，构建1210个样本并实证大模型在开放域视频镜头检索上表现远未达到人类水平，尤其在颜色与风格约束下存在大缺口。


<details>
  <summary>Details</summary>
Motivation: 现有检索多关注文本或静态多模态，缺乏针对富时序和复杂语义的视频镜头检索基准与分析。

Method: 提出基准（关键帧导向的镜头描述与五类单因子约束）并构建1210样本；提出三阶段检索定位管线：查询扩展（视频想象）、候选检索（搜索引擎）、描述引导时序定位。

Result: 在多种闭源与开源模型上实验，发现与人类性能有显著差距；时序定位相对可解，颜色与视觉风格最具挑战性。

Conclusion: 该论文提出了ShotFinder基准与方法，证明开放域视频镜头检索仍有显著挑战。

Abstract: In recent years, large language models (LLMs) have made rapid progress in information retrieval, yet existing research has mainly focused on text or static multimodal settings. Open-domain video shot retrieval, which involves richer temporal structure and more complex semantics, still lacks systematic benchmarks and analysis. To fill this gap, we introduce ShotFinder, a benchmark that formalizes editing requirements as keyframe-oriented shot descriptions and introduces five types of controllable single-factor constraints: Temporal order, Color, Visual style, Audio, and Resolution. We curate 1,210 high-quality samples from YouTube across 20 thematic categories, using large models for generation with human verification. Based on the benchmark, we propose ShotFinder, a text-driven three-stage retrieval and localization pipeline: (1) query expansion via video imagination, (2) candidate video retrieval with a search engine, and (3) description-guided temporal localization. Experiments on multiple closed-source and open-source models reveal a significant gap to human performance, with clear imbalance across constraints: temporal localization is relatively tractable, while color and visual style remain major challenges. These results reveal that open-domain video shot retrieval is still a critical capability that multimodal large models have yet to overcome.

</details>


### [81] [Structured Over Scale: Learning Spatial Reasoning from Educational Video](https://arxiv.org/abs/2601.23251)
*Bishoy Galoaa,Xiangyu Bai,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: 用结构化的教育视频（DoraVQA）和GRPO微调VLM，可以在基础推理任务上显著提升并实现跨域迁移，说明内容的教学结构与数据规模同等重要。


<details>
  <summary>Details</summary>
Motivation: 观察到现有VLM在儿童能解决的基础推理任务上表现不足，作者假设教育视频中有利于教学的结构化内容能提供有效监督信号，帮助模型学习更强的推理能力。

Method: 作者构建了DoraVQA数据集（5344问答对，来自《爱探险的朵拉》8季，带精确时间戳），并采用Group Relative Policy Optimization (GRPO)微调Qwen2和Qwen3模型，利用教育视频中明确的“情境-问题-暂停-回答”结构作为正确性信号和推理轨迹。

Result: 在仅用38小时儿童教育视频训练的情况下，模型在DoraVQA上提升8-14个百分点，对外达到CVBench 86.16%（SOTA），并在Video-MME和NExT-QA上展现良好迁移能力，表明结构化教学内容有助于训练更具通用性的多模态推理能力。

Conclusion: 该论文结论认为，利用教育类视频的结构化教学内容可以显著提升视觉-语言模型（VLM）在基础推理任务（如计数、空间推理和组合理解）上的表现，且这种提升可从窄域的教学内容迁移到更广泛的视频理解基准上。

Abstract: Vision-language models (VLMs) demonstrate impressive performance on standard video understanding benchmarks yet fail systematically on simple reasoning tasks that preschool children can solve, including counting, spatial reasoning, and compositional understanding. We hypothesize that the pedagogically-structured content of educational videos provides an ideal training signal for improving these capabilities. We introduce DoraVQA, a dataset of 5,344 question-answer pairs automatically extracted from 8 seasons of Dora the Explorer with precise timestamp alignment. Each episode follows a consistent \textit{context-question-pause-answer} structure that creates a self-contained learning environment analogous to interactive tutoring. We fine-tune both Qwen2 and Qwen3 using Group Relative Policy Optimization (GRPO), leveraging the clear correctness signals and structured reasoning traces inherent in educational content. Despite training exclusively on 38 hours of children's educational videos, our approach achieves improvements of 8-14 points on DoraVQA and state-of-the-art 86.16\% on CVBench, with strong transfer to Video-MME and NExT-QA, demonstrating effective generalization from narrow pedagogical content to broad multimodal understanding. Through cross-domain benchmarks, we show that VLMs can perform tasks that require robust reasoning learned from structured educational content, suggesting that content structure matters as much as content scale.

</details>


### [82] [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/abs/2601.23253)
*Yi Zhang,Chun-Wun Cheng,Angelica I. Aviles-Rivero,Zhihai He,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: TaTa是一种无训练的测试时适配方法，利用布朗距离协方差进行视觉-文本相关性度量，结合属性增强提示、动态聚类与伪标签细化，能在保持低计算成本的同时提高VLM在域移位场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适配方法通常依赖反向传播和参数更新，计算资源开销大且可能导致模型不稳定；同时多模态适配方法多集中在单一模态或简单的相似度度量（如余弦相似度），难以捕捉复杂的线性与非线性依赖。为此，提出利用能捕捉高阶关系的BDC进行无训练的自适应，以提高效率、稳定性和泛化能力。

Method: 方法核心是利用布朗距离协方差（BDC）作为特征-提示相关性度量，用于无训练的测试时适配。具体包括：1）属性增强的文本提示生成，将视觉属性嵌入文本提示以提高描述性；2）基于BDC的提示选择或提示重排，无需反向传播即可动态适配；3）对未标注测试样本进行动态聚类并生成伪标签，通过伪标签细化提高可靠性；4）推理阶段以BDC分数计算视觉-文本相似性并做最终预测。该流程不依赖模型权重更新，从而节省计算并避免不稳定训练问题。

Result: 作者在多组跨域和跨数据集实验中展示TaTa的有效性，包括在域移位和不同数据分布场景下比较计算成本与性能。结果表明，TaTa显著降低计算成本（因为无反向传播和参数更新）并在多项基准上达到或超过现有测试时适配方法的性能，尤其在稳定性和对视觉属性的适应上表现突出。

Conclusion: TaTa提出了一个无训练、无反向传播的测试时适配方法，基于布朗距离协方差（Brownian Distance Covariance, BDC）度量多模态特征与文本提示之间的相关性，从而在目标域上动态选择/重排文本提示及伪标签，实现对视觉语言模型（VLMs）的域自适应。方法避免了参数更新，提高了效率和稳定性，并结合属性增强的提示、动态聚类与伪标签细化来提升推理性能。实验证明在多个数据集上，TaTa在计算开销大幅降低的同时，能取得或接近最先进的跨域与跨数据集泛化性能。

Abstract: Vision-language models suffer performance degradation under domain shift, limiting real-world applicability. Existing test-time adaptation methods are computationally intensive, rely on back-propagation, and often focus on single modalities. To address these issues, we propose Training-free Test-Time Adaptation with Brownian Distance Covariance (TaTa). TaTa leverages Brownian Distance Covariance-a powerful statistical measure that captures both linear and nonlinear dependencies via pairwise distances-to dynamically adapt VLMs to new domains without training or back-propagation. This not only improves efficiency but also enhances stability by avoiding disruptive weight updates. TaTa further integrates attribute-enhanced prompting to improve vision-language inference with descriptive visual cues. Combined with dynamic clustering and pseudo-label refinement, it effectively recalibrates the model for novel visual contexts. Experiments across diverse datasets show that TaTa significantly reduces computational cost while achieving state-of-the-art performance in domain and cross-dataset generalization.

</details>


### [83] [User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments](https://arxiv.org/abs/2601.23281)
*Junfeng Lin,Yanming Xiu,Maria Gorlatova*

Main category: cs.CV

TL;DR: TL;DR：在XR的交互提示场景中，OSOD模型对标准/欠细化提示稳健，但对语用模糊（和GroundingDINO对过度细化）敏感；使用提示增强能显著恢复性能，建议在XR系统中应用提示改写/补全策略以提高OSOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 动机：现实交互式XR中用户生成的文本提示通常具有模糊、不完整或过于冗长的问题，现有OSOD基准未充分考察模型在这些真实提示行为下的鲁棒性，因此需要系统评估和改进提示条件下的OSOD性能。

Method: 方法：在真实XR图像上评估两种OSOD模型（GroundingDINO与YOLO-E），使用视觉-语言模型生成四类模拟用户提示（标准、欠细化、过度细化、语用模糊），并测试两种提示增强策略对模型预测的影响。采用mIoU与平均置信度等指标对检测与分类性能进行量化比较。

Result: 结果：两模型在欠细化与标准提示下表现稳定，但在语用模糊提示下性能显著下降；GroundingDINO对过度细化提示更敏感。提示增强策略在模糊提示情形下显著提升性能（mIoU>55%和平均置信度>41%的提升）。基于实验提出了推荐的提示编写与增强方法。

Conclusion: 论文结论：在交互式XR场景中，OSOD模型在面对用户生成提示时表现差异明显：对欠细化和标准提示相对稳健，但在语用模糊提示下性能显著下降；过度细化提示主要影响GroundingDINO。通过提示增强技术可以显著提升在模糊提示下的鲁棒性（mIoU提升>55%，平均置信度提升>41%）。基于此提出了多项提示策略和增强方法以改善XR环境中的OSOD表现。

Abstract: Open-set object detection (OSOD) localizes objects while identifying and rejecting unknown classes at inference. While recent OSOD models perform well on benchmarks, their behavior under realistic user prompting remains underexplored. In interactive XR settings, user-generated prompts are often ambiguous, underspecified, or overly detailed. To study prompt-conditioned robustness, we evaluate two OSOD models, GroundingDINO and YOLO-E, on real-world XR images and simulate diverse user prompting behaviors using vision-language models. We consider four prompt types: standard, underdetailed, overdetailed, and pragmatically ambiguous, and examine the impact of two enhancement strategies on these prompts. Results show that both models exhibit stable performance under underdetailed and standard prompts, while they suffer degradation under ambiguous prompts. Overdetailed prompts primarily affect GroundingDINO. Prompt enhancement substantially improves robustness under ambiguity, yielding gains exceeding 55% mIoU and 41% average confidence. Based on the findings, we propose several prompting strategies and prompt enhancement methods for OSOD models in XR environments.

</details>


### [84] [VideoGPA: Distilling Geometry Priors for 3D-Consistent Video Generation](https://arxiv.org/abs/2601.23286)
*Hongyang Du,Junjie Ye,Xiaoyan Cong,Runhao Li,Jingcheng Ni,Aman Agarwal,Zeqi Zhou,Zekun Li,Randall Balestriero,Yue Wang*

Main category: cs.CV

TL;DR: VideoGPA用几何模型自动生成偏好信号并通过DPO微调视频扩散模型，显著改善3D结构一致性与时间稳定性，无需人工标注且数据高效。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在时间上和空间上往往难以保持三维结构一致，导致物体变形或空间漂移。作者认为问题根源在于普通去噪目标未明确鼓励几何一致性，因此需要引入几何偏好监督。

Method: 使用几何基础模型自动生成用于偏好学习的成对样本（密集偏好信号），并将这些偏好通过DPO整合进视频扩散模型的训练，使生成分布倾向具有更好几何一致性的解空间。该框架为自监督、数据高效；训练时不需要人工标注。

Result: 实验证明，VideoGPA在使用极少偏好对的情况下显著提升了时间稳定性、物理合理性与运动连贯性，并在大量对比实验中持续优于最先进基线。

Conclusion: 本论文提出VideoGPA，通过利用几何基础模型生成密集偏好信号并用直接偏好优化（DPO）引导视频扩散模型，从而提高3D结构一致性。

Abstract: While recent video diffusion models (VDMs) produce visually impressive results, they fundamentally struggle to maintain 3D structural consistency, often resulting in object deformation or spatial drift. We hypothesize that these failures arise because standard denoising objectives lack explicit incentives for geometric coherence. To address this, we introduce VideoGPA (Video Geometric Preference Alignment), a data-efficient self-supervised framework that leverages a geometry foundation model to automatically derive dense preference signals that guide VDMs via Direct Preference Optimization (DPO). This approach effectively steers the generative distribution toward inherent 3D consistency without requiring human annotations. VideoGPA significantly enhances temporal stability, physical plausibility, and motion coherence using minimal preference pairs, consistently outperforming state-of-the-art baselines in extensive experiments.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [85] [An innovating approach to teaching applied to database design. Improvement of Action Learning in Lifelong Learning](https://arxiv.org/abs/2601.22175)
*Christophe Béchade*

Main category: cs.DB

TL;DR: Action Learning通过项目驱动、教师监管与咨询经验传授，使无数据处理背景的从业者能在真实企业项目中学会数据库设计，促进校企合作与职业能力提升。


<details>
  <summary>Details</summary>
Motivation: 弥补非数据处理专业员工在数据库设计领域的能力短板，同时实现高校与企业合作、提升合同数量与质量。

Method: 在大学继续教育学院框架下，采用项目驱动的教学模式，由教师担任项目监督、咨询师提供实践经验，学员以真实企业流程为基础参与数据库设计项目。

Result: 实践十年表明，该模式继承并整合了技术课程的项目式教学、职业培训的适应性和服务提供者的专业能力，达成了可评估且持久的教学与职业目标。

Conclusion: Action Learning能有效将高校、企业与职业培训的优势结合，推动非信息专业从业者在数据库设计方面的能力建设，支持大学与企业合作发展。

Abstract: For now 10 years, the Action Learning has allowed employees of University of Angers, private and public Companies to be initiated with the design of database, on projects financed by professional structures. These innovating training periods are carried out within the framework of the University College of Further Education of the University of Angers. Database design is a process initially reserved to the professional data processing specialists, coming from French Level-2 technological courses (2-year degrees) or Engineer Schools (Master). The pedagogical model of technological courses has integrated for more than 20 years transverse semester projects, in order to give the students the opportunity to apply newly acquired knowledge, coordinated by teachers. Action Learning requires teachers to assume the role of supervisors for the project management. The objective of Action Learning is to transmit not only knowledge from teachers, but also the experience of consultants to trainees having no competence in data processing, but who have the knowledge of their business process. The present paper shows that Action Learning puts together the factors for success of French technological courses, the adaptability of pedagogy provided to the vocational training, and finally the competence of service provider, Keeping the best parts of those three complementary approaches makes it possible for this kind of formation to achieve teaching and professional, assessable and long lasting goals. Action Learning belongs to the French policy that aims to improve the volume and the quality of the contracts between Universities and companies.

</details>


### [86] [Discovering High-utility Sequential Rules with Increasing Utility Ratio](https://arxiv.org/abs/2601.22178)
*Zhenqiang Ye,Wensheng Gan,Gengsen Huang,Tianlong Gu,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出SRIU，通过双向扩展、IPEUP剪枝与紧凑数据结构，高效挖掘效用比递增的高效用序列规则，实验验证了方法的有效性与规则质量提升。


<details>
  <summary>Details</summary>
Motivation: 现有规则增长方法中，新增项对已有规则的效用或置信度影响不明确，导致无法保证规则在扩展后效用/置信度的单调性；因此提出挖掘效用比递增的HUSRs问题，以提高规则可靠性与可解释性。

Method: 核心方法包括两种扩展方式（左-右扩展和右-左扩展）、项目对估计效用剪枝（IPEUP）、为两种扩展设计的上界与相应剪枝策略、使用Bitmap降低内存开销以及紧凑效用表以加速挖掘过程。

Result: SRIU在真实与合成数据集上的大量实验显示其能有效减少搜索空间并生成更相关的规则；使用置信度与说服度（conviction）等度量表明SRIU挖出的规则在可靠性与相关性上有所提升。

Conclusion: 本文提出了一种名为SRIU的新算法，用于挖掘满足“效用比递增”性质的高效用序列规则（HUSRs），并通过双向扩展与剪枝策略有效减少搜索空间。实验表明SRIU在效率和规则相关性上均优于基线方法。

Abstract: Utility-driven mining is an essential task in data science, as it can provide deeper insight into the real world. High-utility sequential rule mining (HUSRM) aims at discovering sequential rules with high utility and high confidence. It can certainly provide reliable information for decision-making because it uses confidence as an evaluation metric, as well as some algorithms like HUSRM and US-Rule. However, in current rule-growth mining methods, the linkage between HUSRs and their generation remains ambiguous. Specifically, it is unclear whether the addition of new items affects the utility or confidence of the former rule, leading to an increase or decrease in their values. Therefore, in this paper, we formulate the problem of mining HUSRs with an increasing utility ratio. To address this, we introduce a novel algorithm called SRIU for discovering all HUSRs with an increasing utility ratio using two distinct expansion methods, including left-right expansion and right-left expansion. SRIU also utilizes the item pair estimated utility pruning strategy (IPEUP) to reduce the search space. Moreover, for the two expansion methods, two sets of upper bounds and corresponding pruning strategies are introduced. To enhance the efficiency of SRIU, several optimizations are incorporated. These include utilizing the Bitmap to reduce memory consumption and designing a compact utility table for the mining procedure. Finally, extensive experimental results from both real-world and synthetic datasets demonstrate the effectiveness of the proposed method. Moreover, to better assess the quality of the generated sequential rules, metrics such as confidence and conviction are employed, which further demonstrate that SRIU can improve the relevance of mining results.

</details>


### [87] [High-utility Sequential Rule Mining Utilizing Segmentation Guided by Confidence](https://arxiv.org/abs/2601.22179)
*Chunkai Zhang,Jiarui Deng,Maohua Lyu,Wensheng Gan,Philip S. Yu*

Main category: cs.DB

TL;DR: 提出RSC：置信度引导分段、预计算分段置信度、效用链接表与更严格的剩余效用上界，从而减少冗余效用计算并提升高效用序列规则挖掘效率。


<details>
  <summary>Details</summary>
Motivation: 现有高效用序列规则挖掘算法在含有重复项或相同序列但不同规则的情况下存在大量冗余效用计算，导致性能低下，因而需要一种能通过分段和预计算置信度来减少重复计算的改进方法。

Method: RSC首先基于置信度进行序列分段，预计算候选子序列的支持以得到分段置信度；在确定分割点后，同时生成具有不同先导项和后继项的所有规则；使用效用链接表加速候选序列生成，并提出“序列的剩余效用缩减”作为更严格的上界来处理重复项序列，从而剪枝。

Result: 在多个公开数据集上的实验表明，RSC在运行时间、候选数和/或内存消耗上均优于最新方法，说明减少冗余计算与更严格上界能有效提升性能。

Conclusion: 该论文提出了一种基于置信度引导分段的高效序列规则挖掘算法RSC，通过预先计算分段规则的置信度并并行生成不同前件/后件的规则，减少了重复的效用计算，改进了候选序列生成并引入更严格的效用上界。实验表明RSC在多个数据集上优于现有方法。

Abstract: Within the domain of data mining, one critical objective is the discovery of sequential rules with high utility. The goal is to discover sequential rules that exhibit both high utility and strong confidence, which are valuable in real-world applications. However, existing high-utility sequential rule mining algorithms suffer from redundant utility computations, as different rules may consist of the same sequence of items. When these items can form multiple distinct rules, additional utility calculations are required. To address this issue, this study proposes a sequential rule mining algorithm that utilizes segmentation guided by confidence (RSC), which employs confidence-guided segmentation to reduce redundant utility computation. It adopts a method that precomputes the confidence of segmented rules by leveraging the support of candidate subsequences in advance. Once the segmentation point is determined, all rules with different antecedents and consequents are generated simultaneously. RSC uses a utility-linked table to accelerate candidate sequence generation and introduces a stricter utility upper bound, called the reduced remaining utility of a sequence, to address sequences with duplicate items. Finally, the proposed RSC method was evaluated on multiple datasets, and the results demonstrate improvements over state-of-the-art approaches.

</details>


### [88] [COL-Trees: Efficient Hierarchical Object Search in Road Networks](https://arxiv.org/abs/2601.22183)
*Tenindra Abeywickrama,Muhammad Aamir Cheema,Sabine Storandt*

Main category: cs.DB

TL;DR: 提出一种基于路标的压缩对象-路标树（COL-Tree），用于在路网图上高效处理AkNN、kFN等复杂空间查询，实验显示显著加速且预处理代价低。


<details>
  <summary>Details</summary>
Motivation: 传统kNN和层次搜索方法多依赖欧几里得距离启发式，在路网等图结构上效果下降；许多应用需要考虑多代理或远对象查询（AkNN、kFN），因此需要更适合图结构的分层启发式索引。

Method: 构建COL-Tree：利用路标距离下界估计替代欧几里得启发式，对对象集合进行压缩分层索引；提出基于COL-Tree的查询算法，用于AkNN、kFN等查询，通过上界/下界剪枝加速遍历。

Result: 在真实和合成数据集上实验表明，COL-Tree的查询性能比现有方法高出最多4个数量级，同时预处理开销较小，理论与实际开销均可接受。

Conclusion: 本文提出了COL-Tree（Compacted Object-Landmark Tree），一种基于路标（landmark）启发式的分层图遍历数据结构，能在非欧几里得图（如路网）上高效支持多种查询，包括Aggregate k Nearest Neighbor (AkNN) 和 k Farthest Neighbor (kFN)。

Abstract: Location-based services rely heavily on efficient methods that search for relevant points-of-interest (POIs) near a given location. A k Nearest Neighbor (kNN) query is one such example that finds the k closest POIs from an agent's location. While most existing techniques focus on retrieving nearby POIs for a single agent, these search heuristics do not translate to many other applications. For example, Aggregate k Nearest Neighbor (AkNN) queries require POIs that are close to multiple agents. k Farthest Neighbor (kFN) queries require POIs that are the antithesis of nearest. Such problems naturally benefit from a hierarchical approach, but existing methods rely on Euclidean-based heuristics, which have diminished effectiveness in graphs such as road networks. We propose a novel data structure, COL-Tree (Compacted Object-Landmark Tree), to address this gap by enabling efficient hierarchical graph traversal using a more accurate landmark-based heuristic. We then present query algorithms that utilize COL-Trees to efficiently answer AkNN, kFN, and other queries. In our experiments on real-world and synthetic datasets, we demonstrate that our techniques significantly outperform existing approaches, achieving up to 4 orders of magnitude improvement. Moreover, this comes at a small pre-processing overhead in both theory and practice.

</details>
