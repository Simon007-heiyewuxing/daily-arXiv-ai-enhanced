<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 68]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [AI-Based Culvert-Sewer Inspection](https://arxiv.org/abs/2601.15366)
*Christina Thrainer*

Main category: cs.CV

TL;DR: 本文针对涵洞和污水管道缺陷分割，提出三种在标注数据稀缺情况下提升性能的方法：增强数据的预处理策略（含动态标签注入）、新型轻量网络FORTRESS（深度可分离卷积+自适应KAN+多尺度注意力）、以及基于双向注意力原型网络的少样本语义分割。三者在IoU和F1等指标上均显著提升，并在参数量与计算成本上有优势。


<details>
  <summary>Details</summary>
Motivation: 涵洞和污水管道缺陷标注难、成本高，数据集规模受限，亟需适用于小样本、低标注场景的高效分割方法。

Method: 1) 预处理与数据增强（传统增强+动态标签注入）；2) FORTRESS架构：深度可分离卷积、适应性Kolmogorov-Arnold网络（KAN）、多尺度注意力模块；3) 少样本语义分割：双向带注意力的原型网络。

Result: 在涵洞与管道缺陷数据集上，所提预处理方法提升了IoU和F1分数，FORTRESS达到了数据集上的最先进结果并显著减少参数量与计算量，少样本方法在评估指标上也取得令人满意的表现。

Conclusion: 通过数据增强与动态标签注入、设计轻量高效的FORTRESS网络，以及采用双向原型注意力的少样本分割方法，论文在有限标注数据条件下显著提高了涵洞与管道缺陷的语义分割性能，同时降低了模型复杂度，验证了这些方法在现实工程场景中的可行性。

Abstract: Culverts and sewer pipes are critical components of drainage systems, and their failure can lead to serious risks to public safety and the environment. In this thesis, we explore methods to improve automated defect segmentation in culverts and sewer pipes. Collecting and annotating data in this field is cumbersome and requires domain knowledge. Having a large dataset for structural defect detection is therefore not feasible. Our proposed methods are tested under conditions with limited annotated data to demonstrate applicability to real-world scenarios. Overall, this thesis proposes three methods to significantly enhance defect segmentation and handle data scarcity. This can be addressed either by enhancing the training data or by adjusting a models architecture.
  First, we evaluate preprocessing strategies, including traditional data augmentation and dynamic label injection. These techniques significantly improve segmentation performance, increasing both Intersection over Union (IoU) and F1 score. Second, we introduce FORTRESS, a novel architecture that combines depthwise separable convolutions, adaptive Kolmogorov-Arnold Networks (KAN), and multi-scale attention mechanisms. FORTRESS achieves state-of-the-art performance on the culvert sewer pipe defect dataset, while significantly reducing the number of trainable parameters, as well as its computational cost. Finally, we investigate few-shot semantic segmentation and its applicability to defect detection. Few-shot learning aims to train models with only limited data available. By employing a bidirectional prototypical network with attention mechanisms, the model achieves richer feature representations and achieves satisfactory results across evaluation metrics.

</details>


### [2] [Evaluating Multimodal Large Language Models for Heterogeneous Face Recognition](https://arxiv.org/abs/2601.15406)
*Hatef Otroshi Shahreza,Anjith George,Sébastien Marcel*

Main category: cs.CV

TL;DR: MLLMs currently underperform classical face recognition in cross-spectral HFR; significant gaps remain especially in VIS-THERMAL and VIS-SWIR scenarios.


<details>
  <summary>Details</summary>
Motivation: Assess whether recent advances in MLLMs translate to robust performance in challenging biometric tasks like cross-spectral face recognition.

Method: Benchmarked multiple open-source MLLMs across VIS-NIR, VIS-SWIR, VIS-THERMAL using standard biometric protocols; measured Acquire Rate, EER, TAR.

Result: Systematic evaluation of open-source Multimodal Large Language Models (MLLMs) for heterogeneous face recognition (HFR) across VIS, NIR, SWIR, and thermal modalities.

Conclusion: Current open-source MLLMs are not yet suitable replacements for purpose-built face recognition systems in HFR; rigorous biometric evaluation is essential before deployment.

Abstract: Multimodal Large Language Models (MLLMs) have recently demonstrated strong performance on a wide range of vision-language tasks, raising interest in their potential use for biometric applications. In this paper, we conduct a systematic evaluation of state-of-the-art MLLMs for heterogeneous face recognition (HFR), where enrollment and probe images are from different sensing modalities, including visual (VIS), near infrared (NIR), short-wave infrared (SWIR), and thermal camera. We benchmark multiple open-source MLLMs across several cross-modality scenarios, including VIS-NIR, VIS-SWIR, and VIS-THERMAL face recognition. The recognition performance of MLLMs is evaluated using biometric protocols and based on different metrics, including Acquire Rate, Equal Error Rate (EER), and True Accept Rate (TAR). Our results reveal substantial performance gaps between MLLMs and classical face recognition systems, particularly under challenging cross-spectral conditions, in spite of recent advances in MLLMs. Our findings highlight the limitations of current MLLMs for HFR and also the importance of rigorous biometric evaluation when considering their deployment in face recognition systems.

</details>


### [3] [CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation](https://arxiv.org/abs/2601.15408)
*Pablo Messina,Andrés Villa,Juan León Alcázar,Karen Sánchez,Carlos Hinojosa,Denis Parra,Álvaro Soto,Bernard Ghanem*

Main category: cs.CV

TL;DR: CURE uses error-aware curriculum learning to fine-tune multimodal models on phrase grounding and anatomy-grounded report tasks, dynamically sampling harder examples to boost alignment and reduce hallucinations, improving IoU, CXRFEScore, and lowering hallucinations with no extra data


<details>
  <summary>Details</summary>
Motivation: Improve visual grounding and factual consistency in radiology report generation without extra data by emphasizing harder samples via error-aware curriculum learning

Method: Fine-tune multimodal instructional model with curriculum

Result: Improved grounding accuracy (+0.37 IoU), increased report quality (+0.188 CXRFEScore), reduced hallucinations by 18.6%

Conclusion: CURE is a data-efficient framework that enhances spatial and textual alignment in medical VLMs, producing more reliable, better-grounded radiology reports and code/models are publicly available

Abstract: Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure

</details>


### [4] [DuFal: Dual-Frequency-Aware Learning for High-Fidelity Extremely Sparse-view CBCT Reconstruction](https://arxiv.org/abs/2601.15416)
*Cuong Tran Van,Trong-Thang Pham,Ngoc-Son Nguyen,Duy Minh Ho Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DuFal通过频域与空域双路径及跨注意力融合，有效恢复稀疏投影CT的高频细节，且参数友好，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 常规CNN偏向低频信息，难以从稀疏投影中恢复细腻的解剖高频结构；因此需要同时利用频域的全局频率建模和空域的局部空间信息以保留细节。

Method: 提出Dual-Frequency-Aware Learning（DuFal），包含高频局部分解傅里叶神经算子（包含全局高频增强FNO和局部高频增强FNO）、谱通道分解以减参数、及跨注意力频率融合模块，最终通过特征解码和强度场解码生成CT体积。

Result: 在LUNA16与ToothFairy数据集上的实验显示，DuFal在极稀疏视角下显著优于最先进方法，能更好保留高频解剖特征。

Conclusion: DuFal在稀疏投影CT重建中通过双路径频域与空域融合，显著提升了高频细节的恢复能力，尤其在极稀疏视角下优于现有方法。

Abstract: Sparse-view Cone-Beam Computed Tomography reconstruction from limited X-ray projections remains a challenging problem in medical imaging due to the inherent undersampling of fine-grained anatomical details, which correspond to high-frequency components. Conventional CNN-based methods often struggle to recover these fine structures, as they are typically biased toward learning low-frequency information. To address this challenge, this paper presents DuFal (Dual-Frequency-Aware Learning), a novel framework that integrates frequency-domain and spatial-domain processing via a dual-path architecture. The core innovation lies in our High-Local Factorized Fourier Neural Operator, which comprises two complementary branches: a Global High-Frequency Enhanced Fourier Neural Operator that captures global frequency patterns and a Local High-Frequency Enhanced Fourier Neural Operator that processes spatially partitioned patches to preserve spatial locality that might be lost in global frequency analysis. To improve efficiency, we design a Spectral-Channel Factorization scheme that reduces the Fourier Neural Operator parameter count. We also design a Cross-Attention Frequency Fusion module to integrate spatial and frequency features effectively. The fused features are then decoded through a Feature Decoder to produce projection representations, which are subsequently processed through an Intensity Field Decoding pipeline to reconstruct a final Computed Tomography volume. Experimental results on the LUNA16 and ToothFairy datasets demonstrate that DuFal significantly outperforms existing state-of-the-art methods in preserving high-frequency anatomical features, particularly under extremely sparse-view settings.

</details>


### [5] [DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection](https://arxiv.org/abs/2601.15453)
*Morteza Poudineh,Marc Lalonde*

Main category: cs.CV

TL;DR: 提出一种结合可学习提示和偏差评分的FNSAD方法，通过Top-K MIL和高斯偏差建模提升了异常定位与可解释性，在标准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 在少样本场景中，现有基于CLIP的提示方法区分正常与异常能力弱，且缺乏合理的patch级评分机制，难以精准定位异常。

Method: 用可学习的共享前缀提示向量和异常特定的后缀token构建文本提示；引入基于偏差的损失与Top-K MIL，将patch特征视为相对于正常分布的高斯偏差，以统计显著性评分来判定异常程度。

Result: 在MVTecAD和VISA数据集上，该方法在像素级检测上优于PromptAD和其他基线，消融实验证明可学习提示、偏差评分和Top-K MIL的有效性。

Conclusion: 该论文提出了一个偏差引导的提示学习框架，在少样本正常样本下提高异常检测的分割与定位性能，通过可学习的提示向量和偏差损失增强了正常与异常的可分性。

Abstract: Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.

</details>


### [6] [Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR NeRF from Single-Exposure Images and Events](https://arxiv.org/abs/2601.15475)
*Yunshan Qi,Lin Zhu,Nan Bao,Yifan Zhao,Jia Li*

Main category: cs.CV

TL;DR: 提出基于传感器物理的NeRF方法，利用事件与单次曝光模糊LDR图像，通过像素级RGB映射域和事件映射域对齐物理辐射与传感器输出，联合优化以生成清晰HDR新视角。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略相机输出与物理世界辐射的不匹配，导致在极端光照下HDR和去模糊性能受限；通过传感器物理建模可以更准确地利用事件和LDR数据恢复真实HDR场景。

Method: 使用NeRF表示场景HDR辐射，显式模拟传感器像素接受的HDR光线；引入像素级RGB映射场将渲染HDR对齐为输入LDR图像；设计事件映射场将物理场景动态映射为事件输出；三者联合训练以利用事件的时空信息进行去模糊与HDR恢复。

Result: The paper proposes a sensor-physics grounded NeRF framework that fuses events and single-exposure blurry LDR images to recover sharp HDR novel views. It introduces two mapping fields (pixel-wise RGB and event mapping) to align rendered HDR radiance with sensor LDR outputs and event sensor signals, jointly optimized with NeRF. Experiments show state-of-the-art performance.

Conclusion: 基于传感器物理建模并引入像素级RGB与事件映射域，有效桥接相机输出与实际场景辐照率，显著提升单曝模糊LDR+事件条件下的去模糊与HDR新视图合成质量。

Abstract: Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.

</details>


### [7] [Hybrid Vision Transformer_GAN Attribute Neutralizer for Mitigating Bias in Chest X_Ray Diagnosis](https://arxiv.org/abs/2601.15490)
*Jobeal Solomon,Ali Mohammed Mansoor Alsahag,Seyed Sahand Mohammadi Ziabari*

Main category: cs.CV

TL;DR: 用DeiT-S作为中和器编码器的ViT在减少性别属性泄露方面优于U-Net，同时保持疾病预测性能接近基线，提供了实现更公平胸片AI的可行途径。


<details>
  <summary>Details</summary>
Motivation: 像性别和年龄这样的属性会被胸片分类器当作捷径，导致对少数群体的系统性漏诊；之前基于像素空间的中和方法（U-Net编码器）在临床可用的编辑强度下未能完全消除属性泄露，故探索更擅长捕捉全局特征的ViT作为替代。

Method: 在ChestX-ray14数据集上训练了一个基于DeiT-S的属性中和器（Attribute-Neutral Framework），在11个编辑强度级别生成编辑影像；使用独立的AI评判器评估属性泄露（性别识别AUC）并用ConvNet评估15种影像所见的疾病预测（ROC AUC、最差子群AUC）。

Result: 在中等编辑强度（alpha=0.5）下，ViT中和器将患者性别识别AUC降低到约0.80，较原始U-Net约低10个百分点，同时15项所见的宏ROC AUC在未编辑基线附近（在5个百分点以内），最差子群AUC维持在约0.70，表明属性泄露显著减少且诊断效用基本保留。

Conclusion: 本文表明将U-Net卷积编码器替换为Vision Transformer（ViT）可在保留诊断性能的同时进一步减少性别等人口属性泄露，从而改善胸片AI的公平性。

Abstract: Bias in chest X-ray classifiers frequently stems from sex- and age-related shortcuts, leading to systematic underdiagnosis of minority subgroups. Previous pixel-space attribute neutralizers, which rely on convolutional encoders, lessen but do not fully remove this attribute leakage at clinically usable edit strengths. This study evaluates whether substituting the U-Net convolutional encoder with a Vision Transformer backbone in the Attribute-Neutral Framework can reduce demographic attribute leakage while preserving diagnostic accuracy. A data-efficient Image Transformer Small (DeiT-S) neutralizer was trained on the ChestX-ray14 dataset. Its edited images, generated across eleven edit-intensity levels, were evaluated with an independent AI judge for attribute leakage and with a convolutional neural network (ConvNet) for disease prediction. At a moderate edit level (alpha = 0.5), the Vision Transformer (ViT) neutralizer reduces patient sex-recognition area under the curve (AUC) to approximately 0.80, about 10 percentage points below the original framework's convolutional U-Net encoder, despite being trained for only half as many epochs. Meanwhile, macro receiver operating characteristic area under the curve (ROC AUC) across 15 findings stays within five percentage points of the unedited baseline, and the worst-case subgroup AUC remains near 0.70. These results indicate that global self-attention vision models can further suppress attribute leakage without sacrificing clinical utility, suggesting a practical route toward fairer chest X-ray AI.

</details>


### [8] [Controllable Layered Image Generation for Real-World Editing](https://arxiv.org/abs/2601.15507)
*Jinrui Yang,Qing Liu,Yijun Li,Mengwei Ren,Letian Zhang,Zhe Lin,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出LASAGNA：联合生成背景与高质量透明前景（含阴影、反射等真实视觉效果）的分层图像生成框架，并发布LASAGNA-48K数据集与LASAGNABENCH基准，用于提升可控编辑与分层一致性。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成和层表示方法在可控编辑、层间合成一致性及前景真实效果（阴影/反射）方面表现不足，需一种能共同生成图像及其组成层以支持高质量、可控的分层编辑。

Method: 构建统一生成框架，输入可为文本、前景、背景、位置mask等多种条件，训练数据包含物理合理的RGBA前景与干净背景，模型学习正确的图像合成关系以产生带真实视觉效果的前景并与背景一致地合成。

Result: Propose LASAGNA, a unified framework to generate photorealistic image with layers (background and RGBA foreground) enabling controllable editing and realistic visual effects; introduces dataset LASAGNA-48K and benchmark LASAGNABENCH; supports diverse conditioning inputs and better compositing consistency.

Conclusion: LASAGNA在同时生成多个层面上表现出高度一致性与连贯性，能保留身份和真实视觉效果，适用于多种后期编辑任务。数据集与基准将公开发布以促进行业研究。

Abstract: Recent image generation models have shown impressive progress, yet they often struggle to yield controllable and consistent results when users attempt to edit specific elements within an existing image. Layered representations enable flexible, user-driven content creation, but existing approaches often fail to produce layers with coherent compositing relationships, and their object layers typically lack realistic visual effects such as shadows and reflections. To overcome these limitations, we propose LASAGNA, a novel, unified framework that generates an image jointly with its composing layers--a photorealistic background and a high-quality transparent foreground with compelling visual effects. Unlike prior work, LASAGNA efficiently learns correct image composition from a wide range of conditioning inputs--text prompts, foreground, background, and location masks--offering greater controllability for real-world applications. To enable this, we introduce LASAGNA-48K, a new dataset composed of clean backgrounds and RGBA foregrounds with physically grounded visual effects. We also propose LASAGNABENCH, the first benchmark for layer editing. We demonstrate that LASAGNA excels in generating highly consistent and coherent results across multiple image layers simultaneously, enabling diverse post-editing applications that accurately preserve identity and visual effects. LASAGNA-48K and LASAGNABENCH will be publicly released to foster open research in the community. The project page is https://rayjryang.github.io/LASAGNA-Page/.

</details>


### [9] [DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views](https://arxiv.org/abs/2601.15516)
*William Huang,Siyou Pei,Leyi Zou,Eric J. Gonzalez,Ishan Chatterjee,Yang Zhang*

Main category: cs.CV

TL;DR: Leverage dorsal skin deformation via dual-stream delta encoder contrasting dynamic vs relaxed features; improves occluded finger pose estimation and enables interactions like isometric click with smaller models


<details>
  <summary>Details</summary>
Motivation: Egocentric hand pose estimation suffers from finger occlusions; dorsal skin deformation contains useful cues unlocked by dense visual featurizers

Method: Dual-stream delta encoder on dorsal hand images

Result: Using only cropped dorsal images, method reduces MPJAE by 18% in self-occluded scenarios versus SOTA that use whole-hand geometry and large backbones

Conclusion: Dorsal deformation features and contrastive delta encoding provide robust, compact solution for occluded egocentric hand pose estimation, improving downstream interaction reliability and enabling new paradigms

Abstract: The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >=50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface "click" without visible movement while minimizing model size.

</details>


### [10] [VIOLA: Towards Video In-Context Learning with Minimal Annotations](https://arxiv.org/abs/2601.15549)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: 提出VIOLA：在极少标注下，通过密度-不确定性加权采样选择代表性样本，并用置信感知检索与提示在混合池中区分真实标签和伪标签，实现对视频MLLM的高效域适配。


<details>
  <summary>Details</summary>
Motivation: 在专业视频领域标注昂贵且样本稀缺时，如何在不大规模标注的前提下让MLLM通过ICL适应新域，最大化少量专家标注价值并安全利用大量未标注数据。

Method: 1) 使用密度估计与不确定性结合加权采样，从未标注视频中选出既代表性又信息量大的样本供专家标注。2) 构建混合池（真实标签+伪标签），引入置信感知检索：基于相似度与置信度复合得分检索示例；置信感知提示：在上下文中明确区分高置信和低置信示例以避免噪声传播。3) 在多任务视频问答等任务上进行评估。

Result: VIOLA framework for label-efficient video ICL combining density-uncertainty-weighted sampling and confidence-aware retrieval/prompting to use minimal expert labels plus unlabeled data.

Conclusion: VIOLA在九个基准和四个MLLM上显著优于多种低资源基线，证明密度-不确定性采样与置信感知机制能在极少注释下实现稳健适配。

Abstract: Generalizing Multimodal Large Language Models (MLLMs) to novel video domains is essential for real-world deployment but remains challenging due to the scarcity of labeled data. While In-Context Learning (ICL) offers a training-free adaptation path, standard methods rely on large annotated pools, which are often impractical in specialized environments like industrial or surgical settings since they require the experts' annotations. To bridge this gap, we introduce VIOLA (Video In-cOntext Learning with minimal Annotation), a label-efficient framework that synergizes minimal expert supervision with abundant unlabeled data. First, to maximize the efficiency of a strict annotation budget, we propose density-uncertainty-weighted sampling. Unlike standard diversity or uncertainty strategies that risk selecting visual outliers, our method leverages density estimation to identify samples that are simultaneously diverse, representative, and informative. Second, to utilize the remaining unlabeled data without noise propagation, we construct a hybrid pool and introduce confidence-aware retrieval and confidence-aware prompting. These mechanisms explicitly model label reliability, retrieving demonstrations based on a composite score of similarity and confidence while enabling the MLLM to adaptively distinguish between verified ground truths and noisy pseudo-labels. Extensive experiments across nine diverse benchmarks using four MLLMs demonstrate that our framework significantly outperforms various baselines in low-resource settings, achieving robust adaptation with minimal annotation costs.

</details>


### [11] [Relative Classification Accuracy: A Calibrated Metric for Identity Consistency in Fine-Grained K-pop Face Generation](https://arxiv.org/abs/2601.15560)
*Sylvey Lin,Eranki Vasistha*

Main category: cs.CV

TL;DR: 针对K-pop偶像32x32人脸生成，作者提出Relative Classification Accuracy (RCA)指标校准生成器性能，发现高FID并不等同于语义一致性，模型在身份可控性上严重崩溃（RCA=0.27）。


<details>
  <summary>Details</summary>
Motivation: 标准的生成评估指标（FID/IS）无法检测单域细粒度身份错配，尤其在K-pop偶像这样高类内相似性的场景中，需更精细的语义可控性评估。

Method: 使用Class-Conditional DDPM在K-pop偶像32x32数据上训练；提出RCA作为将生成器分类准确率相对于“oracle”分类器基线归一化的度量；通过混淆矩阵分析识别语义崩溃的类别与属性相关性。

Result: 该论文探讨了在K-pop偶像小尺寸人脸（32x32）生成任务中，条件扩散模型（Class-Conditional DDPMs）在语义可控性方面的表现，提出了新的校准指标，并分析了模型出现语义模态崩溃的原因。

Conclusion: 高视觉质量并不能保证语义可控性；在高相似度单域任务中需用RCA等校准指标评估生成模型，并关注分辨率与性别内相似性带来的失败模式。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable success in high-fidelity image generation. However, evaluating their semantic controllability-specifically for fine-grained, single-domain tasks-remains challenging. Standard metrics like FID and Inception Score (IS) often fail to detect identity misalignment in such specialized contexts. In this work, we investigate Class-Conditional DDPMs for K-pop idol face generation (32x32), a domain characterized by high inter-class similarity. We propose a calibrated metric, Relative Classification Accuracy (RCA), which normalizes generative performance against an oracle classifier's baseline. Our evaluation reveals a critical trade-off: while the model achieves high visual quality (FID 8.93), it suffers from severe semantic mode collapse (RCA 0.27), particularly for visually ambiguous identities. We analyze these failure modes through confusion matrices and attribute them to resolution constraints and intra-gender ambiguity. Our framework provides a rigorous standard for verifying identity consistency in conditional generative models.

</details>


### [12] [Region-aware Spatiotemporal Modeling with Collaborative Domain Generalization for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2601.15615)
*Weiwei Wu,Yueyang Li,Yuhu Shi,Weiming Zeng,Lang Qin,Yang Yang,Ke Zhou,Zhiguo Zhang,Wai Ting Siok,Nizhuan Wang*

Main category: cs.CV

TL;DR: 提出RSM-CoDG：基于功能区的区域级时空建模+协同域泛化，有效提升跨被试EEG情感识别泛化性能并在SEED上取得领先结果。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情感识别面临被试间变异大和情感神经表征在时空上复杂多尺度演化的挑战，现有方法常在空间、时间或泛化策略上单一改进，难以同时对齐表示并抑制被试偏差。

Method: 构建基于功能脑区的区域级空间表示，采用多尺度时间卷积或注意力模块捕捉情感相关的动态演变，并通过多维约束（协同域泛化）在训练阶段减弱被试特异性偏差以应对完全未知目标被试。

Result: 在SEED系列数据集上的大量实验显示RSM-CoDG优于现有对比方法，在稳健性和泛化能力上有显著提升。

Conclusion: RSM-CoDG通过融合功能区分割的区域级空间建模、多尺度时间建模与协同域泛化策略，有效减小被试间分布差异并提升跨被试情感识别的泛化性能。

Abstract: Cross-subject EEG-based emotion recognition (EER) remains challenging due to strong inter-subject variability, which induces substantial distribution shifts in EEG signals, as well as the high complexity of emotion-related neural representations in both spatial organization and temporal evolution. Existing approaches typically improve spatial modeling, temporal modeling, or generalization strategies in isolation, which limits their ability to align representations across subjects while capturing multi-scale dynamics and suppressing subject-specific bias within a unified framework. To address these gaps, we propose a Region-aware Spatiotemporal Modeling framework with Collaborative Domain Generalization (RSM-CoDG) for cross-subject EEG emotion recognition. RSM-CoDG incorporates neuroscience priors derived from functional brain region partitioning to construct region-level spatial representations, thereby improving cross-subject comparability. It also employs multi-scale temporal modeling to characterize the dynamic evolution of emotion-evoked neural activity. In addition, the framework employs a collaborative domain generalization strategy, incorporating multidimensional constraints to reduce subject-specific bias in a fully unseen target subject setting, which enhances the generalization to unknown individuals. Extensive experimental results on SEED series datasets demonstrate that RSM-CoDG consistently outperforms existing competing methods, providing an effective approach for improving robustness. The source code is available at https://github.com/RyanLi-X/RSM-CoDG.

</details>


### [13] [Explainable Deepfake Detection with RL Enhanced Self-Blended Images](https://arxiv.org/abs/2601.15624)
*Ning Jiang,Dingheng Zeng,Yanhong Liu,Haiyang Yi,Shijie Yu,Minghe Weng,Haifeng Shen,Ying Li*

Main category: cs.CV

TL;DR: 通过自融图像自动生成CoT文本并结合RL优化，本文在可解释深伪检测上实现了低注释成本且具竞争力的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测缺乏可解释输出且高质量的伪造归因文本标注昂贵难得。随着MLLM的兴起，希望在减少人工注释成本的前提下，使其应用于解释性深伪检测，并探索RL在提升视觉任务跨域泛化中的潜力。

Method: 首先利用Self-Blended Images自动生成具有伪造属性的图像及对应的CoT文本描述，构建大规模带有归因信息的训练数据；然后在此数据上微调多模态大语言模型(MLLM)以实现可解释检测；引入RL策略（定制奖励机制与反馈驱动的合成数据生成）对检测器进行迭代优化以提高跨数据集鲁棒性。

Result: 大量实验证明所提CoT数据构建流水线、定制奖励机制与反馈驱动合成数据生成有效，所提出方法在多个跨数据集基准上达到与SOTA相近的性能。

Conclusion: 本文提出基于Self-Blended Images的自动化Chain-of-Thought(CoT)数据生成框架，并结合强化学习(RL)增强的深度伪造检测方法，旨在降低文本注释成本并提升跨域泛化与可解释性。

Abstract: Most prior deepfake detection methods lack explainable outputs. With the growing interest in multimodal large language models (MLLMs), researchers have started exploring their use in interpretable deepfake detection. However, a major obstacle in applying MLLMs to this task is the scarcity of high-quality datasets with detailed forgery attribution annotations, as textual annotation is both costly and challenging - particularly for high-fidelity forged images or videos. Moreover, multiple studies have shown that reinforcement learning (RL) can substantially enhance performance in visual tasks, especially in improving cross-domain generalization. To facilitate the adoption of mainstream MLLM frameworks in deepfake detection with reduced annotation cost, and to investigate the potential of RL in this context, we propose an automated Chain-of-Thought (CoT) data generation framework based on Self-Blended Images, along with an RL-enhanced deepfake detection framework. Extensive experiments validate the effectiveness of our CoT data construction pipeline, tailored reward mechanism, and feedback-driven synthetic data generation approach. Our method achieves performance competitive with state-of-the-art (SOTA) approaches across multiple cross-dataset benchmarks. Implementation details are available at https://github.com/deon1219/rlsbi.

</details>


### [14] [Evolving Without Ending: Unifying Multimodal Incremental Learning for Continual Panoptic Perception](https://arxiv.org/abs/2601.15643)
*Bo Yuan,Danpei Zhao,Wentao Li,Tian Li,Zhiguo Jiang*

Main category: cs.CV

TL;DR: 将持续学习扩展到多模态多任务的全景感知，设计了CCE、知识继承（对比特征+实例蒸馏）、跨模态一致性（CPP+）和非对称伪标签，无需样本回放，在细粒度任务上效果好。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习多聚焦于单任务，难以应对多模态多任务场景，除灾难性遗忘外还存在跨模态语义混淆，亟需一种能在像素、实例与图像级别联合解释下保持对旧知识的对齐与继承的方法。

Method: 构建一个端到端的CPP模型：1) 协作跨模态编码器(CCE)用于融合视觉/其他模态嵌入；2) 可塑知识继承模块，采用对比特征蒸馏和实例级蒸馏保持旧任务知识；3) 跨模态一致性损失（CPP+）用于对齐语义；4) 非对称伪标签策略替代样本回放实现无监督增量更新。训练结合多任务损失与蒸馏损失。

Result: 提出了Continual Panoptic Perception (CPP)模型，扩展了持续学习（CL）到多模态多任务的全景感知，包含协作跨模态编码器（CCE）、可塑知识继承模块（通过对比特征蒸馏和实例蒸馏）以及跨模态一致性约束和CPP+，并使用非对称伪标签在不回放范例的情况下更新模型。实验显示在多模态数据集和细粒度CL任务上表现优越。

Conclusion: 通过协作跨模态编码、对比特征与实例蒸馏的知识继承以及跨模态一致性约束，CPP/CPP+能缓解多任务多模态持续学习中的语义混淆和灾难性遗忘，支持无回放增量更新并在实验中取得显著提升。

Abstract: Continual learning (CL) is a great endeavour in developing intelligent perception AI systems. However, the pioneer research has predominantly focus on single-task CL, which restricts the potential in multi-task and multimodal scenarios. Beyond the well-known issue of catastrophic forgetting, the multi-task CL also brings semantic obfuscation across multimodal alignment, leading to severe model degradation during incremental training steps. In this paper, we extend CL to continual panoptic perception (CPP), integrating multimodal and multi-task CL to enhance comprehensive image perception through pixel-level, instance-level, and image-level joint interpretation. We formalize the CL task in multimodal scenarios and propose an end-to-end continual panoptic perception model. Concretely, CPP model features a collaborative cross-modal encoder (CCE) for multimodal embedding. We also propose a malleable knowledge inheritance module via contrastive feature distillation and instance distillation, addressing catastrophic forgetting from task-interactive boosting manner. Furthermore, we propose a cross-modal consistency constraint and develop CPP+, ensuring multimodal semantic alignment for model updating under multi-task incremental scenarios. Additionally, our proposed model incorporates an asymmetric pseudo-labeling manner, enabling model evolving without exemplar replay. Extensive experiments on multimodal datasets and diverse CL tasks demonstrate the superiority of the proposed model, particularly in fine-grained CL tasks.

</details>


### [15] [SuperOcc: Toward Cohesive Temporal Modeling for Superquadric-based Occupancy Prediction](https://arxiv.org/abs/2601.15644)
*Zichen Yu,Quanli Liu,Wei Wang,Liyong Zhang,Xiaoguang Zhao*

Main category: cs.CV

TL;DR: SuperOcc: sparse superquadric-based 3D occupancy prediction with cohesive temporal modeling, multi-superquadric decoding, and efficient splatting, achieving state-of-the-art accuracy and efficiency


<details>
  <summary>Details</summary>
Motivation: Dense representations ignore scene sparsity; superquadrics offer sparse, expressive geometry but current methods lack temporal modeling, expressive-sparsity trade-off, and efficient splatting

Method: Paper proposes SuperOcc for 3D occupancy using superquadrics

Result: SuperOcc with temporal modeling, multi-superquadric decoding, and efficient splatting achieves SOTA on SurroundOcc and Occ3D with better efficiency

Conclusion: SuperOcc overcomes prior limitations of superquadric frameworks, balancing sparsity and expressiveness and improving efficiency, validated on benchmarks

Abstract: 3D occupancy prediction plays a pivotal role in the realm of autonomous driving, as it provides a comprehensive understanding of the driving environment. Most existing methods construct dense scene representations for occupancy prediction, overlooking the inherent sparsity of real-world driving scenes. Recently, 3D superquadric representation has emerged as a promising sparse alternative to dense scene representations due to the strong geometric expressiveness of superquadrics. However, existing superquadric frameworks still suffer from insufficient temporal modeling, a challenging trade-off between query sparsity and geometric expressiveness, and inefficient superquadric-to-voxel splatting. To address these issues, we propose SuperOcc, a novel framework for superquadric-based 3D occupancy prediction. SuperOcc incorporates three key designs: (1) a cohesive temporal modeling mechanism to simultaneously exploit view-centric and object-centric temporal cues; (2) a multi-superquadric decoding strategy to enhance geometric expressiveness without sacrificing query sparsity; and (3) an efficient superquadric-to-voxel splatting scheme to improve computational efficiency. Extensive experiments on the SurroundOcc and Occ3D benchmarks demonstrate that SuperOcc achieves state-of-the-art performance while maintaining superior efficiency. The code is available at https://github.com/Yzichen/SuperOcc.

</details>


### [16] [Event-VStream: Event-Driven Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2601.15655)
*Zhenghui Guo,Yuanbin Man,Junyuan Sheng,Bowen Lin,Ahmed Ahmed,Bo Jiang,Boyuan Zhang,Miao Yin,Sian Jin,Omprakash Gnawal,Chengming Zhang*

Main category: cs.CV

TL;DR: Event-VStream通过事件边界检测与事件级持久记忆，减少冗余解码并保留关键信息，实现低延迟且具有长时记忆的实时长视频理解，显著提升各类基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有流式系统在固定间隔解码或缓存修剪下产生重复输出或丢失重要时间信息的问题，提升多模态大模型对长视频流的实时理解能力。

Method: 提出事件感知框架：融合运动、语义与预测线索进行事件边界检测，仅在边界触发语言生成；将每个事件的嵌入写入持久化记忆库以支持长时推理。

Result: 在OVOBench-Realtime和长时Ego4D评测上表现优异：比VideoLLM-Online-8B提高+10.4分；在只用通用LLaMA-3-8B文本骨干情况下接近Flash-VStream-7B；在2小时Ego4D流上保持约70%的GPT-5胜率。

Conclusion: Event-VStream通过事件触发生成和持久记忆库解决了长视频流实时理解中的冗余处理与快速遗忘问题，能够在保持低延迟下实现长时推理。

Abstract: Real-time understanding of long video streams remains challenging for multimodal large language models (VLMs) due to redundant frame processing and rapid forgetting of past context. Existing streaming systems rely on fixed-interval decoding or cache pruning, which either produce repetitive outputs or discard crucial temporal information. We introduce Event-VStream, an event-aware framework that represents continuous video as a sequence of discrete, semantically coherent events. Our system detects meaningful state transitions by integrating motion, semantic, and predictive cues, and triggers language generation only at those boundaries. Each event embedding is consolidated into a persistent memory bank, enabling long-horizon reasoning while maintaining low latency. Across OVOBench-Realtime, and long-form Ego4D evaluations, Event-VStream achieves competitive performance. It improves over a VideoLLM-Online-8B baseline by +10.4 points on OVOBench-Realtime, achieves performance close to Flash-VStream-7B despite using only a general-purpose LLaMA-3-8B text backbone, and maintains around 70% GPT-5 win rate on 2-hour Ego4D streams.

</details>


### [17] [Skywork UniPic 3.0: Unified Multi-Image Composition via Sequence Modeling](https://arxiv.org/abs/2601.15664)
*Hongyang Wei,Hongbo Liu,Zidong Wang,Yi Peng,Baixin Xu,Size Wu,Xuying Zhang,Xianglong He,Zexiang Liu,Peiyu Wang,Xuchen Song,Yangguang Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: 提出统一的Skywork UniPic 3.0，专注HOI多图合成，700K高质量数据+序列建模+推理加速，在质量与速度上均领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 社区对多图合成兴趣激增，尤其关注Human-Object Interaction（HOI），但现有方法在高质量融合细节上缺乏公开说明，促使作者系统实现并优化HOI为中心的多图合成方案。

Method: 构建统一多模态框架，支持1~6张输入、任意分辨率（受1024x1024像素预算限制），并提出将多图合成视为序列建模的训练范式；引入轨迹映射与分布匹配在后训练阶段用于加速推理。

Result: 使用700K高质量训练样本并结合数据收集/过滤/合成流水线，模型在单图编辑基准上达到SOTA，并在多图合成基准上超越Nano-Banana和Seedream 4.0；推理仅需8步，较标准采样加速12.5x。

Conclusion: Skywork UniPic 3.0在单图编辑和多图合成（尤其是HOI任务）上均取得了先进性能，数据处理和训练范式是成功关键。

Abstract: The recent surge in popularity of Nano-Banana and Seedream 4.0 underscores the community's strong interest in multi-image composition tasks. Compared to single-image editing, multi-image composition presents significantly greater challenges in terms of consistency and quality, yet existing models have not disclosed specific methodological details for achieving high-quality fusion. Through statistical analysis, we identify Human-Object Interaction (HOI) as the most sought-after category by the community. We therefore systematically analyze and implement a state-of-the-art solution for multi-image composition with a primary focus on HOI-centric tasks. We present Skywork UniPic 3.0, a unified multimodal framework that integrates single-image editing and multi-image composition. Our model supports an arbitrary (1~6) number and resolution of input images, as well as arbitrary output resolutions (within a total pixel budget of 1024x1024). To address the challenges of multi-image composition, we design a comprehensive data collection, filtering, and synthesis pipeline, achieving strong performance with only 700K high-quality training samples. Furthermore, we introduce a novel training paradigm that formulates multi-image composition as a sequence-modeling problem, transforming conditional generation into unified sequence synthesis. To accelerate inference, we integrate trajectory mapping and distribution matching into the post-training stage, enabling the model to produce high-fidelity samples in just 8 steps and achieve a 12.5x speedup over standard synthesis sampling. Skywork UniPic 3.0 achieves state-of-the-art performance on single-image editing benchmark and surpasses both Nano-Banana and Seedream 4.0 on multi-image composition benchmark, thereby validating the effectiveness of our data pipeline and training paradigm. Code, models and dataset are publicly available.

</details>


### [18] [Consistency-Regularized GAN for Few-Shot SAR Target Recognition](https://arxiv.org/abs/2601.15681)
*Yikui Zhai,Shikuang Liu,Wenlve Zhou,Hongsheng Zhang,Zhiheng Zhou,Xiaolin Tian,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 为了解决少样本下GAN训练不稳定问题，Cr-GAN通过双分支判别器、通道特征插值与双域循环一致性，在极少数据下合成高质量多样样本，从而显著提升SSL+微调的少样本识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAR图像少样本识别中数据极度稀缺导致GAN无法稳定训练的问题，进而通过合成数据提升自监督预训练效果和下游少样本微调性能。

Method: 设计双分支判别器把对抗损失与表征学习分离；在生成器或判别器特征空间做通道级插值以合成新潜特征；引入像素域与特征域的双域循环一致性损失确保语义保持；将生成的数据用于SSL预训练后微调少量有标签样本。

Result: 提出Consistency-regularized GAN (Cr-GAN)，采用双分支判别器解耦对抗训练与表征学习，通道级特征插值生成新潜特征，双域循环一致性维持语义完整。Cr-GAN能在极少数据下生成多样高保真样本，提升多种SSL算法，在MSTAR和SRSDD 8-shot设置下分别达71.21%和51.64%，参数量约为扩散模型的1/5。

Conclusion: Cr-GAN在少样本SAR任务中能稳定生成高质量样本，显著提升自监督预训练和下游识别效果，同时参数开销小，可广泛适配多种GAN结构。

Abstract: Few-shot recognition in synthetic aperture radar (SAR) imagery remains a critical bottleneck for real-world applications due to extreme data scarcity. A promising strategy involves synthesizing a large dataset with a generative adversarial network (GAN), pre-training a model via self-supervised learning (SSL), and then fine-tuning on the few labeled samples. However, this approach faces a fundamental paradox: conventional GANs themselves require abundant data for stable training, contradicting the premise of few-shot learning. To resolve this, we propose the consistency-regularized generative adversarial network (Cr-GAN), a novel framework designed to synthesize diverse, high-fidelity samples even when trained under these severe data limitations. Cr-GAN introduces a dual-branch discriminator that decouples adversarial training from representation learning. This architecture enables a channel-wise feature interpolation strategy to create novel latent features, complemented by a dual-domain cycle consistency mechanism that ensures semantic integrity. Our Cr-GAN framework is adaptable to various GAN architectures, and its synthesized data effectively boosts multiple SSL algorithms. Extensive experiments on the MSTAR and SRSDD datasets validate our approach, with Cr-GAN achieving a highly competitive accuracy of 71.21% and 51.64%, respectively, in the 8-shot setting, significantly outperforming leading baselines, while requiring only ~5 of the parameters of state-of-the-art diffusion models. Code is available at: https://github.com/yikuizhai/Cr-GAN.

</details>


### [19] [Performance-guided Reinforced Active Learning for Object Detection](https://arxiv.org/abs/2601.15688)
*Zhixuan Liang,Xingyu Zeng,Rui Zhao,Ping Luo*

Main category: cs.CV

TL;DR: 提出将mAP作为奖励的强化学习主动检测方法，通过期望输出变化度量信息量并用无监督查表近似mAP，显著提升了在VOC/COCO上的AL性能。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习评估信息量多基于数据分布或内在信息，与下游检测性能（如mAP）未直接关联，导致选择样本不一定有助于提高最终目标性能。作者希望通过直接将mAP作为优化目标，弥合信息量评估与检测性能之间的差距。

Method: 提出了基于强化学习的采样智能体：以期望模型输出变化表征信息量，采用策略梯度并以mAP提升作为奖励来优化批次选择；为应对mAP估计的成本，设计了基于无监督快速查表的mAP近似方法以降低计算开销。

Result: 在PASCAL VOC与COCO基准数据集上，MGRAL在主动学习曲线上取得了最高表现，并用可视化结果证明了方法的有效性，展示出在有限标注预算下提高检测mAP的能力。

Conclusion: MGRAL通过将mAP提升作为奖励，用强化学习策略梯度优化批次选择，有效缓解了传统AL方法与最终检测性能脱节的问题，并在PASCAL VOC和COCO上展示了优越的AL曲线，实现了基于性能引导的主动检测新范式。

Abstract: Active learning (AL) strategies aim to train high-performance models with minimal labeling efforts, only selecting the most informative instances for annotation. Current approaches to evaluating data informativeness predominantly focus on the data's distribution or intrinsic information content and do not directly correlate with downstream task performance, such as mean average precision (mAP) in object detection. Thus, we propose Performance-guided (i.e. mAP-guided) Reinforced Active Learning for Object Detection (MGRAL), a novel approach that leverages the concept of expected model output changes as informativeness. To address the combinatorial explosion challenge of batch sample selection and the non-differentiable correlation between model performance and selected batches, MGRAL skillfully employs a reinforcement learning-based sampling agent that optimizes selection using policy gradient with mAP improvement as reward. Moreover, to reduce the computational overhead of mAP estimation with unlabeled samples, MGRAL utilizes an unsupervised way with fast look-up tables, ensuring feasible deployment. We evaluate MGRAL's active learning performance on detection tasks over PASCAL VOC and COCO benchmarks. Our approach demonstrates the highest AL curve with convincing visualizations, establishing a new paradigm in reinforcement learning-driven active object detection.

</details>


### [20] [Beyond Visual Safety: Jailbreaking Multimodal Large Language Models for Harmful Image Generation via Semantic-Agnostic Inputs](https://arxiv.org/abs/2601.15698)
*Mingyu Yu,Lana Liu,Zhehao Zhao,Wei Wang,Sujuan Qin*

Main category: cs.CV

TL;DR: BVS通过“重建-再生成”策略，使用中性化的视觉拼接和归纳重组，将恶意意图与原始输入解耦，从而诱导模型生成有害图像，对GPT-5（2026-01-12）实现98.21%越狱成功率。


<details>
  <summary>Details</summary>
Motivation: 弥补对MLLM视觉安全边界研究的不足，识别并量化视觉安全对齐的脆弱性。

Method: 采用“重建-再生成”策略，先中性化图像拼接以隐藏恶意提示，再通过归纳重组诱导模型按恶意意图生成图像。

Result: BVS提出了一种用于探究多模态大模型视觉安全边界的图文配对越狱框架。

Conclusion: 研究表明当前MLLM在视觉安全对齐存在重大漏洞，BVS可高效突破这些防护。

Abstract: The rapid advancement of Multimodal Large Language Models (MLLMs) has introduced complex security challenges, particularly at the intersection of textual and visual safety. While existing schemes have explored the security vulnerabilities of MLLMs, the investigation into their visual safety boundaries remains insufficient. In this paper, we propose Beyond Visual Safety (BVS), a novel image-text pair jailbreaking framework specifically designed to probe the visual safety boundaries of MLLMs. BVS employs a "reconstruction-then-generation" strategy, leveraging neutralized visual splicing and inductive recomposition to decouple malicious intent from raw inputs, thereby leading MLLMs to be induced into generating harmful images. Experimental results demonstrate that BVS achieves a remarkable jailbreak success rate of 98.21\% against GPT-5 (12 January 2026 release). Our findings expose critical vulnerabilities in the visual safety alignment of current MLLMs.

</details>


### [21] [Enhanced LULC Segmentation via Lightweight Model Refinements on ALOS-2 SAR Data](https://arxiv.org/abs/2601.15705)
*Ali Caglayan,Nevrez Imamoglu,Toru Kouyama*

Main category: cs.CV

TL;DR: 在SAR自监督预训练基础上，作者提出三项轻量改进：高分辨率特征注入、多尺度进阶上采样精化头、以及用于平衡长尾重权的α缩放因子，显著改善日本范围内ALOS-2 LULC与水体检测，尤其提升少数类表现。


<details>
  <summary>Details</summary>
Motivation: 解决SAR密集预测在边界过度平滑、细长结构遗漏和长尾类别退化三大常见失败模式，同时保持流水线简单性。

Method: 基于SAR-W-MixMAE自监督预训练，采用轻量改动：注入高分辨率特征到解码器、多阶段交替卷积与逐步上采样的refine-up头，以及在focal+dice损失中加入α比例调节类重权。

Result: Paper proposes lightweight refinements to SAR semantic segmentation using ALOS-2 HH data for Japan-wide LULC and water detection, improving boundaries, thin structures, and rare classes without added complexity.

Conclusion: 引入高分辨率特征、多步精化上采样和α缩放平衡后，模型在日本ALOS-2 LULC基准上整体性能提升，少数类和边界细节恢复最明显，水体检测指标亦有改进。

Abstract: This work focuses on national-scale land-use/land-cover (LULC) semantic segmentation using ALOS-2 single-polarization (HH) SAR data over Japan, together with a companion binary water detection task. Building on SAR-W-MixMAE self-supervised pretraining [1], we address common SAR dense-prediction failure modes, boundary over-smoothing, missed thin/slender structures, and rare-class degradation under long-tailed labels, without increasing pipeline complexity. We introduce three lightweight refinements: (i) injecting high-resolution features into multi-scale decoding, (ii) a progressive refine-up head that alternates convolutional refinement and stepwise upsampling, and (iii) an $α$-scale factor that tempers class reweighting within a focal+dice objective. The resulting model yields consistent improvements on the Japan-wide ALOS-2 LULC benchmark, particularly for under-represented classes, and improves water detection across standard evaluation metrics.

</details>


### [22] [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/abs/2601.15711)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 提出三层诊断性评估，发现VLMs在细粒度分类强但在判定属性是否适用（NA）上弱，且高效模型具备实际部署价值。


<details>
  <summary>Details</summary>
Motivation: 时尚零样本属性预测对目录丰富化、视觉搜索和推荐至关重要，但现有VLM在多属性且带有条件（如NA）标签的任务上缺乏系统性评估，且需要区分属性不可用与分类错误的来源。

Method: 使用DeepFashion-MultiModal数据集（显式包含NA标签），在5,000张图像、18个属性上对9个VLM（覆盖旗舰、效率和超效率等级）和基于预训练Fashion-CLIP嵌入训练的逻辑回归分类器进行零样本与有监督基线比较。评估分为三层：整体（含NA）、属性适用性检测，以及在可判断情况下的细粒度分类。

Result: 零样本VLM在宏观F1上达64.0%，较基线提升近三倍；在Tier3细粒度分类F1为70.8%，而属性适用性（NA）检测仅34.1% NA-F1，成为性能瓶颈；高效模型能达到旗舰模型90%以上性能。

Conclusion: 本文提出并验证了一个用于时尚多属性预测的三层评估框架，揭示了视觉-语言模型（VLM）在可细化分类方面表现优异但在属性适用性检测上存在明显短板，指出高效模型能以低成本接近旗舰模型性能。

Abstract: Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, "outer fabric" is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn't exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.

</details>


### [23] [VideoThinker: Building Agentic VideoLLMs with LLM-Guided Tool Reasoning](https://arxiv.org/abs/2601.15724)
*Chenglin Li,Qianglong Chen,Feng Han,Yikun Wang,Xingxi Yin,Yan Gong,Ruilin Li,Yin Zhang,Jiaqi Wang*

Main category: cs.CV

TL;DR: Convert videos to captions, synthesize multi-step tool-use trajectories with agentic LLM, map back to frames to create training data, train VideoThinker on this synthetic agentic dataset to enable adaptive, tool-augmented long-form video understanding that outperforms baselines.


<details>
  <summary>Details</summary>
Motivation: Long-form video understanding is hard; static sampling causes temporal info loss; agentic tools can adaptively explore but need models with long-form comprehension, creating circular dependency.

Method: Generate rich captions for videos, use an agentic LLM to produce tool-usage sequences in caption space, replace captions with corresponding frames to create interleaved video-tool trajectories, and train a Video LLM (VideoThinker) on this synthetic dataset to learn dynamic, multi-step tool use and adaptive temporal/spatial zooming.

Result: VideoThinker, trained on synthetic tool interaction trajectories generated in caption space and grounded back to videos, achieves dynamic reasoning, adaptive temporal exploration, and multi-step tool use, outperforming caption-only and video baselines on long-video benchmarks.

Conclusion: Synthetic agentic training in caption space grounded to video effectively breaks the circular dependency and yields a Video LLM capable of superior long-form understanding via adaptive retrieval and zooming tools.

Abstract: Long-form video understanding remains a fundamental challenge for current Video Large Language Models. Most existing models rely on static reasoning over uniformly sampled frames, which weakens temporal localization and leads to substantial information loss in long videos. Agentic tools such as temporal retrieval, spatial zoom, and temporal zoom offer a natural way to overcome these limitations by enabling adaptive exploration of key moments. However, constructing agentic video understanding data requires models that already possess strong long-form video comprehension, creating a circular dependency. We address this challenge with VideoThinker, an agentic Video Large Language Model trained entirely on synthetic tool interaction trajectories. Our key idea is to convert videos into rich captions and employ a powerful agentic language model to generate multi-step tool use sequences in caption space. These trajectories are subsequently grounded back to video by replacing captions with the corresponding frames, yielding a large-scale interleaved video and tool reasoning dataset without requiring any long-form understanding from the underlying model. Training on this synthetic agentic dataset equips VideoThinker with dynamic reasoning capabilities, adaptive temporal exploration, and multi-step tool use. Remarkably, VideoThinker significantly outperforms both caption-only language model agents and strong video model baselines across long-video benchmarks, demonstrating the effectiveness of tool augmented synthetic data and adaptive retrieval and zoom reasoning for long-form video understanding.

</details>


### [24] [FAIR-ESI: Feature Adaptive Importance Refinement for Electrophysiological Source Imaging](https://arxiv.org/abs/2601.15731)
*Linyong Zou,Liang Zhang,Xiongfei Wang,Jia-Hong Gao,Yi Sun,Shurong Sheng,Kuntao Xiao,Wanli Yang,Pengfei Teng,Guoming Luan,Zhao Lv,Zikang Xu*

Main category: cs.CV

TL;DR: Provide a short TL;DR of the paper.


<details>
  <summary>Details</summary>
Motivation: Explain why feature refinement across multiple views helps ESI.

Method: Please analyze the methods in the paper and explain how FAIR-ESI works.

Result: Summarize the main findings and performance improvements claimed in the abstract.

Conclusion: Give brief conclusions and potential future work.

Abstract: An essential technique for diagnosing brain disorders is electrophysiological source imaging (ESI). While model-based optimization and deep learning methods have achieved promising results in this field, the accurate selection and refinement of features remains a central challenge for precise ESI. This paper proposes FAIR-ESI, a novel framework that adaptively refines feature importance across different views, including FFT-based spectral feature refinement, weighted temporal feature refinement, and self-attention-based patch-wise feature refinement. Extensive experiments on two simulation datasets with diverse configurations and two real-world clinical datasets validate our framework's efficacy, highlighting its potential to advance brain disorder diagnosis and offer new insights into brain function.

</details>


### [25] [Sub-Region-Aware Modality Fusion and Adaptive Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2601.15734)
*Shadi Alijani,Fereshteh Aghaee Meibodi,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 这篇论文提出了一个将基础模型（foundation models）适配到多模态医学影像的新框架，核心包括子区域感知的模态注意力和自适应提示工程，用于对肿瘤各子区域（尤其坏死核）进行模态融合与微调提示，从而提升分割性能。在BraTS2020数据集上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在多模态医学影像上难以有效融合不同成像模态的信息，并且病变组织具有高度异质性，导致分割性能有限；因此需要一种能在子区域级别进行模态自适应融合并利用提示机制挖掘基础模型潜力的方法。

Method: 方法包含两部分：1) 子区域感知模态注意力：对每个肿瘤子区域学习不同的模态加权，动态决定不同模态（如MRI序列）在该子区域的贡献；2) 自适应提示工程：设计可调整的提示（prompt）机制使基础模型利用已有能力并在细粒度分割任务上进行微调。整体架构将两者结合于基础模型适配流程，并在训练过程中优化注意力与提示参数。

Result: 在BraTS2020脑肿瘤分割上相比基线方法整体性能更好，坏死核子区域提升尤为明显（文中宣称显著优于基线），证明所提方法在多模态融合和提示策略上具有实际效果。

Conclusion: 引入子区域感知模态注意力与自适应提示工程能够更好地融合多模态信息并提升肿瘤分割精度，特别是在坏死核心子区域表现显著提升，展示了在多模态医学影像适配基础模型的有效性。

Abstract: The successful adaptation of foundation models to multi-modal medical imaging is a critical yet unresolved challenge. Existing models often struggle to effectively fuse information from multiple sources and adapt to the heterogeneous nature of pathological tissues. To address this, we introduce a novel framework for adapting foundation models to multi-modal medical imaging, featuring two key technical innovations: sub-region-aware modality attention and adaptive prompt engineering. The attention mechanism enables the model to learn the optimal combination of modalities for each tumor sub-region, while the adaptive prompting strategy leverages the inherent capabilities of foundation models to refine segmentation accuracy. We validate our framework on the BraTS 2020 brain tumor segmentation dataset, demonstrating that our approach significantly outperforms baseline methods, particularly in the challenging necrotic core sub-region. Our work provides a principled and effective approach to multi-modal fusion and prompting, paving the way for more accurate and robust foundation model-based solutions in medical imaging.

</details>


### [26] [Breaking the Resolution Barrier: Arbitrary-resolution Deep Image Steganography Framework](https://arxiv.org/abs/2601.15739)
*Xinjue Hu,Chi Wang,Boyu Wang,Xiang Zhang,Zhenshan Tan,Zhangjie Fu*

Main category: cs.CV

TL;DR: 提出ARDIS，一个能隐写任意分辨率秘密图像并恢复到原始分辨率的框架，通过频率解耦在固定封面分辨率下隐藏全局基底与高频残差，并用隐式重建器在恢复阶段结合解码的细节潜变量在连续空间重建高频细节；并引入隐式分辨率编码实现盲恢复。实验表明在隐蔽性和跨分辨率恢复保真度上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度图像隐写要求秘密图像与封面具有相同分辨率，导致分辨率不一致时必须重采样，造成细节丢失且无法在未知分辨率下恢复原图。ARDIS旨在解决跨分辨率恢复与盲恢复的问题。

Method: 1) 隐藏阶段：频率解耦架构将秘密图像分解为分辨率对齐的全局基底和与分辨率无关的高频潜变量，并将二者与隐式分辨率编码一起嵌入固定分辨率的封面图像的特征冗余空间。2) 恢复阶段：潜变量引导的隐式重建器使用解码的高频潜变量来调制连续隐式函数，在任意查询坐标上渲染高频残差并叠加到全局基底，从而在连续空间恢复原始分辨率细节。3) 隐式分辨率编码：把离散分辨率映射为稠密特征图并隐藏，以实现盲解码。

Result: 在隐蔽性（invisibility）与跨分辨率恢复保真度方面，ARDIS显著优于最先进方法，能在保持封面无明显扰动的同时恢复高保真细节。

Conclusion: ARDIS成功实现了任意分辨率的深度图像隐写与盲恢复，通过频率解耦和潜变量引导的隐式重建，有效保留高频细节并在不知道分辨率的情况下恢复原始分辨率，性能优于现有工作。

Abstract: Deep image steganography (DIS) has achieved significant results in capacity and invisibility. However, current paradigms enforce the secret image to maintain the same resolution as the cover image during hiding and revealing. This leads to two challenges: secret images with inconsistent resolutions must undergo resampling beforehand which results in detail loss during recovery, and the secret image cannot be recovered to its original resolution when the resolution value is unknown. To address these, we propose ARDIS, the first Arbitrary Resolution DIS framework, which shifts the paradigm from discrete mapping to reference-guided continuous signal reconstruction. Specifically, to minimize the detail loss caused by resolution mismatch, we first design a Frequency Decoupling Architecture in hiding stage. It disentangles the secret into a resolution-aligned global basis and a resolution-agnostic high-frequency latent to hide in a fixed-resolution cover. Second, for recovery, we propose a Latent-Guided Implicit Reconstructor to perform deterministic restoration. The recovered detail latent code modulates a continuous implicit function to accurately query and render high-frequency residuals onto the recovered global basis, ensuring faithful restoration of original details. Furthermore, to achieve blind recovery, we introduce an Implicit Resolution Coding strategy. By transforming discrete resolution values into dense feature maps and hiding them in the redundant space of the feature domain, the reconstructor can correctly decode the secret's resolution directly from the steganographic representation. Experimental results demonstrate that ARDIS significantly outperforms state-of-the-art methods in both invisibility and cross-resolution recovery fidelity.

</details>


### [27] [White-Box mHC: Electromagnetic Spectrum-Aware and Interpretable Stream Interactions for Hyperspectral Image Classification](https://arxiv.org/abs/2601.15757)
*Yimin Zhu,Lincoln Linlin Xu,Zhengsen Xu,Zack Dewis,Mabel Heffring,Saeid Taleghanidoozdoozan,Motasem Alkayid,Quinn Ledingham,Megan Greenwood*

Main category: cs.CV

TL;DR: ES-mHC 通过结构化、有向的超连接矩阵显式建模光谱组交互，实现部分白盒的 HSIC，提升可解释性并揭示有意义的空间与方向性交互模式。


<details>
  <summary>Details</summary>
Motivation: 现有深度模型在光谱-空间特征混合上不透明，难以解释内部决策机制，需引入物理光谱认知以提高可解释性。

Method: 提出 ES-mHC 框架：在 mHC 的残差流上用结构化、有向矩阵显式建模电磁谱组间交互，分离特征表示与交互结构，支持可视化和空间分析。

Result: 在 HSIC 任务中，学习到的超连接矩阵表现出连贯的空间模式与不对称交互行为；提高 expansion rate 加速结构化模式出现，表明模型机制性增强。

Conclusion: ES-mHC 将 HSIC 从黑盒转为部分白盒，通过显式建模光谱组间的交互并分离表示与交互结构，提升可解释性并保留性能。

Abstract: In hyperspectral image classification (HSIC), most deep learning models rely on opaque spectral-spatial feature mixing, limiting their interpretability and hindering understanding of internal decision mechanisms. We present physical spectrum-aware white-box mHC, named ES-mHC, a hyper-connection framework that explicitly models interactions among different electromagnetic spectrum groupings (residual stream in mHC) interactions using structured, directional matrices. By separating feature representation from interaction structure, ES-mHC promotes electromagnetic spectrum grouping specialization, reduces redundancy, and exposes internal information flow that can be directly visualized and spatially analyzed. Using hyperspectral image classification as a representative testbed, we demonstrate that the learned hyper-connection matrices exhibit coherent spatial patterns and asymmetric interaction behaviors, providing mechanistic insight into the model internal dynamics. Furthermore, we find that increasing the expansion rate accelerates the emergence of structured interaction patterns. These results suggest that ES-mHC transforms HSIC from a purely black-box prediction task into a structurally transparent, partially white-box learning process.

</details>


### [28] [Atlas-Assisted Segment Anything Model for Fetal Brain MRI (FeTal-SAM)](https://arxiv.org/abs/2601.15759)
*Qi Zeng,Weide Liu,Bo Li,Ryne Didier,P. Ellen Grant,Davood Karimi*

Main category: cs.CV

TL;DR: FeTal-SAM 将多图谱提示引入 SAM 实现按结构的二值分割并融合为 3D 分割，提供对任意用户指定结构的灵活分割能力，在高对比结构上性能接近专门训练模型，是一种有潜力的通用胎儿脑 MRI 分割方案，但在低对比小结构上性能还有提升空间。


<details>
  <summary>Details</summary>
Motivation: 减少对大量为固定标签定义手工注释数据的依赖，避免为不同标签定义反复训练，同时揭示分割是由图像对比驱动还是由空间先验驱动，提高方法在临床/研究需求变化时的适应性。

Method: 通过多图谱配准生成与目标图像空间对齐的标签模板作为稠密提示（dense prompts），并结合边界框提示喂入 SAM 的分割解码器；对每个结构执行二值分割后融合重建 3D 分割；在两个数据集（dHCP 与自建数据集）上评估性能。

Result: 在皮质板 (cortical plate) 与小脑(cerebellum) 等高对比结构上，FeTal-SAM 达到与专门训练的基线相当的 Dice；在海马与杏仁核等低对比、精细结构上准确率略低；总体表现随胎龄变化保持稳健，展示出不需针对每种标签定义重训练即可进行可插拔的结构分割能力。

Conclusion: FeTal-SAM 提出了一种将 SAM 与多图谱配准结合的适配方法，实现了按结构逐一二值化分割并融合为完整三维胎儿脑 MRI 分割体积，在保留对任意用户指定结构的灵活性前提下，对高对比度结构达到了与为特定数据与标签定义专门训练的基线相当的 Dice 性能，对低对比度小结构性能略逊。

Abstract: This paper presents FeTal-SAM, a novel adaptation of the Segment Anything Model (SAM) tailored for fetal brain MRI segmentation. Traditional deep learning methods often require large annotated datasets for a fixed set of labels, making them inflexible when clinical or research needs change. By integrating atlas-based prompts and foundation-model principles, FeTal-SAM addresses two key limitations in fetal brain MRI segmentation: (1) the need to retrain models for varying label definitions, and (2) the lack of insight into whether segmentations are driven by genuine image contrast or by learned spatial priors. We leverage multi-atlas registration to generate spatially aligned label templates that serve as dense prompts, alongside a bounding-box prompt, for SAM's segmentation decoder. This strategy enables binary segmentation on a per-structure basis, which is subsequently fused to reconstruct the full 3D segmentation volumes. Evaluations on two datasets, the dHCP dataset and an in-house dataset demonstrate FeTal-SAM's robust performance across gestational ages. Notably, it achieves Dice scores comparable to state-of-the-art baselines which were trained for each dataset and label definition for well-contrasted structures like cortical plate and cerebellum, while maintaining the flexibility to segment any user-specified anatomy. Although slightly lower accuracy is observed for subtle, low-contrast structures (e.g., hippocampus, amygdala), our results highlight FeTal-SAM's potential to serve as a general-purpose segmentation model without exhaustive retraining. This method thus constitutes a promising step toward clinically adaptable fetal brain MRI analysis tools.

</details>


### [29] [LL-GaussianMap: Zero-shot Low-Light Image Enhancement via 2D Gaussian Splatting Guided Gain Maps](https://arxiv.org/abs/2601.15766)
*Yuhan Chen,Ying Fang,Guofa Li,Wenxuan Yu,Yicui Shi,Jingrui Zhang,Kefei Qian,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: Introduce LL-GaussianMap, first unsupervised low-light enhancement using 2D Gaussian Splatting to generate gain maps guided by explicit Gaussian primitives; achieves superior results and low storage.


<details>
  <summary>Details</summary>
Motivation: To incorporate explicit structural priors from 2D Gaussian Splatting into low-light image enhancement, addressing neglect of geometric structures in existing pixel-domain or implicit feature methods.

Method: Two-stage method: (1) high-fidelity structural reconstruction via 2D Gaussian Splatting; (2) render data-driven enhancement dictionary coefficients through Gaussian rasterization in a unified enhancement module to produce gain maps for image enhancement without paired data.

Result: Proposed LL-GaussianMap: an unsupervised framework using 2DGS to generate gain maps via rasterization of Gaussian splatting; two-stage pipeline—structural reconstruction with 2DGS then rendering enhancement dictionary coefficients through a unified enhancement module—yielding better edge preservation, artifact suppression, and low storage footprint.

Conclusion: Explicit 2D Gaussian representations effectively provide structural priors that improve unsupervised low-light enhancement by preserving edges, reducing artifacts, and requiring minimal storage.

Abstract: Significant progress has been made in low-light image enhancement with respect to visual quality. However, most existing methods primarily operate in the pixel domain or rely on implicit feature representations. As a result, the intrinsic geometric structural priors of images are often neglected. 2D Gaussian Splatting (2DGS) has emerged as a prominent explicit scene representation technique characterized by superior structural fitting capabilities and high rendering efficiency. Despite these advantages, the utilization of 2DGS in low-level vision tasks remains unexplored. To bridge this gap, LL-GaussianMap is proposed as the first unsupervised framework incorporating 2DGS into low-light image enhancement. Distinct from conventional methodologies, the enhancement task is formulated as a gain map generation process guided by 2DGS primitives. The proposed method comprises two primary stages. First, high-fidelity structural reconstruction is executed utilizing 2DGS. Then, data-driven enhancement dictionary coefficients are rendered via the rasterization mechanism of Gaussian splatting through an innovative unified enhancement module. This design effectively incorporates the structural perception capabilities of 2DGS into gain map generation, thereby preserving edges and suppressing artifacts during enhancement. Additionally, the reliance on paired data is circumvented through unsupervised learning. Experimental results demonstrate that LL-GaussianMap achieves superior enhancement performance with an extremely low storage footprint, highlighting the effectiveness of explicit Gaussian representations for image enhancement.

</details>


### [30] [LL-GaussianImage: Efficient Image Representation for Zero-shot Low-Light Enhancement with 2D Gaussian Splatting](https://arxiv.org/abs/2601.15772)
*Yuhan Chen,Wenxuan Yu,Guofa Li,Yijun Xu,Ying Fang,Yicui Shi,Long Cao,Wenbo Chu,Keqiang Li*

Main category: cs.CV

TL;DR: 提出首个针对2DGS压缩表示域的零样本无监督低光增强方法LL-GaussianImage，通过语义引导MoE、自适应变换、多目标损失和两阶段优化，在压缩域直接完成高质量低光增强，避免解压流程并保持高压缩率。


<details>
  <summary>Details</summary>
Motivation: 现有低光增强主要在像素域进行，针对2DGS压缩图像必须经过费时的解压-增强-再压缩流程，效率低且引入二次退化，因而提出在2DGS压缩表示域直接进行增强以提高效率并保持高压缩比优势。

Method: 构建语义引导的Mixture-of-Experts动态自适应变换模块，在2DGS稀疏属性空间上进行变换；设计多目标协同损失函数以约束平滑性和保真度，抑制伪影并提升视觉质量；采用两阶段优化策略（先单尺度重建以保证基础表示精度，再细化以提升鲁棒性和增强效果）。

Result: 实验表明，该方法能在保持高压缩比的同时，显著提升低光图像的可视质量，抑制伪影并保持重建精度，验证了在压缩表示域直接处理的可行性和优越性。

Conclusion: 该论文提出了一种基于2D Gaussian Splatting（2DGS）压缩表示域的零样本无监督低光照增强框架LL-GaussianImage，成功实现了在压缩域内直接增强，避免了解压-增强-再压缩流程带来的效率下降和二次退化。

Abstract: 2D Gaussian Splatting (2DGS) is an emerging explicit scene representation method with significant potential for image compression due to high fidelity and high compression ratios. However, existing low-light enhancement algorithms operate predominantly within the pixel domain. Processing 2DGS-compressed images necessitates a cumbersome decompression-enhancement-recompression pipeline, which compromises efficiency and introduces secondary degradation. To address these limitations, we propose LL-GaussianImage, the first zero-shot unsupervised framework designed for low-light enhancement directly within the 2DGS compressed representation domain. Three primary advantages are offered by this framework. First, a semantic-guided Mixture-of-Experts enhancement framework is designed. Dynamic adaptive transformations are applied to the sparse attribute space of 2DGS using rendered images as guidance to enable compression-as-enhancement without full decompression to a pixel grid. Second, a multi-objective collaborative loss function system is established to strictly constrain smoothness and fidelity during enhancement, suppressing artifacts while improving visual quality. Third, a two-stage optimization process is utilized to achieve reconstruction-as-enhancement. The accuracy of the base representation is ensured through single-scale reconstruction and network robustness is enhanced. High-quality enhancement of low-light images is achieved while high compression ratios are maintained. The feasibility and superiority of the paradigm for direct processing within the compressed representation domain are validated through experimental results.

</details>


### [31] [Diffusion Model-Based Data Augmentation for Enhanced Neuron Segmentation](https://arxiv.org/abs/2601.15779)
*Liuyun Jiang,Yanchao Zhang,Jinyue Guo,Yizhuo Lu,Ruining Zhou,Hua Han*

Main category: cs.CV

TL;DR: 提出NeuroDiff：基于分辨率感知的条件扩散与生物学掩码重塑的EM神经元分割数据增强框架，在低标注下显著提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分割方法依赖大量人工标注，传统几何/光度增强样本与原图高度相关且缺乏结构多样性，故需生成结构合理且多样的合成数据以提升模型泛化。

Method: 采用分辨率感知的条件扩散模型，结合多尺度条件和EM分辨率先验，实现从3D掩码到体素级图像合成；并设计生物学引导的掩码重塑模块以生成更具结构真实感的增强掩码。

Result: 在AC3和AC4数据集的低标注实验中，与两种后处理方法结合时，ARAND分别提升了32.1%和30.7%，并公开了代码。

Conclusion: 本文提出了一种基于扩散模型的数据增强框架，用于在低标注条件下生成多样且结构合理的3D EM图像-标签对，从而提升神经元分割性能。

Abstract: Neuron segmentation in electron microscopy (EM) aims to reconstruct the complete neuronal connectome; however, current deep learning-based methods are limited by their reliance on large-scale training data and extensive, time-consuming manual annotations. Traditional methods augment the training set through geometric and photometric transformations; however, the generated samples remain highly correlated with the original images and lack structural diversity. To address this limitation, we propose a diffusion-based data augmentation framework capable of generating diverse and structurally plausible image-label pairs for neuron segmentation. Specifically, the framework employs a resolution-aware conditional diffusion model with multi-scale conditioning and EM resolution priors to enable voxel-level image synthesis from 3D masks. It further incorporates a biology-guided mask remodeling module that produces augmented masks with enhanced structural realism. Together, these components effectively enrich the training set and improve segmentation performance. On the AC3 and AC4 datasets under low-annotation regimes, our method improves the ARAND metric by 32.1% and 30.7%, respectively, when combined with two different post-processing methods. Our code is available at https://github.com/HeadLiuYun/NeuroDiff.

</details>


### [32] [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/abs/2601.15780)
*Pascal Benschop,Justin Dauwels,Jan van Gemert*

Main category: cs.CV

TL;DR: 新基准通过最小视频对测试场景感知（有害/无害）和空间感知（施害者身份绑定、轨迹对齐），近期VLM在零训练下表现接近随机，颜色提示能部分缓解施害者混淆但不能解决核心问题。


<details>
  <summary>Details</summary>
Motivation: 评估并暴露VLM在处理依赖微妙时间或几何线索的语义推理时的缺陷，为开发轻量级空间先验补充大规模预训练提供可复现的诊断工具并促进后续研究。

Method: 构造最小化差异的视频对，围绕三个任务：暴力识别、视角间施害者绑定、精细轨迹比对；在训练自由（zero-shot）设置下测试多种VLMs；引入稳定颜色作为辅助线索并分析其效果。

Result: Synthetic benchmark probing situational and spatial awareness in VLMs; training-free eval shows near-chance performance; stable color cues partially help.

Conclusion: 当前大型视觉语言模型在需要细微时空推理的任务上脆弱，需引入轻量级的空间/时序先验或结构化机制来补强预训练能力。

Abstract: Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.

</details>


### [33] [A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks](https://arxiv.org/abs/2601.15810)
*Mustafa Yurdakul,Enes Ayan,Fahrettin Horasan,Sakir Tasdemir*

Main category: cs.CV

TL;DR: 研究开发了一款基于CNN的花卉识别移动应用，比较MobileNet、DenseNet121和Xception三个模型与七种优化算法，最终DenseNet-121+SGD实现约96%的分类性能。


<details>
  <summary>Details</summary>
Motivation: 开发基于卷积神经网络的移动应用，以便为非专业人员提供快速、便捷的花卉识别服务，弥补专家无法随时随地访问的问题。

Method: 使用MobileNet、DenseNet121和Xception三种预训练/构建的卷积神经网络模型，分别用七种优化算法训练并比较分类性能，选出在准确率、精确率、召回率和F1分数上表现最好的模型用于移动应用开发。

Result: 在三个CNN模型（MobileNet、DenseNet121、Xception）和七种优化算法的组合比较中，DenseNet-121配合随机梯度下降(SGD)优化器表现最佳，达到95.84%准确率，以及96.00%的精确率、召回率和F1分数。

Conclusion: CNN模型可有效用于移动端花卉分类，DenseNet-121结合SGD在本研究数据与设置下效果最佳。

Abstract: A convolutional neural network (CNN) is a deep learning algorithm that has been specifically designed for computer vision applications. The CNNs proved successful in handling the increasing amount of data in many computer vision problems, where classical machine learning algorithms were insufficient. Flowers have many uses in our daily lives, from decorating to making medicines to detoxifying the environment. Identifying flower types requires expert knowledge. However, accessing experts at any time and in any location may not always be feasible. In this study a mobile application based on CNNs was developed to recognize different types of flowers to provide non-specialists with quick and easy access to information about flower types. The study employed three distinct CNN models, namely MobileNet, DenseNet121, and Xception, to determine the most suitable model for the mobile application. The classification performances of the models were evaluated by training them with seven different optimization algorithms. The DenseNet-121 architecture, which uses the stochastic gradient descent (SGD) optimization algorithm, was the most successful, achieving 95.84 % accuracy, 96.00% precision, recall, and F1-score. This result shows that CNNs can be used for flower classification in mobile applications.

</details>


### [34] [Beyond Off-the-Shelf Models: A Lightweight and Accessible Machine Learning Pipeline for Ecologists Working with Image Data](https://arxiv.org/abs/2601.15813)
*Clare Chemery,Hendrik Edelhoff,Ludwig Bothmann*

Main category: cs.CV

TL;DR: 作者开发了一个面向生态学家的轻量级ML实验流水线，结合CLI与GUI，帮助非专业机器学习人员在有限本地数据上训练并比较模型；在红鹿年龄与性别分类任务上取得了90.8%与96.2%的准确率，展示了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 降低生态学研究中应用机器学习的门槛，使研究者能根据本地数据和特定任务构建紧凑的、面向问题的分类器，而非仅依赖预训练的通用模型。

Method: 设计了包含命令行接口（用于预处理、训练与评估）和图形界面（用于标注、错误分析与模型比较）的工具；使用3392张相机监控图像，裁剪出4352张含单个个体的图像并由专家标注；训练并对比多种骨干网络、参数配置及数据增强策略。

Result: 在有限数据条件下，最佳模型在年龄分类上达90.77%准确率，在性别分类上达96.15%准确率，证明在狭义生态问题上可获得可靠的人口学分类结果。

Conclusion: 该工作提供了一个轻量级、可操作的实验流水线，使生态学家可在无需深厚ML背景下自行构建和比较图像分类模型，成功在有限数据上实现了对红鹿年龄和性别的高精度分类。

Abstract: We introduce a lightweight experimentation pipeline designed to lower the barrier for applying machine learning (ML) methods for classifying images in ecological research. We enable ecologists to experiment with ML models independently, thus they can move beyond off-the-shelf models and generate insights tailored to local datasets and specific classification tasks and target variables. Our tool combines a simple command-line interface for preprocessing, training, and evaluation with a graphical interface for annotation, error analysis, and model comparison. This design enables ecologists to build and iterate on compact, task-specific classifiers without requiring advanced ML expertise. As a proof of concept, we apply the pipeline to classify red deer (Cervus elaphus) by age and sex from 3392 camera trap images collected in the Veldenstein Forest, Germany. Using 4352 cropped images containing individual deer labeled by experts, we trained and evaluated multiple backbone architectures with a wide variety of parameters and data augmentation strategies. Our best-performing models achieved 90.77% accuracy for age classification and 96.15% for sex classification. These results demonstrate that reliable demographic classification is feasible even with limited data to answer narrow, well-defined ecological problems. More broadly, the framework provides ecologists with an accessible tool for developing ML models tailored to specific research questions, paving the way for broader adoption of ML in wildlife monitoring and demographic analysis.

</details>


### [35] [Towards Realistic Remote Sensing Dataset Distillation with Discriminative Prototype-guided Diffusion](https://arxiv.org/abs/2601.15829)
*Yonghao Xu,Pedram Ghamisi,Qihao Weng*

Main category: cs.CV

TL;DR: 该工作通过在扩散模型中注入分类一致性引导并结合潜在聚类与视觉语言描述，将遥感数据集蒸馏为小型合成数据集，既降低成本又缓解泄露风险，对场景分类任务表现良好。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习在遥感影像解释中依赖大量标注数据，导致高昂的存储与计算成本以及敏感类别的数据泄露风险，因而需要更紧凑且安全的数据表示。

Method: 使用文本到图像扩散模型生成合成样本，并在扩散训练中引入分类一致性损失（由预训练分类器提供）作为分类驱动的引导；对训练样本进行潜在空间聚类以选择多样的原型作为视觉风格指导，并利用视觉语言模型生成聚合文本描述。

Result: 在三个高分辨率遥感场景分类基准上，所提方法能蒸馏出真实且多样的样本，用于下游模型训练，实验结果证明其有效性，并提供了代码与预训练模型。

Conclusion: 该论文首次将数据集蒸馏引入遥感影像理解领域，通过训练文本到图像的扩散模型，将大规模遥感数据集压缩为紧凑、高代表性的蒸馏数据集，解决了存储/计算成本和数据泄露风险问题。

Abstract: Recent years have witnessed the remarkable success of deep learning in remote sensing image interpretation, driven by the availability of large-scale benchmark datasets. However, this reliance on massive training data also brings two major challenges: (1) high storage and computational costs, and (2) the risk of data leakage, especially when sensitive categories are involved. To address these challenges, this study introduces the concept of dataset distillation into the field of remote sensing image interpretation for the first time. Specifically, we train a text-to-image diffusion model to condense a large-scale remote sensing dataset into a compact and representative distilled dataset. To improve the discriminative quality of the synthesized samples, we propose a classifier-driven guidance by injecting a classification consistency loss from a pre-trained model into the diffusion training process. Besides, considering the rich semantic complexity of remote sensing imagery, we further perform latent space clustering on training samples to select representative and diverse prototypes as visual style guidance, while using a visual language model to provide aggregated text descriptions. Experiments on three high-resolution remote sensing scene classification benchmarks show that the proposed method can distill realistic and diverse samples for downstream model training. Code and pre-trained models are available online (https://github.com/YonghaoXu/DPD).

</details>


### [36] [An IoT-Based Smart Plant Monitoring and Irrigation System with Real-Time Environmental Sensing, Automated Alerts, and Cloud Analytics](https://arxiv.org/abs/2601.15830)
*Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 该论文提出了基于物联网的智能植物监测系统，使用ESP32和多种传感器实现实时监测、自动灌溉与云端分析，声称在土壤湿度维持和节水方面效果显著，且成本低廉。


<details>
  <summary>Details</summary>
Motivation: 为响应可持续农业需求，减少人工观察和水资源浪费，提升植物健康管理的实时性与自动化。

Method: 使用ESP32采集DHT22、HC-SR04和土壤湿度传感器数据，OLED显示和蜂鸣器报警，通过Wi-Fi将数据上传到ThingSpeak云平台进行可视化与历史分析，并驱动水泵实现自动灌溉。

Result: 实验结果显示系统实现了实时环境监控、92%土壤湿度准确率、约40%节水率，且总成本为45.20美元，具备可扩展性和实用性。

Conclusion: 系统能够实时监测环境参数并自动灌溉，报告在土壤湿度检测上达92%准确率并将用水量减少约40%，适用于家庭和小型商业农业。

Abstract: The increasing global demand for sustainable agriculture necessitates intelligent monitoring systems that optimize resource utilization and plant health management. Traditional farming methods rely on manual observation and periodic watering, often leading to water wastage, inconsistent plant growth, and delayed response to environmental changes. This paper presents a comprehensive IoT-based smart plant monitoring system that integrates multiple environmental sensors with automated irrigation and cloud analytics. The proposed system utilizes an ESP32 microcontroller to collect real-time data from DHT22 (temperature/humidity), HC-SR04 (water level), and soil moisture sensors, with visual feedback through an OLED display and auditory alerts via a buzzer. All sensor data is wirelessly transmitted to the ThingSpeak cloud platform for remote monitoring, historical analysis, and automated alert generation. Experimental results demonstrate the system's effectiveness in maintaining optimal soil moisture levels (with 92\% accuracy), providing real-time environmental monitoring, and reducing water consumption by approximately 40\% compared to conventional irrigation methods. The integrated web dashboard offers comprehensive visualization of plant health parameters, making it suitable for both small-scale gardening and commercial agriculture applications. With a total implementation cost of \$45.20, this system provides an affordable, scalable solution for precision agriculture and smart farming.

</details>


### [37] [TinySense: Effective CSI Compression for Scalable and Accurate Wi-Fi Sensing](https://arxiv.org/abs/2601.15838)
*Toan Gian,Dung T. Tran,Viet Quoc Pham,Francesco Restuccia,Van-Dinh Nguyen*

Main category: cs.CV

TL;DR: TinySense uses VQGAN-based compression, K-means clustering for dynamic bitrates, and a Transformer to recover bitrate loss, enabling more efficient Wi-Fi-based HPE with higher accuracy and lower latency/network use.


<details>
  <summary>Details</summary>
Motivation: Reduce CSI data for scalable, privacy-preserving Wi-Fi human pose estimation while keeping HPE accuracy and lowering latency/network overhead.

Method: Vector quantization + VQGAN + Transformer + K-means bitrate clustering

Result: TinySense compresses CSI using a VQGAN codebook, K-means for bitrate adjustment, and Transformer for robustness; achieves up to 1.5x higher PCK20 at same compression, reduces latency up to 5x and network overhead up to 2.5x, validated on Jetson Nano and Raspberry Pi testbed.

Conclusion: TinySense effectively compresses CSI with minimal HPE accuracy loss and improved robustness, outperforming prior compression schemes and demonstrating practical gains in latency and network resource reduction.

Abstract: With the growing demand for device-free and privacy-preserving sensing solutions, Wi-Fi sensing has emerged as a promising approach for human pose estimation (HPE). However, existing methods often process vast amounts of channel state information (CSI) data directly, ultimately straining networking resources. This paper introduces TinySense, an efficient compression framework that enhances the scalability of Wi-Fi-based human sensing. Our approach is based on a new vector quantization-based generative adversarial network (VQGAN). Specifically, by leveraging a VQGAN-learned codebook, TinySense significantly reduces CSI data while maintaining the accuracy required for reliable HPE. To optimize compression, we employ the K-means algorithm to dynamically adjust compression bitrates to cluster a large-scale pre-trained codebook into smaller subsets. Furthermore, a Transformer model is incorporated to mitigate bitrate loss, enhancing robustness in unreliable networking conditions. We prototype TinySense on an experimental testbed using Jetson Nano and Raspberry Pi to measure latency and network resource use. Extensive results demonstrate that TinySense significantly outperforms state-of-the-art compression schemes, achieving up to 1.5x higher HPE accuracy score (PCK20) under the same compression rate. It also reduces latency and networking overhead, respectively, by up to 5x and 2.5x. The code repository is available online at here.

</details>


### [38] [A Lightweight Brain-Inspired Machine Learning Framework for Coronary Angiography: Hybrid Neural Representation and Robust Learning Strategies](https://arxiv.org/abs/2601.15865)
*Jingsong Xia,Siqi Wang*

Main category: cs.CV

TL;DR: 作者提出一种轻量级且生物启发的冠状动脉造影分类框架：用预训练CNN构建混合表示，选择性微调、注意力调制（Focal Loss+标签平滑）、类不平衡采样与余弦退火热重启，提高难例敏感性并在资源受限下实现稳健性能。


<details>
  <summary>Details</summary>
Motivation: 现实临床冠状动脉造影影像存在复杂病变形态、严重类别不平衡、标注不确定性以及计算资源受限，这些问题使得传统深度学习方法在鲁棒性和泛化性方面面临挑战。

Method: 基于预训练卷积神经网络构建轻量级混合表征，通过选择性神经可塑性策略只更新部分参数以提高效率；采用将Focal Loss与标签平滑结合的注意力调制损失函数以增强对难例和标注不确定性的敏感性；使用类不平衡感知采样策略和带热重启的余弦退火学习率来模拟生物节律性的调节。

Result: 在二分类任务上，该轻量级大脑启发模型在准确率、召回率、F1分数和AUC等指标上表现出强稳的竞争力，同时保持较高的计算效率，适合部署。

Conclusion: 该研究表明，将大脑启发的学习机制（如注意力调制的损失函数、类不平衡感知采样和节律性学习率调度）整合到轻量级神经网络中，可以在计算资源受限的环境下实现对冠状动脉造影图像的稳健二分类性能，并具有可部署性。

Abstract: Background: Coronary angiography (CAG) is a cornerstone imaging modality for assessing coronary artery disease and guiding interventional treatment decisions. However, in real-world clinical settings, angiographic images are often characterized by complex lesion morphology, severe class imbalance, label uncertainty, and limited computational resources, posing substantial challenges to conventional deep learning approaches in terms of robustness and generalization.Methods: The proposed framework is built upon a pretrained convolutional neural network to construct a lightweight hybrid neural representation. A selective neural plasticity training strategy is introduced to enable efficient parameter adaptation. Furthermore, a brain-inspired attention-modulated loss function, combining Focal Loss with label smoothing, is employed to enhance sensitivity to hard samples and uncertain annotations. Class-imbalance-aware sampling and cosine annealing with warm restarts are adopted to mimic rhythmic regulation and attention allocation mechanisms observed in biological neural systems.Results: Experimental results demonstrate that the proposed lightweight brain-inspired model achieves strong and stable performance in binary coronary angiography classification, yielding competitive accuracy, recall, F1-score, and AUC metrics while maintaining high computational efficiency.Conclusion: This study validates the effectiveness of brain-inspired learning mechanisms in lightweight medical image analysis and provides a biologically plausible and deployable solution for intelligent clinical decision support under limited computational resources.

</details>


### [39] [Out-of-Distribution Detection Based on Total Variation Estimation](https://arxiv.org/abs/2601.15867)
*Dabiao Ma,Zhiba Su,Jian Yang,Haojun Fei*

Main category: cs.CV

TL;DR: 提出基于Total Variation Network Estimator的TV-OOD，通过计算每个输入对总变差的贡献作为分数进行OOD检测，在图像分类上取得了优于或相当于SOTA的方法表现。


<details>
  <summary>Details</summary>
Motivation: 实际部署中模型面对分布变化风险，现有OOD检测方法虽有效但仍有改进空间，本文通过总变差视角提供新的判别证据。

Method: 提出Total Variation Network Estimator来估计每个输入对模型预测分布的总变差贡献，定义为total variation score；基于该分数进行OOD判别。通过多模型、多数据集实验评估性能并与主流OOD方法比较。

Result: 在多个模型和数据集的图像分类任务上，TV-OOD在所有评估指标上取得了与或优于主流OOD检测方法的结果，展示出稳健性与普适性。

Conclusion: TV-OOD能有效利用输入对整体总变差的贡献区分ID与OOD样本，在图像分类任务上对比现有SOTA方法表现出相当或更好效果。

Abstract: This paper introduces a novel approach to securing machine learning model deployments against potential distribution shifts in practical applications, the Total Variation Out-of-Distribution (TV-OOD) detection method. Existing methods have produced satisfactory results, but TV-OOD improves upon these by leveraging the Total Variation Network Estimator to calculate each input's contribution to the overall total variation. By defining this as the total variation score, TV-OOD discriminates between in- and out-of-distribution data. The method's efficacy was tested across a range of models and datasets, consistently yielding results in image classification tasks that were either comparable or superior to those achieved by leading-edge out-of-distribution detection techniques across all evaluation metrics.

</details>


### [40] [PMPBench: A Paired Multi-Modal Pan-Cancer Benchmark for Medical Image Synthesis](https://arxiv.org/abs/2601.15884)
*Yifan Chen,Fei Yin,Hao Chen,Jia Wu,Chao Li*

Main category: cs.CV

TL;DR: 首次公开的完全配对、多器官、含MRI三相DCE与CT对照增强的影像数据集PMPBench，并提供基准与评测，推动无造影合成研究。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据主要集中于脑部且多为部分配对或缺失相位/模态标注，阻碍了合成造影的广泛研究；因此需要一个覆盖多器官、完全配对且带有清晰相位标注的数据集来推动该领域发展。

Method: 收集并筛选覆盖11个器官的影像，确保MRI包含DCE三期（DCE1-DCE3）、CT包含非增强与增强（CTC）配对；进行解剖配准和质量控制；基于该数据构建多种一对一、一对多、多对多翻译任务并用当前典型的图像到图像翻译方法（例如GAN、CycleGAN、Pix2Pix等）做基线评测。

Result: 构建了第一个公开、完全配对的泛癌症影像数据集，涵盖11个人体器官，包含MRI三阶段DCE序列和CT非增强与增强配对，数据经过解剖对应校验；在此基础上建立了图像翻译基准并提供代表性方法的评估结果，公开数据与代码。

Conclusion: 该工作提供了解决造影剂合成研究中数据短缺与不完整配对问题的关键资源，将促进多器官肿瘤影像中安全有效的合成造影开发与验证。

Abstract: Contrast medium plays a pivotal role in radiological imaging, as it amplifies lesion conspicuity and improves detection for the diagnosis of tumor-related diseases. However, depending on the patient's health condition or the medical resources available, the use of contrast medium is not always feasible. Recent work has explored AI-based image translation to synthesize contrast-enhanced images directly from non-contrast scans, aims to reduce side effects and streamlines clinical workflows. Progress in this direction has been constrained by data limitations: (1) existing public datasets focus almost exclusively on brain-related paired MR modalities; (2) other collections include partially paired data but suffer from missing modalities/timestamps and imperfect spatial alignment; (3) explicit labeling of CT vs. CTC or DCE phases is often absent; (4) substantial resources remain private. To bridge this gap, we introduce the first public, fully paired, pan-cancer medical imaging dataset spanning 11 human organs. The MR data include complete dynamic contrast-enhanced (DCE) sequences covering all three phases (DCE1-DCE3), while the CT data provide paired non-contrast and contrast-enhanced acquisitions (CTC). The dataset is curated for anatomical correspondence, enabling rigorous evaluation of 1-to-1, N-to-1, and N-to-N translation settings (e.g., predicting DCE phases from non-contrast inputs). Built upon this resource, we establish a comprehensive benchmark. We report results from representative baselines of contemporary image-to-image translation. We release the dataset and benchmark to catalyze research on safe, effective contrast synthesis, with direct relevance to multi-organ oncology imaging workflows. Our code and dataset are publicly available at https://github.com/YifanChen02/PMPBench.

</details>


### [41] [Understanding the Transfer Limits of Vision Foundation Models](https://arxiv.org/abs/2601.15888)
*Shiqi Huang,Yipei Wang,Natasha Thorley,Alexander Ng,Shaheer Saeed,Mark Emberton,Shonit Punwani,Veeru Kasivisvanathan,Dean Barratt,Daniel Alexander,Yipeng Hu*

Main category: cs.CV

TL;DR: 预训练目标与下游医疗影像任务对齐度决定了迁移效果；用MMD评估对齐可解释性能差异，建议设计更贴近下游需求的预训练策略。


<details>
  <summary>Details</summary>
Motivation: 调查视觉基础模型(VFMs)在临床影像下游任务上迁移性能不稳的原因，假设是预训练目标与下游任务需求不匹配。

Method: 在前列腺多参数MRI五项任务上对两种预训练模型进行微调与评估，比较微调前后特征的MMD以及对应的性能与收敛速度。

Result: 比较两种VFM（基于重建的ProFound与基于对比的ProViCNet）在前列腺多参数MRI的五项任务上的迁移表现，发现预训练与下游任务特征分布的对齐度（用MMD衡量）越高，微调后性能提升越大且收敛更快。

Conclusion: 强调将下游适用性纳入预训练目标设计与分析，以提升VFM在临床影像任务上的稳健迁移性能。

Abstract: Foundation models leverage large-scale pretraining to capture extensive knowledge, demonstrating generalization in a wide range of language tasks. By comparison, vision foundation models (VFMs) often exhibit uneven improvements across downstream tasks, despite substantial computational investment. We postulate that this limitation arises from a mismatch between pretraining objectives and the demands of downstream vision-and-imaging tasks. Pretraining strategies like masked image reconstruction or contrastive learning shape representations for tasks such as recovery of generic visual patterns or global semantic structures, which may not align with the task-specific requirements of downstream applications including segmentation, classification, or image synthesis. To investigate this in a concrete real-world clinical area, we assess two VFMs, a reconstruction-focused MAE-based model (ProFound) and a contrastive-learning-based model (ProViCNet), on five prostate multiparametric MR imaging tasks, examining how such task alignment influences transfer performance, i.e., from pretraining to fine-tuning. Our findings indicate that better alignment between pretraining and downstream tasks, measured by simple divergence metrics such as maximum-mean-discrepancy (MMD) between the same features before and after fine-tuning, correlates with greater performance improvements and faster convergence, emphasizing the importance of designing and analyzing pretraining objectives with downstream applicability in mind.

</details>


### [42] [RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture](https://arxiv.org/abs/2601.15891)
*Anas Anwarul Haq Khan,Mariam Husain,Kshitij Jadhav*

Main category: cs.CV

TL;DR: 提出RadJEPA，一种无需语言监督的自监督放射影像表征学习框架。基于联合嵌入预测架构，模型通过预测被遮挡区域的潜在表征进行预训练，在胸部X光上优于包括Rad-DINO在内的最先进方法。


<details>
  <summary>Details</summary>
Motivation: 受限于配对图像-文本数据的稀缺，研究是否能在没有语言监督的情况下学到稳健的放射影像编码器。探索一种不同于对齐全局表征的方法，通过显式预测被遮挡区域的潜在向量来获取更丰富的视觉表示。

Method: 构建Joint Embedding Predictive Architecture：在图像上随机遮挡部分区域，编码器学习预测被遮挡区域的潜在表示。与图像-文本预训练和DINO式自蒸馏不同，RadJEPA侧重于潜在空间的预测而非全局表示对齐。仅用无标注胸片进行预训练。

Result: 在疾病分类、语义分割和报告生成等下游基准上，RadJEPA的性能超过了包括Rad-DINO在内的最先进方法，证明了其在无文本监督场景下的有效性。

Conclusion: RadJEPA能在没有文本配对的情况下，仅用未标注胸部X光学到强大的放射学编码器，其在分类、分割和报告生成等任务上的表现超过现有最优方法，表明显式的潜在空间预测是有效的自监督策略。

Abstract: Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.

</details>


### [43] [ThermoSplat: Cross-Modal 3D Gaussian Splatting with Feature Modulation and Geometry Decoupling](https://arxiv.org/abs/2601.15897)
*Zhaoqi Su,Shihai Chen,Xinyan Lin,Liqin Huang,Zhipeng Su,Xiaoqiang Lu*

Main category: cs.CV

TL;DR: ThermoSplat通过跨模态FiLM调制和模态自适应几何解耦，把3D Gaussian Splatting成功扩展到RGB-热红外重建，显著提升双谱渲染质量。


<details>
  <summary>Details</summary>
Motivation: 当前将3D Gaussian Splatting扩展到多光谱场景时难以充分利用跨模态互补信息，既有方法要么忽视跨模态关联，要么使用的共享表示无法自适应处理光谱间复杂结构相关性与物理差异。

Method: 提出Cross-Modal FiLM Modulation用于以热结构先验动态调制共享潜在特征；提出Modality-Adaptive Geometric Decoupling为热分支学习独立不透明度偏移并执行独立光栅化；采用混合渲染管线结合显式球谐函数与隐式神经解码。

Result: 在RGBT-Scenes数据集上，ThermoSplat在可见光与热谱渲染质量上均达到或超越现有最先进水平，保持语义一致性并保留高频细节。

Conclusion: ThermoSplat有效提升了可见光与热红外的多模态3D重建质量，通过跨模态条件调制和模态自适应几何解耦解决了光谱间结构差异与物理不一致性问题，实现了在RGB与热图像上的高质量渲染。

Abstract: Multi-modal scene reconstruction integrating RGB and thermal infrared data is essential for robust environmental perception across diverse lighting and weather conditions. However, extending 3D Gaussian Splatting (3DGS) to multi-spectral scenarios remains challenging. Current approaches often struggle to fully leverage the complementary information of multi-modal data, typically relying on mechanisms that either tend to neglect cross-modal correlations or leverage shared representations that fail to adaptively handle the complex structural correlations and physical discrepancies between spectrums. To address these limitations, we propose ThermoSplat, a novel framework that enables deep spectral-aware reconstruction through active feature modulation and adaptive geometry decoupling. First, we introduce a Cross-Modal FiLM Modulation mechanism that dynamically conditions shared latent features on thermal structural priors, effectively guiding visible texture synthesis with reliable cross-modal geometric cues. Second, to accommodate modality-specific geometric inconsistencies, we propose a Modality-Adaptive Geometric Decoupling scheme that learns independent opacity offsets and executes an independent rasterization pass for the thermal branch. Additionally, a hybrid rendering pipeline is employed to integrate explicit Spherical Harmonics with implicit neural decoding, ensuring both semantic consistency and high-frequency detail preservation. Extensive experiments on the RGBT-Scenes dataset demonstrate that ThermoSplat achieves state-of-the-art rendering quality across both visible and thermal spectrums.

</details>


### [44] [Opening the Black Box: Preliminary Insights into Affective Modeling in Multimodal Foundation Models](https://arxiv.org/abs/2601.15906)
*Zhen Zhang,Runhao Zeng,Sicheng Zhao,Xiping Hu*

Main category: cs.CV

TL;DR: 该论文系统性研究了多模态基础模型中的情感建模机制，发现情感适配主要集中在前馈门控投影（gate_proj）模块，而非注意力模块。通过模块转移、单模块适配和消融实验，证明gate_proj对情感理解与生成是充分、有效且必要的。仅调优约24.5%的参数即可达到AffectGPT 96.6%的平均性能，显示出显著的参数效率。


<details>
  <summary>Details</summary>
Motivation: 尽管情感模型在经验性能上已有很大提升，但我们对它们内部架构如何支持情感理解与生成缺乏机械性理解；因此需要系统性研究以定位模型中承载情感能力的结构组件。

Method: 在多种架构、训练策略和情感任务上，作者通过分析情感监督如何重塑内部参数，采用控制模块转移、单模块定向调优和破坏性消融实验来识别关键模块（gate_proj）。比较了全模型适配（AffectGPT）与仅调优特定模块的性能与参数效率。

Result: 发现情感适配一致地集中在gate_proj，调优该模块能以约24.5%的参数量实现AffectGPT 96.6%的平均性能，证明了gate_proj的充分性、效率和必要性。

Conclusion: 情感能力在基础模型中由前馈门控机制结构性介导，gate_proj是情感建模的关键结构位点；针对该模块的定向调优能够高效恢复大部分情感任务性能。

Abstract: Understanding where and how emotions are represented in large-scale foundation models remains an open problem, particularly in multimodal affective settings. Despite the strong empirical performance of recent affective models, the internal architectural mechanisms that support affective understanding and generation are still poorly understood. In this work, we present a systematic mechanistic study of affective modeling in multimodal foundation models. Across multiple architectures, training strategies, and affective tasks, we analyze how emotion-oriented supervision reshapes internal model parameters. Our results consistently reveal a clear and robust pattern: affective adaptation does not primarily focus on the attention module, but instead localizes to the feed-forward gating projection (\texttt{gate\_proj}). Through controlled module transfer, targeted single-module adaptation, and destructive ablation, we further demonstrate that \texttt{gate\_proj} is sufficient, efficient, and necessary for affective understanding and generation. Notably, by tuning only approximately 24.5\% of the parameters tuned by AffectGPT, our approach achieves 96.6\% of its average performance across eight affective tasks, highlighting substantial parameter efficiency. Together, these findings provide empirical evidence that affective capabilities in foundation models are structurally mediated by feed-forward gating mechanisms and identify \texttt{gate\_proj} as a central architectural locus of affective modeling.

</details>


### [45] [The Latency Wall: Benchmarking Off-the-Shelf Emotion Recognition for Real-Time Virtual Avatars](https://arxiv.org/abs/2601.15914)
*Yarin Benyamin*

Main category: cs.CV

TL;DR: Face detection reliable; classification too slow/inaccurate on CPU — need lightweight, domain-specific models for real-time VR therapy.


<details>
  <summary>Details</summary>
Motivation: Evaluate feasibility of real-time emotion recognition for VR-based ASD therapy by benchmarking SOTA zero-shot FER models on stylized avatars, focusing on latency-accuracy trade-offs for CPU-only setups.

Method: Benchmarked YOLO (v8/v11/v12) Medium/Nano for detection and CLIP/SigLIP/ViT-FER for zero-shot FER using UIBVFED avatars; measured accuracy and CPU-only inference latency (MTP constraint <140 ms).

Result: Face detection on stylized avatars is robust (100%), but classification faces a 'Latency Wall'. YOLOv11n best for detection (~54 ms). Transformers like CLIP/SigLIP/ViT-FER underperform in accuracy (<23%) or latency (>150 ms).

Conclusion: Current general-purpose SOTA models are not suitable for accessible real-time FER on commodity hardware; develop lightweight, domain-specific architectures to meet latency-accuracy constraints.

Abstract: In the realm of Virtual Reality (VR) and Human-Computer Interaction (HCI), real-time emotion recognition shows promise for supporting individuals with Autism Spectrum Disorder (ASD) in improving social skills. This task requires a strict latency-accuracy trade-off, with motion-to-photon (MTP) latency kept below 140 ms to maintain contingency. However, most off-the-shelf Deep Learning models prioritize accuracy over the strict timing constraints of commodity hardware. As a first step toward accessible VR therapy, we benchmark State-of-the-Art (SOTA) models for Zero-Shot Facial Expression Recognition (FER) on virtual characters using the UIBVFED dataset. We evaluate Medium and Nano variants of YOLO (v8, v11, and v12) for face detection, alongside general-purpose Vision Transformers including CLIP, SigLIP, and ViT-FER.Our results on CPU-only inference demonstrate that while face detection on stylized avatars is robust (100% accuracy), a "Latency Wall" exists in the classification stage. The YOLOv11n architecture offers the optimal balance for detection (~54 ms). However, general-purpose Transformers like CLIP and SigLIP fail to achieve viable accuracy (<23%) or speed (>150 ms) for real-time loops. This study highlights the necessity for lightweight, domain-specific architectures to enable accessible, real-time AI in therapeutic settings.

</details>


### [46] [A Multi-View Pipeline and Benchmark Dataset for 3D Hand Pose Estimation in Surgery](https://arxiv.org/abs/2601.15918)
*Valery Fischer,Alan Magdaleno,Anna-Katharina Calek,Nicola Cavalcanti,Nathan Hoffman,Christoph Germann,Joschua Wüthrich,Max Krähenmann,Mazda Farshad,Philipp Fürnstahl,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出了一个无需微调的多视图3D手姿估计管线并发布了大规模外科场景标注数据集，在2D和3D误差上分别大幅优于基线，建立了手术视觉研究强基线。


<details>
  <summary>Details</summary>
Motivation: 外科环境光照强烈且局部化、器械或人员频繁遮挡、手套导致手部外观单一、以及标注数据稀缺，这些因素使得传统单视图或需大量领域数据微调的方法效果受限，推动提出无训练或无需微调的稳健方法与数据集。

Method: 管线结合了多视图数据、可靠的人体检测、全身姿态估计、对追踪到的手部裁剪图像进行最先进的2D关键点预测，随后通过约束的3D优化（三角化与位置约束）得到三维手势。全过程依赖预训练模型，无需手术领域专门微调。

Result: 与基线相比，方法在2D平均关节误差上降低了31%，在3D平均每关节位置误差上降低了76%，表明显著提升的精度与鲁棒性。

Conclusion: 该论文在外科环境中提出了一个无需领域特定微调的多视图3D手部姿态估计管线，并提供了一个包含超过68,000帧和3,000个人工标注2D手部姿态及三角化3D真值的新基准数据集，建立了强基线。

Abstract: Purpose: Accurate 3D hand pose estimation supports surgical applications such as skill assessment, robot-assisted interventions, and geometry-aware workflow analysis. However, surgical environments pose severe challenges, including intense and localized lighting, frequent occlusions by instruments or staff, and uniform hand appearance due to gloves, combined with a scarcity of annotated datasets for reliable model training.
  Method: We propose a robust multi-view pipeline for 3D hand pose estimation in surgical contexts that requires no domain-specific fine-tuning and relies solely on off-the-shelf pretrained models. The pipeline integrates reliable person detection, whole-body pose estimation, and state-of-the-art 2D hand keypoint prediction on tracked hand crops, followed by a constrained 3D optimization. In addition, we introduce a novel surgical benchmark dataset comprising over 68,000 frames and 3,000 manually annotated 2D hand poses with triangulated 3D ground truth, recorded in a replica operating room under varying levels of scene complexity.
  Results: Quantitative experiments demonstrate that our method consistently outperforms baselines, achieving a 31% reduction in 2D mean joint error and a 76% reduction in 3D mean per-joint position error.
  Conclusion: Our work establishes a strong baseline for 3D hand pose estimation in surgery, providing both a training-free pipeline and a comprehensive annotated dataset to facilitate future research in surgical computer vision.

</details>


### [47] [Class Confidence Aware Reweighting for Long Tailed Learning](https://arxiv.org/abs/2601.15924)
*Brainard Philemon Jagati,Jitendra Tembhurne,Harsh Goud,Rudra Pratap Singh,Chandrashekhar Meshram*

Main category: cs.CV

TL;DR: 提出Ω(p_t,f_c)类-置信度感知重加权损失，在损失层调节样本贡献，与logit校正互补，在多种长尾数据集上显著提升表现。


<details>
  <summary>Details</summary>
Motivation: 现有长尾学习主要关注在决策层面（如logit校正）补偿类别不平衡，但较少关注因样本置信度差异带来的优化过程偏差，作者旨在通过损失层的重加权来弥补这一不足。

Method: 在损失层面引入类-置信度感知的权重函数Ω(p_t,f_c)，根据预测置信度p_t和类相对频率f_c动态调整样本损失权重，从而在训练过程中平衡不同类别和不同置信度样本的贡献；该方法可与现有logit层校正方法结合使用。

Result: 在CIFAR-100-LT、ImageNet-LT和iNaturalist2018等数据集以及不同不平衡因子设置下，提出的方法带来了显著的性能提升，实验证实了理论分析的有效性。

Conclusion: 本文提出了一种基于样本置信度和类别频率的重加权方案，能够在长尾学习中补偿训练过程中由置信度差异带来的优化偏差，与基于logit校正的方法互补，并在多个长尾数据集上取得显著性能提升。

Abstract: Deep neural network models degrade significantly in the long-tailed data distribution, with the overall training data dominated by a small set of classes in the head, and the tail classes obtaining less training examples. Addressing the imbalance in the classes, attention in the related literature was given mainly to the adjustments carried out in the decision space in terms of either corrections performed at the logit level in order to compensate class-prior bias, with the least attention to the optimization process resulting from the adjustments introduced through the differences in the confidences among the samples. In the current study, we present the design of a class and confidence-aware re-weighting scheme for long-tailed learning. This scheme is purely based upon the loss level and has a complementary nature to the existing methods performing the adjustment of the logits. In the practical implementation stage of the proposed scheme, we use an Ω(p_t, f_c) function. This function enables the modulation of the contribution towards the training task based upon the confidence value of the prediction, as well as the relative frequency of the corresponding class. Our observations in the experiments are corroborated by significant experimental results performed on the CIFAR-100-LT, ImageNet-LT, and iNaturalist2018 datasets under various values of imbalance factors that clearly authenticate the theoretical discussions above.

</details>


### [48] [NeuroMamba: Multi-Perspective Feature Interaction with Visual Mamba for Neuron Segmentation](https://arxiv.org/abs/2601.15929)
*Liuyun Jiang,Yizhuo Lu,Yanchao Zhang,Jiazheng Liu,Hua Han*

Main category: cs.CV

TL;DR: 提出NeuroMamba：融合Visual Mamba的无补丁全局建模与Boundary Discriminative Feature Extractor和跨调制融合，兼顾全局上下文与细粒度边界，适应各向异性/各向同性分辨率，四数据集SOTA。


<details>
  <summary>Details</summary>
Motivation: 针对CNN缺乏长程上下文和Transformer打破体素细节的问题，提出一种既能全局建模又能保持体素级细节的混合框架以改善神经元分割边界精度。

Method: 框架由三部分组成：1) 通道门控的Boundary Discriminative Feature Extractor(BDFE)用于增强局部边界线索；2) Spatial Continuous Feature Extractor(SCFE)在Visual Mamba上加入分辨率感知扫描以实现无补丁的全局依赖建模；3) 跨调制机制融合多视角特征。

Result: NeuroMamba提出一种结合全局与局部建模的神经元分割框架，通过无补丁的全局建模保留长程依赖，并用局部特征提取器恢复体素级细节。实验在四个公开EM数据集上取得SOTA。

Conclusion: NeuroMamba有效弥合了Transformer与CNN在神经元分割上的短板，通过分辨率感知的全局建模与通道门控的边界增强模块，实现了长程依赖与体素细节的平衡，显著提升分割性能。

Abstract: Neuron segmentation is the cornerstone of reconstructing comprehensive neuronal connectomes, which is essential for deciphering the functional organization of the brain. The irregular morphology and densely intertwined structures of neurons make this task particularly challenging. Prevailing CNN-based methods often fail to resolve ambiguous boundaries due to the lack of long-range context, whereas Transformer-based methods suffer from boundary imprecision caused by the loss of voxel-level details during patch partitioning. To address these limitations, we propose NeuroMamba, a multi-perspective framework that exploits the linear complexity of Mamba to enable patch-free global modeling and synergizes this with complementary local feature modeling, thereby efficiently capturing long-range dependencies while meticulously preserving fine-grained voxel details. Specifically, we design a channel-gated Boundary Discriminative Feature Extractor (BDFE) to enhance local morphological cues. Complementing this, we introduce the Spatial Continuous Feature Extractor (SCFE), which integrates a resolution-aware scanning mechanism into the Visual Mamba architecture to adaptively model global dependencies across varying data resolutions. Finally, a cross-modulation mechanism synergistically fuses these multi-perspective features. Our method demonstrates state-of-the-art performance across four public EM datasets, validating its exceptional adaptability to both anisotropic and isotropic resolutions. The source code will be made publicly available.

</details>


### [49] [EVolSplat4D: Efficient Volume-based Gaussian Splatting for 4D Urban Scene Synthesis](https://arxiv.org/abs/2601.15951)
*Sheng Miao,Sijin Li,Pan Wang,Dongfeng Bai,Bingbing Liu,Yue Wang,Andreas Geiger,Yiyi Liao*

Main category: cs.CV

TL;DR: EvolSplat4D用三条专门分支统一体积与像素级Gaussian表示，在不做逐场景优化的前提下，实现对城市静态与动态场景的高质量一致性重建，优于现有优化和前馈方法。


<details>
  <summary>Details</summary>
Motivation: 解决城市场景新视图合成在重建时间与质量之间的权衡问题，尤其在静态与动态混合环境中避免现有前馈方法的多视图不一致性与基于场景优化方法的高时间成本。

Method: 设计三分支网络：1) 近景静态分支从3D特征体积预测跨帧一致的3D Gaussians，并通过语义增强的图像渲染模块生成外观；2) 动态对象分支基于物体中心的规范空间与运动校正渲染聚合时间信息，处理运动先验噪声；3) 远景分支采用每像素Gaussian以高效覆盖远场。三分支融合策略在训练时兼顾几何一致性和外观保真，并在多个数据集上进行对比实验验证。

Result: 提出EvolSplat4D，一个三分支前馈框架，结合体素/像素混合的3D Gaussian预测：对近景静态区域用3D特征体积预测一致的3D Gaussians并用语义增强的图像渲染预测外观；对动态对象用物体中心规范空间与运动调整渲染模块聚合时序特征以保证4D稳定重建；对远景用高效的每像素Gaussian分支覆盖全场景。

Conclusion: EvolSplat4D在KITTI-360、KITTI、Waymo和PandaSet数据集上显示出更高的重建精度与时空一致性，兼顾效率与全景覆盖，适合自动驾驶模拟的新视图合成任务。

Abstract: Novel view synthesis (NVS) of static and dynamic urban scenes is essential for autonomous driving simulation, yet existing methods often struggle to balance reconstruction time with quality. While state-of-the-art neural radiance fields and 3D Gaussian Splatting approaches achieve photorealism, they often rely on time-consuming per-scene optimization. Conversely, emerging feed-forward methods frequently adopt per-pixel Gaussian representations, which lead to 3D inconsistencies when aggregating multi-view predictions in complex, dynamic environments. We propose EvolSplat4D, a feed-forward framework that moves beyond existing per-pixel paradigms by unifying volume-based and pixel-based Gaussian prediction across three specialized branches. For close-range static regions, we predict consistent geometry of 3D Gaussians over multiple frames directly from a 3D feature volume, complemented by a semantically-enhanced image-based rendering module for predicting their appearance. For dynamic actors, we utilize object-centric canonical spaces and a motion-adjusted rendering module to aggregate temporal features, ensuring stable 4D reconstruction despite noisy motion priors. Far-Field scenery is handled by an efficient per-pixel Gaussian branch to ensure full-scene coverage. Experimental results on the KITTI-360, KITTI, Waymo, and PandaSet datasets show that EvolSplat4D reconstructs both static and dynamic environments with superior accuracy and consistency, outperforming both per-scene optimization and state-of-the-art feed-forward baselines.

</details>


### [50] [HyperAlign: Hypernetwork for Efficient Test-Time Alignment of Diffusion Models](https://arxiv.org/abs/2601.15968)
*Xin Xie,Jiaxian Guo,Dong Gong*

Main category: cs.CV

TL;DR: 提出HyperAlign，通过超网络在测试时生成低秩适配权重，动态调制扩散模型的生成算子，实现基于奖励的对齐，兼顾质量与效率；在Stable Diffusion与FLUX上优于现有微调和测试时缩放方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在对齐扩散模型时存在权衡：微调导致多样性损失与奖励过优化，测试时调整计算开销大且优化不足。需要一种既高效又有效的测试时对齐方案。

Method: 构建一个超网络，基于输入潜变量、时间步和提示生成低秩适配权重以调制模型算子；提供不同频率应用的变体以平衡性能与效率；优化目标为奖励分数加偏好数据正则项以减少奖励滥用。

Result: 在Stable Diffusion和FLUX上，HyperAlign相比微调与测试时缩放方法在语义一致性与视觉吸引力上有显著提升，并且在效率上有更好表现。

Conclusion: HyperAlign在多种生成范式上显著提升语义一致性与视觉美感，同时兼顾计算效率，优于传统微调和测试时缩放基线。

Abstract: Diffusion models achieve state-of-the-art performance but often fail to generate outputs that align with human preferences and intentions, resulting in images with poor aesthetic quality and semantic inconsistencies. Existing alignment methods present a difficult trade-off: fine-tuning approaches suffer from loss of diversity with reward over-optimization, while test-time scaling methods introduce significant computational overhead and tend to under-optimize. To address these limitations, we propose HyperAlign, a novel framework that trains a hypernetwork for efficient and effective test-time alignment. Instead of modifying latent states, HyperAlign dynamically generates low-rank adaptation weights to modulate the diffusion model's generation operators. This allows the denoising trajectory to be adaptively adjusted based on input latents, timesteps and prompts for reward-conditioned alignment. We introduce multiple variants of HyperAlign that differ in how frequently the hypernetwork is applied, balancing between performance and efficiency. Furthermore, we optimize the hypernetwork using a reward score objective regularized with preference data to reduce reward hacking. We evaluate HyperAlign on multiple extended generative paradigms, including Stable Diffusion and FLUX. It significantly outperforms existing fine-tuning and test-time scaling baselines in enhancing semantic consistency and visual appeal.

</details>


### [51] [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/abs/2601.16007)
*Chak-Wing Mak,Guanyu Zhu,Boyi Zhang,Hongji Li,Xiaowei Chi,Kevin Zhang,Yichen Wu,Yangfan He,Chun-Kai Fan,Wentao Lu,Kuangzhi Ge,Xinyu Fang,Hongyang He,Kuan Lu,Tianxiang Xu,Li Zhang,Yongxin Ni,Youhua Li,Shanghang Zhang*

Main category: cs.CV

TL;DR: PhysicsMind是一个专注物理规律一致性的多模态基准，包含VQA和视频生成任务，测试质心、力矩与惯性约束，揭示当前模型在力学推理与生成上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 评估多模态大语言模型（MLLMs）与视频世界模型在物理理解方面的能力，弥补现有基准在合成数据、模板化问答或侧重感知质量方面的不足。

Method: 构建包含真实与仿真视频与图像的数据集，设计基于三大物理原理的问答与轨迹生成评估指标；对多种最新MLLM与视频生成模型进行横向评测，分析其错误模式与物理不一致性。

Result: 提出PhysicsMind基准，涵盖真实与仿真环境，围绕质心、杠杆平衡与牛顿第一定律三个物理原理，包含VQA与视频生成两类任务；评估表明现有模型多依赖外观启发式，常违反基本力学规律。

Conclusion: 当前大规模训练与扩展仍不足以保证鲁棒的物理理解；PhysicsMind可作为衡量和推动物理感知与生成能力的标准化测试平台。

Abstract: Modern foundational Multimodal Large Language Models (MLLMs) and video world models have advanced significantly in mathematical, common-sense, and visual reasoning, but their grasp of the underlying physics remains underexplored. Existing benchmarks attempting to measure this matter rely on synthetic, Visual Question Answer templates or focus on perceptual video quality that is tangential to measuring how well the video abides by physical laws. To address this fragmentation, we introduce PhysicsMind, a unified benchmark with both real and simulation environments that evaluates law-consistent reasoning and generation over three canonical principles: Center of Mass, Lever Equilibrium, and Newton's First Law. PhysicsMind comprises two main tasks: i) VQA tasks, testing whether models can reason and determine physical quantities and values from images or short videos, and ii) Video Generation(VG) tasks, evaluating if predicted motion trajectories obey the same center-of-mass, torque, and inertial constraints as the ground truth. A broad range of recent models and video generation models is evaluated on PhysicsMind and found to rely on appearance heuristics while often violating basic mechanics. These gaps indicate that current scaling and training are still insufficient for robust physical understanding, underscoring PhysicsMind as a focused testbed for physics-aware multimodal models. Our data will be released upon acceptance.

</details>


### [52] [Keyframe-Based Feed-Forward Visual Odometry](https://arxiv.org/abs/2601.16020)
*Weichen Dai,Wenhan Su,Da Kong,Yuhang Ming,Wanzeng Kong*

Main category: cs.CV

TL;DR: 提出用RL学习的关键帧策略来改进视觉基础模型驱动的前馈VO，提升效率与精度，在TartanAir上训练并在多个真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Integrate keyframe selection into feed-forward visual foundation model based VO to reduce redundancy and improve accuracy, by using a learned policy via reinforcement learning instead of hand-crafted heuristics.

Method: 在现有前馈视觉基础模型（如VGGT-Long）上加入一个强化学习代理，该代理基于观察到的特征或中间表征决定是否将当前帧作为关键帧。代理在TartanAir数据集上训练，使用奖励函数鼓励提高帧间视差、减少计算成本和提升位姿/重建精度，并在多个真实数据集上进行评估。

Result: A keyframe-based feed-forward VO method where a reinforcement learning agent selects keyframes adaptively, trained on TartanAir, yielding consistent and substantial improvements over state-of-the-art feed-forward VO methods on multiple real-world datasets.

Conclusion: 通过将数据驱动的关键帧选择与视觉基础模型结合，能有效降低计算冗余并提升VO性能，证明了在保留前馈优势的同时融入几何启发式的可行性与优越性。

Abstract: The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.

</details>


### [53] [PAINT: Pathology-Aware Integrated Next-Scale Transformation for Virtual Immunohistochemistry](https://arxiv.org/abs/2601.16024)
*Rongze Ma,Mengkang Lu,Zhenyu Xiang,Yongsheng Pan,Yicheng Wu,Qingjie Zeng,Yong Xia*

Main category: cs.CV

TL;DR: PAINT通过结构优先的自回归生成和3S-Map初始化，从H&E图像更可靠地合成IHC染色，提升结构一致性和临床任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决H&E图像到IHC图像合成中的语义不一致和结构先验不足问题，通过结构先行的自回归生成提高生成的结构保真度和临床相关性。

Method: 提出视觉自回归框架PAINT，设计Spatial Structural Start Map(3S-Map)作为自回归初始结构约束；通过因果顺序在全局结构布局条件下逐步生成分子细节，并在IHC4BC和MIST数据集上进行比较实验。

Result: 提出PAINT框架，引入3S-Map作为自回归初始条件，将分子细节在全局结构布局条件下生成；在IHC4BC和MIST数据集上在结构保真度和下游临床任务上超越现有方法。

Conclusion: 结构引导的自回归建模有效缓解了直接图像翻译的语义错误，PAINT在实验中验证了该方法的优势，证明结构先验对虚拟IHC合成至关重要。

Abstract: Virtual immunohistochemistry (IHC) aims to computationally synthesize molecular staining patterns from routine Hematoxylin and Eosin (H\&E) images, offering a cost-effective and tissue-efficient alternative to traditional physical staining. However, this task is particularly challenging: H\&E morphology provides ambiguous cues about protein expression, and similar tissue structures may correspond to distinct molecular states. Most existing methods focus on direct appearance synthesis to implicitly achieve cross-modal generation, often resulting in semantic inconsistencies due to insufficient structural priors. In this paper, we propose Pathology-Aware Integrated Next-Scale Transformation (PAINT), a visual autoregressive framework that reformulates the synthesis process as a structure-first conditional generation task. Unlike direct image translation, PAINT enforces a causal order by resolving molecular details conditioned on a global structural layout. Central to this approach is the introduction of a Spatial Structural Start Map (3S-Map), which grounds the autoregressive initialization in observed morphology, ensuring deterministic, spatially aligned synthesis. Experiments on the IHC4BC and MIST datasets demonstrate that PAINT outperforms state-of-the-art methods in structural fidelity and clinical downstream tasks, validating the potential of structure-guided autoregressive modeling.

</details>


### [54] [ProGiDiff: Prompt-Guided Diffusion-Based Medical Image Segmentation](https://arxiv.org/abs/2601.16060)
*Yuan Lin,Murong Xu,Marc Hölle,Chinmay Prabhakar,Andreas Maier,Vasileios Belagiannis,Bjoern Menze,Suprosanna Shit*

Main category: cs.CV

TL;DR: ProGiDiff通过ControlNet风格条件化预训练扩散模型，从医学图像生成分割掩码，支持自然语言提示、多类分割、多个提议与跨模态少样本迁移，在CT分割上表现优良并能迁移至MR。


<details>
  <summary>Details</summary>
Motivation: 现有医用分割方法大多是确定性的且难以使用自然语言提示，无法生成多种分割提议、支持人机交互或易于跨模态迁移；而从头训练文本到图像扩散模型需大量数据，不现实，因此利用已有预训练生成模型并进行条件化以弥补上述不足。

Method: 基于预训练扩散生成模型，设计一个类似ControlNet的图像条件模块与定制编码器，将输入医学影像编码为条件信号并注入扩散模型中以生成分割掩码；多类分割通过自然语言提示目标器官实现；为跨模态迁移，采用低秩（low-rank）少样本适配方法对条件机制进行微调。

Result: 在CT器官分割实验中，ProGiDiff较先前方法取得较强性能，能提供多种分割提案并适用于专家在环场景；通过低秩少样本适配，学习到的条件机制可迁移用于MR图像分割。

Conclusion: 该论文提出了一种名为ProGiDiff的新框架，将预训练的文本到图像扩散模型用于医学图像分割，通过ControlNet风格的条件机制和自定义编码器引导扩散模型输出分割掩码，实现多类别分割并支持自然语言提示。方法在CT器官分割上表现良好，支持专家交互生成多个提议，并通过低秩少样本适配迁移到MR图像。

Abstract: Widely adopted medical image segmentation methods, although efficient, are primarily deterministic and remain poorly amenable to natural language prompts. Thus, they lack the capability to estimate multiple proposals, human interaction, and cross-modality adaptation. Recently, text-to-image diffusion models have shown potential to bridge the gap. However, training them from scratch requires a large dataset-a limitation for medical image segmentation. Furthermore, they are often limited to binary segmentation and cannot be conditioned on a natural language prompt. To this end, we propose a novel framework called ProGiDiff that leverages existing image generation models for medical image segmentation purposes. Specifically, we propose a ControlNet-style conditioning mechanism with a custom encoder, suitable for image conditioning, to steer a pre-trained diffusion model to output segmentation masks. It naturally extends to a multi-class setting simply by prompting the target organ. Our experiment on organ segmentation from CT images demonstrates strong performance compared to previous methods and could greatly benefit from an expert-in-the-loop setting to leverage multiple proposals. Importantly, we demonstrate that the learned conditioning mechanism can be easily transferred through low-rank, few-shot adaptation to segment MR images.

</details>


### [55] [DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models](https://arxiv.org/abs/2601.16065)
*Chenyang Li,Jieyuan Liu,Bin Li,Bo Gao,Yilin Yuan,Yangfan He,Yuchen Li,Jingqun Tang*

Main category: cs.CV

TL;DR: 提出DTP：在推理时检测并剪枝图像中干扰token，简单易用且不改模型，能普遍提升VLA模型在机器人操作任务上的成功率。


<details>
  <summary>Details</summary>
Motivation: 发现现有VLA模型容易将注意力放在任务无关的图像区域（“干扰token”），干扰动作生成，降低任务成功率，故希望通过校正注意力模式来提高性能并探索模型性能上界。

Method: DTP在推理阶段作为即插即用模块工作，不改变原始模型结构或输入，通过评估注意力集中于图像各区域的程度来识别“干扰token”，并在生成动作时动态剪除这些token以减少对无关区域的关注。

Result: 在SIMPLER基准上，DTP在多种新型transformer-based VLA模型上均带来了任务成功率的相对提升；进一步分析表明，任务成功率与对无关区域注意力量存在负相关关系。

Conclusion: 本文提出了Distracting Token Pruning (DTP)框架，通过动态检测并剪枝任务无关的图像token，校正视觉注意力分布，从而提升VLA模型在机器人操作任务中的成功率。

Abstract: Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as 'distracting tokens'. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model's visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.

</details>


### [56] [DSFedMed: Dual-Scale Federated Medical Image Segmentation via Mutual Distillation Between Foundation and Lightweight Models](https://arxiv.org/abs/2601.16073)
*Hanwen Zhang,Qiaojin Shen,Yuxi Liu,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: DSFedMed通过生成数据、样本选择和双向知识蒸馏，在联邦医疗图像分割中以极低通信和推理成本显著提升轻量客户端表现，并略微改善中心大模型。


<details>
  <summary>Details</summary>
Motivation: 现有Foundation Models在联邦场景下部署受限于计算、通信和推理成本；需要一种能在资源受限的客户端上保持性能的高效方法。

Method: 框架通过生成高质量医疗图像替代真实公共数据集、采用可学习性引导的样本选择策略，并在双尺度（中心化Foundation Model与轻量客户端模型）上进行互蒸馏，从而实现知识的双向传递。

Result: 在五个医学图像分割数据集上的评估显示，DSFedMed在平均Dice得分上提升约2%，同时相比已有的联邦Foundation Model基线减少近90%的通信成本和推理时间，展示了在资源受限场景下的效率和可扩展性。

Conclusion: 该论文提出了DSFedMed，一种在联邦学习环境中实现大模型与轻量客户端模型之间双向知识蒸馏的框架，旨在医疗图像分割任务中降低通信和推理开销并提升性能。

Abstract: Foundation Models (FMs) have demonstrated strong generalization across diverse vision tasks. However, their deployment in federated settings is hindered by high computational demands, substantial communication overhead, and significant inference costs. We propose DSFedMed, a dual-scale federated framework that enables mutual knowledge distillation between a centralized foundation model and lightweight client models for medical image segmentation. To support knowledge distillation, a set of high-quality medical images is generated to replace real public datasets, and a learnability-guided sample selection strategy is proposed to enhance efficiency and effectiveness in dual-scale distillation. This mutual distillation enables the foundation model to transfer general knowledge to lightweight clients, while also incorporating client-specific insights to refine the foundation model. Evaluations on five medical imaging segmentation datasets show that DSFedMed achieves an average 2 percent improvement in Dice score while reducing communication costs and inference time by nearly 90 percent compared to existing federated foundation model baselines. These results demonstrate significant efficiency gains and scalability for resource-limited federated deployments.

</details>


### [57] [Masked Modeling for Human Motion Recovery Under Occlusions](https://arxiv.org/abs/2601.16079)
*Zhiyin Qian,Siwei Zhang,Bharat Lal Bhatnagar,Federica Bogo,Siyu Tang*

Main category: cs.CV

TL;DR: MoRo用masked modeling和跨模态先验学习，在有遮挡的单目视频中实现了准确、真实且实时的人体动作重建。


<details>
  <summary>Details</summary>
Motivation: 现有回归方法对缺失观测敏感，优化和扩散方法稳健但推理慢且预处理复杂。需要一种既稳健又高效的单目视频人体运动重建方法，特别是在真实场景频繁遮挡的情况下。

Method: 提出跨模态学习方案：1) 轨迹感知的动作先验在MoCap数据上训练；2) 基于图像的姿态先验在图像-姿态数据上训练以捕捉多样的逐帧姿态；3) 视频条件的masked transformer融合上述先验并在视频-动作数据上微调，实现对视觉提示和运动动态的整合。整体采用masked modeling处理遮挡，端到端推理，70FPS实时性能。

Result: 在EgoBody和RICH数据集上，MoRo在遮挡场景下在准确性和动作真实度上显著超越当前最优方法，在非遮挡场景表现相当，并能在单张H200 GPU上达到70FPS实时推理。

Conclusion: MoRo提出了一种基于masked modeling的端到端生成框架，能在遮挡条件下从单目视频高效且稳健地重建人体运动，并在精度与运动真实性上优于现有方法，同时保持实时推理性能。

Abstract: Human motion reconstruction from monocular videos is a fundamental challenge in computer vision, with broad applications in AR/VR, robotics, and digital content creation, but remains challenging under frequent occlusions in real-world settings.Existing regression-based methods are efficient but fragile to missing observations, while optimization- and diffusion-based approaches improve robustness at the cost of slow inference speed and heavy preprocessing steps. To address these limitations, we leverage recent advances in generative masked modeling and present MoRo: Masked Modeling for human motion Recovery under Occlusions. MoRo is an occlusion-robust, end-to-end generative framework that formulates motion reconstruction as a video-conditioned task, and efficiently recover human motion in a consistent global coordinate system from RGB videos. By masked modeling, MoRo naturally handles occlusions while enabling efficient, end-to-end inference. To overcome the scarcity of paired video-motion data, we design a cross-modality learning scheme that learns multi-modal priors from a set of heterogeneous datasets: (i) a trajectory-aware motion prior trained on MoCap datasets, (ii) an image-conditioned pose prior trained on image-pose datasets, capturing diverse per-frame poses, and (iii) a video-conditioned masked transformer that fuses motion and pose priors, finetuned on video-motion datasets to integrate visual cues with motion dynamics for robust inference. Extensive experiments on EgoBody and RICH demonstrate that MoRo substantially outperforms state-of-the-art methods in accuracy and motion realism under occlusions, while performing on-par in non-occluded scenarios. MoRo achieves real-time inference at 70 FPS on a single H200 GPU.

</details>


### [58] [SAMTok: Representing Any Mask with Two Words](https://arxiv.org/abs/2601.16093)
*Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li*

Main category: cs.CV

TL;DR: SAMTok把掩码转换为两个离散token，使得基础MLLM无需架构改动即可学习像素级能力，通过大规模掩码训练+RL在多项像素级任务上取得优秀表现。


<details>
  <summary>Details</summary>
Motivation: 现有像素级多模态LLM难以扩展，原因在于复杂的区域编码器、专门的分割解码器及不兼容的训练目标。作者希望通过将掩码离散化为语言token，简化训练流程并易于扩展。

Method: 基于SAM2，使用掩码编码器与残差矢量量化器在2.09亿多样化掩码上训练，生成离散、紧凑且信息丰富的token；利用5M SAMTok格式的数据对QwenVL系列进行微调并结合文本答案匹配奖励进行高效RL以改善掩码生成。

Result: QwenVL-SAMTok在区域描述、区域VQA、基于引导的对话、指涉分割、场景图解析和多轮交互分割等多项任务上取得了SOTA或可比结果。引入的文本答案匹配奖励在GRES和GCG基准上显著提升掩码生成效果。

Conclusion: 该工作提出了SAMTok，一种将任意区域掩码编码为两个离散特殊token并高保真重建的掩码tokenizer，从而将掩码视为语言token，允许基础多模态大模型通过标准下一个token预测与简单强化学习学习像素级能力，无需修改模型架构或特殊损失。

Abstract: Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.

</details>


### [59] [Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification](https://arxiv.org/abs/2601.16098)
*Zack Dewis,Yimin Zhu,Zhengsen Xu,Mabel Heffring,Saeid Taleghanidoozdoozan,Quinn Ledingham,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: CSSMamba通过聚类引导的空间模块、光谱模块、注意力令牌选择与可学习聚类，生成更短且更有判别力的Mamba令牌序列，从而提升HSI分类性能并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba模型在HSI分类上表现良好但面临令牌序列定义效率和自适应性不足的问题；长序列影响计算效率和特征学习，且空间与光谱信息的协同学习需要改进。因此引入聚类与注意力机制以得到更短、更具判别性的令牌序列，并通过可学习聚类自适应数据分布。

Method: 提出了四个关键模块：1) Cluster-guided Spatial Mamba（CSpaMamba）通过聚类减少空间令牌序列长度并增强特征学习；2) Spectral Mamba（SpeMamba）用于学习光谱信息；3) Attention-Driven Token Selection（ADTS）优化令牌选择以提高关键区域的表征；4) Learnable Clustering Module（LCM）以自适应方式学习簇归属，并将其无缝整合进Mamba架构。整体框架将CSpaMamba与SpeMamba串/并联组合，经过训练在三个公开HSI数据集上验证。

Result: 在Pavia University、Indian Pines和Liao-Ning 01上，CSSMamba在总体精度和边界保持上优于多种基线（包括CNN、Transformer和先前Mamba方法）；报告了更高的分类准确率、抑制噪声和更清晰的边界细节（具体数值未提供）。

Conclusion: 该文提出了CSSMamba（聚类引导的时空光谱Mamba）框架，通过将聚类机制嵌入空间Mamba并结合光谱Mamba、注意力驱动的令牌选择和可学习聚类模块，旨在提高HSI分类的效率与性能；实验证明在Pavia University、Indian Pines和Liao-Ning 01数据集上优于现有CNN、Transformer和Mamba方法。

Abstract: Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.

</details>


### [60] [Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing](https://arxiv.org/abs/2601.16125)
*Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu*

Main category: cs.CV

TL;DR: EDIR is a new fine-grained CIR benchmark with 5,000 edited queries (5 main categories, 15 subcategories) synthesized via controlled image editing; evaluations of 13 models reveal major gaps and benchmark limitations; in-domain training shows some categories are learnable while others expose model limitations.


<details>
  <summary>Details</summary>
Motivation: Construct a more comprehensive and fine-grained benchmark for Composed Image Retrieval (CIR) by leveraging image editing to synthesize diverse, controlled queries, addressing the limitations of existing benchmarks in category coverage and realism.

Method: Use image editing to precisely control modification types and content, synthesize queries across broad categories, assemble 5,000 queries into EDIR, evaluate 13 multimodal embedding models, analyze results and limitations, and run in-domain training experiments to assess learnability per category.

Result: Created EDIR: 5,000 high-quality queries across 5 main categories and 15 subcategories; comprehensive evaluation of 13 multimodal embedding models showing performance gaps; identified limitations in prior benchmarks; in-domain training experiment demonstrating feasibility and distinguishing solvable vs. intrinsic-challenge categories.

Conclusion: EDIR offers a rigorous, diverse benchmark that reveals significant weaknesses in current CIR models and benchmarks; it can guide future model development and dataset design by highlighting which categories require architectural advances versus more data.

Abstract: Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.

</details>


### [61] [Learning to Watermark in the Latent Space of Generative Models](https://arxiv.org/abs/2601.16140)
*Sylvestre-Alvise Rebuffi,Tuan Tran,Valeriu Lacatusu,Pierre Fernandez,Tomáš Souček,Nikola Jovanović,Tom Sander,Hady Elsahar,Alexandre Mourachko*

Main category: cs.CV

TL;DR: DistSeal trains latent-space post-hoc watermarkers for generative models, which can be distilled into the model or decoder to provide efficient, robust, and imperceptible watermarks for diffusion and autoregressive models, with up to 20x speedup


<details>
  <summary>Details</summary>
Motivation: Pixel-space watermarking is slow and can introduce artifacts; latent-space offers efficiency and imperceptibility across diffusion and autoregressive models

Method: Latent space watermarking with DistSeal

Result: Latent watermarkers distilled into models or decoders achieve competitive robustness, similar imperceptibility, and up to 20x speedup versus pixel-space baselines; distilling latent watermarkers outperforms distilling pixel-space ones

Conclusion: Latent watermarking via DistSeal is a more efficient and robust alternative to pixel-space watermarking and supports effective in-model watermarking through distillation

Abstract: Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.

</details>


### [62] [ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion](https://arxiv.org/abs/2601.16148)
*Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier*

Main category: cs.CV

TL;DR: 引入temporal 3D diffusion与时序自编码器，ActionMesh能从多种输入快速生成拓扑一致、无绑定且高质量的动画3D网格，并在标准数据集上取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有生成动画3D对象方法在应用中受限的问题，例如设置复杂、推理慢、质量受限，以及拓扑不一致或需要绑定（rigging）。目标是实现快速、无绑定、拓扑一致且高质量的3D动画生成。

Method: 方法包括两个关键模块：1) 将现有3D扩散模型扩展到时间维度（temporal 3D diffusion），生成一系列同步的时变3D潜编码；2) 设计一个时序3D自编码器，将这些独立形状序列翻译为预定义参考形体的形变，从而生成动画网格。结合这两部分，模型可从视频、文本或带提示的3D网格生成动画。

Result: 在Consistent4D和Objaverse等视频到4D基准上，ActionMesh在几何精确度和时间一致性上达到了最先进的性能，并实现了比以往方法更快的生成速度，且输出为拓扑一致的无绑定网格，便于上色和重定向等后处理。

Conclusion: ActionMesh提出了一个能够快速生成带动作的生产级3D网格的生成模型，兼顾质量、速度和一致性。

Abstract: Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes "in action" in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed "temporal 3D diffusion". Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.

</details>


### [63] [HVD: Human Vision-Driven Video Representation Learning for Text-Video Retrieval](https://arxiv.org/abs/2601.16155)
*Zequn Xie,Xin Liu,Boyun Zhang,Yuxiao Lin,Sihang Cai,Tao Jin*

Main category: cs.CV

TL;DR: 提出模仿人类视觉的HVD模型，先选帧再聚合patch到实体，以减少噪声并实现精确匹配，显著提升文本-视频检索效果并达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前方法在文本稀疏时对关键视觉信息判断困难，存在“盲”特征交互问题；受人类视觉认知（宏观先行、微观精细）的启发来改进检索模型。

Method: 提出两级模块：FFSM用于关键帧选择以去除时间冗余；PFCM通过注意力机制将patch特征压缩为显著视觉实体以实现实体级匹配；基于CLIP特征开展跨模态对齐与检索训练。

Result: 在五个基准数据集上进行广泛实验，表明HVD在捕捉人类关注点的同时实现了最先进的性能（SOTA）。

Conclusion: HVD通过模拟人类的宏观和微观视觉注意力，提出FFSM和PFCM实现粗到细对齐，从而提升文本-视频检索效果。

Abstract: The success of CLIP has driven substantial progress in text-video retrieval. However, current methods often suffer from "blind" feature interaction, where the model struggles to discern key visual information from background noise due to the sparsity of textual queries. To bridge this gap, we draw inspiration from human cognitive behavior and propose the Human Vision-Driven (HVD) model. Our framework establishes a coarse-to-fine alignment mechanism comprising two key components: the Frame Features Selection Module (FFSM) and the Patch Features Compression Module (PFCM). FFSM mimics the human macro-perception ability by selecting key frames to eliminate temporal redundancy. Subsequently, PFCM simulates micro-perception by aggregating patch features into salient visual entities through an advanced attention mechanism, enabling precise entity-level matching. Extensive experiments on five benchmarks demonstrate that HVD not only captures human-like visual focus but also achieves state-of-the-art performance.

</details>


### [64] [360Anything: Geometry-Free Lifting of Images and Videos to 360°](https://arxiv.org/abs/2601.16192)
*Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena*

Main category: cs.CV

TL;DR: 360Anything通过将输入与目标视作token序列，用预训练扩散Transformer在无相机元数据下学习透视->ERP映射，并通过Circular Latent Encoding解决ERP接缝问题，达成SOTA生成与强几何推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖透视与ERP间的显式几何对齐和相机标定，但在野外数据中相机元数据通常缺失或不准，限制应用；希望构建一个无需相机信息也能生成高质量360°全景的系统。

Method: 将透视输入与目标ERP全景视为token序列，使用纯数据驱动的透视到equirectangular映射学习，避免相机信息依赖；识别并修正VAE编码器在ERP边界的零填充问题，提出Circular Latent Encoding以实现无缝生成。

Result: 在图像和视频透视到360°生成任务上取得SOTA性能，优于使用真实相机信息的先前方法；修复ERP边界接缝伪影；在零样本相机FoV与朝向估计基准上表现竞争力，显示出深层几何理解。

Conclusion: 本文提出360Anything，一种基于预训练扩散Transformer的无几何框架，能在无相机元数据情况下将透视图像/视频提升为360°全景。

Abstract: Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything's deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.

</details>


### [65] [Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2601.16208)
*Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie*

Main category: cs.CV

TL;DR: 在大规模T2I任务中，简化后的RAE优于VAE，收敛更快、微调更稳、生成质量更好，并支持视觉理解与生成共享表示空间。


<details>
  <summary>Details</summary>
Motivation: 检验在ImageNet上成功的RAE框架能否扩展到大规模自由形式T2I生成，并比较RAE与VAE在不同规模与数据条件下的表现。

Method: 在冻结的SigLIP-2编码器上扩展RAE解码器，训练数据包含网络图像、合成图像与文本渲染，并系统化测试RAE的设计选择与不同规模的扩散Transformer（0.5B-9.8B），与FLUX VAE进行对比，评估预训练与微调表现。

Result: 规模化带来整体质量提升，但特定域（如文本）需针对性数据；许多复杂设计在大规模下无显著益处；RAE在预训练与微调上均优于VAE，微调时VAE易发生灾难性过拟合。

Conclusion: RAE在大规模文本到图像生成任务中表现优于VAE，具有更快收敛和更稳定的微调行为，且框架可简化。

Abstract: Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.

</details>


### [66] [PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation](https://arxiv.org/abs/2601.16210)
*Onkar Susladkar,Tushar Prakash,Adheesh Juvekar,Kiet A. Nguyen,Dong-Hwan Jang,Inderjit S Dhillon,Ismini Lourentzou*

Main category: cs.CV

TL;DR: PyraTok通过语言对齐的金字塔量化在多尺度学习离散视频表征，显著提升了跨模态对齐、重建与零样本下游任务表现，并支持超高分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有视频tokenizer通常只在单一尺度、较小词表、缺乏深入语言监督下学习码本，导致跨模态对齐差和零样本泛化能力弱，不能很好支持高分辨率和多任务应用。

Method: 基于预训练视频VAE，提出LaPQ模块在多个编码深度使用共享大二进制码本进行金字塔量化，并联合优化多尺度文本引导量化与全局自回归目标，以生成层次化的视频token序列。

Result: 在十个基准测试上，PyraTok实现了SOTA的视频重建结果，持续提升文本生成视频质量，并在视频分割、时间动作定位和视频理解的零样本性能上创下新SOTA，能稳健扩展到4K/8K分辨率。

Conclusion: PyraTok通过多尺度、语言对齐的离散化方法改进视频VAE的tokenizer，实现了更好的跨模态对齐和零样本迁移能力。

Abstract: Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.

</details>


### [67] [Why Can't I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition](https://arxiv.org/abs/2601.16211)
*Geo Ahn,Inwoong Lee,Taeoh Kim,Minho Shim,Dongyoon Wee,Jinwoo Choi*

Main category: cs.CV

TL;DR: 作者发现现有 ZS-CAR 模型会形成“物体驱动的动词捷径”，由监督稀疏/偏斜与动词/物体学习难度不对称导致。提出 RCORE：组合感知的数据增强+时间顺序正则化，强制模型学习基于时序的动词特征，从而改善未见组合的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有 ZS-CAR 在未见动宾组合上泛化差，主要因为训练数据组合稀疏且偏斜，且模型更容易学习物体而非动词，导致学习过程中依赖共现统计而忽视动作的时序证据。

Method: RCORE 包含两部分：(1) 组合感知增强：在保留运动线索的同时多样化动宾组合；(2) 时间顺序正则化损失：显式建模时序结构，惩罚基于物体共现的捷径行为。

Result: RCORE 框架显著提高了零样本组合动作识别中对未见动宾组合的识别准确率，减少了模型对共现偏差的依赖，并在两个基准上实现了稳定的组合增益。

Conclusion: 解决物体驱动的捷径是实现稳健视频组合理解的关键；通过增强动词时序信号并抑制共现捷径，RCORE 在未见组合上取得明显提升。

Abstract: We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.

</details>


### [68] [CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback](https://arxiv.org/abs/2601.16214)
*Wenhang Ge,Guibao Shen,Jiawei Feng,Luozhou Wang,Hao Lu,Xingye Tian,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出CamPilot：将视频潜变量与相机位姿解码为3D高斯以进行几何感知的奖励计算，通过像素级新视图一致性和可见性项改进摄像机可控性，实验证明有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于奖励反馈学习（ReFL）的方法难以直接提升视频与相机的对齐性，主要由于（1）奖励模型无法评估视频-相机对齐；（2）将潜变量解码为RGB视频用于奖励计算开销大；（3）解码过程忽略3D几何信息。为了解决这些限制，提出在解码阶段引入3D几何表示以高效计算与相机相关的奖励。

Method: 设计了一个高效的相机感知3D解码器，将视频潜变量和相机位姿解码为3D高斯体；利用相机位姿既作为输入又作为投影参数，解码后渲染新视图并与真实视图做像素级一致性度量作为奖励；引入可见性项以只监督通过几何配准得到的确定性区域，避免随机性带来的误导。

Result: 在RealEstate10K和WorldScore两个基准上进行了大量实验，结果表明所提方法在摄像机可控性和视频-相机对齐方面优于现有方法（论文提供了项目页面）。

Conclusion: 本论文提出了一种通过将视频潜变量与相机位姿共同解码为3D高斯表示来改进摄像机可控性的框架，并基于渲染视图与真实视图的一致性作为奖励信号来进行优化，从而提升视频-相机对齐能力。

Abstract: Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [NL4ST: A Natural Language Query Tool for Spatio-Temporal Databases](https://arxiv.org/abs/2601.15758)
*Xieyang Wang,Mengyi Liu,Weijia Yi,Jianqiu Xu,Raymond Chi-Wing Wong*

Main category: cs.DB

TL;DR: NL4ST是一个将自然语言转为时空数据库物理执行计划的交互系统，采用知识库、NLU与物理计划生成三层架构，并在多数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 移动设备与定位技术推动了海量时空数据增长，但非专业用户难以用可执行查询语言构造复杂的时空查询，因而需要自然语言查询支持。

Method: 系统采用三层架构：知识库与语料构建用于准备领域知识；自然语言理解模块负责实体识别与链接；物理计划生成层将语义结构转为数据库可执行的物理计划并支持验证。

Result: NL4ST能为范围查询、最近邻查询、连接查询等生成有效的时空物理计划，并在四个真实与合成数据集上进行验证，系统在线并提供演示视频。

Conclusion: NL4ST通过将自然语言解析、实体链接和生成物理执行计划三部分结合，成功把非专业用户的自然语言查询映射为可执行的时空数据库查询计划。

Abstract: The advancement of mobile computing devices and positioning technologies has led to an explosive growth of spatio-temporal data managed in databases. Representative queries over such data include range queries, nearest neighbor queries, and join queries. However, formulating those queries usually requires domain-specific expertise and familiarity with executable query languages, which would be a challenging task for non-expert users. It leads to a great demand for well-supported natural language queries (NLQs) in spatio-temporal databases. To bridge the gap between non-experts and query plans in databases, we present NL4ST, an interactive tool that allows users to query spatio-temporal databases in natural language. NL4ST features a three-layer architecture: (i) knowledge base and corpus for knowledge preparation, (ii) natural language understanding for entity linking, and (iii) generating physical plans. Our demonstration will showcase how NL4ST provides effective spatio-temporal physical plans, verified by using four real and synthetic datasets. We make NL4ST online and provide the demo video at https://youtu.be/-J1R7R5WoqQ.

</details>


### [70] [Efficient Cloud-edge Collaborative Approaches to SPARQL Queries over Large RDF graphs](https://arxiv.org/abs/2601.15992)
*Shidan Ma,Peng Peng,Xu Zhou,M. Tamer Özsu,Lei Zou,Guo Chen*

Main category: cs.DB

TL;DR: 将RDF/SPARQL查询处理下沉到边缘，提出模式诱导子图用于数据定位，并用MINLP联合优化查询分配与资源分配，采用改进分支定界法求解；在真实平台实验中优于基线，代码开源。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备增多与带宽受限或系统高负载的环境下，传统云端RDF/SPARQL处理方案存在性能瓶颈，论文提出将数据与计算迁移到边缘以降低网络延迟与带宽占用，从而提升查询性能。

Method: 提出了模式诱导子图（pattern-induced subgraphs）用于高效数据定位，构建联合考虑数据分布、查询特征、网络通信与计算资源的系统模型，并将查询分配与资源分配问题建模为混合整数非线性规划（MINLP），采用改进的分支定界法求解。实验在真实数据集与云平台上验证方法有效性；并公开了代码。

Result: 实验结果显示，所提方法在查询效率上优于现有最先进基线方法，并在真实云平台上表现出更低的响应时间与更好的资源利用率。

Conclusion: 本论文通过将RDF图数据的存储与查询处理下沉到边缘计算环境，显著提升了SPARQL查询在受限带宽或高负载场景下的性能。研究表明，结合图模式诱导子图的数据定位方法与联合查询分配与计算资源分配的网络调度模型，可在真实云平台上优于现有方法。

Abstract: With the increasing use of RDF graphs, storing and querying such data using SPARQL remains a critical problem. Current mainstream solutions rely on cloud-based data management architectures, but often suffer from performance bottle- necks in environments with limited bandwidth or high system load. To address this issue, this paper explores for the first time the integration of edge computing to move graph data storage and processing to edge environments, thereby improving query performance. This approach requires offloading query processing to edge servers, which involves addressing two challenges: data localization and network scheduling. First, the data localization challenge lies in computing the subgraphs maintained on edge servers to quickly identify the servers that can handle specific queries. To address this challenge, we introduce a new concept of pattern-induced subgraphs. Second, the network scheduling challenge involves efficiently assigning queries to edge and cloud servers to optimize overall system performance. We tackle this by constructing a overall system model that jointly captures data distribution, query characteristics, network communication, and computational resources. Accordingly, we further propose a joint formulation of query assignment and computational resource allocation, modeling it as a Mixed Integer Nonlinear Programming (MINLP) problem and solve this problem using a modified branch-and-bound algorithm. Experimental results on real datasets under a real cloud platform demonstrate that our proposed method outperforms the state-of-the-art baseline methods in terms of efficiency. The codes are available on GitHub

</details>


### [71] [EAIFD: A Fast and Scalable Algorithm for Incremental Functional Dependency Discovery](https://arxiv.org/abs/2601.16025)
*Yajuan Xu,Xixian Han,Xiaolong Wan*

Main category: cs.DB

TL;DR: EAIFD通过差集超图与最小打靶集枚举、MHT与两步校验策略，实现了快速且低内存的增量FD发现，在真实数据上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有静态FD发现需全量重跑，低效；现有增量方法在性能与内存上存在严重瓶颈，需要一种既避免全量重跑又能节省内存与I/O的增量FD发现算法。

Method: 提出EAIFD算法：维护差集的部分超图，使用最小打靶集枚举进行候选生成；设计多属性哈希表(MHT)用于高频键值映射以压缩内存；采用两步校验策略，先用MHT缩减校验空间，再按数据块选择性加载进行批量校验以减少重复I/O。

Result: 在真实数据集上实验表明，EAIFD在运行时间上可达最多一个数量级的加速，内存使用减少超过两个数量级，展现出优秀的扩展性与效率。

Conclusion: EAIFD通过维护差集的部分超图并将增量FD发现转化为超图上最小打靶集枚举，避免了全量重跑，从而在运行时间和内存消耗上均显著优于现有方法。

Abstract: Functional dependencies (FDs) are fundamental integrity constraints in relational databases, but discovering them under incremental updates remains challenging. While static algorithms are inefficient due to full re-execution, incremental algorithms suffer from severe performance and memory bottlenecks. To address these challenges, this paper proposes EAIFD, a novel algorithm for incremental FD discovery. EAIFD maintains the partial hypergraph of difference sets and reframes the incremental FD discovery problem into minimal hitting set enumeration on hypergraph, avoiding full re-runs. EAIFD introduces two key innovations. First, a multi-attribute hash table ($MHT$) is devised for high-frequency key-value mappings of valid FDs, whose memory consumption is proven to be independent of the dataset size. Second, two-step validation strategy is developed to efficiently validate the enumerated candidates, which leverages $MHT$ to effectively reduce the validation space and then selectively loads data blocks for batch validation of remaining candidates, effectively avoiding repeated I/O operations. Experimental results on real-world datasets demonstrate the significant advantages of EAIFD. Compared to existing algorithms, EAIFD achieves up to an order-of-magnitude speedup in runtime while reducing memory usage by over two orders-of-magnitude, establishing it as a highly efficient and scalable solution for incremental FD discovery.

</details>
