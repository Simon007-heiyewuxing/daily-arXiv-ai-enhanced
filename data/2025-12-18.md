<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.DB](#cs.DB) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SkyCap: Bitemporal VHR Optical-SAR Quartets for Amplitude Change Detection and Foundation-Model Evaluation](https://arxiv.org/abs/2512.14755)
*Paul Weinmann,Ferdinand Schenck,Martin Šiklar*

Main category: cs.CV

TL;DR: 构建SkyCap光学-SAR配对数据集并用标签迁移生成SAR变化标签；评测多种基模型与预处理，发现光学基模型在适当预处理下在VHR SAR变化检测上表现最好，且预处理对齐与预训练统计至关重要。


<details>
  <summary>Details</summary>
Motivation: 提出将光学VHR与SAR数据结合以克服云遮挡与注释困难，实现线性基础设施监测的可靠变化检测。

Method: 构建SkyCap双时相数据集（SkySat光学与Capella SAR），通过光学到SAR标签迁移获得SAR振幅变化检测标签，无需SAR专家标注；对SARATR-X进行继续预训练并在不同预处理下将SAR特定基模型与光学基模型在SkyCap上进行基准评测。

Result: 在测试中，光学基模型MTP(ViT-B+RVSA)配合dB+Z-score预处理取得最佳F1_c=45.06，优于直接在Capella数据上继续预训练的SAR特定基模型。发现模型对与预训练统计对齐的预处理敏感，光学模型在光学变化检测的排序不能直接迁移到SAR振幅变化检测。

Conclusion: 首次在VHR SAR振幅变化检测上评估基模型，展示了通过光学到SAR的标签迁移及预处理对齐的重要性，并表明在该任务中某些光学基模型优于SAR专用模型。

Abstract: Change detection for linear infrastructure monitoring requires reliable high-resolution data and regular acquisition cadence. Optical very-high-resolution (VHR) imagery is interpretable and straightforward to label, but clouds break this cadence. Synthetic Aperture Radar (SAR) enables all-weather acquisitions, yet is difficult to annotate. We introduce SkyCap, a bitemporal VHR optical-SAR dataset constructed by archive matching and co-registration of (optical) SkySat and Capella Space (SAR) scenes. We utilize optical-to-SAR label transfer to obtain SAR amplitude change detection (ACD) labels without requiring SAR-expert annotations. We perform continued pretraining of SARATR-X on our SAR data and benchmark the resulting SAR-specific foundation models (FMs) together with SARATR-X against optical FMs on SkyCap under different preprocessing choices. Among evaluated models, MTP(ViT-B+RVSA), an optical FM, with dB+Z-score preprocessing attains the best result (F1$_c$ = 45.06), outperforming SAR-specific FMs further pretrained directly on Capella data. We observe strong sensitivity to preprocessing alignment with pretraining statistics, and the ranking of optical models on optical change detection does not transfer one-to-one to SAR ACD. To our knowledge, this is the first evaluation of foundation models on VHR SAR ACD.

</details>


### [2] [SocialNav-MoE: A Mixture-of-Experts Vision Language Model for Socially Compliant Navigation with Reinforcement Fine-Tuning](https://arxiv.org/abs/2512.14757)
*Tomohito Kawabata,Xinyu Zhang,Ling Xiao*

Main category: cs.CV

TL;DR: 为资源受限机器人提出小型Mixture-of-Experts VLM（SocialNav-MoE）并引入语义相似性奖励，通过强化微调提升社交合规导航性能，兼顾准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 现有社交合规导航研究多聚焦于安全，忽视人类舒适度和社交规范；大型VLM虽有潜力但计算开销大，难以在移动机器人上实时运行，故需研究小型高效模型及优化策略。

Method: 提出小型MoE架构的VLM（SocialNav-MoE），结合强化微调（RFT）和语义相似性奖励（SSR）；比较不同小型语言模型（Phi、Qwen、StableLM）、路由策略和视觉编码器（CLIP与SigLIP，及其是否微调）；在SNEI数据集上进行性能与效率对比实验。

Result: 提出了一种用于社交合规导航的高效小型VLM模型 SocialNav-MoE，通过专家混合机制和强化微调来在有限计算资源上实现低延迟的实时部署。

Conclusion: SocialNav-MoE 在SNEI数据集上实现了导航精度与效率的良好权衡；语义相似性奖励（SSR）在强化微调中优于硬级别和字符级奖励；不同小型语言模型、路由策略及视觉编码器的选择会显著影响性能。

Abstract: For robots navigating in human-populated environments, safety and social compliance are equally critical, yet prior work has mostly emphasized safety. Socially compliant navigation that accounts for human comfort, social norms, and contextual appropriateness remains underexplored. Vision language models (VLMs) show promise for this task; however, large-scale models incur substantial computational overhead, leading to higher inference latency and energy consumption, which makes them unsuitable for real-time deployment on resource-constrained robotic platforms. To address this issue, we investigate the effectiveness of small VLM and propose SocialNav-MoE, an efficient Mixture-of-Experts vision language model for socially compliant navigation with reinforcement fine-tuning (RFT). We further introduce a semantic similarity reward (SSR) to effectively leverage RFT for enhancing the decision-making capabilities. Additionally, we study the effectiveness of different small language model types (Phi, Qwen, and StableLM), routing strategies, and vision encoders (CLIP vs. SigLIP, frozen vs. fine-tuned). Experiments on the SNEI dataset demonstrate that SocialNav-MoE achieves an excellent balance between navigation accuracy and efficiency. The proposed SSR function is more effective than hard-level and character-level rewards. Source code will be released upon acceptance.

</details>


### [3] [The Renaissance of Expert Systems: Optical Recognition of Printed Chinese Jianpu Musical Scores with Lyrics](https://arxiv.org/abs/2512.14758)
*Fan Bu,Rongfeng Li,Zijin Li,Ya Li,Linfeng Fan,Pei Huang*

Main category: cs.CV

TL;DR: Developed interpretable hybrid OMR pipeline for Chinese Jianpu; digitized >5,000 songs (melody) and >1,400 with lyrics; strong accuracy: notes F1 0.951, lyrics F1 0.931.


<details>
  <summary>Details</summary>
Motivation: OMR research is Western-focused; Chinese Jianpu with lyrics is underexplored despite rich resources; need scalable digitization without massive labeled data.

Method: Top-down expert-system pipeline combining traditional computer-vision techniques (phrase correlation, skeleton analysis) and unsupervised deep-learning image embeddings to detect notation and align lyrics, producing MusicXML and MIDI.

Result: A modular expert-system pipeline converting printed Jianpu scores with lyrics into MusicXML and MIDI, using traditional CV plus unsupervised deep-learning embeddings, no large annotated data needed.

Conclusion: Hybrid top-down expert system achieves high-precision OMR for Chinese Jianpu: note-wise F1 0.951 and lyric character-wise F1 0.931 on large dataset, enabling mass digitization.

Abstract: Large-scale optical music recognition (OMR) research has focused mainly on Western staff notation, leaving Chinese Jianpu (numbered notation) and its rich lyric resources underexplored. We present a modular expert-system pipeline that converts printed Jianpu scores with lyrics into machine-readable MusicXML and MIDI, without requiring massive annotated training data. Our approach adopts a top-down expert-system design, leveraging traditional computer-vision techniques (e.g., phrase correlation, skeleton analysis) to capitalize on prior knowledge, while integrating unsupervised deep-learning modules for image feature embeddings. This hybrid strategy strikes a balance between interpretability and accuracy. Evaluated on The Anthology of Chinese Folk Songs, our system massively digitizes (i) a melody-only collection of more than 5,000 songs (> 300,000 notes) and (ii) a curated subset with lyrics comprising over 1,400 songs (> 100,000 notes). The system achieves high-precision recognition on both melody (note-wise F1 = 0.951) and aligned lyrics (character-wise F1 = 0.931).

</details>


### [4] [AquaDiff: Diffusion-Based Underwater Image Enhancement for Addressing Color Distortion](https://arxiv.org/abs/2512.14760)
*Afrah Shaahid,Muzammil Behzad*

Main category: cs.CV

TL;DR: AquaDiff利用色度先验引导的条件扩散与多尺度注意力去噪，并通过跨域一致性损失联合多重约束，实现了优越的水下色彩校正与竞争性的图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决水下图像因波长相关吸收和散射导致的色偏、低对比度与细节丢失问题，为视觉下水应用提供增强图像。

Method: 提出AquaDiff：结合色度先验的色彩补偿与条件扩散过程。在每个去噪步使用交叉注意力将退化输入与噪声潜变量动态融合；使用残差密集块与多分辨率注意力的增强去噪骨干以捕获全局色彩与局部细节；引入跨域一致性损失，联合像素级、感知、结构与频域约束。

Result: 在多个挑战性水下基准上与传统方法、CNN、GAN和其他扩散方法比较，AquaDiff在色彩校正方面表现更优，同时在整体图像质量上具有竞争力，能在多种水下条件下稳定工作。

Conclusion: AquaDiff通过融合色彩先验与条件扩散、改进去噪网络和跨域约束，有效缓解了水下成像退化，实现了更准确的色彩恢复与细节保留，为水下视觉任务提供了更可靠的输入。

Abstract: Underwater images are severely degraded by wavelength-dependent light absorption and scattering, resulting in color distortion, low contrast, and loss of fine details that hinder vision-based underwater applications. To address these challenges, we propose AquaDiff, a diffusion-based underwater image enhancement framework designed to correct chromatic distortions while preserving structural and perceptual fidelity. AquaDiff integrates a chromatic prior-guided color compensation strategy with a conditional diffusion process, where cross-attention dynamically fuses degraded inputs and noisy latent states at each denoising step. An enhanced denoising backbone with residual dense blocks and multi-resolution attention captures both global color context and local details. Furthermore, a novel cross-domain consistency loss jointly enforces pixel-level accuracy, perceptual similarity, structural integrity, and frequency-domain fidelity. Extensive experiments on multiple challenging underwater benchmarks demonstrate that AquaDiff provides good results as compared to the state-of-the-art traditional, CNN-, GAN-, and diffusion-based methods, achieving superior color correction and competitive overall image quality across diverse underwater conditions.

</details>


### [5] [Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification](https://arxiv.org/abs/2512.14770)
*Xixian Wu,Yang Ou,Pengchao Tian,Zian Yang,Jielei Zhang,Peiyi Li,Longwen Gao*

Main category: cs.CV

TL;DR: DAVR结合自我反思与跨模型验证，显著提高VLM在VQA中的可靠性，在ICCV-CLVL 2025竞赛中夺魁。


<details>
  <summary>Details</summary>
Motivation: 解决VLM在VQA任务中易产生幻觉（hallucination）导致自信但错误回答的问题，提高回答可靠性与不确定性估计能力。

Method: 提出DAVR框架，包含双路径：1) 自我反思路径：使用双选择器模块融合VLM潜在特征与QA嵌入以评估响应可靠性；2) 跨模型验证路径：调用外部参考模型进行事实核查以抑制幻觉。两路径结合用于全面的不确定性估计。

Result: 在ICCV-CLVL 2025的Reliable VQA Challenge上，DAVR取得领先成绩，Φ_{100}=39.64，100-AUC=97.22，获得第一名。

Conclusion: DAVR通过自我反思与跨模型核查的双重评估策略有效提升了VLM在VQA场景下的答案可信度，证明该方法能显著缓解幻觉问题并改进不确定性估计。

Abstract: Vision-language models (VLMs) have demonstrated significant potential in Visual Question Answering (VQA). However, the susceptibility of VLMs to hallucinations can lead to overconfident yet incorrect answers, severely undermining answer reliability. To address this, we propose Dual-Assessment for VLM Reliability (DAVR), a novel framework that integrates Self-Reflection and Cross-Model Verification for comprehensive uncertainty estimation. The DAVR framework features a dual-pathway architecture: one pathway leverages dual selector modules to assess response reliability by fusing VLM latent features with QA embeddings, while the other deploys external reference models for factual cross-checking to mitigate hallucinations. Evaluated in the Reliable VQA Challenge at ICCV-CLVL 2025, DAVR achieves a leading $Φ_{100}$ score of 39.64 and a 100-AUC of 97.22, securing first place and demonstrating its effectiveness in enhancing the trustworthiness of VLM responses.

</details>


### [6] [HERBench: A Benchmark for Multi-Evidence Integration in Video Question Answering](https://arxiv.org/abs/2512.14870)
*Dan Ben-Ami,Gabriele Serussi,Kobi Cohen,Chaim Baskin*

Main category: cs.CV

TL;DR: HERBench 强制要求跨时间整合多个证据（≥3 段），构建 26K 多项选择题，平均 MRFS=5.5；13 个视频大模型在该基准上只有 31–42% 的低性能，主要问题是检索遗漏关键帧与无法融合已检索证据。


<details>
  <summary>Details</summary>
Motivation: 现有 VideoQA 基准常可用单一显著线索回答，无法有效测评需要跨时间聚合多证据的推理能力，因此需要一个强制多证据整合的测试集以推动更鲁棒、组合化的视频理解。

Method: 构建 HERBench 数据集：生成 26K 五选多项选择题，确保每题需要整合至少三段不重叠证据；定义 MRFS（Minimum Required Frame-Set）来量化所需整合帧数；将问题划分为十二类组合任务（身份绑定、跨实体关系、时间顺序、共现验证、计数等）；评估并诊断 13 个现有 Video-LLM 的表现，以区分检索与融合缺陷。

Result: HERBench 的平均 MRFS 为 5.5，远高于先前数据集（2.6–4.2）；13 个 Video-LLM 在 HERBench上的准确率普遍为 31–42%，仅略优于 20% 随机基线；分析表明错误来源于（1）检索缺陷：关键证据帧未被选取；（2）融合缺陷：即便提供全部证据模型也无法正确整合。

Conclusion: HERBench 是一个专门评估跨时间多证据整合能力的 VideoQA 基准，设计问题要求整合至少三个不重叠的视频片段证据，平均所需帧集 (MRFS) 为 5.5，显著高于先前数据集，评测 13 个 Video-LLM 显示普遍失败，准确率仅略高于随机猜测，瓶颈在于检索（frame selection）和融合（fusion）两方面。

Abstract: Video Large Language Models (Video-LLMs) are rapidly improving, yet current Video Question Answering (VideoQA) benchmarks often allow questions to be answered from a single salient cue, under-testing reasoning that must aggregate multiple, temporally separated visual evidence. We present HERBench, a VideoQA benchmark purpose-built to assess multi-evidence integration across time. Each question requires aggregating at least three non-overlapping evidential cues across distinct video segments, so neither language priors nor a single snapshot can suffice. HERBench comprises 26K five-way multiple-choice questions organized into twelve compositional tasks that probe identity binding, cross-entity relations, temporal ordering, co-occurrence verification, and counting. To make evidential demand measurable, we introduce the Minimum Required Frame-Set (MRFS), the smallest number of frames a model must fuse to answer correctly, and show that HERBench imposes substantially higher demand than prior datasets (mean MRFS 5.5 vs. 2.6-4.2). Evaluating 13 state-of-the-art Video-LLMs on HERBench reveals pervasive failures: accuracies of 31-42% are only slightly above the 20% random-guess baseline. We disentangle this failure into two critical bottlenecks: (1) a retrieval deficit, where frame selectors overlook key evidence, and (2) a fusion deficit, where models fail to integrate information even when all necessary evidence is provided. By making cross-time evidence both unavoidable and quantifiable, HERBench establishes a principled target for advancing robust, compositional video understanding.

</details>


### [7] [Isolated Sign Language Recognition with Segmentation and Pose Estimation](https://arxiv.org/abs/2512.14876)
*Daniel Perkins,Davis Hunter,Dhrumil Patel,Galen Flanagan*

Main category: cs.CV

TL;DR: Use pose estimation + segmentation + ResNet-Transformer to make ISLR efficient and robust to different signers.


<details>
  <summary>Details</summary>
Motivation: ASL users lack access to advances in language models because sign language relies on visual cues; ISLR can help but faces limited per-sign data, signer variability, and high compute costs.

Method: Pipeline: (1) pose estimation extracts hand/face joint coordinates; (2) segmentation isolates relevant regions; (3) ResNet-Transformer backbone models spatial and temporal dependencies jointly.

Result: The paper presents an ISLR model using pose extraction, segmentation, and a ResNet-Transformer to reduce compute while handling signer variability.

Conclusion: Combining pose-based inputs with segmentation and a ResNet-Transformer backbone yields a computationally efficient and signer-robust ISLR approach, suitable for low-data settings.

Abstract: The recent surge in large language models has automated translations of spoken and written languages. However, these advances remain largely inaccessible to American Sign Language (ASL) users, whose language relies on complex visual cues. Isolated sign language recognition (ISLR) - the task of classifying videos of individual signs - can help bridge this gap but is currently limited by scarce per-sign data, high signer variability, and substantial computational costs. We propose a model for ISLR that reduces computational requirements while maintaining robustness to signer variation. Our approach integrates (i) a pose estimation pipeline to extract hand and face joint coordinates, (ii) a segmentation module that isolates relevant information, and (iii) a ResNet-Transformer backbone to jointly model spatial and temporal dependencies.

</details>


### [8] [Visual-textual Dermatoglyphic Animal Biometrics: A First Case Study on Panthera tigris](https://arxiv.org/abs/2512.14878)
*Wenshuo Li,Majid Mirmehdi,Tilo Burghardt*

Main category: cs.CV

TL;DR: Use forensic-style dermatoglyphic text descriptors plus synthetic paired image-text data to improve and explain tiger re-identification, enabling textual-to-visual retrieval and mitigating scarce image data.


<details>
  <summary>Details</summary>
Motivation: In ecology, automated re-identification of animals often relies only on images; integrating human-interpretable textual descriptors (dermatoglyphic minutiae) could improve accuracy, cross-modal retrieval, explainability, and mitigate data scarcity.

Method: Collected 84,264 manually-labelled minutiae across 3,355 images of 185 tigers; developed text-image co-synthesis pipeline to generate virtual individuals with many paired visuals and dermatoglyphic text; trained/evaluated cross-modal retrieval models using specialist semantics and augmentation.

Result: Dermatoglyphic textual descriptors effectively encode coat topology; augmentation with virtual individuals significantly boosts AI accuracy in cross-modal retrieval and alleviates data scarcity; enables textual-to-visual identity recovery and human-verifiable matchings.

Conclusion: Dermatoglyphic language-guided biometrics overcome vision-only limitations, improve explainability and cross-modal Re-ID, and unify descriptive modalities in ecological monitoring.

Abstract: Biologists have long combined visuals with textual field notes to re-identify (Re-ID) animals. Contemporary AI tools automate this for species with distinctive morphological features but remain largely image-based. Here, we extend Re-ID methodologies by incorporating precise dermatoglyphic textual descriptors-an approach used in forensics but new to ecology. We demonstrate that these specialist semantics abstract and encode animal coat topology using human-interpretable language tags. Drawing on 84,264 manually labelled minutiae across 3,355 images of 185 tigers (Panthera tigris), we evaluate this visual-textual methodology, revealing novel capabilities for cross-modal identity retrieval. To optimise performance, we developed a text-image co-synthesis pipeline to generate 'virtual individuals', each comprising dozens of life-like visuals paired with dermatoglyphic text. Benchmarking against real-world scenarios shows this augmentation significantly boosts AI accuracy in cross-modal retrieval while alleviating data scarcity. We conclude that dermatoglyphic language-guided biometrics can overcome vision-only limitations, enabling textual-to-visual identity recovery underpinned by human-verifiable matchings. This represents a significant advance towards explainability in Re-ID and a language-driven unification of descriptive modalities in ecological monitoring.

</details>


### [9] [Vibe Spaces for Creatively Connecting and Expressing Visual Concepts](https://arxiv.org/abs/2512.14884)
*Huzheng Yang,Katherine Xu,Andrew Lu,Michael D. Grossberg,Yutong Bai,Jianbo Shi*

Main category: cs.CV

TL;DR: 待填


<details>
  <summary>Details</summary>
Motivation: 待填

Method: Summarize methods, results, and conclusions concisely in Chinese.

Result: 待填

Conclusion: 待填

Abstract: Creating new visual concepts often requires connecting distinct ideas through their most relevant shared attributes -- their vibe. We introduce Vibe Blending, a novel task for generating coherent and meaningful hybrids that reveals these shared attributes between images. Achieving such blends is challenging for current methods, which struggle to identify and traverse nonlinear paths linking distant concepts in latent space. We propose Vibe Space, a hierarchical graph manifold that learns low-dimensional geodesics in feature spaces like CLIP, enabling smooth and semantically consistent transitions between concepts. To evaluate creative quality, we design a cognitively inspired framework combining human judgments, LLM reasoning, and a geometric path-based difficulty score. We find that Vibe Space produces blends that humans consistently rate as more creative and coherent than current methods.

</details>


### [10] [PANDA-PLUS-Bench: A Clinical Benchmark for Evaluating Robustness of AI Foundation Models in Prostate Cancer Diagnosis](https://arxiv.org/abs/2512.14922)
*Joshua L. Ebbert,Dennis Della Corte*

Main category: cs.CV

TL;DR: 提出PANDA-PLUS-Bench：9例专家注释的组织切片、两种尺寸与八种增强条件，用于测量模型是否依赖切片特异性伪特征；评测7个基础模型，发现均存在明显的跨切片性能下降，组织特化模型表现最好。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在Gleason分级任务上可能通过学习切片/标本特异性伪特征获得高准确率，从而缺乏真实生物学泛化性，危及临床应用。需要一个专门基准来量化并比较这种失败模式。

Method: 构建PANDA-PLUS-Bench：从9名患者的9张全幅切片中提取不重叠补丁（512x512与224x224），在8种增强条件下生成样本；使用标准化指标评估七个基础模型的‘切片层编码强度’与‘跨切片准确率’，比较within-slide与cross-slide表现差距。

Result: 发现不同模型在切片特征编码与跨切片泛化上差异显著：Virchow2切片编码较低但跨切片准确率也低；HistoEncoder组织特异性训练实现最高跨切片准确率（59.7%）与最高切片编码（90.3%）。所有模型存在19.9到26.9个百分点的within-vs-cross性能差距。

Conclusion: 这篇论文提出了一个针对前列腺癌Gleason分级中模型过拟合样本特异性伪特征问题的基准数据集（PANDA-PLUS-Bench），并用该基准评估了七个基础模型的鲁棒性，发现模型在跨切片泛化能力上存在显著差异，组织特异性训练能提升生物学特征捕获但同时也可能强化切片特征。

Abstract: Artificial intelligence foundation models are increasingly deployed for prostate cancer Gleason grading, where GP3/GP4 distinction directly impacts treatment decisions. However, these models may achieve high validation accuracy by learning specimen-specific artifacts rather than generalizable biological features, limiting real-world clinical utility. We introduce PANDA-PLUS-Bench, a curated benchmark dataset derived from expert-annotated prostate biopsies designed specifically to quantify this failure mode. The benchmark comprises nine carefully selected whole slide images from nine unique patients containing diverse Gleason patterns, with non-overlapping tissue patches extracted at both 512x512 and 224x224 pixel resolutions across eight augmentation conditions. Using this benchmark, we evaluate seven foundation models on their ability to separate biological signal from slide-level confounders. Our results reveal substantial variation in robustness across models: Virchow2 achieved the lowest slide-level encoding among large-scale models (81.0%) yet exhibited the second-lowest cross-slide accuracy (47.2%). HistoEncoder, trained specifically on prostate tissue, demonstrated the highest cross-slide accuracy (59.7%) and the strongest slide-level encoding (90.3%), suggesting tissue-specific training may enhance both biological feature capture and slide-specific signatures. All models exhibited measurable within-slide vs. cross-slide accuracy gaps, though the magnitude varied from 19.9 percentage points to 26.9 percentage points. We provide an open-source Google Colab notebook enabling researchers to evaluate additional foundation models against our benchmark using standardized metrics. PANDA-PLUS-Bench addresses a critical gap in foundation model evaluation by providing a purpose-built resource for robustness assessment in the clinically important context of Gleason grading.

</details>


### [11] [Improving Pre-trained Segmentation Models using Post-Processing](https://arxiv.org/abs/2512.14937)
*Abhijeet Parida,Daniel Capellán-Martín,Zhifan Jiang,Nishad Kulkarni,Krithika Iyer,Austin Tapp,Syed Muhammad Anwar,María J. Ledesma-Carbayo,Marius George Linguraru*

Main category: cs.CV

TL;DR: 作者用自适应后处理方案改善大规模预训练模型对胶质瘤的mpMRI分割，显著提升了BraTS 2025挑战的排名指标（次撒哈拉非洲任务+14.9%，成人胶质瘤任务+0.9%），强调从复杂模型向高效后处理转变的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习提升了自动分割性能，但大规模预训练模型在不同任务上泛化能力差，常出现系统性错误；同时训练大模型资源不均和环境成本高。因此需要低成本且临床相关的后处理方法来提高分割质量与可及性。

Method: 提出一组自适应后处理技术，针对模型常见错误（如假阳性区域过滤、标签一致性校正、切片连续性修复等）对预训练模型输出进行精细化。方法可推广到多种肿瘤类型的预训练模型，且计算开销低。

Result: 在BraTS 2025多个分割任务上验证了方法有效性：次撒哈拉非洲挑战的排名指标提升14.9%，成人胶质瘤挑战提升0.9%，总体上改善了分割的临床适用性并减少了计算资源需求。

Conclusion: 本文提出了一种自适应后处理方法，用于优化大规模预训练模型在多参数MRI（mpMRI）脑胶质瘤分割任务中的输出质量。该方法通过针对常见系统性错误（如假阳性、标签交换、切片不连续）进行精细化处理，提高了分割结果的准确性和临床适应性，同时降低了对高算力资源的依赖，促进更可持续和公平的研究实践。

Abstract: Gliomas are the most common malignant brain tumors in adults and are among the most lethal. Despite aggressive treatment, the median survival rate is less than 15 months. Accurate multiparametric MRI (mpMRI) tumor segmentation is critical for surgical planning, radiotherapy, and disease monitoring. While deep learning models have improved the accuracy of automated segmentation, large-scale pre-trained models generalize poorly and often underperform, producing systematic errors such as false positives, label swaps, and slice discontinuities in slices. These limitations are further compounded by unequal access to GPU resources and the growing environmental cost of large-scale model training. In this work, we propose adaptive post-processing techniques to refine the quality of glioma segmentations produced by large-scale pretrained models developed for various types of tumors. We demonstrated the techniques in multiple BraTS 2025 segmentation challenge tasks, with the ranking metric improving by 14.9 % for the sub-Saharan Africa challenge and 0.9% for the adult glioma challenge. This approach promotes a shift in brain tumor segmentation research from increasingly complex model architectures to efficient, clinically aligned post-processing strategies that are precise, computationally fair, and sustainable.

</details>


### [12] [TalkVerse: Democratizing Minute-Long Audio-Driven Video Generation](https://arxiv.org/abs/2512.14938)
*Zhenzhi Wang,Jian Wang,Ke Ma,Dahua Lin,Bing Zhou*

Main category: cs.CV

TL;DR: 开源大规模TalkVerse语音驱动单人生成数据集（2.3M剪辑，6.3k小时），并基于5B模型实现长视频、低漂移、与14B性能相当但推理成本低10倍的可复现基线，开源数据与权重。


<details>
  <summary>Details</summary>
Motivation: 当前语音驱动视频生成研究受限于封闭数据与高成本模型；需要一个透明、大规模的开源数据集与可复现基线来促进公平比较与研究发展。

Method: Analyze dataset and model

Result: Dataset TalkVerse: 2.3M clips, 6.3k hours, high-res, curated pipeline; Model: 5B DiT baseline (Wan2.2-5B) using video VAE high downsampling, sliding window, motion-frame context; Achieves minute-long generation, comparable to 14B model with 10x lower inference cost; Adds MLLM director and zero-shot dubbing via latent noise; Open-sourced data and checkpoints.

Conclusion: TalkVerse provides a reproducible, large-scale open benchmark and an efficient 5B baseline enabling competitive audio-driven talking video generation with lower compute; useful for fair comparisons and further research.

Abstract: We introduce TalkVerse, a large-scale, open corpus for single-person, audio-driven talking video generation designed to enable fair, reproducible comparison across methods. While current state-of-the-art systems rely on closed data or compute-heavy models, TalkVerse offers 2.3 million high-resolution (720p/1080p) audio-video synchronized clips totaling 6.3k hours. These are curated from over 60k hours of video via a transparent pipeline that includes scene-cut detection, aesthetic assessment, strict audio-visual synchronization checks, and comprehensive annotations including 2D skeletons and structured visual/audio-style captions. Leveraging TalkVerse, we present a reproducible 5B DiT baseline built on Wan2.2-5B. By utilizing a video VAE with a high downsampling ratio and a sliding window mechanism with motion-frame context, our model achieves minute-long generation with low drift. It delivers comparable lip-sync and visual quality to the 14B Wan-S2V model but with 10$\times$ lower inference cost. To enhance storytelling in long videos, we integrate an MLLM director to rewrite prompts based on audio and visual cues. Furthermore, our model supports zero-shot video dubbing via controlled latent noise injection. We open-source the dataset, training recipes, and 5B checkpoints to lower barriers for research in audio-driven human video generation. Project Page: https://zhenzhiwang.github.io/talkverse/

</details>


### [13] [Puzzle Curriculum GRPO for Vision-Centric Reasoning](https://arxiv.org/abs/2512.14944)
*Ahmadreza Jeddi,Hakki Can Karaimer,Hue Nguyen,Zhongling Wang,Ke Zhao,Javad Rajabi,Ran Zhang,Raghav Goyal,Babak Taati,Radek Grzeszczuk*

Main category: cs.CV

TL;DR: PC-GRPO: supervision-free RL with verifiable rewards via three puzzle tasks + difficulty curriculum + RAC monitoring, boosting VLM chain-of-thought reasoning and accuracy.


<details>
  <summary>Details</summary>
Motivation: Remove reliance on costly/noisy annotations or external verifiers, address flat/sparse rewards and reasoning-answer inconsistency in GRPO for VLMs.

Method: Analyze PC-GRPO from abstract

Result: PC-GRPO introduces self-supervised puzzle environments (PatchFit, Rotation, Jigsaw) to provide verifiable rewards without annotations; uses difficulty-aware curriculum and consistency-enforcing reward schemes; improves reasoning and accuracy.

Conclusion: PC-GRPO is a practical, scalable RL post-training approach for VLMs that mitigates reward sparsity, avoids external verifiers, and improves reasoning-answer consistency and downstream performance.

Abstract: Recent reinforcement learning (RL) approaches like outcome-supervised GRPO have advanced chain-of-thought reasoning in Vision Language Models (VLMs), yet key issues linger: (i) reliance on costly and noisy hand-curated annotations or external verifiers; (ii) flat and sparse reward schemes in GRPO; and (iii) logical inconsistency between a chain's reasoning and its final answer. We present Puzzle Curriculum GRPO (PC-GRPO), a supervision-free recipe for RL with Verifiable Rewards (RLVR) that strengthens visual reasoning in VLMs without annotations or external verifiers. PC-GRPO replaces labels with three self-supervised puzzle environments: PatchFit, Rotation (with binary rewards) and Jigsaw (with graded partial credit mitigating reward sparsity). To counter flat rewards and vanishing group-relative advantages, we introduce a difficulty-aware curriculum that dynamically weights samples and peaks at medium difficulty. We further monitor Reasoning-Answer Consistency (RAC) during post-training: mirroring reports for vanilla GRPO in LLMs, RAC typically rises early then degrades; our curriculum delays this decline, and consistency-enforcing reward schemes further boost RAC. RAC correlates with downstream accuracy. Across diverse benchmarks and on Qwen-7B and Qwen-3B backbones, PC-GRPO improves reasoning quality, training stability, and end-task accuracy, offering a practical path to scalable, verifiable, and interpretable RL post-training for VLMs.

</details>


### [14] [Adaptive Multimodal Person Recognition: A Robust Framework for Handling Missing Modalities](https://arxiv.org/abs/2512.14961)
*Aref Farhadipour,Teodora Vukovic,Volker Dellwo,Petr Motlicek,Srikanth Madikeri*

Main category: cs.CV

TL;DR: 提出一种结合语音、面部与手势且对模态缺失鲁棒的Trimodal识别框架，通过跨注意力、门控融合与置信度加权策略，在新数据集CANDOR与VoxCeleb1上取得了优异结果（99.18%与99.92%）。


<details>
  <summary>Details</summary>
Motivation: 解决在实际场景中模态缺失或退化时人员识别性能下降的问题，通过融合语音、面部和手势三模态并保持对模态丢失的鲁棒性。

Method: 使用多任务学习独立处理每个模态，随后通过跨模态注意力（cross-attention）和门控融合（gated fusion）实现模态间交互；引入置信度加权融合策略，根据模态质量动态调整融合权重以适应单模态或双模态缺失情况。并在新引入的CANDOR访谈多模态数据集上进行评估，同时在VoxCeleb1上作为基准测试。

Result: 在CANDOR数据集上，Trimodal系统在人员识别任务上获得99.18%的Top-1准确率，优于单模态及后融合方法；在VoxCeleb1上二模态模式达到99.92%准确率。系统在丢失一到两个模态时仍能保持较高识别准确率。

Conclusion: 提出的Trimodal框架在处理模态缺失和低质量数据时表现出色，是一种适用于真实场景的鲁棒人员识别方法；并公开了代码与数据以促进复现与进一步研究。

Abstract: Person recognition systems often rely on audio, visual, or behavioral cues, but real-world conditions frequently result in missing or degraded modalities. To address this challenge, we propose a Trimodal person identification framework that integrates voice, face, and gesture modalities, while remaining robust to modality loss. Our approach leverages multi-task learning to process each modality independently, followed by a cross-attention and gated fusion mechanisms to facilitate interaction across modalities. Moreover, a confidence-weighted fusion strategy dynamically adapts to missing and low-quality data, ensuring optimal classification even in Unimodal or Bimodal scenarios. We evaluate our method on CANDOR, a newly introduced interview-based multimodal dataset, which we benchmark for the first time. Our results demonstrate that the proposed Trimodal system achieves 99.18% Top-1 accuracy on person identification tasks, outperforming conventional Unimodal and late-fusion approaches. In addition, we evaluate our model on the VoxCeleb1 dataset as a benchmark and reach 99.92% accuracy in Bimodal mode. Moreover, we show that our system maintains high accuracy even when one or two modalities are unavailable, making it a robust solution for real-world person recognition applications. The code and data for this work are publicly available.

</details>


### [15] [Where is the Watermark? Interpretable Watermark Detection at the Block Level](https://arxiv.org/abs/2512.14994)
*Maria Bulychev,Neil G. Marchant,Benjamin I. P. Rubinstein*

Main category: cs.CV

TL;DR: 在离散小波变换域按块嵌入水印，生成可视化的区域检测图，兼顾鲁棒性、对语义篡改敏感性与高不可见性；相较既有方法更具可解释性，能抵抗高达半图裁剪。


<details>
  <summary>Details</summary>
Motivation: 现有图像水印方法多为黑箱化的全局检测，缺乏可解释性与定位能力；需要一种既能后嵌（post-hoc）应用于已生成图像又能提供区域级可解释检测的方案。

Method: Embed watermark in DWT domain using statistical block-wise strategy; generate detection maps showing region-level watermark presence; post-hoc application to existing images

Result: Achieves strong robustness to common transforms; sensitive to semantic manipulations; high imperceptibility; robust to cropping up to half the image; more interpretable detection than prior post-hoc methods

Conclusion: Proposed post-hoc localized watermarking gives interpretable region-level detection maps while maintaining competitive robustness and imperceptibility; useful for tracing and understanding tampering

Abstract: Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.

</details>


### [16] [Beyond Proximity: A Keypoint-Trajectory Framework for Classifying Affiliative and Agonistic Social Networks in Dairy Cattle](https://arxiv.org/abs/2512.14998)
*Sibi Parivendan,Kashfia Sailunaz,Suresh Neethirajan*

Main category: cs.CV

TL;DR: 通过关键点轨迹建模解剖学几何并用SVM分类，本文在奶牛舍场景下将亲和/攻击性互动识别准确率提升到77.5%，优于仅靠接近阈值的方法，可实现近实时运行。


<details>
  <summary>Details</summary>
Motivation: 解决现有围栏/牛舍环境中社会行为自动评估依赖静态接近阈值，无法区分亲和与攻击性行为，可解释性受限。

Method: 提出基于姿态（关键点）的方法，建模解剖学关键点的时空几何，使用YOLOv11检测（mAP@0.50:96.24%）、监督个体识别（98.24%）、ByteTrack跟踪（81.96%）、ZebraPose估计27点关键点，并用基于姿态的距离动态训练SVM进行交互分类。

Result: 在商业奶牛舍标注片段上，仅用姿态信息区分亲和与攻击性行为的分类器准确率为77.51%；相较于仅靠接近性基线，在行为判别上显著提高，尤其是亲和性互动。

Conclusion: 证明了基于视觉的自动社会交互推断在构建交互感知社交网络方面的可行性，并能在普通硬件上接近实时运行。

Abstract: Precision livestock farming requires objective assessment of social behavior to support herd welfare monitoring, yet most existing approaches infer interactions using static proximity thresholds that cannot distinguish affiliative from agonistic behaviors in complex barn environments. This limitation constrains the interpretability of automated social network analysis in commercial settings. We present a pose-based computational framework for interaction classification that moves beyond proximity heuristics by modeling the spatiotemporal geometry of anatomical keypoints. Rather than relying on pixel-level appearance or simple distance measures, the proposed method encodes interaction-specific motion signatures from keypoint trajectories, enabling differentiation of social interaction valence. The framework is implemented as an end-to-end computer vision pipeline integrating YOLOv11 for object detection (mAP@0.50: 96.24%), supervised individual identification (98.24% accuracy), ByteTrack for multi-object tracking (81.96% accuracy), ZebraPose for 27-point anatomical keypoint estimation, and a support vector machine classifier trained on pose-derived distance dynamics. On annotated interaction clips collected from a commercial dairy barn, the classifier achieved 77.51% accuracy in distinguishing affiliative and agonistic behaviors using pose information alone. Comparative evaluation against a proximity-only baseline shows substantial gains in behavioral discrimination, particularly for affiliative interactions. The results establish a proof-of-concept for automated, vision-based inference of social interactions suitable for constructing interaction-aware social networks, with near-real-time performance on commodity hardware.

</details>


### [17] [Evaluating the Capability of Video Question Generation for Expert Knowledge Elicitation](https://arxiv.org/abs/2512.15006)
*Huaying Zhang,Atsushi Hashimoto,Tosho Hirasawa*

Main category: cs.CV

TL;DR: 提出用于评估视频问句生成（VQG）在向人类专家获取未见知识能力的协议与数据集。构建了包含27,666 QA对的EgoExoAsk数据集，基于检索器模拟问答沟通，并在Ego-Exo4D验证集上构建基准，实验显示评价指标合理反映模型利用上下文的能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQG评估侧重能否回答问题（对应VideoQA能力），而非问句自身质量。为持续改进VQG以便向专家获取新知识，需要量化评估问句在引出未知信息方面的有效性。

Method: 提出通过模拟与专家的问答交流来评估问句的能力：使用问句到答案的检索器（question-to-answer retrieval）来模拟专家回答。为训练检索器，构建EgoExoAsk数据集（27,666 QA对），该数据来自Ego-Exo4D的专家解说标注。使用训练集训练检索器，在验证集视频片段上建立评测基准，通过检索到的答案匹配质量评估生成问句。

Result: 实验表明：评测协议能合理区分不同问句生成设置——能访问更丰富上下文的模型获得更好评价，说明检索式模拟协议与预期一致。

Conclusion: 提出的基于检索的评估协议和EgoExoAsk数据集为衡量VQG在引出专家未知信息方面提供了可量化工具，有助于持续改进问句生成模型。数据集已公开。

Abstract: Skilled human interviewers can extract valuable information from experts. This raises a fundamental question: what makes some questions more effective than others? To address this, a quantitative evaluation of question-generation models is essential. Video question generation (VQG) is a topic for video question answering (VideoQA), where questions are generated for given answers. Their evaluation typically focuses on the ability to answer questions, rather than the quality of generated questions. In contrast, we focus on the question quality in eliciting unseen knowledge from human experts. For a continuous improvement of VQG models, we propose a protocol that evaluates the ability by simulating question-answering communication with experts using a question-to-answer retrieval. We obtain the retriever by constructing a novel dataset, EgoExoAsk, which comprises 27,666 QA pairs generated from Ego-Exo4D's expert commentary annotation. The EgoExoAsk training set is used to obtain the retriever, and the benchmark is constructed on the validation set with Ego-Exo4D video segments. Experimental results demonstrate our metric reasonably aligns with question generation settings: models accessing richer context are evaluated better, supporting that our protocol works as intended. The EgoExoAsk dataset is available in https://github.com/omron-sinicx/VQG4ExpertKnowledge .

</details>


### [18] [Model Agnostic Preference Optimization for Medical Image Segmentation](https://arxiv.org/abs/2512.15009)
*Yunseong Nam,Jiwon Jang,Dongkyu Won,Sang Hyun Park,Soopil Kim*

Main category: cs.CV

TL;DR: MAPO: Dropout-driven, model-agnostic preference optimization for medical image segmentation that boosts boundaries and stability without ground-truth labels.


<details>
  <summary>Details</summary>
Motivation: Prior preference-optimization methods for segmentation were model-specific and used low-diversity sampling, limiting generality and robustness; need a scalable, architecture-agnostic approach.

Method: Use Dropout to generate diverse stochastic segmentation hypotheses, compare them to form preference signals, and derive gradients consistent with those preferences to train any segmentation architecture without direct ground-truth supervision.

Result: MAPO introduces a model-agnostic preference-based training that leverages stochastic segmentation hypotheses via Dropout to form preference-consistent gradients, avoiding direct ground-truth; it's compatible with 2D/3D CNNs and Transformers.

Conclusion: MAPO improves boundary adherence, reduces overfitting, and stabilizes optimization across diverse medical segmentation datasets compared to supervised training.

Abstract: Preference optimization offers a scalable supervision paradigm based on relative preference signals, yet prior attempts in medical image segmentation remain model-specific and rely on low-diversity prediction sampling. In this paper, we propose MAPO (Model-Agnostic Preference Optimization), a training framework that utilizes Dropout-driven stochastic segmentation hypotheses to construct preference-consistent gradients without direct ground-truth supervision. MAPO is fully architecture- and dimensionality-agnostic, supporting 2D/3D CNN and Transformer-based segmentation pipelines. Comprehensive evaluations across diverse medical datasets reveal that MAPO consistently enhances boundary adherence, reduces overfitting, and yields more stable optimization dynamics compared to conventional supervised training.

</details>


### [19] [MVGSR: Multi-View Consistent 3D Gaussian Super-Resolution via Epipolar Guidance](https://arxiv.org/abs/2512.15048)
*Kaizhe Zhang,Shinan Chen,Qian Zhao,Weizhan Zhang,Caixia Yan,Yudeng Xin*

Main category: cs.CV

TL;DR: 提出MVGSR：基于相机位姿选择辅助视图，并在3DGS超分辨率网络中引入首个极线约束多视角注意力，以无序多视角数据实现一致且高质量的HR渲染。


<details>
  <summary>Details</summary>
Motivation: 低分辨率图像训练得到的3DGS场景无法直接用于高分辨率渲染。现有方法要么仅用单张图像的超分网络导致跨视角不一致，要么依赖严格时序的视频SR，无法适应无序多视角数据。因此需要一个能融合跨视角互补信息并保持几何一致性的3DGS超分方法。

Method: 1) 提出基于相机位姿的辅助视图选择方法，使方法适用于任意组织的多视角数据而不依赖时间连续性或重排数据。2) 在3DGS超分网络中首次引入基于极线约束的多视角注意力机制，作为核心模块，用于从辅助视图中有选择地聚合与主视图几何一致的信息，从而提升细节保真度和几何一致性。

Result: 在多项物体级与场景级3DGS超分基准上，通过定量指标和定性可视化证明MVGSR在细节恢复和跨视角一致性方面优于现有方法，达到了最先进性能。

Conclusion: 本文提出了MVGSR，一种针对3D Gaussian Splatting (3DGS) 超分辨率的新框架，通过从多视角选择辅助视图并在网络中引入基于极线约束的多视角注意力，有效融合跨视角信息，从而提升高频细节和几何一致性，实验表明在物体级与场景级基准上达到最先进性能。

Abstract: Scenes reconstructed by 3D Gaussian Splatting (3DGS) trained on low-resolution (LR) images are unsuitable for high-resolution (HR) rendering. Consequently, a 3DGS super-resolution (SR) method is needed to bridge LR inputs and HR rendering. Early 3DGS SR methods rely on single-image SR networks, which lack cross-view consistency and fail to fuse complementary information across views. More recent video-based SR approaches attempt to address this limitation but require strictly sequential frames, limiting their applicability to unstructured multi-view datasets. In this work, we introduce Multi-View Consistent 3D Gaussian Splatting Super-Resolution (MVGSR), a framework that focuses on integrating multi-view information for 3DGS rendering with high-frequency details and enhanced consistency. We first propose an Auxiliary View Selection Method based on camera poses, making our method adaptable for arbitrarily organized multi-view datasets without the need of temporal continuity or data reordering. Furthermore, we introduce, for the first time, an epipolar-constrained multi-view attention mechanism into 3DGS SR, which serves as the core of our proposed multi-view SR network. This design enables the model to selectively aggregate consistent information from auxiliary views, enhancing the geometric consistency and detail fidelity of 3DGS representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance on both object-centric and scene-level 3DGS SR benchmarks.

</details>


### [20] [Asynchronous Event Stream Noise Filtering for High-frequency Structure Deformation Measurement](https://arxiv.org/abs/2512.15055)
*Yifei Bian,Banglei Guan,Zibin Liu,Ang Su,Shiyao Zhu,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出一种基于事件相机和LED标记的高频平面形变测量方法，通过滤除噪声并区分闪烁事件与运动事件，使用单目事件相机准确恢复高频形变。


<details>
  <summary>Details</summary>
Motivation: Large-scale structures experience high-frequency deformations under complex loads, and traditional high-speed camera-based measurement methods are limited by harsh lighting conditions and high equipment costs. The paper aims to develop a cost-effective, robust method to measure such deformations using event cameras and LED markers.

Method: Use LED markers that blink to generate characteristic event streams captured by a monocular event camera. First, filter observation noise by leveraging the blinking characteristics and spatiotemporal correlations in the event stream. Differentiate motion-induced events from blinking-induced events to extract LED markers, enabling tracking of high-speed moving markers. Finally, reconstruct high-frequency planar deformations from the tracked LED marker motions.

Result: Experimental results validate the proposed method, showing accurate measurements of high-frequency planar deformations using only a monocular event camera and LED markers.

Conclusion: The method provides an accurate, lower-cost alternative to traditional high-speed camera approaches for measuring high-frequency planar deformations, robust to harsh lighting conditions.

Abstract: Large-scale structures suffer high-frequency deformations due to complex loads. However, harsh lighting conditions and high equipment costs limit measurement methods based on traditional high-speed cameras. This paper proposes a method to measure high-frequency deformations by exploiting an event camera and LED markers. Firstly, observation noise is filtered based on the characteristics of the event stream generated by LED markers blinking and spatiotemporal correlation. Then, LED markers are extracted from the event stream after differentiating between motion-induced events and events from LED blinking, which enables the extraction of high-speed moving LED markers. Ultimately, high-frequency planar deformations are measured by a monocular event camera. Experimental results confirm the accuracy of our method in measuring high-frequency planar deformations.

</details>


### [21] [Tracking spatial temporal details in ultrasound long video via wavelet analysis and memory bank](https://arxiv.org/abs/2512.15066)
*Chenxiao Zhang,Runshi Zhang,Junchen Wang*

Main category: cs.CV

TL;DR: 提出一种基于记忆库的小波滤波融合网络（MWNet），用于超声视频中小目标与器官的高精度分割与长视频跟踪，通过记忆型小波卷积、级联小波压缩、长短时记忆库（含交叉注意与记忆压缩）以及高频感知特征融合模块，提升边界与小目标分割效果，在四个超声视频数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声视频具有低对比度和噪声背景，导致器官边界错分、小物体丢失及边界错误，且长视频中对象跟踪困难，需一种同时增强高频边界细节与时序记忆的分割跟踪方法。

Method: 提出MWNet：在编码器中引入记忆型小波卷积以捕获类别与细节并利用邻域信息；通过级联小波压缩融合多尺度频域特征并扩展每层感受野；设计含交叉注意与记忆压缩的长短时记忆库用于长视频对象跟踪；在解码器使用自适应小波滤波器的高频感知特征融合模块以充分利用边界敏感的高频信息。

Result: 在四个超声视频数据集（两类甲状腺结节、甲状腺、心脏）上的广泛基准测试中，MWNet在分割指标上显著优于现有方法，尤其能更准确地分割小型甲状腺结节，减少边界错误和小目标丢失。

Conclusion: 通过联合小波频域处理与记忆库时序建模，MWNet有效增强了超声视频小目标与边界的分割与长视频跟踪性能，具有在临床超声分割场景中的潜在应用价值。

Abstract: Medical ultrasound videos are widely used for medical inspections, disease diagnosis and surgical planning. High-fidelity lesion area and target organ segmentation constitutes a key component of the computer-assisted surgery workflow. The low contrast levels and noisy backgrounds of ultrasound videos cause missegmentation of organ boundary, which may lead to small object losses and increase boundary segmentation errors. Object tracking in long videos also remains a significant research challenge. To overcome these challenges, we propose a memory bank-based wavelet filtering and fusion network, which adopts an encoder-decoder structure to effectively extract fine-grained detailed spatial features and integrate high-frequency (HF) information. Specifically, memory-based wavelet convolution is presented to simultaneously capture category, detailed information and utilize adjacent information in the encoder. Cascaded wavelet compression is used to fuse multiscale frequency-domain features and expand the receptive field within each convolutional layer. A long short-term memory bank using cross-attention and memory compression mechanisms is designed to track objects in long video. To fully utilize the boundary-sensitive HF details of feature maps, an HF-aware feature fusion module is designed via adaptive wavelet filters in the decoder. In extensive benchmark tests conducted on four ultrasound video datasets (two thyroid nodule, the thyroid gland, the heart datasets) compared with the state-of-the-art methods, our method demonstrates marked improvements in segmentation metrics. In particular, our method can more accurately segment small thyroid nodules, demonstrating its effectiveness for cases involving small ultrasound objects in long video. The code is available at https://github.com/XiAooZ/MWNet.

</details>


### [22] [PMMD: A pose-guided multi-view multi-modal diffusion for person generation](https://arxiv.org/abs/2512.15069)
*Ziyu Shang,Haoran Liu,Rongchao Zhang,Zhiqian Wei,Tongtong Feng*

Main category: cs.CV

TL;DR: PMMD: multimodal diffusion for controllable, consistent person image generation using multi-view, pose, and text; outperforms baselines on DeepFashion.


<details>
  <summary>Details</summary>
Motivation: Address occlusions, garment drift, and pose misalignment in person image generation by leveraging multi-view references and multimodal conditioning to improve fidelity and controllability.

Method: Diffusion-based multimodal person image synthesis using multi-view references, pose maps, and text prompts; ResCVA and cross-modal fusion during denoising.

Result: PMMD achieves higher identity and pose consistency, better detail preservation, and improved controllability compared to baselines on DeepFashion MultiModal.

Conclusion: PMMD effectively integrates multi-view images, pose, and text via a multimodal encoder plus ResCVA and cross-modal fusion, producing more consistent and detailed person images.

Abstract: Generating consistent human images with controllable pose and appearance is essential for applications in virtual try on, image editing, and digital human creation. Current methods often suffer from occlusions, garment style drift, and pose misalignment. We propose Pose-guided Multi-view Multimodal Diffusion (PMMD), a diffusion framework that synthesizes photorealistic person images conditioned on multi-view references, pose maps, and text prompts. A multimodal encoder jointly models visual views, pose features, and semantic descriptions, which reduces cross modal discrepancy and improves identity fidelity. We further design a ResCVA module to enhance local detail while preserving global structure, and a cross modal fusion module that integrates image semantics with text throughout the denoising pipeline. Experiments on the DeepFashion MultiModal dataset show that PMMD outperforms representative baselines in consistency, detail preservation, and controllability. Project page and code are available at https://github.com/ZANMANGLOOPYE/PMMD.

</details>


### [23] [Uni-Parser Technical Report](https://arxiv.org/abs/2512.15098)
*Xi Fang,Haoyi Tao,Shuwen Yang,Suyang Zhong,Haocheng Lu,Han Lyu,Chaozheng Huang,Xinyu Li,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: Uni-Parser is a scalable, modular multi-expert parser for scientific and patent PDFs that preserves cross-modal alignments and delivers high throughput (up to 20 pp/s on 8x4090D GPUs) for cost-efficient, large-scale extraction and downstream use.


<details>
  <summary>Details</summary>
Motivation: Provide an industrial-grade, scalable, and extensible document parsing engine for scientific literature and patents that maintains fine-grained cross-modal alignments and supports high-throughput, cost-efficient processing.

Method: Design a modular, loosely coupled multi-expert architecture preserving cross-modal alignments across text, equations, tables, figures, and chemical structures; implement adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable holistic or modality-specific modes; optimize for large-scale cloud deployment on multi-GPU systems.

Result: Achieves up to 20 PDF pages/sec processing on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference at scale and supporting extraction of literature content, chemical structures, reaction schemes, and bioactivity data.

Conclusion: Uni-Parser provides a scalable, accurate, and extensible solution for parsing scientific documents and patents, facilitating large-scale downstream applications including dataset curation for LLM and AI4Science model training.

Abstract: This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.

</details>


### [24] [Is Nano Banana Pro a Low-Level Vision All-Rounder? A Comprehensive Evaluation on 14 Tasks and 40 Datasets](https://arxiv.org/abs/2512.15110)
*Jialong Zuo,Haoyou Deng,Hanyu Zhou,Jiaxin Zhu,Yicheng Zhang,Yiwei Zhang,Yongxin Yan,Kaixing Huang,Weisen Chen,Yongtai Deng,Rui Jin,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: 零样本下，Nano Banana Pro在视觉主观效果上优，但在像素级参考指标上落后，随机性是关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探讨大型文本到图像生成模型（以Nano Banana Pro为例）能否在无需微调的零样本情况下胜任传统低级视觉任务，填补该领域评估不足。

Method: 在14类低级视觉任务上对40个数据集进行零样本评估，使用简单文本提示驱动Nano Banana Pro生成结果，并将其与专用模型在参考指标和主观视觉质量上比较。

Result: Nano Banana Pro在主观视觉质量上优于专用模型，能生成看似合理的高频细节；但在基于参考的传统量化指标（如PSNR、SSIM等）上表现不如专用模型，反映出生成模型随机性导致难以像素级一致。

Conclusion: Nano Banana Pro可作为低级视觉任务的零样本有力竞争者，但其生成随机性导致的像素不一致性使其难以匹敌领域专用模型在传统参考指标上的高保真度。

Abstract: The rapid evolution of text-to-image generation models has revolutionized visual content creation. While commercial products like Nano Banana Pro have garnered significant attention, their potential as generalist solvers for traditional low-level vision challenges remains largely underexplored. In this study, we investigate the critical question: Is Nano Banana Pro a Low-Level Vision All-Rounder? We conducted a comprehensive zero-shot evaluation across 14 distinct low-level tasks spanning 40 diverse datasets. By utilizing simple textual prompts without fine-tuning, we benchmarked Nano Banana Pro against state-of-the-art specialist models. Our extensive analysis reveals a distinct performance dichotomy: while \textbf{Nano Banana Pro demonstrates superior subjective visual quality}, often hallucinating plausible high-frequency details that surpass specialist models, it lags behind in traditional reference-based quantitative metrics. We attribute this discrepancy to the inherent stochasticity of generative models, which struggle to maintain the strict pixel-level consistency required by conventional metrics. This report identifies Nano Banana Pro as a capable zero-shot contender for low-level vision tasks, while highlighting that achieving the high fidelity of domain specialists remains a significant hurdle.

</details>


### [25] [3DProxyImg: Controllable 3D-Aware Animation Synthesis from Single Image via 2D-3D Aligned Proxy Embedding](https://arxiv.org/abs/2512.15126)
*Yupeng Zhu,Xiongzhen Zhang,Ye Chen,Bingbing Ni*

Main category: cs.CV

TL;DR: 用粗糙3D代理做结构控制，交由图像空间生成模型负责高保真外观渲染，在低算力下实现可交互、保持身份与纹理一致的单图3D动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么沿用昂贵的完整3D管线，要么采用视频合成牺牲3D可控性。单图3D动画面临渲染质量与3D控制之间的权衡。

Method: 构建2D-3D对齐代理表示：使用粗糙的3D几何估计作为运动与交互的结构载体，同时用学习到的图像空间生成先验（可能是基于扩散或GAN的图像渲染模块）合成高保真视图。该方法无需精确几何或昂贵优化，支持背景联动动画。

Result: 在多项实验中，该方法在低算力平台上高效生成动画，优于基于视频的方法，在身份保持、几何/纹理一致性以及精确可交互控制方面表现更好。

Conclusion: 该论文提出了一种轻量级单图3D动画生成框架，通过将几何控制与外观合成解耦，使用一个2D-3D对齐的代理表示（粗略3D结构载体 + 基于图像空间的生成先验）实现高质量渲染与可控运动的平衡。

Abstract: 3D animation is central to modern visual media, yet traditional production pipelines remain labor-intensive, expertise-demanding, and computationally expensive. Recent AIGC-based approaches partially automate asset creation and rigging, but they either inherit the heavy costs of full 3D pipelines or rely on video-synthesis paradigms that sacrifice 3D controllability and interactivity. We focus on single-image 3D animation generation and argue that progress is fundamentally constrained by a trade-off between rendering quality and 3D control.
  To address this limitation, we propose a lightweight 3D animation framework that decouples geometric control from appearance synthesis. The core idea is a 2D-3D aligned proxy representation that uses a coarse 3D estimate as a structural carrier, while delegating high-fidelity appearance and view synthesis to learned image-space generative priors. This proxy formulation enables 3D-aware motion control and interaction comparable to classical pipelines, without requiring accurate geometry or expensive optimization, and naturally extends to coherent background animation. Extensive experiments demonstrate that our method achieves efficient animation generation on low-power platforms and outperforms video-based 3D animation generation in identity preservation, geometric and textural consistency, and the level of precise, interactive control it offers to users.

</details>


### [26] [Borrowing from anything: A generalizable framework for reference-guided instance editing](https://arxiv.org/abs/2512.15138)
*Shengxiao Zhou,Chenghua Li,Jianhao Huang,Qinghao Hu,Yifan Zhang*

Main category: cs.CV

TL;DR: GENIE disentangles reference appearance from extrinsic attributes using spatial alignment, adaptive residual scaling, and progressive attention fusion for better instance editing.


<details>
  <summary>Details</summary>
Motivation: To overcome semantic entanglement in reference-guided instance editing by explicitly disentangling intrinsic appearance and extrinsic attributes to learn what to borrow and how to apply it.

Method: Analyze method, result, conclusion, TLDR, motivation summarizing the given abstract.

Result: GENIE outperforms baselines on AnyInsertion dataset, achieving SOTA fidelity and robustness in instance editing.

Conclusion: Explicit disentanglement via SAM, ARSM, and PAF leads to improved editing fidelity and generalization.

Abstract: Reference-guided instance editing is fundamentally limited by semantic entanglement, where a reference's intrinsic appearance is intertwined with its extrinsic attributes. The key challenge lies in disentangling what information should be borrowed from the reference, and determining how to apply it appropriately to the target. To tackle this challenge, we propose GENIE, a Generalizable Instance Editing framework capable of achieving explicit disentanglement. GENIE first corrects spatial misalignments with a Spatial Alignment Module (SAM). Then, an Adaptive Residual Scaling Module (ARSM) learns what to borrow by amplifying salient intrinsic cues while suppressing extrinsic attributes, while a Progressive Attention Fusion (PAF) mechanism learns how to render this appearance onto the target, preserving its structure. Extensive experiments on the challenging AnyInsertion dataset demonstrate that GENIE achieves state-of-the-art fidelity and robustness, setting a new standard for disentanglement-based instance editing.

</details>


### [27] [Explainable Action Form Assessment by Exploiting Multimodal Chain-of-Thoughts Reasoning](https://arxiv.org/abs/2512.15153)
*Mengshi Qi,Yeteng Wu,Xianlin Zhang,Huadong Ma*

Main category: cs.CV

TL;DR: 构建含链式思维解释的CoT-AFA数据集并提出Explainable Fitness Assessor，通过视觉-语义并行与门控融合，实现对动作是否规范的判断并给出可解释改进建议，显著提升了解释质量与评估性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法关注动作的类别和位置，但无法评估动作是否符合规范并提供改进建议；缺乏带有标准化程度标签与可解释详细反馈的数据集。

Method: 提出新任务AFA并构建CoT-AFA数据集，采用Chain-of-Thought解释范式；设计Explainable Fitness Assessor框架，使用视觉与语义并行流和动态门控融合生成判断、原因和解决方案。

Result: 在解释生成上CIDEr提高约16.0%，动作分类准确率提升约2.7%，质量评估准确率提升约2.1%；展示了CoT-AFA和模型在多任务上改进效果。

Conclusion: 引入可链式推理的动作规范性评估任务与数据集并提出可解释评估框架，促进了动作质量评估的可解释性和性能提升，为相关研究提供数据与基准。

Abstract: Evaluating whether human action is standard or not and providing reasonable feedback to improve action standardization is very crucial but challenging in real-world scenarios. However, current video understanding methods are mainly concerned with what and where the action is, which is unable to meet the requirements. Meanwhile, most of the existing datasets lack the labels indicating the degree of action standardization, and the action quality assessment datasets lack explainability and detailed feedback. Therefore, we define a new Human Action Form Assessment (AFA) task, and introduce a new diverse dataset CoT-AFA, which contains a large scale of fitness and martial arts videos with multi-level annotations for comprehensive video analysis. We enrich the CoT-AFA dataset with a novel Chain-of-Thought explanation paradigm. Instead of offering isolated feedback, our explanations provide a complete reasoning process--from identifying an action step to analyzing its outcome and proposing a concrete solution. Furthermore, we propose a framework named Explainable Fitness Assessor, which can not only judge an action but also explain why and provide a solution. This framework employs two parallel processing streams and a dynamic gating mechanism to fuse visual and semantic information, thereby boosting its analytical capabilities. The experimental results demonstrate that our method has achieved improvements in explanation generation (e.g., +16.0% in CIDEr), action classification (+2.7% in accuracy) and quality assessment (+2.1% in accuracy), revealing great potential of CoT-AFA for future studies. Our dataset and source code is available at https://github.com/MICLAB-BUPT/EFA.

</details>


### [28] [EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence](https://arxiv.org/abs/2512.15160)
*Jiaxu Wan,Xu Wang,Mengwei Xie,Hang Zhang,Mu Xu,Yang Han,Hong Zhang,Ding Yuan,Yifan Yang*

Main category: cs.CV

TL;DR: EagleVision selects keyframes via SPF-DPP under token limits, then uses BEV pose querying with RL-trained agent and spatial grounding reward to iteratively verify 3D hypotheses, achieving SOTA on VSI-Bench


<details>
  <summary>Details</summary>
Motivation: Address weak spatial consistency, limited viewpoint diversity, untraceable evidence chains, and token budget constraints in spatial Chain-of-Thought for multimodal LLMs by integrating compact keyframe selection and BEV-grounded RL verification

Method: Dual-stage framework (macro perception + micro verification): SPF-DPP keyframe selection, BEV-grounded pose querying, RL with spatial grounding reward

Result: State-of-the-art on VSI-Bench for open-source VLMs; improved spatial consistency, viewpoint diversity, traceable evidence chains

Conclusion: EagleVision provides compact token-efficient global perception and iterative BEV-based verification, enabling better spatial CoT and generalizable spatial understanding

Abstract: Recent spatial intelligence approaches typically attach 3D cues to 2D reasoning pipelines or couple MLLMs with black-box reconstruction modules, leading to weak spatial consistency, limited viewpoint diversity, and evidence chains that cannot be traced back to supporting views. Frameworks for "thinking with images" (e.g., ChatGPT-o3 and DeepEyes) show that stepwise multimodal reasoning can emerge by interleaving hypothesis formation with active acquisition of visual evidence, but they do not address three key challenges in spatial Chain-of-Thought (CoT): building global space perception under strict token budgets, explicitly associating 3D hypotheses with video frames for verification, and designing spatially grounded rewards for reinforcement learning. To address these issues, we present EagleVision, a dual-stage framework for progressive spatial cognition through macro perception and micro verification. In the macro perception stage, EagleVision employs a semantics-perspective-fusion determinantal point process (SPF-DPP) to select a compact set of geometry- and semantics-aware keyframes from long videos under a fixed token budget. In the micro verification stage, we formalize spatial CoT as BEV-grounded pose querying: the agent iteratively predicts poses on a BEV plane, retrieves the nearest real frames, and is trained purely by reinforcement learning with a spatial grounding reward that scores the consistency between predicted poses and observed views. On VSI-Bench, EagleVision achieves state-of-the-art performance among open-source vision-language models, demonstrating strong and generalizable spatial understanding.

</details>


### [29] [Cross-modal ultra-scale learning with tri-modalities of renal biopsy images for glomerular multi-disease auxiliary diagnosis](https://arxiv.org/abs/2512.15171)
*Kaixing Long,Danyi Weng,Yun Mi,Zhentai Zhang,Yanmeng Lu,Jian Geng,Zhitao Zhou,Liming Zhong,Qianjin Feng,Wei Yang,Lei Cao*

Main category: cs.CV

TL;DR: 提出CMUS-Net，通过稀疏多实例学习聚合TEM超微信息并使用跨模态尺度注意力实现不同尺度模态特征交互，在三模态两尺度肾小球疾病分类上显著优于现有方法（ACC~95.4%，AUC~99.1%）。


<details>
  <summary>Details</summary>
Motivation: 现有多模态多尺度模型难以融合纳米级TEM与微米级OM/IM特征，导致肾病多分类性能受限；需设计桥接尺度差异的模块以提升诊断辅助性能。

Method: CMUS-Net: cross-modal ultra-scale learning network using sparse multi-instance learning and cross-modal scale attention

Result: On in-house dataset: ACC 95.37±2.41%, AUC 99.05±0.53%, F1 95.32±2.41%; outperforms other multimodal/multiscale methods and generalizes to MN staging

Conclusion: CMUS-Net effectively bridges nano- to micro-scale gap via TEM feature aggregation and cross-modal attention, improving multi-modal glomerular disease classification

Abstract: Constructing a multi-modal automatic classification model based on three types of renal biopsy images can assist pathologists in glomerular multi-disease identification. However, the substantial scale difference between transmission electron microscopy (TEM) image features at the nanoscale and optical microscopy (OM) or immunofluorescence microscopy (IM) images at the microscale poses a challenge for existing multi-modal and multi-scale models in achieving effective feature fusion and improving classification accuracy. To address this issue, we propose a cross-modal ultra-scale learning network (CMUS-Net) for the auxiliary diagnosis of multiple glomerular diseases. CMUS-Net utilizes multiple ultrastructural information to bridge the scale difference between nanometer and micrometer images. Specifically, we introduce a sparse multi-instance learning module to aggregate features from TEM images. Furthermore, we design a cross-modal scale attention module to facilitate feature interaction, enhancing pathological semantic information. Finally, multiple loss functions are combined, allowing the model to weigh the importance among different modalities and achieve precise classification of glomerular diseases. Our method follows the conventional process of renal biopsy pathology diagnosis and, for the first time, performs automatic classification of multiple glomerular diseases including IgA nephropathy (IgAN), membranous nephropathy (MN), and lupus nephritis (LN) based on images from three modalities and two scales. On an in-house dataset, CMUS-Net achieves an ACC of 95.37+/-2.41%, an AUC of 99.05+/-0.53%, and an F1-score of 95.32+/-2.41%. Extensive experiments demonstrate that CMUS-Net outperforms other well-known multi-modal or multi-scale methods and show its generalization capability in staging MN. Code is available at https://github.com/SMU-GL-Group/MultiModal_lkx/tree/main.

</details>


### [30] [Criticality Metrics for Relevance Classification in Safety Evaluation of Object Detection in Automated Driving](https://arxiv.org/abs/2512.15181)
*Jörg Gamerdinger,Sven Teufel,Stephan Amann,Oliver Bringmann*

Main category: cs.CV

TL;DR: 本文首次对用于自动驾驶目标检测安全评估的关键性度量进行深入分析，基于DeepAccident数据集对多种度量进行实证验证，并提出双向评分与多度量聚合两种策略，关键性分类准确率提升高达100%。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的首要目标是确保安全，而安全评估需将检测性能与安全相关性联系起来。传统检测指标（如mAP）无法反映目标对安全的实际影响，因此需要关键性/相关性度量来区分与驾驶安全相关的目标与无关目标，从而提高评估的可靠性。

Method: 文章首先对文献中已有的关键性度量进行归纳与分类；随后在DeepAccident数据集上对这些度量进行定量比较；最后提出两种改进策略：1) 双向关键性评分（bidirectional criticality rating），即从被检测目标与潜在碰撞主体两个方向评估关键性；2) 多度量聚合（multi-metric aggregation），通过融合多种度量的信号以提高判别能力。

Result: 实验证明，不同度量在区分关键与非关键目标上表现各异。采用双向评分与多度量聚合后，在关键性分类任务上最高可实现约100%的相对提升（相较于基线单一度量），表明组合策略在安全评估中具有显著优势。

Conclusion: 本文通过对现有关键性（criticality）度量的系统性综述与实证评估，提出并验证了两种新的应用策略，显著提升了基于关键性的安全评估在目标检测系统中的表现。

Abstract: Ensuring safety is the primary objective of automated driving, which necessitates a comprehensive and accurate perception of the environment. While numerous performance evaluation metrics exist for assessing perception capabilities, incorporating safety-specific metrics is essential to reliably evaluate object detection systems. A key component for safety evaluation is the ability to distinguish between relevant and non-relevant objects - a challenge addressed by criticality or relevance metrics. This paper presents the first in-depth analysis of criticality metrics for safety evaluation of object detection systems. Through a comprehensive review of existing literature, we identify and assess a range of applicable metrics. Their effectiveness is empirically validated using the DeepAccident dataset, which features a variety of safety-critical scenarios. To enhance evaluation accuracy, we propose two novel application strategies: bidirectional criticality rating and multi-metric aggregation. Our approach demonstrates up to a 100% improvement in terms of criticality classification accuracy, highlighting its potential to significantly advance the safety evaluation of object detection systems in automated vehicles.

</details>


### [31] [Robust and Calibrated Detection of Authentic Multimedia Content](https://arxiv.org/abs/2512.15182)
*Sarim Hashmi,Abdelrahman Elsayed,Mohammed Talha Alam,Samuele Poppi,Nils Lukas*

Main category: cs.CV

TL;DR: 用反演+重生成来验证样本是否真实，通过校准控制低误报率，并对计算受限的对手更稳健。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测存在两大问题：事后鉴别有不可避免的高误报（如记忆化样本）以及对抗样本能以低计算成本轻易绕过检测器，迫切需要在高精度、低误报的场景下提高鉴定可靠性和对抗鲁棒性。

Method: 对输入样本进行反演以获得生成器潜在表示，再用相同或受控的生成器重生成样本；通过设计校准策略和相似性度量，控制误报率并判断样本是否可被合理否认；评估重点在高精度低召回下对计算受限对手的鲁棒性。

Result: 提出了一种基于重生成(resynthesis)的深度伪造鉴定框架，旨在在高精度、低召回的场景下对抗计算受限的对手。方法通过将待验证样本经过反演并重生成，比较重生成样本与原样本的相似性来判断真实性。作者宣称：与现有后验检测方法相比，校准的重生成方法在维持可控低误报率(FPR)下更可靠；在对抗鲁棒性方面，对计算受限的对手具有显著优势；并且支持多模态数据，利用了最新的反演技术。

Conclusion: 重生成校准能够在保证低误报率的前提下提高真实样本验证的可靠性，并在有限计算预算的对手面前展现出更强的鲁棒性，适用于多模态场景。

Abstract: Generative models can synthesize highly realistic content, so-called deepfakes, that are already being misused at scale to undermine digital media authenticity. Current deepfake detection methods are unreliable for two reasons: (i) distinguishing inauthentic content post-hoc is often impossible (e.g., with memorized samples), leading to an unbounded false positive rate (FPR); and (ii) detection lacks robustness, as adversaries can adapt to known detectors with near-perfect accuracy using minimal computational resources. To address these limitations, we propose a resynthesis framework to determine if a sample is authentic or if its authenticity can be plausibly denied. We make two key contributions focusing on the high-precision, low-recall setting against efficient (i.e., compute-restricted) adversaries. First, we demonstrate that our calibrated resynthesis method is the most reliable approach for verifying authentic samples while maintaining controllable, low FPRs. Second, we show that our method achieves adversarial robustness against efficient adversaries, whereas prior methods are easily evaded under identical compute budgets. Our approach supports multiple modalities and leverages state-of-the-art inversion techniques.

</details>


### [32] [ERIENet: An Efficient RAW Image Enhancement Network under Low-Light Environment](https://arxiv.org/abs/2512.15186)
*Jianan Wang,Yang Hong,Hesong Li,Tao Wang,Songrong Liu,Ying Fu*

Main category: cs.CV

TL;DR: ERIENet利用并行多尺度结构与绿色通道引导，轻量且快速，显著提升RAW低光增强性能并实现4K实时处理。


<details>
  <summary>Details</summary>
Motivation: 现有RAW低光增强方法多为序列式多尺度处理，导致模型难以轻量化且速度慢；同时忽视RAW图像绿色通道的信息优势，未能充分利用以提升重建质量。

Method: 设计了高效并行多尺度网络和通道感知残差密集块提取特征，另设绿色通道引导分支利用绿色通道信息辅助重建，从而在参数和计算受限下提升性能并实现实时处理。

Result: 提出了一种高效的RAW图像低光增强网络（ERIENet），通过并行多尺度处理和绿色通道引导提高重建质量与速度。

Conclusion: ERIENet在常用低光增强数据集上优于现有方法，参数少、计算量低，并能在RTX 3090上以超过146 FPS处理4K图像，实现实时且高效的RAW图像增强。

Abstract: RAW images have shown superior performance than sRGB images in many image processing tasks, especially for low-light image enhancement. However, most existing methods for RAW-based low-light enhancement usually sequentially process multi-scale information, which makes it difficult to achieve lightweight models and high processing speeds. Besides, they usually ignore the green channel superiority of RAW images, and fail to achieve better reconstruction performance with good use of green channel information. In this work, we propose an efficient RAW Image Enhancement Network (ERIENet), which parallelly processes multi-scale information with efficient convolution modules, and takes advantage of rich information in green channels to guide the reconstruction of images. Firstly, we introduce an efficient multi-scale fully-parallel architecture with a novel channel-aware residual dense block to extract feature maps, which reduces computational costs and achieves real-time processing speed. Secondly, we introduce a green channel guidance branch to exploit the rich information within the green channels of the input RAW image. It increases the quality of reconstruction results with few parameters and computations. Experiments on commonly used low-light image enhancement datasets show that ERIENet outperforms state-of-the-art methods in enhancing low-light RAW images with higher effiency. It also achieves an optimal speed of over 146 frame-per-second (FPS) for 4K-resolution images on a single NVIDIA GeForce RTX 3090 with 24G memory.

</details>


### [33] [TBC: A Target-Background Contrast Metric for Low-Altitude Infrared and Visible Image Fusion](https://arxiv.org/abs/2512.15211)
*Yufeng Xie*

Main category: cs.CV

TL;DR: 提出TBC指标，基于目标与背景对比并惩罚背景噪声，能避免EN/AG的噪声陷阱，在DroneVehicle上与主观感知更一致。


<details>
  <summary>Details</summary>
Motivation: 传统无参考指标在低光/复杂背景下将高频噪声误判为细节，误导融合算法；需要一个侧重目标显著性并抑制背景噪声的评价指标。

Method: Propose a novel evaluation metric called Target-Background Contrast (TBC) for infrared-visible image fusion in low-altitude UAV scenarios; inspired by Weber's Law, compute contrast between detected salient target region and local background, incorporate noise penalty using background variance and spatial frequency weighting.

Result: TBC yields scores that correlate well with subjective human judgments on DroneVehicle dataset, effectively penalizes noisy high-frequency artifacts that traditional metrics (EN, AG) mistakenly reward, and leads to better guidance for fusion algorithm selection.

Conclusion: TBC is more aligned with human perception in low-light UAV imagery, reduces the Noise Trap by punishing background noise and rewarding true target visibility, and can be used as a reliable no-reference metric for evaluation and optimization of fusion algorithms in such scenarios.

Abstract: Infrared and visible image fusion is a pivotal technology in low-altitude UAV reconnaissance missions, providing high-quality data support for downstream tasks such as target detection and tracking by integrating thermal saliency with background texture details.However, traditional no-reference metrics fail(Specifically,like Entropy (EN) and Average Gradient (AG)) in complex low-light environments. They often misinterpret high-frequency sensor noise as valid detail. This creates a "Noise Trap," paradoxically assigning higher scores to noisy images and misguiding fusion algorithms.To address this, we propose the Target-Background Contrast (TBC) metric. Inspired by Weber's Law, TBC focuses on the relative contrast of salient targets rather than global statistics. Unlike traditional metrics, TBC penalizes background noise and rewards target visibility. Experiments on the DroneVehicle dataset demonstrate that TBC aligns better with human perception and provides a reliable standard for low-altitude scenarios.

</details>


### [34] [From Camera to World: A Plug-and-Play Module for Human Mesh Transformation](https://arxiv.org/abs/2512.15212)
*Changhai Ma,Ziyu Wu,Yunkang Zhang,Qijun Ying,Boyan Liu,Xiaohui Cai*

Main category: cs.CV

TL;DR: 提出 Mesh-Plug 模块，将相机坐标系下重建的人体网格准确变换到世界坐标系；通过对人体中心信息（RGB 与从初始网格渲染的深度图）估计相机俯仰角，并结合网格调整模块同时优化根关节朝向和体态；在 SPEC-SYN 与 SPEC-MTP 上优于 SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法常假设相机旋转为零，仅在相机坐标系下工作，但在转换到世界坐标系时会产生显著误差；直接估计环境或使用外部传感器不够通用或不可用。作者希望以仅依赖人体本身信息的方式恢复相机旋转，从而准确将人体网格置于世界坐标系中。

Method: 提出可插拔的 Mesh-Plug 模块。先基于初始重建的网格渲染深度图，结合原始 RGB 图像，训练一个聚焦人体空间布局的相机旋转预测网络，主要估计相机俯仰角（pitch）。随后使用预测的相机参数，将其与初始网格输入到网格调整模块，该模块联合优化根关节方向和身体姿态以修正由于旋转误差带来的偏差。该模块为“plug-and-play”，可与现有相机坐标系下的人体重建方法配合使用。

Result: 在 SPEC-SYN 与 SPEC-MTP 基准上，作者的框架在将网格转换到世界坐标系的精度指标上优于现有最先进方法。具体表现为更准确的相机俯仰估计、改进的根关节方向和更紧致的三维人体对齐。

Conclusion: Mesh-Plug 通过利用 RGB+渲染深度图的“以人为中心”策略，有效估计相机旋转并在网格层面做联合调整，从而实现从相机坐标系到世界坐标系的精确转换；模块具备可插拔性，提升了在真实场景下的人体网格定位精度。

Abstract: Reconstructing accurate 3D human meshes in the world coordinate system from in-the-wild images remains challenging due to the lack of camera rotation information. While existing methods achieve promising results in the camera coordinate system by assuming zero camera rotation, this simplification leads to significant errors when transforming the reconstructed mesh to the world coordinate system. To address this challenge, we propose Mesh-Plug, a plug-and-play module that accurately transforms human meshes from camera coordinates to world coordinates. Our key innovation lies in a human-centered approach that leverages both RGB images and depth maps rendered from the initial mesh to estimate camera rotation parameters, eliminating the dependency on environmental cues. Specifically, we first train a camera rotation prediction module that focuses on the human body's spatial configuration to estimate camera pitch angle. Then, by integrating the predicted camera parameters with the initial mesh, we design a mesh adjustment module that simultaneously refines the root joint orientation and body pose. Extensive experiments demonstrate that our framework outperforms state-of-the-art methods on the benchmark datasets SPEC-SYN and SPEC-MTP.

</details>


### [35] [SLCFormer: Spectral-Local Context Transformer with Physics-Grounded Flare Synthesis for Nighttime Flare Removal](https://arxiv.org/abs/2512.15221)
*Xiyu Zhu,Wei Wang,Xin Yuan,Xiao Wang*

Main category: cs.CV

TL;DR: 提出SLCFormer：频域FFEM+空间DESM，并用ZernikeVAE生成真实散射耀斑，显著提升夜间镜头耀斑去除效果


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理非均匀散射耀斑，需结合光学物理与数据驱动建模以提高泛化性

Method: Spectral-local context transformer (SLCFormer) with FFEM and DESM; ZernikeVAE-based scatter flare generation pipeline

Result: SLCFormer achieves state-of-the-art performance on Flare7K++ dataset, improving quantitative metrics and perceptual quality, generalizes well to real nighttime scenes

Conclusion: Combining frequency-domain global modeling (FFEM) with spatial-domain directional enhancement (DESM) and realistic flare synthesis via ZernikeVAE yields robust removal of nonuniform scattered flares

Abstract: Lens flare is a common nighttime artifact caused by strong light sources scattering within camera lenses, leading to hazy streaks, halos, and glare that degrade visual quality. However, existing methods usually fail to effectively address nonuniform scattered flares, which severely reduces their applicability to complex real-world scenarios with diverse lighting conditions. To address this issue, we propose SLCFormer, a novel spectral-local context transformer framework for effective nighttime lens flare removal. SLCFormer integrates two key modules: the Frequency Fourier and Excitation Module (FFEM), which captures efficient global contextual representations in the frequency domain to model flare characteristics, and the Directionally-Enhanced Spatial Module (DESM) for local structural enhancement and directional features in the spatial domain for precise flare removal. Furthermore, we introduce a ZernikeVAE-based scatter flare generation pipeline to synthesize physically realistic scatter flares with spatially varying PSFs, bridging optical physics and data-driven training. Extensive experiments on the Flare7K++ dataset demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches in both quantitative metrics and perceptual visual quality, and generalizing robustly to real nighttime scenes with complex flare artifacts.

</details>


### [36] [Null-LoRA: Low-Rank Adaptation on Null Space](https://arxiv.org/abs/2512.15233)
*Yi Zhang,Yulei Kang,Haoxuan Chen,Jinxuan Li,ian-Fang Hu*

Main category: cs.CV

TL;DR: Null-LoRA在低秩适配中引入零空间约束和部分冻结机制，提升参数效率并在多项视觉语言任务中取得更好或相当的性能。


<details>
  <summary>Details</summary>
Motivation: 观察到预训练模型包含非平凡的零空间，常规LoRA在全参数空间的低秩适配存在冗余，因而探索在零空间或更小子空间中微调以提升效率。

Method: 1) 分析预训练模型权重的零空间并据此选择需要冻结的低秩矩阵分量；2) 对剩余分量执行低秩适配以提升有效秩；3) 将整个增量更新约束到零空间以最大化参数对于新任务范式的适应性；4) 在图文检索与视觉问答任务上进行大规模对比实验。

Result: Null-LoRA提出了一种基于零空间的低秩微调方法，通过冻结低秩矩阵的部分参数并将增量更新限制在预训练模型的零空间中，提高了参数效率和有效秩，适用于图文检索和视觉问答，大幅减少参数量同时超越现有方法。

Conclusion: 在保持或提升任务性能的前提下，Null-LoRA通过利用预训练模型的零空间和选择性冻结低秩矩阵，显著降低了微调所需的参数量，证明了在子空间内进行微调的可行性与优势。

Abstract: Parameter-efficient fine-tuning methods have gained considerable popularity for adapting large-scale models to downstream tasks, particularly LoRA and its variants. Existing methods perform low-rank adaptation over the full parameter space. However, fine-tuning within a subspace can achieve comparable effectiveness. Inspired by the observation that pre-trained models possess non-trivial null spaces, we propose Null-space based Low-Rank Adaptation (Null-LoRA). Null-LoRA effectively reduces redundancy and enhances effective rank by freezing portions of the low-rank matrices. To further improve parameter efficiency, Null-LoRA constrains the entire incremental update within the null space, maximizing the utilization of incremental updates to adapt to new task paradigms. Null-LoRA surpasses the state of the art with fewer parameters in extensive experiments across image-text retrieval and visual question answering tasks.

</details>


### [37] [Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification](https://arxiv.org/abs/2512.15249)
*Yupeng Zhang,Adam G. Dunn,Usman Naseem,Jinman Kim*

Main category: cs.CV

TL;DR: 提出CMAC-MMD训练框架，通过对齐跨模态表示一致性来标准化不同交叉群体的诊断置信度，显著降低了ΔTPR并提升或保持AUC；在不使用敏感属性做推断的前提下改善公平性与性能。


<details>
  <summary>Details</summary>
Motivation: Address intersectional biases in multimodal medical AI where marginalised subgroups have lower diagnostic confidence; create a method that equalizes decision certainty without requiring demographics at inference.

Method: Contrastive cross-modal regularization to equalize confidence across groups

Result: On HAM10000 (train) and external BCN20000, CMAC-MMD reduced intersectional ΔTPR from 0.50 to 0.26 and increased AUC from 0.94 to 0.97. On Harvard-FairVLMed glaucoma dataset, ΔTPR reduced from 0.41 to 0.31 and AUC improved to 0.72 from 0.71.

Conclusion: CMAC-MMD can reduce intersectional diagnostic confidence gaps and improve or maintain overall discrimination without needing demographic data at inference, offering a scalable fairness-aware training approach for multimodal medical AI.

Abstract: Medical artificial intelligence (AI) systems, particularly multimodal vision-language models (VLM), often exhibit intersectional biases where models are systematically less confident in diagnosing marginalised patient subgroups. Such bias can lead to higher rates of inaccurate and missed diagnoses due to demographically skewed data and divergent distributions of diagnostic certainty. Current fairness interventions frequently fail to address these gaps or compromise overall diagnostic performance to achieve statistical parity among the subgroups. In this study, we developed Cross-Modal Alignment Consistency (CMAC-MMD), a training framework that standardises diagnostic certainty across intersectional patient subgroups. Unlike traditional debiasing methods, this approach equalises the model's decision confidence without requiring sensitive demographic data during clinical inference. We evaluated this approach using 10,015 skin lesion images (HAM10000) with external validation on 12,000 images (BCN20000), and 10,000 fundus images for glaucoma detection (Harvard-FairVLMed), stratifying performance by intersectional age, gender, and race attributes. In the dermatology cohort, the proposed method reduced the overall intersectional missed diagnosis gap (difference in True Positive Rate, $Δ$TPR) from 0.50 to 0.26 while improving the overall Area Under the Curve (AUC) from 0.94 to 0.97 compared to standard training. Similarly, for glaucoma screening, the method reduced $Δ$TPR from 0.41 to 0.31, achieving a better AUC of 0.72 (vs. 0.71 baseline). This establishes a scalable framework for developing high-stakes clinical decision support systems that are both accurate and can perform equitably across diverse patient subgroups, ensuring reliable performance without increasing privacy risks.

</details>


### [38] [Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models](https://arxiv.org/abs/2512.15254)
*Kuinan Hou,Jing Mi,Marco Zorzi,Lamberto Ballan,Alberto Testolin*

Main category: cs.CV

TL;DR: 研究表明大型视觉-语言模型能在开放集计数任务中有效枚举物体，提示中间表达（位置+标签）能显著提升准确率，但在复杂视觉场景下仍无法稳定计数。


<details>
  <summary>Details</summary>
Motivation: 传统专用计数方法依赖预定义类别和特定数据集，限制了开放集计数的通用性。随着大规模多模态视觉-语言模型的发展，需要评估这些通用架构在计数任务中的表现及其潜力。

Method: 系统性比较了最先进的专用计数架构与多种VLMs，在两个流行计数数据集和一个新构建的细粒度可控基准上进行评测；并测试不同提示策略，尤其是要求模型生成中间表示（每个对象的位置和标签）。

Result: 多数VLMs能大致枚举场景中的物体数量，在某些条件下优于专用架构；生成中间表示的提示显著提升计数准确性；但在复杂场景下所有模型表现均不可靠。

Conclusion: VLMs在计数任务上表现接近甚至优于专用计数架构，但在复杂场景下仍然不可靠，需要进一步研究。

Abstract: Counting the number of items in a visual scene remains a fundamental yet challenging task in computer vision. Traditional approaches to solving this problem rely on domain-specific counting architectures, which are trained using datasets annotated with a predefined set of object categories. However, recent progress in creating large-scale multimodal vision-language models (VLMs) suggests that these domain-general architectures may offer a flexible alternative for open-set object counting. In this study, we therefore systematically compare the performance of state-of-the-art specialized counting architectures against VLMs on two popular counting datasets, as well as on a novel benchmark specifically created to have a finer-grained control over the visual properties of test images. Our findings show that most VLMs can approximately enumerate the number of items in a visual scene, matching or even surpassing the performance of specialized computer vision architectures. Notably, enumeration accuracy significantly improves when VLMs are prompted to generate intermediate representations (i.e., locations and verbal labels) of each object to be counted. Nevertheless, none of the models can reliably count the number of objects in complex visual scenes, showing that further research is still needed to create AI systems that can reliably deploy counting procedures in realistic environments.

</details>


### [39] [MMMamba: A Versatile Cross-Modal In Context Fusion Framework for Pan-Sharpening and Zero-Shot Image Enhancement](https://arxiv.org/abs/2512.15261)
*Yingying Wang,Xuanhua He,Chen Wu,Jialing Huang,Suiyun Zhang,Rui Liu,Xinghao Ding,Haoxuan Che*

Main category: cs.CV

TL;DR: 提出基于MMDiT的MMMamba，采用in-context融合与MI交错扫描，在泛锐化与零样本超分上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统CNN和交叉注意力在跨模态信息利用上存在局限，需一种既高效又能保持细粒度对应的融合方法

Method: 从MMDiT与in-context conditioning角度设计跨模态融合框架MMMamba，提出MI交错扫描机制

Result: 在多模态信息高效交互与线性复杂度下，显著优于现有SOTA方法，支持零样本图像超分辨

Conclusion: MMMamba通过Mamba架构与MI扫描实现高效跨模态条件融合，在线性计算代价下提升全谱与空间细节恢复

Abstract: Pan-sharpening aims to generate high-resolution multispectral (HRMS) images by integrating a high-resolution panchromatic (PAN) image with its corresponding low-resolution multispectral (MS) image. To achieve effective fusion, it is crucial to fully exploit the complementary information between the two modalities. Traditional CNN-based methods typically rely on channel-wise concatenation with fixed convolutional operators, which limits their adaptability to diverse spatial and spectral variations. While cross-attention mechanisms enable global interactions, they are computationally inefficient and may dilute fine-grained correspondences, making it difficult to capture complex semantic relationships. Recent advances in the Multimodal Diffusion Transformer (MMDiT) architecture have demonstrated impressive success in image generation and editing tasks. Unlike cross-attention, MMDiT employs in-context conditioning to facilitate more direct and efficient cross-modal information exchange. In this paper, we propose MMMamba, a cross-modal in-context fusion framework for pan-sharpening, with the flexibility to support image super-resolution in a zero-shot manner. Built upon the Mamba architecture, our design ensures linear computational complexity while maintaining strong cross-modal interaction capacity. Furthermore, we introduce a novel multimodal interleaved (MI) scanning mechanism that facilitates effective information exchange between the PAN and MS modalities. Extensive experiments demonstrate the superior performance of our method compared to existing state-of-the-art (SOTA) techniques across multiple tasks and benchmarks.

</details>


### [40] [SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2512.15310)
*Wangyu Wu,Zhenhong Chen,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: 提出ZSWSSS问题和SynthSeg Agents框架，用LLM驱动的多智能体从无真实图像生成语义丰富的合成训练数据，通过CLIP筛选并用ViT重标注，实现在PASCAL VOC和COCO上有竞争力的弱监督分割表现。


<details>
  <summary>Details</summary>
Motivation: 消除对真实带注释图像的依赖，通过LLM与VLM合作自动生成大规模、语义丰富且多样的合成图像及标签，以降低成本并提升可扩展性。

Method: Multi-agent LLM-driven synthetic data generation for zero-shot weakly supervised semantic segmentation

Result: SynthSeg Agents generates synthetic images and image-level labels without any real images. Self Refine Prompt Agent iteratively crafts diverse prompts using memory, exploration, CLIP similarity and diversity filtering. Image Generation Agent uses VLMs to synthesize images; CLIP scores select high-quality samples; ViT classifier relabels dataset for improved semantic precision. Trained segmentation uses these synthetic labels for WSSS tasks.

Conclusion: SynthSeg Agents can produce high-quality training data entirely from synthetic sources, achieving competitive segmentation results on PASCAL VOC 2012 and COCO 2014 without real images, demonstrating feasibility of LLM-driven agent pipelines for scalable, cost-efficient segmentation.

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image level labels aims to produce pixel level predictions without requiring dense annotations. While recent approaches have leveraged generative models to augment existing data, they remain dependent on real world training samples. In this paper, we introduce a novel direction, Zero Shot Weakly Supervised Semantic Segmentation (ZSWSSS), and propose SynthSeg Agents, a multi agent framework driven by Large Language Models (LLMs) to generate synthetic training data entirely without real images. SynthSeg Agents comprises two key modules, a Self Refine Prompt Agent and an Image Generation Agent. The Self Refine Prompt Agent autonomously crafts diverse and semantically rich image prompts via iterative refinement, memory mechanisms, and prompt space exploration, guided by CLIP based similarity and nearest neighbor diversity filtering. These prompts are then passed to the Image Generation Agent, which leverages Vision Language Models (VLMs) to synthesize candidate images. A frozen CLIP scoring model is employed to select high quality samples, and a ViT based classifier is further trained to relabel the entire synthetic dataset with improved semantic precision. Our framework produces high quality training data without any real image supervision. Experiments on PASCAL VOC 2012 and COCO 2014 show that SynthSeg Agents achieves competitive performance without using real training images. This highlights the potential of LLM driven agents in enabling cost efficient and scalable semantic segmentation.

</details>


### [41] [KD360-VoxelBEV: LiDAR and 360-degree Camera Cross Modality Knowledge Distillation for Bird's-Eye-View Segmentation](https://arxiv.org/abs/2512.15311)
*Wenke E,Yixin Sun,Jiaxu Liu,Hubert P. H. Shum,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.CV

TL;DR: First cross-modality distillation for single-panoramic-camera BEV segmentation using LiDAR image rep and voxel-aligned view transformer; teacher-student training yields strong IoU gains and 31.2 FPS inference.


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on expensive/multiple sensors (LiDAR) for BEV segmentation while retaining high performance by transferring knowledge from LiDAR-rich teacher to camera-only student.

Method: Create fused LiDAR image from range, intensity, ambient channels; use voxel-aligned view transformer to preserve spatial fidelity and convert to BEV; train a LiDAR+camera fusion teacher to extract rich features and distil to a single-panoramic-camera student network via cross-modality distillation losses.

Result: The paper proposes a cross-modality distillation framework for BEV segmentation using a LiDAR-camera fusion teacher and a single panoramic camera student. It introduces a LiDAR image representation (range, intensity, ambient) and a voxel-aligned view transformer. Teacher extracts rich spatial/semantic features; student learns via distillation. Experiments on Dur360BEV and KITTI-360 show large IoU improvements and high FPS.

Conclusion: Cross-modality distillation enables a single panoramic camera model to approach LiDAR-fused performance, reducing sensor costs while maintaining competitive accuracy and high speed. The approach generalises to other camera setups.

Abstract: We present the first cross-modality distillation framework specifically tailored for single-panoramic-camera Bird's-Eye-View (BEV) segmentation. Our approach leverages a novel LiDAR image representation fused from range, intensity and ambient channels, together with a voxel-aligned view transformer that preserves spatial fidelity while enabling efficient BEV processing. During training, a high-capacity LiDAR and camera fusion Teacher network extracts both rich spatial and semantic features for cross-modality knowledge distillation into a lightweight Student network that relies solely on a single 360-degree panoramic camera image. Extensive experiments on the Dur360BEV dataset demonstrate that our teacher model significantly outperforms existing camera-based BEV segmentation methods, achieving a 25.6\% IoU improvement. Meanwhile, the distilled Student network attains competitive performance with an 8.5\% IoU gain and state-of-the-art inference speed of 31.2 FPS. Moreover, evaluations on KITTI-360 (two fisheye cameras) confirm that our distillation framework generalises to diverse camera setups, underscoring its feasibility and robustness. This approach reduces sensor complexity and deployment costs while providing a practical solution for efficient, low-cost BEV segmentation in real-world autonomous driving.

</details>


### [42] [Automated Motion Artifact Check for MRI (AutoMAC-MRI): An Interpretable Framework for Motion Artifact Detection and Severity Assessment](https://arxiv.org/abs/2512.15315)
*Antony Jerald,Dattesh Shanbhag,Sudhanya Chatterjee*

Main category: cs.CV

TL;DR: AutoMAC-MRI使用监督对比学习学习运动严重度的判别性特征，并在该特征空间计算每个等级的亲和力分数以量化与各等级的接近度，实验证明在5000+专家标注的脑MRI切片上与专家判断一致。


<details>
  <summary>Details</summary>
Motivation: 运动伪影降低MRI图像质量并增加患者复扫，需要自动化且可解释的质量评估工具，而现有方法多为二分类且缺乏可解释性。

Method: 采用监督对比学习构建对运动严重度敏感的嵌入空间；基于此特征空间计算每个等级的中心或原型并定义亲和力（距离或相似度）分数，从而为每张图像提供多等级的亲和力向量并据此做分级决策。

Result: 在超过5000张专家标注的脑MRI切片（多对比度、多视角）上评估，亲和力分数与专家标签高度对齐，分级准确且解释性强，可用于inline质量控制以减少不必要复扫并提升工作流程效率。

Conclusion: 本文提出了一个可解释的MRI运动伪影分级框架AutoMAC-MRI，能在多种对比度和视角上评估运动伪影严重度，并通过分级亲和力分数提供透明可解释的评分依据。

Abstract: Motion artifacts degrade MRI image quality and increase patient recalls. Existing automated quality assessment methods are largely limited to binary decisions and provide little interpretability. We introduce AutoMAC-MRI, an explainable framework for grading motion artifacts across heterogeneous MR contrasts and orientations. The approach uses supervised contrastive learning to learn a discriminative representation of motion severity. Within this feature space, we compute grade-specific affinity scores that quantify an image's proximity to each motion grade, thereby making grade assignments transparent and interpretable. We evaluate AutoMAC-MRI on more than 5000 expert-annotated brain MRI slices spanning multiple contrasts and views. Experiments assessing affinity scores against expert labels show that the scores align well with expert judgment, supporting their use as an interpretable measure of motion severity. By coupling accurate grade detection with per-grade affinity scoring, AutoMAC-MRI enables inline MRI quality control, with the potential to reduce unnecessary rescans and improve workflow efficiency.

</details>


### [43] [Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection](https://arxiv.org/abs/2512.15319)
*Yuxin Jiang,Yunkang Cao,Weiming Shen*

Main category: cs.CV

TL;DR: 针对少样本异常检测的域差距问题，作者提出PCSNet，结合原型引导的特征适配与伪异常辅助的上下文感知分割，显著提升了异常定位与检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前FSAD方法依赖预训练特征，忽视了预训练表征与目标场景间的域差距，导致目标场景下特征可分性不足，从而影响异常检测效果。

Method: PCSNet由PFA与CAS两部分组成：PFA提取原型特征以增强正常样本的类内紧凑性并通过像素级差异分类损失扩大异常-正常的分离；CAS子网利用伪造异常进行像素级分割训练以实现异常定位。实验在MVTec、MPDD及实际工业数据集上验证效果。

Result: 提出PCSNet，一种包含原型特征适配(PFA)与上下文感知分割(CAS)的网络，用于解决预训练表征与目标FSAD场景的域差距，提高特征判别性与少样本异常检测性能。

Conclusion: PCSNet在MVTec与MPDD上实现了优越性能（8-shot情形下图像级AUROC分别为94.9%与80.2%），并在汽车塑料件检测的实际应用中表现良好，证明其在有限样本条件下的实用性。

Abstract: Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. Existing FSAD methods largely rely on pre-trained feature representations to detect anomalies, but the inherent domain gap between pre-trained representations and target FSAD scenarios is often overlooked. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap, thereby improving feature descriptiveness in target scenarios and enhancing FSAD performance. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples. Code is available at https://github.com/yuxin-jiang/PCSNet.

</details>


### [44] [MECAD: A multi-expert architecture for continual anomaly detection](https://arxiv.org/abs/2512.15323)
*Malihe Dahmardeh,Francesco Setti*

Main category: cs.CV

TL;DR: MECAD用多专家、coreset和重放缓冲实现无需重训练的持续异常检测，5专家在MVTec AD上平均AUROC=0.8259，较单专家更能防止遗忘。


<details>
  <summary>Details</summary>
Motivation: 现实工业场景中产品种类不断变化，需系统能增量学习新类别同时保留旧知识，避免频繁重训练。MECAD旨在提供计算高效且可扩展的持续异常检测方案。

Method: 设计多专家架构，按特征相似性将类别分配给不同专家；使用优化的coreset选择和专门的重放缓冲机制管理记忆，实现增量学习而无需完全重训练；评估不同专家数量对性能与遗忘的影响，并在MVTec AD上实验。

Result: MECAD提出了一种多专家（multi-expert）架构用于持续异常检测（continual anomaly detection），通过基于特征相似性动态分配专家到物体类别，并使用高效的记忆管理保存先前类别的知识。该方法使用优化的coreset选择和专门的重放缓冲机制，实现增量学习而无需完全重训练模型。实验在MVTec AD数据集上，最优的5专家配置在15个类别上平均AUROC为0.8259，并且相比单专家方法显著降低了知识退化。

Conclusion: MECAD能在保持计算效率的同时，兼顾类别专用知识保留与适应性，适用于产品类型不断变化的工业场景；5专家配置在MVTec AD上表现最好，但整体AUROC仍有提升空间。

Abstract: In this paper we propose MECAD, a novel approach for continual anomaly detection using a multi-expert architecture. Our system dynamically assigns experts to object classes based on feature similarity and employs efficient memory management to preserve the knowledge of previously seen classes. By leveraging an optimized coreset selection and a specialized replay buffer mechanism, we enable incremental learning without requiring full model retraining. Our experimental evaluation on the MVTec AD dataset demonstrates that the optimal 5-expert configuration achieves an average AUROC of 0.8259 across 15 diverse object categories while significantly reducing knowledge degradation compared to single-expert approaches. This framework balances computational efficiency, specialized knowledge retention, and adaptability, making it well-suited for industrial environments with evolving product types.

</details>


### [45] [A Masked Reverse Knowledge Distillation Method Incorporating Global and Local Information for Image Anomaly Detection](https://arxiv.org/abs/2512.15326)
*Yuxin Jiang,Yunkang Can,Weiming Shen*

Main category: cs.CV

TL;DR: 提出Masked Reverse Knowledge Distillation (MRKD)，通过图像级掩码(ILM)与特征级掩码(FLM)将重建任务转为修复任务，减少知识蒸馏中过度泛化，提升异常检测定位性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于知识蒸馏的图像异常检测因输入与监督信号相似而易产生过度泛化，导致模型难以区分正常与异常细节。需要方法增强语境感知并引入局部异常信息以抑制过泛化。

Method: 提出MRKD：在蒸馏框架中加入图像级掩码(ILM)以遮挡部分输入，迫使模型利用全局上下文恢复被遮挡区域，从而减少输入-监督信号相似性；同时在特征层引入合成异常的特征级掩码(FLM)，在训练中注入局部异常特征以促使学习到区分正常/异常的局部表示。二者结合将重建任务转为修复任务，提高对全局与局部信息的捕获。

Result: 在MVTec数据集上表现优异：图像级AU-ROC 98.9%、像素级AU-ROC 98.4%、AU-PRO 95.3%。消融实验显示ILM和FLM均对降低过度泛化与提升性能有显著贡献。

Conclusion: MRKD通过ILM与FLM有效缓解了知识蒸馏在异常检测中的过度泛化问题，增强了模型的全局语境与局部异常识别能力，在标准基准上取得了领先性能。

Abstract: Knowledge distillation is an effective image anomaly detection and localization scheme. However, a major drawback of this scheme is its tendency to overly generalize, primarily due to the similarities between input and supervisory signals. In order to address this issue, this paper introduces a novel technique called masked reverse knowledge distillation (MRKD). By employing image-level masking (ILM) and feature-level masking (FLM), MRKD transforms the task of image reconstruction into image restoration. Specifically, ILM helps to capture global information by differentiating input signals from supervisory signals. On the other hand, FLM incorporates synthetic feature-level anomalies to ensure that the learned representations contain sufficient local information. With these two strategies, MRKD is endowed with stronger image context capture capacity and is less likely to be overgeneralized. Experiments on the widely-used MVTec anomaly detection dataset demonstrate that MRKD achieves impressive performance: image-level 98.9% AU-ROC, pixel-level 98.4% AU-ROC, and 95.3% AU-PRO. In addition, extensive ablation experiments have validated the superiority of MRKD in mitigating the overgeneralization problem.

</details>


### [46] [Vision-based module for accurately reading linear scales in a laboratory](https://arxiv.org/abs/2512.15327)
*Parvesh Saini,Soumyadipta Maiti,Beena Rai*

Main category: cs.CV

TL;DR: 提出一种基于视觉的线性刻度读数系统，针对注射器和量筒，通过方向校正、兴趣区域提取和特征检测（主刻线、数字、液位指示）计算读数，并与人工读数对比，结果准确匹配。


<details>
  <summary>Details</summary>
Motivation: 机器视觉模型在检测和分类方面表现优异，但能像人眼直接从图像中准确量取线性刻度（如注射器、量筒）并不常见。对于实验室机器人实现高自主性，读取仪器刻度是必要技能，因此设计一个鲁棒、高效的刻度读数系统。

Method: 对随机朝向的注射器先进行方向校正，然后裁剪出仅包含线性刻度的感兴趣区域以提高效率。提取特征包括主要刻度线位置、对应数字位置与液位指示位置，基于这些特征计算最终读数。方法包含图像变换、刻度线与数字的检测与匹配、以及液位定位。

Result: 系统在注射器和量筒样本上给出读数，并与人工读取值对比，二者高度一致，证明方法在准确性上可达人类水平。

Conclusion: 所提出的人类启发的线性刻度读数方法在实验室常见器具（注射器、量筒）上表现良好，具有实现机器人自主读取仪器刻度的潜力。

Abstract: Capabilities and the number of vision-based models are increasing rapidly. And these vision models are now able to do more tasks like object detection, image classification, instance segmentation etc. with great accuracy. But models which can take accurate quantitative measurements form an image, as a human can do by just looking at it, are rare. For a robot to work with complete autonomy in a Laboratory environment, it needs to have some basic skills like navigation, handling objects, preparing samples etc. to match human-like capabilities in an unstructured environment. Another important capability is to read measurements from instruments and apparatus. Here, we tried to mimic a human inspired approach to read measurements from a linear scale. As a test case we have picked reading level from a syringe and a measuring cylinder. For a randomly oriented syringe we carry out transformations to correct the orientation. To make the system efficient and robust, the area of interest is reduced to just the linear scale containing part of the image. After that, a series of features were extracted like the major makers, the corresponding digits, and the level indicator location, from which the final reading was calculated. Readings obtained using this system were also compared against human read values of the same instances and an accurate correspondence was observed.

</details>


### [47] [Towards Seamless Interaction: Causal Turn-Level Modeling of Interactive 3D Conversational Head Dynamics](https://arxiv.org/abs/2512.15340)
*Junjie Chen,Fei Wang,Zhihao Huang,Qing Zhou,Kun Li,Dan Guo,Linfeng Zhang,Xun Yang*

Main category: cs.CV

TL;DR: TIMAR: causal turn-level auto-regressive model with diffusion head, improves 3D head generation (15-30% lower FD/MSE).


<details>
  <summary>Details</summary>
Motivation: Human conversation involves continuous exchanges of speech and nonverbal cues; existing models fail to capture causal, interleaved dynamics across turns.

Method: TIMAR: Turn-level Interleaved Masked AutoRegression; fuses multimodal info within turns; uses turn-level causal attention to accumulate history; lightweight diffusion head predicts continuous 3D head dynamics.

Result: On DualTalk, TIMAR reduces Fréchet Distance and MSE by 15-30% on test set and similar gains on OOD data.

Conclusion: TIMAR provides a causal, turn-aware framework improving temporal coherence and expressive variability for 3D conversational head generation.

Abstract: Human conversation involves continuous exchanges of speech and nonverbal cues such as head nods, gaze shifts, and facial expressions that convey attention and emotion. Modeling these bidirectional dynamics in 3D is essential for building expressive avatars and interactive robots. However, existing frameworks often treat talking and listening as independent processes or rely on non-causal full-sequence modeling, hindering temporal coherence across turns. We present TIMAR (Turn-level Interleaved Masked AutoRegression), a causal framework for 3D conversational head generation that models dialogue as interleaved audio-visual contexts. It fuses multimodal information within each turn and applies turn-level causal attention to accumulate conversational history, while a lightweight diffusion head predicts continuous 3D head dynamics that captures both coordination and expressive variability. Experiments on the DualTalk benchmark show that TIMAR reduces Fréchet Distance and MSE by 15-30% on the test set, and achieves similar gains on out-of-distribution data. The source code will be released in the GitHub repository https://github.com/CoderChen01/towards-seamleass-interaction.

</details>


### [48] [Expand and Prune: Maximizing Trajectory Diversity for Effective GRPO in Generative Models](https://arxiv.org/abs/2512.15347)
*Shiran Ge,Chenyi Huang,Yuang Ai,Qihang Fan,Huaibo Huang,Ran He*

Main category: cs.CV

TL;DR: GRPO suffers from reward clustering; OVF helps but is wasteful; Pro-GRPO prunes trajectories early using latent features and applies multi-step OVF after expanding samples, reducing cost and improving performance across generative models.


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost of GRPO while preserving or improving alignment performance by addressing reward clustering and leveraging latent-based early pruning to avoid sampling waste.

Method: Empirical analysis of GRPO and proposal of Pro-GRPO

Result: Found reward clustering; OVF selecting high-variance subset outperforms unfiltered groups; Pro-GRPO prunes trajectories early via latent features and uses Expand-and-Prune with multi-step OVF to reduce cost and improve diversity; validated on diffusion and flow models

Conclusion: Pro-GRPO achieves computational savings and better alignment by proactive latent-based pruning and staged OVF, outperforming static OVF and standard GRPO

Abstract: Group Relative Policy Optimization (GRPO) is a powerful technique for aligning generative models, but its effectiveness is bottlenecked by the conflict between large group sizes and prohibitive computational costs. In this work, we investigate the trade-off through empirical studies, yielding two key observations. First, we discover the reward clustering phenomenon in which many trajectories collapse toward the group-mean reward, offering limited optimization value. Second, we design a heuristic strategy named Optimal Variance Filtering (OVF), and verify that a high-variance subset of trajectories, selected by OVF can outperform the larger, unfiltered group. However, this static, post-sampling OVF approach still necessitates critical computational overhead, as it performs unnecessary sampling for trajectories that are ultimately discarded. To resolve this, we propose Pro-GRPO (Proactive GRPO), a novel dynamic framework that integrates latent feature-based trajectory pruning into the sampling process. Through the early termination of reward-clustered trajectories, Pro-GRPO reduces computational overhead. Leveraging its efficiency, Pro-GRPO employs an "Expand-and-Prune" strategy. This strategy first expands the size of initial sampling group to maximize trajectory diversity, then it applies multi-step OVF to the latents, avoiding prohibitive computational costs. Extensive experiments on both diffusion-based and flow-based models demonstrate the generality and effectiveness of our Pro-GRPO framework.

</details>


### [49] [SemanticBridge -- A Dataset for 3D Semantic Segmentation of Bridges and Domain Gap Analysis](https://arxiv.org/abs/2512.15369)
*Maximilian Kellner,Mariana Ferrandon Cervantes,Yuandong Pan,Ruodan Lu,Ioannis Brilakis,Alexander Reiterer*

Main category: cs.CV

TL;DR: 新数据集用于桥梁3D语义分割，评测三种模型；传感器差异引起的域迁移可导致最高11.4% mIoU下降。


<details>
  <summary>Details</summary>
Motivation: 为桥梁结构健康监测提供专门的高质量3D语义分割数据集，并研究不同传感器引起的域差异对模型性能的影响。

Method: 收集多国不同桥梁的高分辨率3D扫描数据并逐点标注语义标签；选取并训练/测试三种SOTA 3D语义分割架构；用来自多种传感器的数据进行跨传感器测试以量化域差距。

Result: The paper introduces a new high-resolution 3D bridge dataset with semantic labels and analyzes domain gaps across sensors; evaluates three SOTA 3D segmentation models and finds up to 11.4% mIoU drop due to sensor variation.

Conclusion: The new dataset enables benchmarking for bridge component segmentation; while current models perform well overall, sensor-induced domain gaps significantly reduce accuracy and should be addressed in future work.

Abstract: We propose a novel dataset that has been specifically designed for 3D semantic segmentation of bridges and the domain gap analysis caused by varying sensors. This addresses a critical need in the field of infrastructure inspection and maintenance, which is essential for modern society. The dataset comprises high-resolution 3D scans of a diverse range of bridge structures from various countries, with detailed semantic labels provided for each. Our initial objective is to facilitate accurate and automated segmentation of bridge components, thereby advancing the structural health monitoring practice. To evaluate the effectiveness of existing 3D deep learning models on this novel dataset, we conduct a comprehensive analysis of three distinct state-of-the-art architectures. Furthermore, we present data acquired through diverse sensors to quantify the domain gap resulting from sensor variations. Our findings indicate that all architectures demonstrate robust performance on the specified task. However, the domain gap can potentially lead to a decline in the performance of up to 11.4% mIoU.

</details>


### [50] [Emotion Recognition in Signers](https://arxiv.org/abs/2512.15376)
*Kotaro Funakoshi,Yaoxiong Zhu*

Main category: cs.CV

TL;DR: 本文提出eJSL手语情感识别基准与跨语种训练策略，结合BOBSL与文本情感知识缓解数据稀缺，并验证时段选择与手部运动信息的重要性，建立比口语LLM更强的基线。


<details>
  <summary>Details</summary>
Motivation: 手语情感识别面临两大挑战：面部同时承担语法与情感表达导致混淆，以及训练数据稀缺。作者在跨语种设定下希望利用口语文本情感资源提升手语情感识别性能，并构建日手语数据集评估方法。

Method: 1) 构建eJSL数据集：2名受试者、78句、7种情绪，共1092个片段；2) 跨语种训练：利用BOBSL（英手语带字幕）与口语文本情感模型迁移知识到日手语；3) 模型设计包括面部与手部特征、以及基于时间段的关键片段选择机制；4) 比较基线为口语LLM。

Result: 实验证明：1) 口语文本情感识别可缓解手语数据稀缺问题，提升手语情感识别性能；2) 不同时间段选择显著影响识别结果，合理的段选择能提升性能；3) 融入手部运动特征对识别有显著增益；4) 建立的跨语种方法在eJSL上优于口语LLM基线。

Conclusion: 在跨语种训练与多模态特征（面部+手部）以及时间段选择的结合下，可有效缓解手语情感识别的两大挑战，且提出的基线为该任务提供更可靠的参考。

Abstract: Recognition of signers' emotions suffers from one theoretical challenge and one practical challenge, namely, the overlap between grammatical and affective facial expressions and the scarcity of data for model training. This paper addresses these two challenges in a cross-lingual setting using our eJSL dataset, a new benchmark dataset for emotion recognition in Japanese Sign Language signers, and BOBSL, a large British Sign Language dataset with subtitles. In eJSL, two signers expressed 78 distinct utterances with each of seven different emotional states, resulting in 1,092 video clips. We empirically demonstrate that 1) textual emotion recognition in spoken language mitigates data scarcity in sign language, 2) temporal segment selection has a significant impact, and 3) incorporating hand motion enhances emotion recognition in signers. Finally we establish a stronger baseline than spoken language LLMs.

</details>


### [51] [See It Before You Grab It: Deep Learning-based Action Anticipation in Basketball](https://arxiv.org/abs/2512.15386)
*Arnau Barrera Roy,Albert Clapés Sintes*

Main category: cs.CV

TL;DR: 该论文提出在篮球转播视频中预测投篮后哪支球队将获得球权的新任务，构建了一个包含100,000个视频片段、300小时以上视频、2000多个手工标注反弹事件的数据集，并用现有动作预判方法给出基线结果，同时探讨了反弹分类和反弹定位两个补充任务，展示了该数据集在篮球视频理解上的多用途。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量运动视频理解工作（如物体/球员跟踪、姿态估计、动作定位和犯规识别），但在体育视频中预判动作（anticipation）、尤其是预测投篮后哪队将获得球权的研究很少。作者希望通过新任务和数据集推动该方向，支持实时转播增强和赛后分析。

Method: 作者收集并自建了包含100,000个篮球视频片段的数据集（300+小时），其中2000多个为人工标注的篮板事件。采用并评估了多种先进的动作预判深度学习方法作为基线，另行设计或应用模型用于两个相关任务：篮板分类（预测哪队拿到球）和篮板定位（检测事件发生时间/位置）。

Result: 在基线实验中，深度学习方法首次应用于篮球篮板预测，结果显示预测是可行的但具挑战性：在准确率和时效性上仍有较大提升空间。补充任务（分类与定位）也展示了数据集的有效性，实验揭示了数据量、遮挡、多主体互动和视角变化等困难。

Conclusion: 论文通过引入新任务和大规模标注数据，填补了篮球视频中动作预判的空白，为后续研究提供基准和资源。该工作可促进实时自动广播功能和赛后分析工具的发展，但现有方法在复杂多主体场景下仍需改进。

Abstract: Computer vision and video understanding have transformed sports analytics by enabling large-scale, automated analysis of game dynamics from broadcast footage. Despite significant advances in player and ball tracking, pose estimation, action localization, and automatic foul recognition, anticipating actions before they occur in sports videos has received comparatively little attention. This work introduces the task of action anticipation in basketball broadcast videos, focusing on predicting which team will gain possession of the ball following a shot attempt. To benchmark this task, a new self-curated dataset comprising 100,000 basketball video clips, over 300 hours of footage, and more than 2,000 manually annotated rebound events is presented. Comprehensive baseline results are reported using state-of-the-art action anticipation methods, representing the first application of deep learning techniques to basketball rebound prediction. Additionally, two complementary tasks, rebound classification and rebound spotting, are explored, demonstrating that this dataset supports a wide range of video understanding applications in basketball, for which no comparable datasets currently exist. Experimental results highlight both the feasibility and inherent challenges of anticipating rebounds, providing valuable insights into predictive modeling for dynamic multi-agent sports scenarios. By forecasting team possession before rebounds occur, this work enables applications in real-time automated broadcasting and post-game analysis tools to support decision-making.

</details>


### [52] [SMART: Semantic Matching Contrastive Learning for Partially View-Aligned Clustering](https://arxiv.org/abs/2512.15396)
*Liang Peng,Yixuan Ye,Cheng Liu,Hangjun Che,Fei Wang,Zhiwen Yu,Si Wu,Hau-San Wong*

Main category: cs.CV

TL;DR: SMART：缓解跨视图分布漂移，进行语义匹配对比学习，改进部分视图对齐聚类，实验证明效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实场景中视图严格对齐困难，PVC任务需要同时利用对齐与未对齐数据，但现有方法未充分利用未对齐样本的共享语义且受视图异质性导致的表示分布漂移影响，影响跨视图语义对应建立。

Method: 提出SMART模型，通过降低跨视图分布漂移（可能包括对齐前的分布校正或域适配机制）并引入语义匹配对比学习损失来学习更鲁棒的跨视图表示与对应关系，从而提升PVC聚类性能。

Result: 提出了一种用于部分视图对齐聚类（PVC）的新方法：Semantic MAtching contRasTive learning (SMART)。该方法通过缓解跨视图表示分布漂移，进行语义匹配对比学习，从而更好地利用对齐与未对齐数据的语义关系。实验证明在八个基准数据集上优于现有方法。

Conclusion: SMART通过减少视图间分布差异并结合语义匹配对比学习，有效提升了PVC任务中对齐与未对齐样本的聚类性能，并在多个数据集上取得了显著优越性。

Abstract: Multi-view clustering has been empirically shown to improve learning performance by leveraging the inherent complementary information across multiple views of data. However, in real-world scenarios, collecting strictly aligned views is challenging, and learning from both aligned and unaligned data becomes a more practical solution. Partially View-aligned Clustering aims to learn correspondences between misaligned view samples to better exploit the potential consistency and complementarity across views, including both aligned and unaligned data. However, most existing PVC methods fail to leverage unaligned data to capture the shared semantics among samples from the same cluster. Moreover, the inherent heterogeneity of multi-view data induces distributional shifts in representations, leading to inaccuracies in establishing meaningful correspondences between cross-view latent features and, consequently, impairing learning effectiveness. To address these challenges, we propose a Semantic MAtching contRasTive learning model (SMART) for PVC. The main idea of our approach is to alleviate the influence of cross-view distributional shifts, thereby facilitating semantic matching contrastive learning to fully exploit semantic relationships in both aligned and unaligned data. Extensive experiments on eight benchmark datasets demonstrate that our method consistently outperforms existing approaches on the PVC problem.

</details>


### [53] [Preserving Marker Specificity with Lightweight Channel-Independent Representation Learning](https://arxiv.org/abs/2512.15410)
*Simon Gutwein,Arthur Longuefosse,Jun Seita,Sabine Taschner-Mandl,Roxane Licandro*

Main category: cs.CV

TL;DR: Shallow, channel-independent models (CIM-S) learn better marker-specific representations than deep early-fusion CNNs for multiplex imaging, despite having far fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Early channel fusion assumes shared structure across markers, which may erase marker-specific signals important in multiplex tissue imaging; investigate if preserving marker independence is a better inductive bias than scaling models up.

Method: Compare early-fusion CNNs vs channel-separated architectures (marker-aware baseline and CIM-S with 5.5K params) on a 49-marker Hodgkin lymphoma CODEX dataset using contrastive pretraining and linear evaluation; test across self-supervised frameworks, augmentations, and marker-reduced settings.

Result: Channel-independent shallow models outperform early-fusion deep CNNs for multiplexed tissue imaging representation learning.

Conclusion: Preserving marker independence with shallow, channel-separated architectures (notably CIM-S) yields stronger, more marker-specific representations than large early-fusion models, especially for rare-cell discrimination; compact models can match/surpass larger models.

Abstract: Multiplexed tissue imaging measures dozens of protein markers per cell, yet most deep learning models still apply early channel fusion, assuming shared structure across markers. We investigate whether preserving marker independence, combined with deliberately shallow architectures, provides a more suitable inductive bias for self-supervised representation learning in multiplex data than increasing model scale. Using a Hodgkin lymphoma CODEX dataset with 145,000 cells and 49 markers, we compare standard early-fusion CNNs with channel-separated architectures, including a marker-aware baseline and our novel shallow Channel-Independent Model (CIM-S) with 5.5K parameters. After contrastive pretraining and linear evaluation, early-fusion models show limited ability to retain marker-specific information and struggle particularly with rare-cell discrimination. Channel-independent architectures, and CIM-S in particular, achieve substantially stronger representations despite their compact size. These findings are consistent across multiple self-supervised frameworks, remain stable across augmentation settings, and are reproducible across both the 49-marker and reduced 18-marker settings. These results show that lightweight, channel-independent architectures can match or surpass deep early-fusion CNNs and foundation models for multiplex representation learning. Code is available at https://github.com/SimonBon/CIM-S.

</details>


### [54] [Photorealistic Phantom Roads in Real Scenes: Disentangling 3D Hallucinations from Physical Geometry](https://arxiv.org/abs/2512.15423)
*Hoang Nguyen,Xiaohao Xu,Xiaonan Huang*

Main category: cs.CV

TL;DR: 本文定义并研究了“3D Mirage”现象，提出基准、度量与一种高效蒸馏方法来减少幻觉，同时保持背景知识。


<details>
  <summary>Details</summary>
Motivation: 单目深度模型学习语义先验虽提升泛化，但会在平面场景中生成虚假的三维结构，需要专门的探测与抑制方法。

Method: 请简要总结论文提出的方法、主要结果与结论

Result: 提出了一个端到端框架：3D-Mirage基准、Laplacian评估框架（DCS和CCS）与Grounded Self-Distillation策略，对单目深度基线幻觉（3D Mirage）进行探测、量化与抑制

Conclusion: 呼吁从像素精度评价转向结构与语境鲁棒性评价，并提供代码与基准以促进研究

Abstract: Monocular depth foundation models achieve remarkable generalization by learning large-scale semantic priors, but this creates a critical vulnerability: they hallucinate illusory 3D structures from geometrically planar but perceptually ambiguous inputs. We term this failure the 3D Mirage. This paper introduces the first end-to-end framework to probe, quantify, and tame this unquantified safety risk. To probe, we present 3D-Mirage, the first benchmark of real-world illusions (e.g., street art) with precise planar-region annotations and context-restricted crops. To quantify, we propose a Laplacian-based evaluation framework with two metrics: the Deviation Composite Score (DCS) for spurious non-planarity and the Confusion Composite Score (CCS) for contextual instability. To tame this failure, we introduce Grounded Self-Distillation, a parameter-efficient strategy that surgically enforces planarity on illusion ROIs while using a frozen teacher to preserve background knowledge, thus avoiding catastrophic forgetting. Our work provides the essential tools to diagnose and mitigate this phenomenon, urging a necessary shift in MDE evaluation from pixel-wise accuracy to structural and contextual robustness. Our code and benchmark will be publicly available to foster this exciting research direction.

</details>


### [55] [Step-GUI Technical Report](https://arxiv.org/abs/2512.15431)
*Haolong Yan,Jia Wang,Xin Huang,Yeqing Shen,Ziyang Meng,Zhimin Fan,Kaijun Tan,Jin Gao,Lieyu Shi,Mi Yang,Shiliang Yang,Zhirui Wang,Brian Li,Kang An,Chenyang Li,Lei Lei,Mengmeng Duan,Danxun Liang,Guodong Liu,Hang Cheng,Hao Wu,Jie Dong,Junhao Huang,Mei Chen,Renjie Yu,Shunshan Li,Xu Zhou,Yiting Dai,Yineng Deng,Yingdan Liang,Zelin Chen,Wen Sun,Chengxu Yan,Chunqin Xu,Dong Li,Fengqiong Xiao,Guanghao Fan,Guopeng Li,Guozhen Peng,Hongbing Li,Hang Li,Hongming Chen,Jingjing Xie,Jianyong Li,Jingyang Zhang,Jiaju Ren,Jiayu Yuan,Jianpeng Yin,Kai Cao,Liang Zhao,Liguo Tan,Liying Shi,Mengqiang Ren,Min Xu,Manjiao Liu,Mao Luo,Mingxin Wan,Na Wang,Nan Wu,Ning Wang,Peiyao Ma,Qingzhou Zhang,Qiao Wang,Qinlin Zeng,Qiong Gao,Qiongyao Li,Shangwu Zhong,Shuli Gao,Shaofan Liu,Shisi Gao,Shuang Luo,Xingbin Liu,Xiaojia Liu,Xiaojie Hou,Xin Liu,Xuanti Feng,Xuedan Cai,Xuan Wen,Xianwei Zhu,Xin Liang,Xin Liu,Xin Zhou,Yingxiu Zhao,Yukang Shi,Yunfang Xu,Yuqing Zeng,Yixun Zhang,Zejia Weng,Zhonghao Yan,Zhiguo Huang,Zhuoyu Wang,Zheng Ge,Jing Li,Yibo Zhu,Binxing Jiao,Xiangyu Zhang,Daxin Jiang*

Main category: cs.CV

TL;DR: 引入一种基于校准步骤奖励系统的自演化训练管线，大幅降低标注成本并保证>90%准确率，提出Step-GUI模型（4B/8B）与GUI-MCP隐私保护协议，并发布真实场景基准AndroidDaily，提升GUI智能体的实用部署潜力。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大模型在GUI自动化中高质量训练数据难以高效获取且注释可靠性不足的问题，同时满足跨设备标准接口与隐私保护需求，以促进实际部署。

Method: 提出Calibrated Step Reward System对模型生成的轨迹进行轨迹级校准以生成可靠训练信号；基于此构建自演化训练流程；训练Step-GUI（4B/8B）模型；设计GUI-MCP层次化协议，将低级原子操作与高级任务委派到本地专家模型以保护隐私；构建AndroidDaily基准用于评估。

Result: 实验显示该校准系统能以10-100倍更低成本达到>90%注释准确率；Step-GUI 8B在多项基准上达到SOTA（AndroidWorld 80.2%，OSWorld 48.5%，ScreenShot-Pro 62.6%）；AndroidDaily上静态动作与端到端任务表现分别为89.91%与52.50%。

Conclusion: This paper presents a self-evolving training pipeline using a Calibrated Step Reward System to convert model-generated GUI trajectories into high-quality training signals, enabling cost-effective annotation (>90% accuracy with 10–100x lower cost). It introduces Step-GUI models (4B/8B) achieving SOTA GUI performance, proposes GUI-MCP protocol for privacy-preserving hierarchical execution across devices, and provides AndroidDaily benchmark for real-world evaluation. Overall, it advances practical GUI agent deployment.

Abstract: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.

</details>


### [56] [CLIP-FTI: Fine-Grained Face Template Inversion via CLIP-Driven Attribute Conditioning](https://arxiv.org/abs/2512.15433)
*Longchen Dai,Zixuan Shen,Zhiheng Zhou,Peipeng Yu,Zhihua Xia*

Main category: cs.CV

TL;DR: 本文提出CLIP-FTI，一种基于CLIP的细粒度属性条件化人脸模板反演方法，通过将CLIP提取的面部部件语义嵌入与泄露的人脸模板融合，并映射到StyleGAN中间潜在空间，生成保留身份且在部件属性上更清晰的重建人脸。实验表明在识别精度、属性相似度和跨模型转移攻击方面均优于先前方法，首次使用模板外额外信息实现SOTA反演。


<details>
  <summary>Details</summary>
Motivation: 现有人脸模板反演生成的图像部件（眼睛、鼻子、嘴等）过度平滑、细节不足且跨模型转移性差，威胁隐私和安全。作者希望通过引入额外的语义信息来实现更细粒度的属性恢复，从而提高反演效果和攻击转移性。

Method: 提出CLIP-FTI：利用CLIP提取面部部件的语义嵌入，设计跨模态特征交互网络将这些嵌入与泄露的人脸模板融合，随后将融合特征投射到预训练StyleGAN的中间潜在空间，使用StyleGAN生成具有相同身份但在面部部件属性上更清晰的重建图像。

Result: 在多个人脸识别骨干网络与数据集上，CLIP-FTI在识别准确率和属性相似度上提升明显，能恢复更清晰的部件级语义属性，并且在跨模型攻击转移性上优于先前反演方法。

Conclusion: 首次利用模板外的CLIP语义信息进行人脸模板反演，显著提升重建细节与攻击效果，为模板安全评估和隐私风险研究提供了新的方向與SOTA成果。

Abstract: Face recognition systems store face templates for efficient matching. Once leaked, these templates pose a threat: inverting them can yield photorealistic surrogates that compromise privacy and enable impersonation. Although existing research has achieved relatively realistic face template inversion, the reconstructed facial images exhibit over-smoothed facial-part attributes (eyes, nose, mouth) and limited transferability. To address this problem, we present CLIP-FTI, a CLIP-driven fine-grained attribute conditioning framework for face template inversion. Our core idea is to use the CLIP model to obtain the semantic embeddings of facial features, in order to realize the reconstruction of specific facial feature attributes. Specifically, facial feature attribute embeddings extracted from CLIP are fused with the leaked template via a cross-modal feature interaction network and projected into the intermediate latent space of a pretrained StyleGAN. The StyleGAN generator then synthesizes face images with the same identity as the templates but with more fine-grained facial feature attributes. Experiments across multiple face recognition backbones and datasets show that our reconstructions (i) achieve higher identification accuracy and attribute similarity, (ii) recover sharper component-level attribute semantics, and (iii) improve cross-model attack transferability compared to prior reconstruction attacks. To the best of our knowledge, ours is the first method to use additional information besides the face template attack to realize face template inversion and obtains SOTA results.

</details>


### [57] [ST-DETrack: Identity-Preserving Branch Tracking in Entangled Plant Canopies via Dual Spatiotemporal Evidence](https://arxiv.org/abs/2512.15445)
*Yueqianji Chen,Kevin Williams,John H. Doonan,Paolo Remagnino,Jo Hepworth*

Main category: cs.CV

TL;DR: 提出ST-DETrack：空间解码器结合几何先验、时间解码器利用运动一致性，自适应门控平衡两者，并加入负向向地性约束，BMA=93.6%。


<details>
  <summary>Details</summary>
Motivation: 应对植物生长过程中由于非刚性形变和枝条交织导致的身份碎片化，确保从萌芽到开花阶段的长期身份保持，满足高通量表型需求。

Method: 设计双解码器网络（空间解码器利用位置、角度等几何先验，时间解码器利用帧间运动一致性），加入自适应门控机制在不同生长阶段动态权衡时空线索，并结合基于负向向地性的生物学约束来减少垂直生长歧义。

Result: ST-DETrack提出了一种融合时空信息的双解码器网络，用于从时序植物影像中保持枝条身份跟踪。

Conclusion: 在Brassica napus数据集上，BMA达到93.6%，显著超过纯空间和纯时间基线，展示了在复杂植株结构中维持长期身份一致性的能力。

Abstract: Automated extraction of individual plant branches from time-series imagery is essential for high-throughput phenotyping, yet it remains computationally challenging due to non-rigid growth dynamics and severe identity fragmentation within entangled canopies. To overcome these stage-dependent ambiguities, we propose ST-DETrack, a spatiotemporal-fusion dual-decoder network designed to preserve branch identity from budding to flowering. Our architecture integrates a spatial decoder, which leverages geometric priors such as position and angle for early-stage tracking, with a temporal decoder that exploits motion consistency to resolve late-stage occlusions. Crucially, an adaptive gating mechanism dynamically shifts reliance between these spatial and temporal cues, while a biological constraint based on negative gravitropism mitigates vertical growth ambiguities. Validated on a Brassica napus dataset, ST-DETrack achieves a Branch Matching Accuracy (BMA) of 93.6%, significantly outperforming spatial and temporal baselines by 28.9 and 3.3 percentage points, respectively. These results demonstrate the method's robustness in maintaining long-term identity consistency amidst complex, dynamic plant architectures.

</details>


### [58] [Evaluation of deep learning architectures for wildlife object detection: A comparative study of ResNet and Inception](https://arxiv.org/abs/2512.15480)
*Malach Obisa Amonga,Benard Osero,Edna Too*

Main category: cs.CV

TL;DR: ResNet-101和Inception v3在野生动物检测上均表现良好（准确率分别94%和95%，mAP分别0.91和0.92），Inception v3略优；但对相似外观物种与恶劣拍摄条件仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估两种经典深度网络在环境多变、物种相似与类内差异等现实挑战下的野生动物目标检测性能，为保护相关计算机视觉应用提供依据。

Method: 使用标准预处理（最长边800像素、转RGB、转PyTorch张量），将数据按70:30分为训练/验证，分别训练ResNet-101和Inception v3并比较分类准确率与mAP指标。

Result: ResNet-101: 准确率94%、mAP 0.91；Inception v3: 准确率95%、mAP 0.92。两模型在光照差、遮挡及相似物种检测上表现欠佳。

Conclusion: 两种模型均表现出色，但在相似物种、低光和遮挡情况下仍有不足。

Abstract: Wildlife object detection plays a vital role in biodiversity conservation, ecological monitoring, and habitat protection. However, this task is often challenged by environmental variability, visual similarities among species, and intra-class diversity. This study investigates the effectiveness of two individual deep learning architectures ResNet-101 and Inception v3 for wildlife object detection under such complex conditions. The models were trained and evaluated on a wildlife image dataset using a standardized preprocessing approach, which included resizing images to a maximum dimension of 800 pixels, converting them to RGB format, and transforming them into PyTorch tensors. A ratio of 70:30 training and validation split was used for model development. The ResNet-101 model achieved a classification accuracy of 94% and a mean Average Precision (mAP) of 0.91, showing strong performance in extracting deep hierarchical features. The Inception v3 model performed slightly better, attaining a classification accuracy of 95% and a mAP of 0.92, attributed to its efficient multi-scale feature extraction through parallel convolutions. Despite the strong results, both models exhibited challenges when detecting species with similar visual characteristics or those captured under poor lighting and occlusion. Nonetheless, the findings confirm that both ResNet-101 and Inception v3 are effective models for wildlife object detection tasks and provide a reliable foundation for conservation-focused computer vision applications.

</details>


### [59] [RUMPL: Ray-Based Transformers for Universal Multi-View 2D to 3D Human Pose Lifting](https://arxiv.org/abs/2512.15488)
*Seyed Abolfazl Ghasemzadeh,Alexandre Alahi,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: RUMPL: camera-agnostic transformer 3D-pose lifter using 3D ray representation and view fusion, significantly improving MPJPE and generalization to in-the-wild multi-view settings.


<details>
  <summary>Details</summary>
Motivation: Address limitations of multi-view 3D human pose estimation caused by scarcity of large-scale calibrated multi-view datasets and poor generalization of existing methods to in-the-wild scenarios.

Method: Propose RUMPL, a transformer-based 3D pose lifter using a 3D ray-based representation of 2D keypoints to remove dependency on camera calibration and number of views; introduce View Fusion Transformer that aggregates fused-ray tokens along rays for multi-view consistency.

Result: Extensive experiments show RUMPL reduces MPJPE by up to 53% vs triangulation and >60% vs transformer image-representation baselines; robust performance on in-the-wild multi-view and multi-person datasets.

Conclusion: RUMPL enables camera-agnostic, view-number-agnostic 3D pose lifting with improved accuracy and generalization, validated across benchmarks; code released.

Abstract: Estimating 3D human poses from 2D images remains challenging due to occlusions and projective ambiguity. Multi-view learning-based approaches mitigate these issues but often fail to generalize to real-world scenarios, as large-scale multi-view datasets with 3D ground truth are scarce and captured under constrained conditions. To overcome this limitation, recent methods rely on 2D pose estimation combined with 2D-to-3D pose lifting trained on synthetic data. Building on our previous MPL framework, we propose RUMPL, a transformer-based 3D pose lifter that introduces a 3D ray-based representation of 2D keypoints. This formulation makes the model independent of camera calibration and the number of views, enabling universal deployment across arbitrary multi-view configurations without retraining or fine-tuning. A new View Fusion Transformer leverages learned fused-ray tokens to aggregate information along rays, further improving multi-view consistency. Extensive experiments demonstrate that RUMPL reduces MPJPE by up to 53% compared to triangulation and over 60% compared to transformer-based image-representation baselines. Results on new benchmarks, including in-the-wild multi-view and multi-person datasets, confirm its robustness and scalability. The framework's source code is available at https://github.com/aghasemzadeh/OpenRUMPL

</details>


### [60] [The LUMirage: An independent evaluation of zero-shot performance in the LUMIR challenge](https://arxiv.org/abs/2512.15505)
*Rohit Jena,Pratik Chaudhari,James C. Gee*

Main category: cs.CV

TL;DR: DL equals iterative on T1w; DL worse on T2/T2*/FLAIR; DL fails at 0.6mm; sensitive to preprocessing; zero-shot superiority overstated.


<details>
  <summary>Details</summary>
Motivation: To verify LUMIR challenge claims of zero-shot generalization of DL registration across contrasts/resolutions and address potential instrumentation bias.

Method: Independent reevaluation with rigorous protocols, controlling for preprocessing/instrumentation, comparing DL and iterative methods across in-distribution and out-of-distribution contrasts, species, and resolutions; measured effect sizes (Cohen's d) and assessed runtime/scalability and sensitivity analyses.

Result: Re-evaluation of LUMIR zero-shot claims

Conclusion: Deep learning methods are not universally superior zero-shot; they match iterative methods in-distribution but degrade on unseen contrasts, fail on very high resolution, and are sensitive to preprocessing.

Abstract: The LUMIR challenge represents an important benchmark for evaluating deformable image registration methods on large-scale neuroimaging data. While the challenge demonstrates that modern deep learning methods achieve competitive accuracy on T1-weighted MRI, it also claims exceptional zero-shot generalization to unseen contrasts and resolutions, assertions that contradict established understanding of domain shift in deep learning. In this paper, we perform an independent re-evaluation of these zero-shot claims using rigorous evaluation protocols while addressing potential sources of instrumentation bias. Our findings reveal a more nuanced picture: (1) deep learning methods perform comparably to iterative optimization on in-distribution T1w images and even on human-adjacent species (macaque), demonstrating improved task understanding; (2) however, performance degrades significantly on out-of-distribution contrasts (T2, T2*, FLAIR), with Cohen's d scores ranging from 0.7-1.5, indicating substantial practical impact on downstream clinical workflows; (3) deep learning methods face scalability limitations on high-resolution data, failing to run on 0.6 mm isotropic images, while iterative methods benefit from increased resolution; and (4) deep methods exhibit high sensitivity to preprocessing choices. These results align with the well-established literature on domain shift and suggest that claims of universal zero-shot superiority require careful scrutiny. We advocate for evaluation protocols that reflect practical clinical and research workflows rather than conditions that may inadvertently favor particular method classes.

</details>


### [61] [Off The Grid: Detection of Primitives for Feed-Forward 3D Gaussian Splatting](https://arxiv.org/abs/2512.15508)
*Arthur Moreau,Richard Shaw,Michal Nazarczuk,Jisu Shin,Thomas Tanay,Zhensong Zhang,Songcen Xu,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: 提出一种受关键点检测启发的前馈式“离网格”3D高斯分布方法，通过多分辨率解码器在补丁级别进行亚像素检测，与3D重建自监督联合训练，实现更少基元下的高质量、实时新视角合成，并能提升相机位姿估计。


<details>
  <summary>Details</summary>
Motivation: 现有前馈式3DGS依赖密集刚性像素网格来放置像素对齐基元，导致基元浪费、细节捕捉不足和效率低下。论文旨在通过自适应的亚像素级基元检测替代固定网格，以提高质量与效率并减少伪影。

Method: 设计一个多分辨率解码器，受关键点检测启发，在图像补丁上学会在亚像素位置分布3D高斯基元（“Off The Grid”）。该检测模块与3D重建主干网络端到端自监督训练，无需相机姿态标签，输出姿态无关的实时场景生成。

Result: 在新视角合成任务上，本方法在前馈模型类别中达到了最先进的性能，同时使用更少的基元，实现更精确的细节还原与伪影减少。额外结果显示，联合训练能改善相机位姿估计，支持无标签训练的可行性。

Conclusion: 本文提出了一种无需姿态信息的前馈式3D高斯点阵（3DGS）模型改进，通过亚像素级关键点式检测器将原先刚性像素网格替换为自适应的“离网格（Off The Grid）”分布，从而更高效、更精确地布置3D高斯基元。该方法通过多分辨率解码器在图像补丁上学习基元分布，并与3D重建主干网络端到端自监督训练，显著提升了新视角合成质量与效率，同时减少了基元数量和伪影。额外发现是，学习渲染3D高斯有助于提升相机位姿估计，显示了无需标签训练基础模型的潜力。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models enable real-time scene generation but are hindered by suboptimal pixel-aligned primitive placement, which relies on a dense, rigid grid and limits both quality and efficiency. We introduce a new feed-forward architecture that detects 3D Gaussian primitives at a sub-pixel level, replacing the pixel grid with an adaptive, "Off The Grid" distribution. Inspired by keypoint detection, our multi-resolution decoder learns to distribute primitives across image patches. This module is trained end-to-end with a 3D reconstruction backbone using self-supervised learning. Our resulting pose-free model generates photorealistic scenes in seconds, achieving state-of-the-art novel view synthesis for feed-forward models. It outperforms competitors while using far fewer primitives, demonstrating a more accurate and efficient allocation that captures fine details and reduces artifacts. Moreover, we observe that by learning to render 3D Gaussians, our 3D reconstruction backbone improves camera pose estimation, suggesting opportunities to train these foundational models without labels.

</details>


### [62] [VAAS: Vision-Attention Anomaly Scoring for Image Manipulation Detection in Digital Forensics](https://arxiv.org/abs/2512.15512)
*Opeyemi Bamigbade,Mark Scanlon,John Sheppard*

Main category: cs.CV

TL;DR: VAAS通过ViT注意力和SegFormer补丁一致性融合，给出连续可解释的异常评分与可视化异常图，在DF2023和CASIA v2.0上取得良好检测与定位效果，代码开源。


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测器易被视觉一致的伪造绕过，且通常缺乏表示操控强度的连续度量，导致难以量化篡改严重性和提供人类可理解的证据。

Method: 提出双模块混合方法：1) 使用预训练ViT提取全局注意力图并学习异常估计器，输出像素/区域级的注意力驱动异常强度；2) 基于SegFormer提取补丁嵌入，评估补丁间自洽性以生成局部一致性分数；两部分融合为连续异常分数并生成可视化异常图，训练在包含篡改标签的数据上进行监督与多任务损失。

Result: 在DF2023和CASIA v2.0数据集上，VAAS在F1和IoU指标上达到与现有方法相当或更优的性能，并通过注意力引导的异常图提供更好的可视化解释性，且作者已开源实验代码以支持重现。

Conclusion: VAAS提出了一种结合ViT全局注意力异常估计与SegFormer补丁级自洽性评分的混合框架，能够生成连续且可解释的异常强度分数，兼顾定位与量化操控程度，实验在DF2023和CASIA v2.0上表现出有竞争力的F1和IoU并增强可视化可解释性。

Abstract: Recent advances in AI-driven image generation have introduced new challenges for verifying the authenticity of digital evidence in forensic investigations. Modern generative models can produce visually consistent forgeries that evade traditional detectors based on pixel or compression artefacts. Most existing approaches also lack an explicit measure of anomaly intensity, which limits their ability to quantify the severity of manipulation. This paper introduces Vision-Attention Anomaly Scoring (VAAS), a novel dual-module framework that integrates global attention-based anomaly estimation using Vision Transformers (ViT) with patch-level self-consistency scoring derived from SegFormer embeddings. The hybrid formulation provides a continuous and interpretable anomaly score that reflects both the location and degree of manipulation. Evaluations on the DF2023 and CASIA v2.0 datasets demonstrate that VAAS achieves competitive F1 and IoU performance, while enhancing visual explainability through attention-guided anomaly maps. The framework bridges quantitative detection with human-understandable reasoning, supporting transparent and reliable image integrity assessment. The source code for all experiments and corresponding materials for reproducing the results are available open source.

</details>


### [63] [DeX-Portrait: Disentangled and Expressive Portrait Animation via Explicit and Latent Motion Representations](https://arxiv.org/abs/2512.15524)
*Yuxiang Shi,Zhe Li,Yanwen Wang,Hao Zhu,Xun Cao,Ligang Liu*

Main category: cs.CV

TL;DR: 提出DeX-Portrait，通过将头部姿态建模为显式全局变换、表情建模为隐式潜码，并在扩散模型中采用双分支姿态条件与交叉注意表情注入，以及进阶混合无分类器引导，实现高保真可解耦的人像动画。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的单图驱动人像动画方法无法实现头部姿态与面部表情的高保真可解耦控制，限制了仅表情或仅姿态编辑等应用。

Method: （1）姿态用显式全局变换表示，表情用隐式潜码表示；（2）设计motion trainer学习姿态与表情的编码器以精确分解驱动信号；（3）在扩散模型中通过双分支机制注入姿态变换，通过交叉注意注入表情潜码；（4）提出渐进式混合无分类器引导以保持身份一致性。

Result: 实验证明在动画质量与解耦可控性上优于现有最先进方法。

Conclusion: DeX-Portrait实现了对姿态与表情的高保真解耦控制，提升了人像动画的表达性与编辑灵活性，适合表情/姿态单独编辑的应用场景。

Abstract: Portrait animation from a single source image and a driving video is a long-standing problem. Recent approaches tend to adopt diffusion-based image/video generation models for realistic and expressive animation. However, none of these diffusion models realizes high-fidelity disentangled control between the head pose and facial expression, hindering applications like expression-only or pose-only editing and animation. To address this, we propose DeX-Portrait, a novel approach capable of generating expressive portrait animation driven by disentangled pose and expression signals. Specifically, we represent the pose as an explicit global transformation and the expression as an implicit latent code. First, we design a powerful motion trainer to learn both pose and expression encoders for extracting precise and decomposed driving signals. Then we propose to inject the pose transformation into the diffusion model through a dual-branch conditioning mechanism, and the expression latent through cross attention. Finally, we design a progressive hybrid classifier-free guidance for more faithful identity consistency. Experiments show that our method outperforms state-of-the-art baselines on both animation quality and disentangled controllability.

</details>


### [64] [EmoCaliber: Advancing Reliable Visual Emotion Comprehension via Confidence Verbalization and Calibration](https://arxiv.org/abs/2512.15528)
*Daiqing Wu,Dongbao Yang,Can Ma. Yu Zhou*

Main category: cs.CV

TL;DR: 作者针对VEC任务的主观性，提出让MLLM输出情感预测同时口头化置信度；通过三阶段训练得到EmoCaliber，在统一基准VECBench上表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前将VEC视为确定性标签预测忽略了情感判断的主观性，缺乏对替代解释和模型自信度的表达，导致实际应用中可靠性不足。

Method: 设计三阶段训练流程：1）赋予模型结构化推理能力以解释情感线索；2）训练模型将预测伴随可解释的置信度表述；3）对置信度输出进行校准以确保与实际准确率一致，最终形成EmoCaliber。

Result: 在VECBench上的公平综合评估显示EmoCaliber在情感分类与置信度估计两方面均优于现有方法，证明了置信度表述与校准能提升VEC系统的可靠性。

Conclusion: 该论文提出将置信度表达引入多模态大语言模型（MLLMs）以改进视觉情感理解（VEC），并通过三阶段训练框架（结构化推理、置信度口头化、置信度校准）构建了EmoCaliber模型，从而在情感预测和置信度估计上优于现有方法。

Abstract: Visual Emotion Comprehension (VEC) aims to infer sentiment polarities or emotion categories from affective cues embedded in images. In recent years, Multimodal Large Language Models (MLLMs) have established a popular paradigm in VEC, leveraging their generalizability to unify VEC tasks defined under diverse emotion taxonomies. While this paradigm achieves notable success, it typically formulates VEC as a deterministic task, requiring the model to output a single, definitive emotion label for each image. Such a formulation insufficiently accounts for the inherent subjectivity of emotion perception, overlooking alternative interpretations that may be equally plausible to different viewers. To address this limitation, we propose equipping MLLMs with capabilities to verbalize their confidence in emotion predictions. This additional signal provides users with an estimate of both the plausibility of alternative interpretations and the MLLMs' self-assessed competence, thereby enhancing reliability in practice. Building on this insight, we introduce a three-stage training framework that progressively endows with structured reasoning, teaches to verbalize confidence, and calibrates confidence expression, culminating in EmoCaliber, a confidence-aware MLLM for VEC. Through fair and comprehensive evaluations on the unified benchmark VECBench, EmoCaliber demonstrates overall superiority against existing methods in both emotion prediction and confidence estimation. These results validate the effectiveness of our approach and mark a feasible step toward more reliable VEC systems. Project page: https://github.com/wdqqdw/EmoCaliber.

</details>


### [65] [An Efficient and Effective Encoder Model for Vision and Language Tasks in the Remote Sensing Domain](https://arxiv.org/abs/2512.15531)
*João Daniel Silva,Joao Magalhaes,Devis Tuia,Bruno Martins*

Main category: cs.CV

TL;DR: 提出了GeoMELT，一种基于encoder-only的紧凑多任务模型，能在遥感图像文本生成和跨模态检索上高效表现，降低训练和推理成本


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型在遥感领域虽功能强大但参数多、训练推理成本高；需寻找更节省参数和计算资源的多任务解决方案，尤其覆盖图像生成文本与跨模态检索等任务

Method: Encoder-only multi-task model (GeoMELT)

Result: GeoMELT achieves effective multi-task performance (image-to-text generation and cross-modal retrieval) while being compact and parameter-efficient

Conclusion: Encoder-only architectures can provide a practical, efficient alternative to LVLMs for multi-task remote sensing applications, reducing computational and parameter costs without major performance loss

Abstract: The remote sensing community has recently seen the emergence of methods based on Large Vision and Language Models (LVLMs) that can address multiple tasks at the intersection of computer vision and natural language processing. To fully exploit the potential of such models, a significant focus has been given to the collection of large amounts of training data that cover multiple remote sensing-specific tasks, such as image captioning or visual question answering. However, the cost of using and training LVLMs is high, due to the large number of parameters. While multiple parameter-efficient adaptation techniques have been explored, the computational costs of training and inference with these models can remain prohibitive for most institutions. In this work, we explore the use of encoder-only architectures and propose a model that can effectively address multi-task learning while remaining compact in terms of the number of parameters. In particular, our model tackles combinations of tasks that are not typically explored in a unified model: the generation of text from remote sensing images and cross-modal retrieval. The results of our GeoMELT model - named from Multi-task Efficient Learning Transformer - in established benchmarks confirm the efficacy and efficiency of the proposed approach.

</details>


### [66] [BLANKET: Anonymizing Faces in Infant Video Recordings](https://arxiv.org/abs/2512.15542)
*Ditmar Hadera,Jan Cech,Miroslav Purkrabek,Matej Hoffmann*

Main category: cs.CV

TL;DR: BLANKET 用扩散模型生成与原身份兼容的随机面孔并通过时序一致的面部替换保留表情和关键点，针对婴儿视频去识别化，在保留属性与下游任务性能方面优于 DeepPrivacy2，代码开源。


<details>
  <summary>Details</summary>
Motivation: 婴儿视频含敏感人脸信息，现有去识别方法往往在去识别与属性保留间难以兼顾；需要一种既能有效去识别又能保留表情、关键点等对下游分析（如姿态估计）重要信息的方法。

Method: 两阶段流程：1) 使用扩散模型对面部区域进行修补（inpainting），生成一个与原身份兼容的随机新面孔；2) 在视频每帧上进行时序一致的面部替换，同时通过关键点/表情传递保持表情和面部关键点一致性，以减少伪影并维持下游任务性能。

Result: 在婴儿短视频数据集上与 DeepPrivacy2 比较，两个方法均实现身份改变，但 BLANKET 在面部属性保留、对人体姿态估计等下游任务影响更小、伪影更少等方面表现更好。

Conclusion: BLANKET 提出了一种针对婴儿视频面部的去识别化方法，兼顾关键点一致性与面部属性保留，通过两阶段（扩散模型修补生成随机面孔 + 时序一致的面部替换与表情传递）实现，比 DeepPrivacy2 在去识别外的指标（面部属性保留、下游任务影响、伪影控制）表现更好，并提供开源代码。

Abstract: Ensuring the ethical use of video data involving human subjects, particularly infants, requires robust anonymization methods. We propose BLANKET (Baby-face Landmark-preserving ANonymization with Keypoint dEtection consisTency), a novel approach designed to anonymize infant faces in video recordings while preserving essential facial attributes. Our method comprises two stages. First, a new random face, compatible with the original identity, is generated via inpainting using a diffusion model. Second, the new identity is seamlessly incorporated into each video frame through temporally consistent face swapping with authentic expression transfer. The method is evaluated on a dataset of short video recordings of babies and is compared to the popular anonymization method, DeepPrivacy2. Key metrics assessed include the level of de-identification, preservation of facial attributes, impact on human pose estimation (as an example of a downstream task), and presence of artifacts. Both methods alter the identity, and our method outperforms DeepPrivacy2 in all other respects. The code is available as an easy-to-use anonymization demo at https://github.com/ctu-vras/blanket-infant-face-anonym.

</details>


### [67] [GRAN-TED: Generating Robust, Aligned, and Nuanced Text Embedding for Diffusion Models](https://arxiv.org/abs/2512.15560)
*Bozhou Li,Sihan Yang,Yushuo Guan,Ruichuan An,Xinlong Chen,Yang Shi,Pengfei Wan,Wentao Zhang,Yuanxing zhang*

Main category: cs.CV

TL;DR: GRAN-TED：提出TED-6K文本基准以高效预测编码器在生成任务中的表现，并设计两阶段训练得到更强文本编码器，提升下游扩散生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本编码器在扩散模型中的发展受限于缺乏高效可靠的评估基准和难以从预训练语言模型迁移到视觉生成任务，因而需一个评估框架和专门的适配策略。

Method: 1) 设计TED-6K文本基准，通过统一轻量适配器评估编码器表示质量；2) 提出两阶段训练：先在多模态大模型上微调以获得更好的视觉相关表示，随后采用层级加权方法提取细腻文本特征；3) 在扩散生成任务上验证效果。

Result: 提出了GRAN-TED框架，用于生成适用于扩散模型的鲁棒、对齐且细腻的文本嵌入。

Conclusion: 通过TED-6K基准评估文本编码器并使用两阶段训练（先在多模态大模型上微调，再进行层权重提取）来提升编码器表现，最终在文本到图像/视频生成任务中取得性能提升。

Abstract: The text encoder is a critical component of text-to-image and text-to-video diffusion models, fundamentally determining the semantic fidelity of the generated content. However, its development has been hindered by two major challenges: the lack of an efficient evaluation framework that reliably predicts downstream generation performance, and the difficulty of effectively adapting pretrained language models for visual synthesis. To address these issues, we introduce GRAN-TED, a paradigm to Generate Robust, Aligned, and Nuanced Text Embeddings for Diffusion models. Our contribution is twofold. First, we propose TED-6K, a novel text-only benchmark that enables efficient and robust assessment of an encoder's representational quality without requiring costly end-to-end model training. We demonstrate that performance on TED-6K, standardized via a lightweight, unified adapter, strongly correlates with an encoder's effectiveness in downstream generation tasks. Second, guided by this validated framework, we develop a superior text encoder using a novel two-stage training paradigm. This process involves an initial fine-tuning stage on a Multimodal Large Language Model for better visual representation, followed by a layer-wise weighting method to extract more nuanced and potent text features. Our experiments show that the resulting GRAN-TED encoder not only achieves state-of-the-art performance on TED-6K but also leads to demonstrable performance gains in text-to-image and text-to-video generation. Our code is available at the following link: https://anonymous.4open.science/r/GRAN-TED-4FCC/.

</details>


### [68] [On the Effectiveness of Textual Prompting with Lightweight Fine-Tuning for SAM3 Remote Sensing Segmentation](https://arxiv.org/abs/2512.15564)
*Roni Blushtein-Livnon,Osher Rafaeli,David Ioffe,Amir Boger,Karen Sandberg Esquenazi,Tal Svoray*

Main category: cs.CV

TL;DR: 本文评估了基于SAM3的概念驱动分割在遥感影像（RS）上的适应性。比较文本、几何和混合提示策略，在零样本和轻量微调下不同标注规模的表现。结论是混合提示最好，纯文本最差，几何标注少量即可显著提升，且精度与IoU差距表明存在欠分割与边界误差问题。


<details>
  <summary>Details</summary>
Motivation: 遥感影像标注稀缺且与用于训练基础模型的自然影像存在域差异，需在有限监督下实现有效适应。借助SAM3能基于文本生成掩码，可能无须任务特定修改就能促进适应，因此评估其在RS数据上的可行性与不同提示策略的效果。

Method: 在四类目标上评估SAM3，比较三种提示策略（文本、几何、混合），并在多种轻量微调规模下以及零样本下测试。指标包括Precision和IoU等，考察不同监督规模下性能变化与误差模式。

Result: 混合提示在所有目标和指标上表现最佳；纯文本提示表现最差，对不规则目标差距更大，反映文本语义和航拍外观对齐不足。轻量微调显著优于零-shot，且随着标注规模增加收益递减。少量几何标注即可达到较好适应。Precision普遍高于IoU，表明存在欠分割和边界不准问题，尤其对不规则和稀有目标。

Conclusion: SAM3在遥感领域可通过混合提示与适量几何标注实现有效适配；纯文本提示受限于语义对齐问题但在规则目标上是成本低效的折衷。未来需改进边界建模与细粒度语义对齐以减少欠分割与边界误差。

Abstract: Remote sensing (RS) image segmentation is constrained by the limited availability of annotated data and a gap between overhead imagery and natural images used to train foundational models. This motivates effective adaptation under limited supervision. SAM3 concept-driven framework generates masks from textual prompts without requiring task-specific modifications, which may enable this adaptation. We evaluate SAM3 for RS imagery across four target types, comparing textual, geometric, and hybrid prompting strategies, under lightweight fine-tuning scales with increasing supervision, alongside zero-shot inference. Results show that combining semantic and geometric cues yields the highest performance across targets and metrics. Text-only prompting exhibits the lowest performance, with marked score gaps for irregularly shaped targets, reflecting limited semantic alignment between SAM3 textual representations and their overhead appearances. Nevertheless, textual prompting with light fine-tuning offers a practical performance-effort trade-off for geometrically regular and visually salient targets. Across targets, performance improves between zero-shot inference and fine-tuning, followed by diminishing returns as the supervision scale increases. Namely, a modest geometric annotation effort is sufficient for effective adaptation. A persistent gap between Precision and IoU further indicates that under-segmentation and boundary inaccuracies remain prevalent error patterns in RS tasks, particularly for irregular and less prevalent targets.

</details>


### [69] [MoonSeg3R: Monocular Online Zero-Shot Segment Anything in 3D with Reconstructive Foundation Priors](https://arxiv.org/abs/2512.15577)
*Zhipeng Du,Duolikun Danier,Jan Eric Lenssen,Hakan Bilen*

Main category: cs.CV

TL;DR: Use CUT3R to get geometry from single RGB; refine 2D masks into 3D queries, use temporal memory and a CUT3R token to fuse masks across frames; achieves competitive results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods require posed RGB-D sequences; need online method using only RGB stream leveraging reconstructive foundation models.

Method: Propose method: leverage CUT3R to generate geometric priors; introduce MoonSeg3R with three modules: self-supervised query refinement with spatial-semantic distillation, 3D query index memory, state-distribution token for mask identity.

Result: Online monocular 3D instance segmentation competitive with RGB-D methods on ScanNet200 and SceneNN; first to enable online monocular 3D segmentation.

Conclusion: MoonSeg3R successfully uses RFM-derived geometry to allow zero-shot online monocular 3D instance segmentation, achieving competitive results to RGB-D approaches.

Abstract: In this paper, we focus on online zero-shot monocular 3D instance segmentation, a novel practical setting where existing approaches fail to perform because they rely on posed RGB-D sequences. To overcome this limitation, we leverage CUT3R, a recent Reconstructive Foundation Model (RFM), to provide reliable geometric priors from a single RGB stream. We propose MoonSeg3R, which introduces three key components: (1) a self-supervised query refinement module with spatial-semantic distillation that transforms segmentation masks from 2D visual foundation models (VFMs) into discriminative 3D queries; (2) a 3D query index memory that provides temporal consistency by retrieving contextual queries; and (3) a state-distribution token from CUT3R that acts as a mask identity descriptor to strengthen cross-frame fusion. Experiments on ScanNet200 and SceneNN show that MoonSeg3R is the first method to enable online monocular 3D segmentation and achieves performance competitive with state-of-the-art RGB-D-based systems. Code and models will be released.

</details>


### [70] [IMKD: Intensity-Aware Multi-Level Knowledge Distillation for Camera-Radar Fusion](https://arxiv.org/abs/2512.15581)
*Shashank Mishra,Karan Patil,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 提出IMKD：三阶段强度感知知识蒸馏（LiDAR→Radar、LiDAR→Fused、Camera↔Radar）用于雷达-相机融合，nuScenes上达到67.0% NDS和61.0% mAP。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法直接传递模态特定特征，会破坏传感器各自的特性并削弱单传感器优势，需一种既保留各自特性又能增强互补性的蒸馏策略。

Method: IMKD包含三阶段强度感知蒸馏：1) LiDAR→Radar的强度感知特征蒸馏，用LiDAR的精细结构信息增强雷达表示；2) LiDAR→Fused的强度引导蒸馏，在融合层有选择地突出有用的几何与深度信息以促进互补而非强制对齐；3) Camera-Radar的强度引导融合机制，帮助特征对齐与校准。整体在多层级架构中应用蒸馏以丰富融合表示。

Result: 在nuScenes基准上，IMKD实现67.0% NDS和61.0% mAP，优于所有先前基于蒸馏的雷达-相机融合方法；代码与模型已开源。

Conclusion: IMKD通过多层次、强度感知的知识蒸馏策略，在不依赖LiDAR推理的前提下提升雷达-相机的3D目标检测性能，既保留了各传感器的固有特性，又增强了模态间互补性。

Abstract: High-performance Radar-Camera 3D object detection can be achieved by leveraging knowledge distillation without using LiDAR at inference time. However, existing distillation methods typically transfer modality-specific features directly to each sensor, which can distort their unique characteristics and degrade their individual strengths. To address this, we introduce IMKD, a radar-camera fusion framework based on multi-level knowledge distillation that preserves each sensor's intrinsic characteristics while amplifying their complementary strengths. IMKD applies a three-stage, intensity-aware distillation strategy to enrich the fused representation across the architecture: (1) LiDAR-to-Radar intensity-aware feature distillation to enhance radar representations with fine-grained structural cues, (2) LiDAR-to-Fused feature intensity-guided distillation to selectively highlight useful geometry and depth information at the fusion level, fostering complementarity between the modalities rather than forcing them to align, and (3) Camera-Radar intensity-guided fusion mechanism that facilitates effective feature alignment and calibration. Extensive experiments on the nuScenes benchmark show that IMKD reaches 67.0% NDS and 61.0% mAP, outperforming all prior distillation-based radar-camera fusion methods. Our code and models are available at https://github.com/dfki-av/IMKD/.

</details>


### [71] [FlexAvatar: Learning Complete 3D Head Avatars with Partial Supervision](https://arxiv.org/abs/2512.15599)
*Tobias Kirschstein,Simon Giebenhain,Matthias Nießner*

Main category: cs.CV

TL;DR: FlexAvatar通过bias sinks在Transformer中解耦驱动信号与目标视角，统一单目与多视角训练，得到既能泛化又具完整性的3D头像表示。


<details>
  <summary>Details</summary>
Motivation: 单目训练因驱动信号与目标视角耦合，导致生成不完整的3D头部；多视角数据稀缺。目标是设计可同时利用两类数据的方法，弥补各自短板。

Method: 提出基于Transformer的3D肖像动画网络，加入可学习的数据源token（bias sinks）实现统一训练，结合多视角监督与单目视频的泛化能力，训练出平滑的潜在头像空间，支持身份插值与多观测拟合。

Result: FlexAvatar提出了一种从单张图像创建高质量完整3D头部头像的方法，通过在单目和多视角数据上统一训练来兼顾泛化与完整性。

Conclusion: 引入可学习的数据源token（bias sinks）和基于Transformer的动画模型，实现了跨数据源训练，并在单视图与稀少样本设置下生成完整的3D头部和逼真表情动画。

Abstract: We introduce FlexAvatar, a method for creating high-quality and complete 3D head avatars from a single image. A core challenge lies in the limited availability of multi-view data and the tendency of monocular training to yield incomplete 3D head reconstructions. We identify the root cause of this issue as the entanglement between driving signal and target viewpoint when learning from monocular videos. To address this, we propose a transformer-based 3D portrait animation model with learnable data source tokens, so-called bias sinks, which enables unified training across monocular and multi-view datasets. This design leverages the strengths of both data sources during inference: strong generalization from monocular data and full 3D completeness from multi-view supervision. Furthermore, our training procedure yields a smooth latent avatar space that facilitates identity interpolation and flexible fitting to an arbitrary number of input observations. In extensive evaluations on single-view, few-shot, and monocular avatar creation tasks, we verify the efficacy of FlexAvatar. Many existing methods struggle with view extrapolation while FlexAvatar generates complete 3D head avatars with realistic facial animations. Website: https://tobias-kirschstein.github.io/flexavatar/

</details>


### [72] [Qwen-Image-Layered: Towards Inherent Editability via Layer Decomposition](https://arxiv.org/abs/2512.15603)
*Shengming Yin,Zekai Zhang,Zecheng Tang,Kaiyuan Gao,Xiao Xu,Kun Yan,Jiahao Li,Yilei Chen,Yuxiang Chen,Heung-Yeung Shum,Lionel M. Ni,Jingren Zhou,Junyang Lin,Chenfei Wu*

Main category: cs.CV

TL;DR: 提出端到端扩散模型Qwen-Image-Layered，将RGB图像解构为可变数目的RGBA层，结合RGBA-VAE、VLD-MMDiT架构与多阶段训练，并构建PSD数据集，显著提高分解与编辑一致性。


<details>
  <summary>Details</summary>
Motivation: 栅格图像的内容纠缠导致编辑一致性差，而专业设计工具使用图层表征实现隔离编辑。受此启发，目标是让生成模型输出易编辑的图层化表示，从而支持不破坏全局一致性的局部改动。

Method: 引入三大组件：1) RGBA-VAE统一RGB与RGBA的潜空间；2) VLD-MMDiT用于可变层数的分解；3) 多阶段训练策略将预训练图像生成模型适配为多层分解器。并从PSD文件抽取注释多层数据以缓解训练数据稀缺。

Result: 实验证明在分解质量、语义分离与下游编辑任务上的性能优于现有方法，提升了图层可编辑性与编辑后一致性，并公开了代码与模型。

Conclusion: 本文提出的Qwen-Image-Layered通过将单张RGB图像分解为多个语义可分离的RGBA图层，实现了内建可编辑性，每层可独立操控而不影响其它内容。该方法在分解质量和编辑一致性上较现有方法显著提升，为图像编辑引入了图层化范式。

Abstract: Recent visual generative models often struggle with consistency during image editing due to the entangled nature of raster images, where all visual content is fused into a single canvas. In contrast, professional design tools employ layered representations, allowing isolated edits while preserving consistency. Motivated by this, we propose \textbf{Qwen-Image-Layered}, an end-to-end diffusion model that decomposes a single RGB image into multiple semantically disentangled RGBA layers, enabling \textbf{inherent editability}, where each RGBA layer can be independently manipulated without affecting other content. To support variable-length decomposition, we introduce three key components: (1) an RGBA-VAE to unify the latent representations of RGB and RGBA images; (2) a VLD-MMDiT (Variable Layers Decomposition MMDiT) architecture capable of decomposing a variable number of image layers; and (3) a Multi-stage Training strategy to adapt a pretrained image generation model into a multilayer image decomposer. Furthermore, to address the scarcity of high-quality multilayer training images, we build a pipeline to extract and annotate multilayer images from Photoshop documents (PSD). Experiments demonstrate that our method significantly surpasses existing approaches in decomposition quality and establishes a new paradigm for consistent image editing. Our code and models are released on \href{https://github.com/QwenLM/Qwen-Image-Layered}{https://github.com/QwenLM/Qwen-Image-Layered}

</details>


### [73] [Robust Multi-view Camera Calibration from Dense Matches](https://arxiv.org/abs/2512.15608)
*Johannes Hägerlind,Bao-Long Tran,Urs Waldmann,Per-Erik Forssén*

Main category: cs.CV

TL;DR: 提出用于多摄像机姿态估计与标定的鲁棒SfM改进，重点在于从密集匹配器对对应点进行子采样以及增量添加视图的选择策略，在扭曲较强相机上显著提高精度，并在全局SfM中验证了方法通用性。


<details>
  <summary>Details</summary>
Motivation: 在动物行为和监控取证等场景中，常有多个刚性固定的相机从不同视角拍摄，现有SfM在强径向畸变和稠密匹配噪声下表现欠佳；需要一种更鲁棒的姿态估计与相机标定流程。

Method: 分析SfM流水线各组件，提出两项设计：1) 从密集匹配器输出的对应关系中设计子采样策略，以选出更可靠且分布均匀的点对用于估计；2) 设计视图增量加入的选择准则，决定哪一帧先用于初始化以减少累积误差。并在全局SfM中以VGGT初始化验证对应子采样的效果。

Result: 在严格的定量评测中，尤其对具有强径向畸变的相机，方法显著优于基线（例如：对比VGGT的40.4%，本文方法达79.9%），整体在多种相机配置上表现良好。

Conclusion: 通过针对对应点子采样和视图选择的设计改进，能显著提升存在强径向畸变和密集匹配噪声时的相机姿态估计与标定鲁棒性，适用于动物行为与取证分析等多摄像机场景。

Abstract: Estimating camera intrinsics and extrinsics is a fundamental problem in computer vision, and while advances in structure-from-motion (SfM) have improved accuracy and robustness, open challenges remain. In this paper, we introduce a robust method for pose estimation and calibration. We consider a set of rigid cameras, each observing the scene from a different perspective, which is a typical camera setup in animal behavior studies and forensic analysis of surveillance footage. Specifically, we analyse the individual components in a structure-from-motion (SfM) pipeline, and identify design choices that improve accuracy. Our main contributions are: (1) we investigate how to best subsample the predicted correspondences from a dense matcher to leverage them in the estimation process. (2) We investigate selection criteria for how to add the views incrementally. In a rigorous quantitative evaluation, we show the effectiveness of our changes, especially for cameras with strong radial distortion (79.9% ours vs. 40.4 vanilla VGGT). Finally, we demonstrate our correspondence subsampling in a global SfM setting where we initialize the poses using VGGT. The proposed pipeline generalizes across a wide range of camera setups, and could thus become a useful tool for animal behavior and forensic analysis.

</details>


### [74] [Persistent feature reconstruction of resident space objects (RSOs) within inverse synthetic aperture radar (ISAR) images](https://arxiv.org/abs/2512.15618)
*Morgan Coe,Gruffudd Jones,Leah-Nani Alconcel,Marina Gashinova*

Main category: cs.CV

TL;DR: 本文通过仿射对齐、梯度比率边缘检测和双权重霍夫变换，在子THz ISAR图像序列上进行线性特征检测与跟踪，提升卫星外部结构识别的置信度，并演示了对阴影等特征的鲁棒检测。


<details>
  <summary>Details</summary>
Motivation: 提升近地空间态势感知（SDA），通过子太赫兹逆合成孔径雷达（ISAR）在太空中实现对在轨物体（RSOs）高分辨率成像，以便识别其外部结构与状态

Method: 使用元启发式模拟器生成ISAR图像序列，先通过仿射变换实现帧间对齐；在单帧中用梯度比率方法进行边缘检测，得到边缘幅值和方向；采用基于幅值与方向的双权重霍夫变换检测线性特征，并在序列中跟踪这些特征以分析其演化

Result: 在模拟的多种遭遇场景中，提出的方法实现了高精度线性特征检测；通过序列特征跟踪，提高了检测与分类的置信度；给出了阴影作为稳健特征的检测示例

Conclusion: 在子THz ISAR序列中引入基于方向和幅值加权的霍夫变换与序列跟踪可提升对卫星外部结构的识别可靠性，支持空间态势感知任务

Abstract: With the rapidly growing population of resident space objects (RSOs) in the near-Earth space environment, detailed information about their condition and capabilities is needed to provide Space Domain Awareness (SDA). Space-based sensing will enable inspection of RSOs at shorter ranges, independent of atmospheric effects, and from all aspects. The use of a sub-THz inverse synthetic aperture radar (ISAR) imaging and sensing system for SDA has been proposed in previous work, demonstrating the achievement of sub-cm image resolution at ranges of up to 100 km. This work focuses on recognition of external structures by use of sequential feature detection and tracking throughout the aligned ISAR images of the satellites. The Hough transform is employed to detect linear features, which are tracked throughout the sequence. ISAR imagery is generated via a metaheuristic simulator capable of modelling encounters for a variety of deployment scenarios. Initial frame-to-frame alignment is achieved through a series of affine transformations to facilitate later association between image features. A gradient-by-ratio method is used for edge detection within individual ISAR images, and edge magnitude and direction are subsequently used to inform a double-weighted Hough transform to detect features with high accuracy. Feature evolution during sequences of frames is analysed. It is shown that the use of feature tracking within sequences with the proposed approach will increase confidence in feature detection and classification, and an example use-case of robust detection of shadowing as a feature is presented.

</details>


### [75] [OccSTeP: Benchmarking 4D Occupancy Spatio-Temporal Persistence](https://arxiv.org/abs/2512.15621)
*Yu Zheng,Jie Hu,Kailun Yang,Jiaming Zhang*

Main category: cs.CV

TL;DR: 提出4D OccSTeP任务与基准，设计OccSTeP-WM世界模型，以体素记忆+线性注意力+循环状态实现鲁棒在线时空预测，显著提升语义与占据IoU。


<details>
  <summary>Details</summary>
Motivation: 提出4D占据时空持久性（OccSTeP）以解决自动驾驶中对三维场景的持久理解与基于时间扰动和未来动作推断的需求。

Method: 建立OccSTeP基准（包含语义标签错误和丢帧等挑战），并提出OccSTeP-WM：无分词器的世界模型，使用稠密体素场景状态，线性复杂度注意力骨干和循环状态空间模块，结合自车运动补偿进行时空上下文增量融合，实现在线推理和鲁棒记忆更新。

Result: 在基准上，OccSTeP-WM在语义mIoU达到23.70%（+6.56%提升），占据IoU达到35.89%（+9.26%提升），显示出在有缺失或噪声历史传感器输入时依然稳健。

Conclusion: OccSTeP作为新的任务定义有效，所提OccSTeP-WM通过稠密体素记忆与高效注意力-循环结构实现了对时空持久性的建模，为反应式与主动式预测提供有力方案，且将开源数据与代码。

Abstract: Autonomous driving requires a persistent understanding of 3D scenes that is robust to temporal disturbances and accounts for potential future actions. We introduce a new concept of 4D Occupancy Spatio-Temporal Persistence (OccSTeP), which aims to address two tasks: (1) reactive forecasting: ''what will happen next'' and (2) proactive forecasting: "what would happen given a specific future action". For the first time, we create a new OccSTeP benchmark with challenging scenarios (e.g., erroneous semantic labels and dropped frames). To address this task, we propose OccSTeP-WM, a tokenizer-free world model that maintains a dense voxel-based scene state and incrementally fuses spatio-temporal context over time. OccSTeP-WM leverages a linear-complexity attention backbone and a recurrent state-space module to capture long-range spatial dependencies while continually updating the scene memory with ego-motion compensation. This design enables online inference and robust performance even when historical sensor input is missing or noisy. Extensive experiments prove the effectiveness of the OccSTeP concept and our OccSTeP-WM, yielding an average semantic mIoU of 23.70% (+6.56% gain) and occupancy IoU of 35.89% (+9.26% gain). The data and code will be open source at https://github.com/FaterYU/OccSTeP.

</details>


### [76] [Towards Physically-Based Sky-Modeling For Image Based Lighting](https://arxiv.org/abs/2512.15632)
*Ian J. Maquignaz*

Main category: cs.CV

TL;DR: 该论文提出AllSky，一种基于物理捕获HDRI学习的全天气云天空模型，支持用户控制太阳和云位置，覆盖全天光照22档动态范围，实现比现有DNN天空模型更真实的光照重现。作者分析了输入模态、色调映射、条件化和评估方法，证明现有DNN方法无法替代实测HDRI或参数化天空模型进行准确照明。


<details>
  <summary>Details</summary>
Motivation: 现有DNN生成的HDRI在视觉质量上虽有提升，但不能在重光（re-light）时再现与真实HDRI相同的色调、阴影和照明，且无法覆盖户外照明所需的22个曝光档（FDR）。因此需要一个既能实现光照物理准确又能支持直观用户控制的全天气天空模型。

Method: 提出AllSky：基于物理捕获的HDRI数据直接学习的全天气天空模型。研究并比较输入模态、色调映射、条件化方式和评估指标。提供用户可控的太阳和云位置调整，使生成的环境映射保留全动态范围并用于真实重光。

Result: AllSky在作者设计的评估框架下取得了当前最先进的天空建模性能，生成的环境图在光照重现方面更接近实测HDRI，并允许直观的用户控制。评估显示现有DNN天空模型与实测HDRI或参数化模型不可互换，存在限制阻碍扩展和下游应用中的准确照明。

Conclusion: AllSky证明了通过直接学习实测HDRI可以同时实现FDR支持与光照真实感，扩展了用户可控的环境图功能，并表明当前DNN方法在精确重光和可扩展性上仍有显著不足。

Abstract: Accurate environment maps are a key component for rendering photorealistic outdoor scenes with coherent illumination. They enable captivating visual arts, immersive virtual reality, and a wide range of engineering and scientific applications. Recent works have extended sky-models to be more comprehensive and inclusive of cloud formations but, as we demonstrate, existing methods fall short in faithfully recreating natural skies. Though in recent years the visual quality of DNN-generated High Dynamic Range Imagery (HDRI) has greatly improved, the environment maps generated by DNN sky-models do not re-light scenes with the same tones, shadows, and illumination as physically captured HDR imagery. In this work, we demonstrate progress in HDR literature to be tangential to sky-modelling as current works cannot support both photorealism and the 22 f-stops required for the Full Dynamic Range (FDR) of outdoor illumination. We achieve this by proposing AllSky, a flexible all-weather sky-model learned directly from physically captured HDRI which we leverage to study the input modalities, tonemapping, conditioning, and evaluation of sky-models. Per user-controlled positioning of the sun and cloud formations, AllSky expands on current functionality by allowing for intuitive user control over environment maps and achieves state-of-the-art sky-model performance. Through our proposed evaluation, we demonstrate existing DNN sky-models are not interchangeable with physically captured HDRI or parametric sky-models, with current limitations being prohibitive of scalability and accurate illumination in downstream applications

</details>


### [77] [IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning](https://arxiv.org/abs/2512.15635)
*Yuanhang Li,Yiren Song,Junzhe Bai,Xinran Liang,Hu Yang,Libiao Jin,Qi Mao*

Main category: cs.CV

TL;DR: 提出IC-Effect：DiT为基础、指令驱动、采用两阶段训练与Effect-LoRA并结合时空稀疏token化的少样本视频VFX编辑方法，能高效生成逼真且时序一致的复杂特效并严格保留背景。


<details>
  <summary>Details</summary>
Motivation: 视频VFX编辑要求注入特效与背景无缝融合、背景完全不变、以及从有限配对数据中高效学习特效模式；现有方法难以同时满足这些严格需求。

Method: 1) 将源视频作为干净的上下文条件，利用DiT模型的上下文学习能力进行精确背景保留与自然特效注入。2) 两阶段训练：先做通用编辑适配以强化指令跟随能力，再用Effect-LoRA进行特效专门学习以高效建模特效模式。3) 引入时空稀疏token化以减少计算量并保持高保真。4) 构建并公开包含15种风格的配对VFX数据集用于训练与评估。

Result: IC-Effect提出了一种基于DiT且受指令引导的少样本视频特效编辑框架，能在严格保持空间和时间一致性的同时合成复杂特效（如火焰、粒子、卡通角色）。

Conclusion: 通过利用源视频作为干净的上下文条件、两阶段训练（通用编辑适配 + Effect-LoRA的特效专门学习）、以及时空稀疏token化，IC-Effect在背景保留、指令遵从和时序一致性方面均显著优于现有方法。作者还提供了包含15种高质量视觉风格的配对数据集，实验结果表明方法在可控性和视觉质量上表现优异。

Abstract: We propose \textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation.

</details>


### [78] [InpaintDPO: Mitigating Spatial Relationship Hallucinations in Foreground-conditioned Inpainting via Diverse Preference Optimization](https://arxiv.org/abs/2512.15644)
*Qirui Li,Yizhe Tang,Ran Yi,Guangben Lu,Fangyuan Zou,Peng Shu,Huan Yu,Jie Jiang*

Main category: cs.CV

TL;DR: 提出InpaintDPO，通过MaskDPO、Conditional Asymmetric Preference Optimization和Shared Commonality Preference Optimization，专注背景偏好优化，提升前景-背景空间合理性，减少空间错觉。


<details>
  <summary>Details</summary>
Motivation: 当前前景条件修复存在前景与背景之间空间关系幻觉（尺度、位置、视角不合理），且空间合理性主观难量化，阻碍基于奖励的RLHF方法。需要专门的偏好优化框架来约束背景生成的空间关系，同时保持前景完整性。

Method: 使用Direct Preference Optimization作为框架基础；MaskDPO在赢-输对中只对背景区域进行偏好优化以避免梯度冲突并保留前景修复损失；Conditional Asymmetric Preference Optimization通过差异裁剪与全局偏好提升前景-背景边界一致性；Shared Commonality Preference Optimization利用获胜样本的共性增强模型对高质量空间关系的理解。

Result: This paper presents InpaintDPO, a Direct Preference Optimization framework tailored to foreground-conditioned inpainting to address Spatial Relationship Hallucinations. It introduces MaskDPO to restrict preference optimization to background regions, Conditional Asymmetric Preference Optimization to improve boundary coherence via asymmetric cropping and global preference optimization, and Shared Commonality Preference Optimization to reinforce common spatial patterns among winning samples.

Conclusion: InpaintDPO effectively mitigates spatial hallucinations by focusing preference learning on background spatial relationships, improving foreground preservation and boundary coherence, and leveraging shared commonalities among high-quality samples to promote spatial rationality.

Abstract: Foreground-conditioned inpainting, which aims at generating a harmonious background for a given foreground subject based on the text prompt, is an important subfield in controllable image generation. A common challenge in current methods, however, is the occurrence of Spatial Relationship Hallucinations between the foreground subject and the generated background, including inappropriate scale, positional relationships, and viewpoints. Critically, the subjective nature of spatial rationality makes it challenging to quantify, hindering the use of traditional reward-based RLHF methods. To address this issue, we propose InpaintDPO, the first Direct Preference Optimization (DPO) based framework dedicated to spatial rationality in foreground-conditioned inpainting, ensuring plausible spatial relationships between foreground and background elements. To resolve the gradient conflicts in standard DPO caused by identical foreground in win-lose pairs, we propose MaskDPO, which confines preference optimization exclusively to the background to enhance background spatial relationships, while retaining the inpainting loss in the foreground region for robust foreground preservation. To enhance coherence at the foreground-background boundary, we propose Conditional Asymmetric Preference Optimization, which samples pairs with differentiated cropping operations and applies global preference optimization to promote contextual awareness and enhance boundary coherence. Finally, based on the observation that winning samples share a commonality in plausible spatial relationships, we propose Shared Commonality Preference Optimization to enhance the model's understanding of spatial commonality across high-quality winning samples, further promoting shared spatial rationality.

</details>


### [79] [Hard Labels In! Rethinking the Role of Hard Labels in Mitigating Local Semantic Drift](https://arxiv.org/abs/2512.15647)
*Jiacheng Cui,Bingkui Tong,Xinyue Bi,Xiaohan Zhao,Jiacheng Liu,Zhiqiang shen*

Main category: cs.CV

TL;DR: 针对少量裁剪下软标签导致的局部语义漂移，作者提出用硬标签作为校正信号的HALD方法，理论证明并在ImageNet-1K等实验中显著提升性能（较LPLD高9%）。


<details>
  <summary>Details</summary>
Motivation: 软标签提供比硬标签更丰富的监督，但在每张图像只有少量裁剪时会发生局部语义漂移：裁剪可能更像其他类别，导致软嵌入偏离原始图像的语义。该漂移引起训练与测试间分布不对齐，并产生系统性错误。作者想重新评估硬标签的作用，利用硬标签校准语义漂移。

Method: 理论分析在少量软标签监督下漂移产生的机制，证明混合软硬标签可以恢复视觉内容与语义监督的对齐。提出HALD（Hard Label for Alleviating Local Semantic Drift）范式：在训练中将硬标签作为中间校正信号，同时保留软标签的细粒度信息。在数据集蒸馏与大规模分类任务中集成该策略。

Result: 在多个数据集蒸馏和常规模型分类基准上，HALD稳健提升泛化性能。特别是在ImageNet-1K上，使用285M存储的软标签实现42.7%（准确率或其他指标）表现，比之前最先进方法LPLD提升9.0%。

Conclusion: 硬标签在软标签主导的训练范式中仍是重要且互补的工具。适当混合硬与软标签可缓解局部语义漂移，恢复训练-测试对齐，并显著改善蒸馏与分类性能。作者呼吁重新思考硬标签的地位。

Abstract: Soft labels generated by teacher models have become a dominant paradigm for knowledge transfer and recent large-scale dataset distillation such as SRe2L, RDED, LPLD, offering richer supervision than conventional hard labels. However, we observe that when only a limited number of crops per image are used, soft labels are prone to local semantic drift: a crop may visually resemble another class, causing its soft embedding to deviate from the ground-truth semantics of the original image. This mismatch between local visual content and global semantic meaning introduces systematic errors and distribution misalignment between training and testing. In this work, we revisit the overlooked role of hard labels and show that, when appropriately integrated, they provide a powerful content-agnostic anchor to calibrate semantic drift. We theoretically characterize the emergence of drift under few soft-label supervision and demonstrate that hybridizing soft and hard labels restores alignment between visual content and semantic supervision. Building on this insight, we propose a new training paradigm, Hard Label for Alleviating Local Semantic Drift (HALD), which leverages hard labels as intermediate corrective signals while retaining the fine-grained advantages of soft labels. Extensive experiments on dataset distillation and large-scale conventional classification benchmarks validate our approach, showing consistent improvements in generalization. On ImageNet-1K, we achieve 42.7% with only 285M storage for soft labels, outperforming prior state-of-the-art LPLD by 9.0%. Our findings re-establish the importance of hard labels as a complementary tool, and call for a rethinking of their role in soft-label-dominated training.

</details>


### [80] [VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?](https://arxiv.org/abs/2512.15649)
*Hongbo Zhao,Meng Wang,Fei Zhu,Wenzhuo Liu,Bolin Ni,Fanhu Zeng,Gaofeng Meng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出首个VTC基准并评测多种VLM，发现大多数模型在VTC压缩文本下无法有效理解长上下文关联，表明未来需改进模型架构与训练策略以处理高密度视觉文本表示。


<details>
  <summary>Details</summary>
Motivation: 分析VTC（vision-text compression）对扩展LLM上下文窗口影响的研究空白，评估其对视觉-语言模型（VLM）长期上下文理解能力的影响。

Method: 提出首个VTC基准（包含VTC-Retrieval、VTC-Reasoning、VTC-Memory三种任务）并建立VTCBench-Wild以模拟多样输入；对开源与专有VLM进行全面评测，检查其在压缩文本（2D视觉表示）下的信息检索、推理与记忆表现。

Result: 尽管VLM在解码文本（如OCR）方面表现良好，大多数模型在处理VTC压缩信息时的长期上下文理解能力显著不足，难以捕捉长距离关联或依赖，影响检索、推理和长期记忆问答性能。

Conclusion: 当前VLM在面对高信息密度的VTC表示时存在严重能力缺陷，研究为未来设计更高效、可扩展的VLM提供基准与方向。

Abstract: The computational and memory overheads associated with expanding the context window of LLMs severely limit their scalability. A noteworthy solution is vision-text compression (VTC), exemplified by frameworks like DeepSeek-OCR and Glyph, which convert long texts into dense 2D visual representations, thereby achieving token compression ratios of 3x-20x. However, the impact of this high information density on the core long-context capabilities of vision-language models (VLMs) remains under-investigated. To address this gap, we introduce the first benchmark for VTC and systematically assess the performance of VLMs across three long-context understanding settings: VTC-Retrieval, which evaluates the model's ability to retrieve and aggregate information; VTC-Reasoning, which requires models to infer latent associations to locate facts with minimal lexical overlap; and VTC-Memory, which measures comprehensive question answering within long-term dialogue memory. Furthermore, we establish the VTCBench-Wild to simulate diverse input scenarios.We comprehensively evaluate leading open-source and proprietary models on our benchmarks. The results indicate that, despite being able to decode textual information (e.g., OCR) well, most VLMs exhibit a surprisingly poor long-context understanding ability with VTC-compressed information, failing to capture long associations or dependencies in the context.This study provides a deep understanding of VTC and serves as a foundation for designing more efficient and scalable VLMs.

</details>


### [81] [Stylized Synthetic Augmentation further improves Corruption Robustness](https://arxiv.org/abs/2512.15675)
*Georg Siedel,Rojan Regmi,Abhirami Anand,Weijia Shao,Silvia Vock,Andrey Morozov*

Main category: cs.CV

TL;DR: Stylizing synthetic images for augmentation boosts corruption robustness significantly, achieving SOTA on small benchmarks despite poorer FID


<details>
  <summary>Details</summary>
Motivation: Deep vision models are vulnerable to common corruptions; synthetic data alone or style transfer alone insufficient, so combining them may improve robustness

Method: Training-data augmentation pipeline combining synthetic images and neural style transfer; empirical hyperparameter study; integration with rule-based augmentations

Result: Stylized synthetic images improve corruption robustness despite worse FID; SOTA robust accuracy on CIFAR-10-C (93.54%), CIFAR-100-C (74.9%), TinyImageNet-C (50.86%)

Conclusion: Combining synthetic data with neural style transfer yields complementary benefits and improves robustness to common corruptions, compatible with some rule-based augmentations

Abstract: This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively

</details>


### [82] [Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning](https://arxiv.org/abs/2512.15693)
*Yifei Li,Wenzhao Zheng,Yanran Zhang,Runze Sun,Yu Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出Skyra：一个基于可感知伪影的可解释AI视频检测MLLM；构建了首个大规模细粒度伪影标注数据集ViF-CoT-4K；采用两阶段训练提升时空伪影感知与解释能力；并通过ViF-Bench进行全面评估，结果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成视频检测方法多为二分类且缺乏可解释性，社会担忧增加，需要能给出人类可理解证据的检测系统。

Method: 构建ViF-CoT-4K细粒度伪影标注集；设计两阶段训练策略（增强时空伪影感知与解释能力）；训练多模态大语言模型Skyra以检测并生成基于视觉伪影的解释。

Result: 在多个基准上Skyra优于现有方法；通过3K样本的ViF-Bench评估，证明了检测准确性与解释质量的提升，并为可解释AI视频检测提出了洞见。

Conclusion: Skyra通过专门的多模态大模型定位并利用可被人类感知的视觉伪影作为检测与解释的依据，在人工智能生成视频的可解释检测任务上实现了显著提升。

Abstract: The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model's spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.

</details>


### [83] [VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression](https://arxiv.org/abs/2512.15701)
*Kyle Sargent,Ruiqi Gao,Philipp Henzler,Charles Herrmann,Aleksander Holynski,Li Fei-Fei,Jiajun Wu,Jason Zhang*

Main category: cs.CV

TL;DR: 作者发现现有VLM能零样本复现人类2AFC偏好。提出VLIC：一种用VLM二元偏好进行后训练的扩散式图像压缩系统，避免训练独立感知损失网络，实验证明在感知对齐压缩上达到或接近最新水平，并进行了奖励设计与训练细节分析。


<details>
  <summary>Details</summary>
Motivation: 传统像素级失真（如MSE）与人类主观感受不一致，已有方法用神经网络感知损失需要大规模人工标注。若能用已有大规模训练的VLM零样本预测人类偏好，可省去大量标注并更直接对齐人类感知。

Method: 提出VLIC：基于扩散模型的图像压缩系统，使用VLM对两幅重建图像进行二元比较得到偏好信号，直接将此偏好作为奖励对扩散模型进行后训练（post-training），而不是蒸馏出单独的感知损失网络。

Result: 实验证明VLM在2AFC任务上零样本复现人类判断，VLIC通过在VLM偏好上校准后在若干数据集上根据感知指标和大规模用户研究达到或接近最先进水平，并提供了对奖励设计和训练过程的深入分析。

Conclusion: 该论文展示了利用视觉-语言模型（VLMs）进行零样本视觉判断，以指导基于扩散模型的图像压缩后训练，从而实现对齐人类主观感受的压缩模型。

Abstract: Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic

</details>


### [84] [End-to-End Training for Autoregressive Video Diffusion via Self-Resampling](https://arxiv.org/abs/2512.15702)
*Yuwei Guo,Ceyuan Yang,Hao He,Yang Zhao,Meng Wei,Zhenheng Yang,Weilin Huang,Dahua Lin*

Main category: cs.CV

TL;DR: Introduce Resampling Forcing: self-resampling degrades history frames during training + sparse causal mask for parallel causal diffusion loss + history routing for top-k relevant past frame retrieval; matches distillation baselines and improves long-term temporal consistency.


<details>
  <summary>Details</summary>
Motivation: Address exposure bias in autoregressive video diffusion without relying on bidirectional teacher models or online discriminators, enabling scalable end-to-end training from scratch.

Method: Resampling Forcing with self-resampling, sparse causal mask, history routing

Result: Comparable to distillation-based baselines; superior temporal consistency on longer videos; enables parallel training and native-length training

Conclusion: Teacher-free, end-to-end training of autoregressive video diffusion models that mitigates exposure bias and scales to long-horizon generation

Abstract: Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.

</details>


### [85] [GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection](https://arxiv.org/abs/2512.15707)
*Yu Wang,Juhyung Ha,Frangil M. Ramirez,Yuchen Wang,David J. Crandall*

Main category: cs.CV

TL;DR: GateFusion用HiGate在多层Transformer中按需注入跨模态信息，并设计MAL与OPP辅助约束，显著提升ASD性能并具备较好泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有ASD方法多采用后期融合，难以捕捉细粒度跨模态交互。作者希望在保留强大单模态预训练编码器的同时，实现更深层次的跨模态信息交互以提高鲁棒性和泛化能力。

Method: 将预训练的视觉和音频编码器与一个分层门控融合解码器（HiGate）结合；HiGate在Transformer的多个层次通过学得的双模态条件门控器有选择地将一种模态的上下文注入到另一模态；同时引入Masked Alignment Loss将单模态输出对齐到多模态预测，和Over-Positive Penalty以抑制仅视频驱动的虚假激活。

Result: GateFusion提出一种用于主动发言人检测的多模态融合架构，通过在Transformer不同层次使用双模态条件门控的分层融合解码器（HiGate），实现逐步注入跨模态上下文，并辅以Masked Alignment Loss和Over-Positive Penalty两个辅助损失，显著提升了在多个基准上的mAP。

Conclusion: 分层门控的多层注入结合对齐与抑制噪声的辅助损失，是GateFusion性能提升的关键，展示了与强大单模态编码器联合训练进行深度跨模态交互的有效性。

Abstract: Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.

</details>


### [86] [Multi-View Foundation Models](https://arxiv.org/abs/2512.15708)
*Leo Segre,Or Hirschorn,Shai Avidan*

Main category: cs.CV

TL;DR: 将单视图基础模型扩展为多视图基础模型，通过中间的3D感知注意力层在不同视图间对齐特征，显著提升跨视图匹配和下游任务表现。


<details>
  <summary>Details</summary>
Motivation: 单视图基础模型在多视图场景下对同一3D点生成的不一致特征限制了利用多视图信息的能力；需要一种方法在图像空间直接实现视图间特征一致性，而无需构建完整3D特征模型。

Method: 在Transformer架构中插入中间3D-aware attention层，接受一组图像作为输入并输出每张图像的特征图；这些层利用几何关系（如相机参数/视图几何）在视图间进行注意力配对，从而使对应3D点的特征一致。

Result: Multi-view foundation model converts single-view foundation models to produce consistent multi-view features.

Conclusion: Augmenting transformer-based foundation models with 3D-aware attention layers yields substantially more consistent cross-view features and improves downstream tasks like surface normal estimation and multi-view segmentation.

Abstract: Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.

</details>


### [87] [Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering](https://arxiv.org/abs/2512.15711)
*Divam Gupta,Anuj Pahuja,Nemanja Bartolovic,Tomas Simon,Forrest Iandola,Giljoo Nam*

Main category: cs.CV

TL;DR: 提出 GPiCA：用三角网格表示皮肤等表面、用3D高斯表示头发等体积，结合统一可微渲染与网络解码，实现既真实又高效的头像渲染，适配移动端。


<details>
  <summary>Details</summary>
Motivation: 现有头像方法通常在真实感与渲染效率之间权衡：纯高斯（或体积）方法真实但昂贵，纯网格方法高效但难表现复杂体毛与半透明结构。目标是兼顾两者优点，用混合表示在移动端实现高真实感。

Method: 引入混合表示：三角网格负责表面区域，3D各向异性高斯负责非表面（如头发、胡须）；将网格作为半透明层嵌入到3D Gaussian Splatting的体积渲染范式中，构建统一可微渲染器。训练一个网络将表情编码解码为三维面网格、RGBA纹理与一组3D高斯，并以多视角图像监督端到端训练。

Result: 在多视角数据上，GPiCA 在视觉真实感上接近纯高斯方法，同时在渲染速度和内存占用上匹配网格方法，证明了混合表示在质量与效率之间的平衡。

Conclusion: GPiCA 提出了一种混合表示（网格+各向异性3D高斯），通过可微渲染将两者统一渲染，并训练网络从表情编码解码出网格、RGBA纹理和3D高斯，达到高真实感且移动端高效渲染。

Abstract: We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.

</details>


### [88] [DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models](https://arxiv.org/abs/2512.15713)
*Lunbin Zeng,Jingfeng Yao,Bencheng Liao,Hongyuan Tao,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出DiffusionVL：一种可从强大自回归（AR）模型转化得到的扩散视觉语言模型（dVLM）。通过简单微调，将AR预训练模型适配为扩散范式，并设计块解码支持任意长度生成和KV缓存重用，实现推理加速。尽管训练数据量 <5%，在多项视觉与认知基准上显著提升性能（MMMU-Pro↑34.4%，MME↑37.5%）并获得2×推理速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前dVLM受限于基础扩散语言模型能力，性能远落后于主流AR模型。问题是能否基于现有强AR模型构建高性能dVLM，从而兼得扩散范式优势与AR模型能力。

Method: 提出DiffusionVL框架：通过对现有AR预训练模型进行简单微调，将其转换为扩散范式的dVLM。同时引入块解码设计以支持任意长度生成并复用KV缓存，提升推理效率。实施细节包括使用少量（<5%）训练数据完成适配与微调。

Result: 在广泛实验中，DiffusionVL在MMMU-Pro（视觉）上提升34.4%，在MME（认知）上提升37.5%；同时实现2倍推理速度加速。转换自AR模型的dVLM与LLaVA风格的视觉指令微调表现相当。

Conclusion: 验证了将AR模型迁移到扩散范式的可行性与有效性；DiffusionVL在性能与效率上均取得显著改进，且对训练数据需求大幅降低，具有实际应用潜力。

Abstract: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.

</details>


### [89] [In Pursuit of Pixel Supervision for Visual Pre-training](https://arxiv.org/abs/2512.15715)
*Lihe Yang,Shang-Wen Li,Yang Li,Xinjie Lei,Dong Wang,Abdelrahman Mohamed,Hengshuang Zhao,Hu Xu*

Main category: cs.CV

TL;DR: Pixio: enhanced MAE trained on 2B images; competitive vs DINOv3 across tasks; pixel-space SSL viable


<details>
  <summary>Details</summary>
Motivation: To show autoencoder-based SSL remains competitive and useful for many downstream tasks

Method: Analyse abstract

Result: See fields

Conclusion: See fields

Abstract: At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed "Pixio", is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.

</details>


### [90] [Spatia: Video Generation with Updatable Spatial Memory](https://arxiv.org/abs/2512.15716)
*Jinjing Zhao,Fangyun Wei,Zhening Liu,Hongyang Zhang,Chang Xu,Yan Lu*

Main category: cs.CV

TL;DR: Spatia stores a persistent 3D point cloud memory updated by SLAM and generates clips conditioned on it to improve consistency and enable 3D-aware controls.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current video generation models in maintaining long-term spatial and temporal consistency due to high-dimensional dense video signals.

Method: Iteratively generate video clips conditioned on a persistent 3D point cloud spatial memory, continuously update memory via visual SLAM, and disentangle dynamic entities from static scene geometry to preserve consistency and enable explicit camera/3D editing.

Result: Spatia framework proposes preserving a 3D scene point cloud as persistent spatial memory to improve long-term spatial and temporal consistency in video generation.

Conclusion: Using a dynamic-static disentanglement with visual SLAM updates enables better spatial consistency, camera control, and 3D-aware editing while keeping realistic dynamics.

Abstract: Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model's ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [91] [MS-Index: Fast Top-k Subsequence Search for Multivariate Time Series under Euclidean Distance](https://arxiv.org/abs/2512.14723)
*Jens E. d'Hondt,Teun Kortekaas,Odysseas Papapetrou,Themis Palpanas*

Main category: cs.DB

TL;DR: 提出MS-Index：支持按需选择通道的精确MTS子序列检索，查询速度远超现有方法（快10–100倍），且对通道数扩展次线性。


<details>
  <summary>Details</summary>
Motivation: 实际中多元时间序列包含多通道，但不同查询只关注其中少数通道，需支持查询时动态指定相关通道以提高检索效率和精度。

Method: 设计MS-Index索引结构和检索流程，允许在查询时只利用指定通道进行欧氏距离计算，使用能够剪枝与聚合通道信息的技术以实现次线性扩展；算法为精确检索，兼顾原始与归一化子序列情形。

Result: 在34个数据集上比较，MS-Index对原始和归一化子序列检索均比最先进方法快1–2个数量级，证明了其高效性与可扩展性。

Conclusion: 本文提出了MS-Index，一种支持查询时任意选择通道的精确多元时间序列子序列最近邻检索算法；算法在欧氏距离下具有次线性地随查询通道数扩展的查询性能，并在34个数据集上的实验中对原始和归一化子序列均比现有方法快1至2个数量级。

Abstract: Modern applications frequently collect and analyze temporal data in the form of multivariate time series (MTS) -- time series that contain multiple channels. A common task in this context is subsequence search, which involves identifying all MTS that contain subsequences highly similar to a query time series. In practical scenarios, not all channels of an MTS are relevant to every query. For instance, airplane sensors may gather data on a plethora of components and subsystems, but only a few of these are relevant to a specific query, such as identifying the cause of a malfunctioning landing gear, or a specific flight maneuver. Consequently, the relevant query channels are often specified at query time. In this work, we introduce the Multivariate Subsequence Index (MS-Index), a novel algorithm for nearest neighbor MTS subsequence search under Euclidean distance that supports ad-hoc selection of query channels. The algorithm is exact and demonstrates query performance that scales sublinearly to the number of query channels. We examine the properties of \name with a thorough experimental evaluation over 34 datasets, and show that it outperforms the state-of-the-art one to two orders of magnitude for both raw and normalized subsequences.

</details>


### [92] [Extracting node comparison insights for the interactive exploration of property graphs](https://arxiv.org/abs/2512.15157)
*Cristina Aguiar,Jacques Chabin,Alexandre Chanson,Mirian Halfeld-Ferrari,Nicolas Hiot,Nicolas Labroche,Patrick Marcel,Verónika Peralta,Felipe Vasconcelos*

Main category: cs.DB

TL;DR: 针对属性图中基于节点属性的比较问题，本文提出自动提取节点比较的方法：用节点上下文构造比较指标，形式化定义组划分问题以确保比较既显著又非平凡，并提出多种启发式解法。实测表明简单启发式可在几分钟内提供洞见，而高质量洞见需更慢的方法。


<details>
  <summary>Details</summary>
Motivation: 虽然图中节点重要性评分（如中心性）研究已久，但基于属性图中节点属性进行比较的自动化方法尚未被广泛研究。作者旨在支持交互式探索性分析，自动发现有意义且非平凡的节点比较，帮助分析者快速获得洞见。

Method: 1) 利用被比较节点的上下文信息构造比较指标（comparison indicators）。2) 正式化定义使用这些指标对节点进行分组的问题，约束为：比较需显著且非显而易见。3) 提出若干启发式算法来求解该组划分问题，平衡效率与比较质量。

Result: 在真实属性图数据库上的测试显示：快速、简单的启发式算法能在几分钟内生成有用的比较洞见；更复杂、耗时的启发式算法则能产出质量更高的比较结果。

Conclusion: 提出的方法可自动从属性图中提取有意义的节点比较，支持交互式探索分析。不同启发式在速度与质量间做权衡，适用于实时分析或深度研究。

Abstract: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.

</details>


### [93] [Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting](https://arxiv.org/abs/2512.15308)
*Basil Ell*

Main category: cs.DB

TL;DR: 提出GPARs：一种对有向标记多重图（如RDF）适用的图模式关联规则；采用no-repeated-anything语义并在概率空间中导出置信度、提升度等指标，分析其与传统项集指标的关系与一致性


<details>
  <summary>Details</summary>
Motivation: 拓展关联规则到能考虑图拓扑和多重/有向标记边的场景，弥补现有图约束和规则形式不能同时支持生成性与评估性任务的不足

Method: Introduce GPARs for directed labeled multigraphs; define no-repeated-anything semantics; define probability space; derive metrics (confidence, lift, leverage, conviction); analyze relation to itemset metrics

Result: GPAR framework supports generative and evaluative tasks; more expressive than related formalisms; metrics adapted to probabilistic graph setting

Conclusion: GPARs provide a principled, probabilistic extension of association rule metrics to graph patterns allowing topology-aware rule evaluation; under certain conditions classical metric properties are preserved

Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.

</details>


### [94] [Revisiting Task-Oriented Dataset Search in the Era of Large Language Models: Challenges, Benchmark, and Solution](https://arxiv.org/abs/2512.15363)
*Zixin Wei,Yucan Guo,Jinyang Li,Xiaolin Han,Xiaolong Jin,Chenhao Ma*

Main category: cs.DB

TL;DR: 提出KATS，一个从非结构化科研文献中进行任务导向数据集检索的端到端系统。通过离线构建任务-数据集知识图谱与在线结合向量检索和图排序的混合查询引擎，并推出CS-TDS基准，实验证明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究者以高层次任务描述查找合适数据集时，面临意图模糊、任务到数据集映射缺失、基准空白和实体歧义等挑战，现有检索系统难以满足需求。

Method: KATS包含离线知识库构建与在线查询处理两部分。离线通过协同多智能体信息抽取自动构建可动态更新的任务-数据集知识图谱，并使用语义机制进行任务实体链接和数据集实体消歧。在线采用向量检索与图结构排序结合的混合查询引擎检索相关数据集。另提出CS-TDS评估基准。

Result: 在所提CS-TDS基准上，KATS在效果与效率上显著优于最先进的检索增强生成（RAG）框架，证明其在任务导向数据集检索上的优势。

Conclusion: KATS为从文献中发现数据集提供了实用且高效的解决方案，通过知识图谱与语义消歧结合混合检索，弥补了任务-数据集映射与评估基准的空白，指明了数据集检索系统的发展方向。

Abstract: The search for suitable datasets is the critical "first step" in data-driven research, but it remains a great challenge. Researchers often need to search for datasets based on high-level task descriptions. However, existing search systems struggle with this task due to ambiguous user intent, task-to-dataset mapping and benchmark gaps, and entity ambiguity. To address these challenges, we introduce KATS, a novel end-to-end system for task-oriented dataset search from unstructured scientific literature. KATS consists of two key components, i.e., offline knowledge base construction and online query processing. The sophisticated offline pipeline automatically constructs a high-quality, dynamically updatable task-dataset knowledge graph by employing a collaborative multi-agent framework for information extraction, thereby filling the task-to-dataset mapping gap. To further address the challenge of entity ambiguity, a unique semantic-based mechanism is used for task entity linking and dataset entity resolution. For online retrieval, KATS utilizes a specialized hybrid query engine that combines vector search with graph-based ranking to generate highly relevant results. Additionally, we introduce CS-TDS, a tailored benchmark suite for evaluating task-oriented dataset search systems, addressing the critical gap in standardized evaluation. Experiments on our benchmark suite show that KATS significantly outperforms state-of-the-art retrieval-augmented generation frameworks in both effectiveness and efficiency, providing a robust blueprint for the next generation of dataset discovery systems.

</details>


### [95] [ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata](https://arxiv.org/abs/2512.15365)
*Gajendra Doniparthi,Shashank Balu Pandhare,Stefan Deßloch,Timo Mühlhaus*

Main category: cs.DB

TL;DR: ArcBERT: LLM-based semantic search for RDM metadata, supports natural language and understands metadata hierarchies.


<details>
  <summary>Details</summary>
Motivation: Traditional RDM search uses keyword queries; need natural language understanding and semantic matching for better dataset discovery.

Method: Introduce ArcBERT, an LLM-based system that accepts natural language queries, uses semantic matching, and models metadata structure and hierarchies for integrated exploration.

Result: ArcBERT effectively handles diverse querying patterns and improves discovery compared to traditional keyword search.

Conclusion: LLM-driven semantic search that understands metadata structure enhances RDM metadata exploration.

Abstract: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.

</details>
