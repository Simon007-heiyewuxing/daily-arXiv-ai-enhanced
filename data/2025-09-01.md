<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 59]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving](https://arxiv.org/abs/2508.21080)
*Ali K. AlShami,Ryan Rabinowitz,Maged Shoman,Jianwu Fang,Lukas Picek,Shao-Yuan Lo,Steve Cruz,Khang Nhut Lam,Nachiket Kamod,Lei-Lei Li,Jugal Kalita,Terrance E. Boult*

Main category: cs.CV

TL;DR: 2COOOL 研讨会在ICCV 2025上聚焦自动驾驶中对未知/标外危险的检测与处理，目标推动方法、基准和工业实践的进步。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶在真实世界中处理新颖场景与未知危险的能力，通过汇聚学术与工业界的研究推动异常检测、开集识别、开词汇模型等方法在自动驾驶领域的应用与基准建设。

Method: 组织学术与工业讲座、论文与演示，聚焦异常/开集检测、视觉-语言危险理解、基准和方法学发展，并通过研讨会推动新算法与系统的落地。

Result: 组织第二届2COOOL研讨会（ICCV 2025），汇集学术和工业界，推动新方法、基准与实践，用于感知、决策与危险规避；继WACV 2025首届成功后继续扩展影响。

Conclusion: 通过专门研讨会促进跨界交流与基准化，推动自动驾驶系统在新颖场景下的安全性与鲁棒性提升，为实现更安全的无人驾驶迈出关键一步。

Abstract: As the computer vision community advances autonomous driving algorithms,
integrating vision-based insights with sensor data remains essential for
improving perception, decision making, planning, prediction, simulation, and
control. Yet we must ask: Why don't we have entirely safe self-driving cars
yet? A key part of the answer lies in addressing novel scenarios, one of the
most critical barriers to real-world deployment. Our 2COOOL workshop provides a
dedicated forum for researchers and industry experts to push the state of the
art in novelty handling, including out-of-distribution hazard detection,
vision-language models for hazard understanding, new benchmarking and
methodologies, and safe autonomous driving practices. The 2nd Workshop on the
Challenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be held
at the International Conference on Computer Vision (ICCV) 2025 in Honolulu,
Hawaii, on October 19, 2025. We aim to inspire the development of new
algorithms and systems for hazard avoidance, drawing on ideas from anomaly
detection, open-set recognition, open-vocabulary modeling, domain adaptation,
and related fields. Building on the success of its inaugural edition at the
Winter Conference on Applications of Computer Vision (WACV) 2025, the workshop
will feature a mix of academic and industry participation.

</details>


### [2] [Advanced Deep Learning Techniques for Classifying Dental Conditions Using Panoramic X-Ray Images](https://arxiv.org/abs/2508.21088)
*Alireza Golkarieh,Kiana Kiashemshaki,Sajjad Rezvani Boroujeni*

Main category: cs.CV

TL;DR: Using CNN feature extraction combined with Random Forest yields best classification (85.4%) on a dataset of 1,512 panoramic X-rays; hybrid approach outperforms custom CNN and most pre-trained models, but larger datasets and clinical validation are needed.


<details>
  <summary>Details</summary>
Motivation: Automate classification of dental conditions (fillings, cavities, implants, impacted teeth) in panoramic radiographs to support clinicians and improve efficiency and reliability.

Method: Dataset of 1,512 panoramic X-rays with 11,137 annotations; preprocessing and class balancing; evaluated custom CNN, hybrid (CNN features + traditional classifiers like Random Forest), and fine-tuned pre-trained models (VGG16, Xception, ResNet50); 5-fold cross-validation; metrics: accuracy, precision, recall, F1.

Result: Hybrid CNN + Random Forest achieved 85.4% accuracy (best); custom CNN baseline 74.3%; VGG16 82.3%, Xception and ResNet50 lower; hybrid models better at distinguishing morphologically similar conditions.

Conclusion: Hybrid CNN + Random Forest provides best performance (85.4%) and outperforms custom CNN; approach promising for automated dental diagnosis but needs larger datasets and clinical validation.

Abstract: This study investigates deep learning methods for automated classification of
dental conditions in panoramic X-ray images. A dataset of 1,512 radiographs
with 11,137 expert-verified annotations across four conditions fillings,
cavities, implants, and impacted teeth was used. After preprocessing and class
balancing, three approaches were evaluated: a custom convolutional neural
network (CNN), hybrid models combining CNN feature extraction with traditional
classifiers, and fine-tuned pre-trained architectures. Experiments employed 5
fold cross validation with accuracy, precision, recall, and F1 score as
evaluation metrics. The hybrid CNN Random Forest model achieved the highest
performance with 85.4% accuracy, surpassing the custom CNN baseline of 74.3%.
Among pre-trained models, VGG16 performed best at 82.3% accuracy, followed by
Xception and ResNet50. Results show that hybrid models improve discrimination
of morphologically similar conditions and provide efficient, reliable
performance. These findings suggest that combining CNN-based feature extraction
with ensemble classifiers offers a practical path toward automated dental
diagnostic support, while also highlighting the need for larger datasets and
further clinical validation.

</details>


### [3] [Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment](https://arxiv.org/abs/2508.21090)
*Namu Kim,Wonbin Kweon,Minsoo Kim,Hwanjo Yu*

Main category: cs.CV

TL;DR: Q-Align通过用Query-Query对齐并结合Key-Value重排与注意力细化，解决了注意力泄漏问题，显著提升零-shot外观迁移的外观保真度。


<details>
  <summary>Details</summary>
Motivation: 发现大规模图像生成模型在零-shot外观迁移时存在注意力泄漏问题——语义映射被Query-Key对齐错误捕捉，导致外观与结构对齐不佳。

Method: 提出Q-Align框架，包含(1) Query-Query对齐用于捕捉图像间复杂的空间语义映射；(2) Key-Value重排用于通过重对齐增强特征对应关系；(3) 使用重排后的Key和Value进行注意力细化以保持语义一致性。

Result: 大量实验证明Q-Align在外观保真度上优于现有最先进方法，同时在结构保持上保持竞争力。

Conclusion: 该方法通过用Query-Query对齐替代传统的Query-Key对齐，有效缓解了注意力泄漏，提升了零-shot外观迁移的语义一致性和外观保真度。

Abstract: We observe that zero-shot appearance transfer with large-scale image
generation models faces a significant challenge: Attention Leakage. This
challenge arises when the semantic mapping between two images is captured by
the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing
Query-Query alignment to mitigate attention leakage and improve the semantic
alignment in zero-shot appearance transfer. Q-Align incorporates three core
contributions: (1) Query-Query alignment, facilitating the sophisticated
spatial semantic mapping between two images; (2) Key-Value rearrangement,
enhancing feature correspondence through realignment; and (3) Attention
refinement using rearranged keys and values to maintain semantic consistency.
We validate the effectiveness of Q-Align through extensive experiments and
analysis, and Q-Align outperforms state-of-the-art methods in appearance
fidelity while maintaining competitive structure preservation.

</details>


### [4] [ERTACache: Error Rectification and Timesteps Adjustment for Efficient Diffusion](https://arxiv.org/abs/2508.21091)
*Xurui Peng,Hong Liu,Chenqian Yan,Rui Ma,Fangmin Chen,Xing Wang,Zhihua Wu,Songwei Liu,Mingbao Lin*

Main category: cs.CV

TL;DR: 提出ERTACache，分解并校正缓存引入的特征移位与步长放大误差，通过残差分析与轨迹感知校正实现激进缓存重用，在多项生成任务上达到约2x的加速且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型迭代推理成本高，特征缓存可加速但会带来累积质量退化，需分析并消除缓存引入的误差以实现激进重用。

Method: 提出了一个包含离线残差分析、基于轨迹的校正系数动态调整时间步间隔、以及通过残差线性化的封闭形式近似来估计缓存误差的缓存框架。

Result: 在图像和视频生成基准上实现最多2倍加速，同时保持或改善视觉质量；在Wan2.1视频扩散模型上实现2x加速且VBench退化极小。

Conclusion: ERTACache通过联合校正缓存误差的两个来源（特征偏移和步长放大误差），在保持或提升生成质量的同时实现了显著的推理加速。

Abstract: Diffusion models suffer from substantial computational overhead due to their
inherently iterative inference process. While feature caching offers a
promising acceleration strategy by reusing intermediate outputs across
timesteps, naive reuse often incurs noticeable quality degradation. In this
work, we formally analyze the cumulative error introduced by caching and
decompose it into two principal components: feature shift error, caused by
inaccuracies in cached outputs, and step amplification error, which arises from
error propagation under fixed timestep schedules. To address these issues, we
propose ERTACache, a principled caching framework that jointly rectifies both
error types. Our method employs an offline residual profiling stage to identify
reusable steps, dynamically adjusts integration intervals via a
trajectory-aware correction coefficient, and analytically approximates
cache-induced errors through a closed-form residual linearization model.
Together, these components enable accurate and efficient sampling under
aggressive cache reuse. Extensive experiments across standard image and video
generation benchmarks show that ERTACache achieves up to 2x inference speedup
while consistently preserving or even improving visual quality. Notably, on the
state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x
acceleration with minimal VBench degradation, effectively maintaining baseline
fidelity while significantly improving efficiency. The code is available at
https://github.com/bytedance/ERTACache.

</details>


### [5] [Video-LLMs with Temporal Visual Screening](https://arxiv.org/abs/2508.21094)
*Zheyu Fan,Jiateng Liu,Yuji Zhang,Zihan Wang,Yi R.,Fung,Manling Li,Heng Ji*

Main category: cs.CV

TL;DR: 提出TVS，通过选取关键时间段与同步简化查询作为前端模块，改善Video-LLMs的时间推理能力。构建基准并提出ReSimplifyIt基线，实验证明在训练和推理中均显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前Video-LLMs因稀疏帧采样和训练阶段缺乏帧间推理监督，难以捕捉细粒度时间语义；受认知科学中人类时间筛选行为启发，希望通过模拟人类拖动进度条关注关键时段的方法来降低认知负担并增强模型对时间信息的利用。

Method: 将TVS作为模块化前端适配器，包含（1）保留关注关键的片段（视频裁剪/修整）；（2）同步重写查询为最直接形式并保持答案一致性（查询简化/重写）；（3）保持任意可能答案下的不变性和一致性。构建TVS基准并提出ReSimplifyIt作为基线方法。

Result: 构建首个TVS基准，ReSimplifyIt在视频裁剪任务上相比先前类似任务F1提高0.47，并在查询重写上表现竞争性。将TVS融入训练带来7.33%相对提升，推理阶段带来34.6%相对提升，证明了时间信息筛选对视频-语言理解的有效性。

Conclusion: 该论文提出了Temporal Visual Screening (TVS)任务，通过选择视频中的关键时间段并同步简化查询，改善Video-LLMs对细粒度时间语义的理解，从而提高视频问答和指令微调的效果。

Abstract: Humans naturally perform temporal screening by dragging the progress bar and
focusing on salient temporal segments, but current Video Large Language Models
(Video-LLMs) struggle to capture fine-grained temporal semantics due to sparse
frame sampling and insufficient inter-frame reasoning supervision during their
training. To address this, Inspired by well-established cognitive science
principles, we propose Temporal Visual Screening (TVS), a new task that
universally pre-processes video question answering and instruction tuning data
by: (1) retaining focus-critical video segments, (2) synchronously
reconstructing queries to their most direct form while preserving answer
consistency, and (3) keeping the invariance and consistency for any possible
answer. TVS is formulated as a modular front-end adapter task that can be
seamlessly integrated into both Video Instruction Tuning (training) and Video
Question Answering (inference) pipelines. TVS optimizes distribution of
reasoning burden and cognitive load; during training, it aligns queries with
focus-critical visual information; at inference, it enables query-aware segment
focus and streamlined query representations. In particular, we curate the first
benchmark for TVS and propose ReSimplifyIt, a baseline outperforming prior
approaches on seemingly similar tasks by 0.47 in F-1 score on video trimming
while achieving competitive query rewriting performance. Experiments
demonstrate that incorporating TVS yields relative gains of 7.33% (training)
and 34.6% (inference), demonstrating the effectiveness of temporal information
screening for improving video-language understanding.

</details>


### [6] [ROBUST-MIPS: A Combined Skeletal Pose and Instance Segmentation Dataset for Laparoscopic Surgical Instruments](https://arxiv.org/abs/2508.21096)
*Zhe Han,Charlie Budd,Gongyu Zhang,Huanyu Tian,Christos Bergeles,Tom Vercauteren*

Main category: cs.CV

TL;DR: 作者提出骨架位姿作为更高效的手术器械标注方式，发布了ROBU ST-MIPS数据集与基线评测/工具，验证位姿标注可实现高质量的器械定位。


<details>
  <summary>Details</summary>
Motivation: 现有分割标注获取成本高且数据稀缺，作者认为骨架位姿标注信息丰富且标注更高效，有助于加速标注数据扩展和提升定位性能。

Method: 基于已有ROBUST-MIS数据集，作者为工具添加了骨架位姿和实例分割标注，构建ROBU ST-MIPS数据集；使用流行的姿态估计方法建立基准评测，并发布标注软件与基线模型。

Result: 发布了结合工具位姿与实例分割的ROBU ST-MIPS数据集；基准测试显示使用位姿标注可达到高质量工具定位结果；同时开放了标注工具与基线模型供社区使用。

Conclusion: 该文提出以骨架位姿标注替代或补充分割标注，并发布了ROBU ST-MIPS数据集及基线模型，证明位姿标注对工具定位有效。

Abstract: Localisation of surgical tools constitutes a foundational building block for
computer-assisted interventional technologies. Works in this field typically
focus on training deep learning models to perform segmentation tasks.
Performance of learning-based approaches is limited by the availability of
diverse annotated data. We argue that skeletal pose annotations are a more
efficient annotation approach for surgical tools, striking a balance between
richness of semantic information and ease of annotation, thus allowing for
accelerated growth of available annotated data. To encourage adoption of this
annotation style, we present, ROBUST-MIPS, a combined tool pose and tool
instance segmentation dataset derived from the existing ROBUST-MIS dataset. Our
enriched dataset facilitates the joint study of these two annotation styles and
allow head-to-head comparison on various downstream tasks. To demonstrate the
adequacy of pose annotations for surgical tool localisation, we set up a simple
benchmark using popular pose estimation methods and observe high-quality
results. To ease adoption, together with the dataset, we release our benchmark
models and custom tool pose annotation software.

</details>


### [7] [Safe-Control: A Safety Patch for Mitigating Unsafe Content in Text-to-Image Generation Models](https://arxiv.org/abs/2508.21099)
*Xiangtao Meng,Yingkai Dong,Ning Yu,Li Wang,Zheng Li,Shanqing Guo*

Main category: cs.CV

TL;DR: 提出Safe-Control：可插拔的安全补丁注入方法，兼容相似去噪架构的T2I模型，能在不改模型参数下显著减少不安全生成且不损害正常生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有T2I模型安全机制在分布偏移下易被规避或需复杂模型调整的问题，提供一种无需改动模型参数、可插拔的安全修补机制。

Method: 基于数据驱动与安全感知条件，构建安全控制信号作为补丁注入到锁定模型；开发可组合的多种安全补丁并融合为统一补丁；在六个公开T2I模型上评估并与七种基线方法比较。

Result: 提出Safe-Control，一种数据驱动的安全补丁，通过注入安全控制信号到锁定的T2I模型中，在多模型上显著降低不安全内容生成概率（降至7%），同时保持图像质量和文本对齐，优于七种现有防御方法。

Conclusion: Safe-Control为T2I模型提供了一种高效、可组合、适配性强的安全解决方案，能在多模型与对抗攻击下保持稳健性并 outperform 多种现有防御。

Abstract: Despite the advancements in Text-to-Image (T2I) generation models, their
potential for misuse or even abuse raises serious safety concerns. Model
developers have made tremendous efforts to introduce safety mechanisms that can
address these concerns in T2I models. However, the existing safety mechanisms,
whether external or internal, either remain susceptible to evasion under
distribution shifts or require extensive model-specific adjustments. To address
these limitations, we introduce Safe-Control, an innovative plug-and-play
safety patch designed to mitigate unsafe content generation in T2I models.
Using data-driven strategies and safety-aware conditions, Safe-Control injects
safety control signals into the locked T2I model, acting as an update in a
patch-like manner. Model developers can also construct various safety patches
to meet the evolving safety requirements, which can be flexibly merged into a
single, unified patch. Its plug-and-play design further ensures adaptability,
making it compatible with other T2I models of similar denoising architecture.
We conduct extensive evaluations on six diverse and public T2I models.
Empirical results highlight that Safe-Control is effective in reducing unsafe
content generation across six diverse T2I models with similar generative
architectures, yet it successfully maintains the quality and text alignment of
benign images. Compared to seven state-of-the-art safety mechanisms, including
both external and internal defenses, Safe-Control significantly outperforms all
baselines in reducing unsafe content generation. For example, it reduces the
probability of unsafe content generation to 7%, compared to approximately 20%
for most baseline methods, under both unsafe prompts and the latest adversarial
attacks.

</details>


### [8] [GENNAV: Polygon Mask Generation for Generalized Referring Navigable Regions](https://arxiv.org/abs/2508.21102)
*Kei Katsumata,Yui Iioka,Naoki Hosomi,Teruhisa Misu,Kentaro Yamada,Komei Sugiura*

Main category: cs.CV

TL;DR: GENNAV predicts existence and segments multiple stuff-type targets; new GRiN-Drive benchmark; strong zero-shot real-world performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail on stuff-type regions, absent/multiple targets; need existence prediction and segmentation for ambiguous boundaries.

Method: We propose GENNAV for identifying locations of target regions from language and front camera images.

Result: GENNAV outperforms baselines on GRiN-Drive benchmark and shows robust zero-shot transfer in real-world tests across 5 cities with 4 cars.

Conclusion: GENNAV effectively handles stuff-type, no-target, and multi-target cases, improving over baselines and transferring to diverse real environments.

Abstract: We focus on the task of identifying the location of target regions from a
natural language instruction and a front camera image captured by a mobility.
This task is challenging because it requires both existence prediction and
segmentation, particularly for stuff-type target regions with ambiguous
boundaries. Existing methods often underperform in handling stuff-type target
regions, in addition to absent or multiple targets. To overcome these
limitations, we propose GENNAV, which predicts target existence and generates
segmentation masks for multiple stuff-type target regions. To evaluate GENNAV,
we constructed a novel benchmark called GRiN-Drive, which includes three
distinct types of samples: no-target, single-target, and multi-target. GENNAV
achieved superior performance over baseline methods on standard evaluation
metrics. Furthermore, we conducted real-world experiments with four automobiles
operated in five geographically distinct urban areas to validate its zero-shot
transfer performance. In these experiments, GENNAV outperformed baseline
methods and demonstrated its robustness across diverse real-world environments.
The project page is available at https://gennav.vercel.app/.

</details>


### [9] [R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs via Bi-Mode Annealing and Reinforce Learning](https://arxiv.org/abs/2508.21113)
*Jie Jiang,Qi Yang,Bolin Ni,Shiming Xiang,Han Hu,Houwen Peng*

Main category: cs.CV

TL;DR: R-4B是一种能自适应“要不要思考”的多模态大模型，通过双模退火与BPO训练，在保持推理能力的同时提升简单任务效率，并在25项基准上取得SOTA或接近更大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 针对MLLMs在复杂问题上虽然表现优异但在简单问题上冗余思考导致效率低下的问题，设计能够自适应决定是否进行复杂思考的模型以节省计算资源和降低延迟。

Method: 提出双模退火训练策略，使模型兼具思考与非思考两种生成能力；引入双模策略优化(Bi-mode Policy Optimization, BPO)并在改进的GRPO框架下训练策略模型，强制为每个输入在两种模式下生成响应，以提升判定是否思考的准确性。

Result: 在25个基准测试上达到或超过最先进水平，在多数任务上优于Qwen2.5-VL-7B，并在需要强推理的任务上以更低计算成本达到可比于16B模型（如Kimi-VL-A3B-Thinking-2506）的性能。

Conclusion: R-4B通过自适应决定是否进行逐步思考，在确保推理能力的同时显著提升简单问题的响应效率，实现了性能与计算成本的良好平衡。

Abstract: Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking
capabilities have demonstrated remarkable performance on complex reasoning
problems. However, this thinking process is redundant for simple problems
solvable without complex reasoning. To address this inefficiency, we propose
R-4B, an auto-thinking MLLM, which can adaptively decide when to think based on
problem complexity. The central idea of R-4B is to empower the model with both
thinking and non-thinking capabilities using bi-mode annealing, and apply
Bi-mode Policy Optimization~(BPO) to improve the model's accuracy in
determining whether to activate the thinking process. Specifically, we first
train the model on a carefully curated dataset spanning various topics, which
contains samples from both thinking and non-thinking modes. Then it undergoes a
second phase of training under an improved GRPO framework, where the policy
model is forced to generate responses from both modes for each input query.
Experimental results show that R-4B achieves state-of-the-art performance
across 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks
and achieves performance comparable to larger models such as
Kimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower
computational cost.

</details>


### [10] [HiddenObject: Modality-Agnostic Fusion for Multimodal Hidden Object Detection](https://arxiv.org/abs/2508.21135)
*Harris Song,Tuan-Anh Vu,Sanjith Menon,Sriram Narasimhan,M. Khalid Jawed*

Main category: cs.CV

TL;DR: 提出基于Mamba的RGB-热-深度融合框架HiddenObject，针对遮挡/伪装目标检测表现优越，多个基准上表现SOTA或接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统RGB检测在遮挡、伪装与光照变化下性能急剧下降，单一模态不足以覆盖所有场景，因而需要一种鲁棒的多模态融合方法来整合互补信息并提升隐蔽目标检测性能。

Method: 采用模态特定分支提取RGB、热图与深度特征，应用Mamba-based模块进行交互融合以获得统一表示；融合后通过检测头进行目标定位与分类。训练在多模态标注数据上进行，包含数据增强与对齐策略，并在基准上评估。

Result: HiddenObject提出一个多模态融合框架，将RGB、热成像和深度数据通过Mamba融合机制整合，用于检测被遮挡或伪装的目标。方法通过识别模态特定特征并在统一表示中融合，提升在复杂和视觉退化条件下的检测性能。在多个基准数据集上取得了SOTA或具有竞争力的结果，同时揭示了单模态和简单融合策略的局限性。

Conclusion: Mamba融合架构能有效提取并整合多模态互补信息，显著提升在遮挡、伪装和光照变化等困难条件下的目标检测性能，但仍存在对模态缺失、计算开销与泛化性方面的限制。

Abstract: Detecting hidden or partially concealed objects remains a fundamental
challenge in multimodal environments, where factors like occlusion, camouflage,
and lighting variations significantly hinder performance. Traditional RGB-based
detection methods often fail under such adverse conditions, motivating the need
for more robust, modality-agnostic approaches. In this work, we present
HiddenObject, a fusion framework that integrates RGB, thermal, and depth data
using a Mamba-based fusion mechanism. Our method captures complementary signals
across modalities, enabling enhanced detection of obscured or camouflaged
targets. Specifically, the proposed approach identifies modality-specific
features and fuses them in a unified representation that generalizes well
across challenging scenarios. We validate HiddenObject across multiple
benchmark datasets, demonstrating state-of-the-art or competitive performance
compared to existing methods. These results highlight the efficacy of our
fusion design and expose key limitations in current unimodal and na\"ive fusion
strategies. More broadly, our findings suggest that Mamba-based fusion
architectures can significantly advance the field of multimodal object
detection, especially under visually degraded or complex conditions.

</details>


### [11] [RadGS-Reg: Registering Spine CT with Biplanar X-rays via Joint 3D Radiative Gaussians Reconstruction and 3D/3D Registration](https://arxiv.org/abs/2508.21154)
*Ao Shen,Xueming Fu,Junfeng Jiang,Qiang Zeng,Ye Tang,Zhengming Chen,Luming Nong,Feng Wang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 提出RadGS-Reg：结合RadGS三维重建与3D/3D配准，利用反事实注意力和患者特异性预训练，提高了在噪声双平面X光下的椎体级CT/X光配准精度与稳健性，实验显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决CT/X光配准在图像引导导航中对高精度和实时性的严格要求，克服传统“渲染与比较”方法的空间信息丢失和域差问题，以及双平面X光重建对密集视角和噪声敏感的限制。

Method: 核心方法包括：1) 基于学习的三维RadGS重建模块，从双平面X光推断放射性高斯体表示；2) 反事实注意力学习(CAL)，在噪声X光中聚焦椎体区域以提升重建质量；3) 患者特异性预训练策略，从模拟数据逐步适配真实数据并学习个体化椎体形状先验；4) 联合RadGS重建与3D/3D配准以完成椎体级配准任务。

Result: 提出RadGS-Reg框架，通过联合3D放射性高斯(RadGS)重建和3D/3D配准，实现椎体级别的CT/X光配准。引入基于学习的RadGS重建模块、反事实注意力学习(CAL)机制与患者特异性预训练策略，在in-house数据集上优于现有方法并达到SOTA性能。代码公开。

Conclusion: RadGS-Reg通过学习驱动的RadGS重建、CAL机制和患者特异性预训练，有效解决密集视角需求和X光噪声问题，显著提升椎体级CT/X光配准性能，为临床图像引导导航提供可行方案。

Abstract: Computed Tomography (CT)/X-ray registration in image-guided navigation
remains challenging because of its stringent requirements for high accuracy and
real-time performance. Traditional "render and compare" methods, relying on
iterative projection and comparison, suffer from spatial information loss and
domain gap. 3D reconstruction from biplanar X-rays supplements spatial and
shape information for 2D/3D registration, but current methods are limited by
dense-view requirements and struggles with noisy X-rays. To address these
limitations, we introduce RadGS-Reg, a novel framework for vertebral-level
CT/X-ray registration through joint 3D Radiative Gaussians (RadGS)
reconstruction and 3D/3D registration. Specifically, our biplanar X-rays
vertebral RadGS reconstruction module explores learning-based RadGS
reconstruction method with a Counterfactual Attention Learning (CAL) mechanism,
focusing on vertebral regions in noisy X-rays. Additionally, a patient-specific
pre-training strategy progressively adapts the RadGS-Reg from simulated to real
data while simultaneously learning vertebral shape prior knowledge. Experiments
on in-house datasets demonstrate the state-of-the-art performance for both
tasks, surpassing existing methods. The code is available at:
https://github.com/shenao1995/RadGS_Reg.

</details>


### [12] [SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4](https://arxiv.org/abs/2508.21169)
*Kevin Mayer,Alex Vesel,Xinyi Zhao,Martin Fischer*

Main category: cs.CV

TL;DR: SYNBUILD-3D: 6.2M synthetic LoD4 residential buildings with tri-modal data (wireframe graphs, floor plans, roof point clouds) and semantic annotations to foster generative modeling and automated 3D building reconstruction.


<details>
  <summary>Details</summary>
Motivation: Lack of large-scale annotated LoD4 3D building datasets hinders development of automated, semantically rich 3D building modeling; synthetic data can fill this gap to train and evaluate generative models.

Method: They procedurally generate diverse residential building models, render floor plans, derive semantic labels (rooms, doors, windows) to annotate LoD4 wireframes, and synthesize LiDAR-like roof point clouds; dataset and code released on GitHub.

Result: The paper introduces SYNBUILD-3D, a large synthetic dataset of 6.2M residential 3D buildings at LoD4, with three modalities: 3D semantically labeled wireframe graphs, floor plan images, and LiDAR-like roof point clouds. It aims to enable generative AI for creating LoD4 building models with semantic-geometric consistency; data and code are publicly available.

Conclusion: Providing a massive, multi-modal synthetic dataset lowers the data barrier for training generative and reconstruction models for detailed 3D buildings, enabling research into semantics-aware LoD4 model generation and validation.

Abstract: 3D building models are critical for applications in architecture, energy
simulation, and navigation. Yet, generating accurate and semantically rich 3D
buildings automatically remains a major challenge due to the lack of
large-scale annotated datasets in the public domain. Inspired by the success of
synthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,
and multi-modal dataset of over 6.2 million synthetic 3D residential buildings
at Level of Detail (LoD) 4. In the dataset, each building is represented
through three distinct modalities: a semantically enriched 3D wireframe graph
at LoD 4 (Modality I), the corresponding floor plan images (Modality II), and a
LiDAR-like roof point cloud (Modality III). The semantic annotations for each
building wireframe are derived from the corresponding floor plan images and
include information on rooms, doors, and windows. Through its tri-modal nature,
future work can use SYNBUILD-3D to develop novel generative AI algorithms that
automate the creation of 3D building models at LoD 4, subject to predefined
floor plan layouts and roof geometries, while enforcing semantic-geometric
consistency. Dataset and code samples are publicly available at
https://github.com/kdmayer/SYNBUILD-3D.

</details>


### [13] [Radially Distorted Homographies, Revisited](https://arxiv.org/abs/2508.21190)
*Mårten Wadenbäck,Marcus Valtonen Örnhag,Johan Edstedt*

Main category: cs.CV

TL;DR: 论文提出一种统一方法同时处理三种径向畸变下的单应矩阵估计问题（仅一张图像畸变、两张相同畸变、两张独立畸变），并据此构建若干快速、稳定、准确的极小解算器。实验表明在速度上优于现有方法且精度相当，包含鱼眼相机数据的基准测试证明了方法有效性。


<details>
  <summary>Details</summary>
Motivation: 现实图像常含相机镜头造成的径向几何畸变，若不同时估计畸变与单应，会导致单应估计错误。因此需要快速稳定的极小样本解算器以便在RANSAC等鲁棒框架中高效运行。

Method: 通过对带径向畸变的两视图单应模型进行代数处理，统一表述三类不同畸变配置，随后导出相应的极小样本解算器（closed-form或代数求解）并结合数值稳定性优化（如合适的参数化、消元策略、数值正则化）。最后在基准数据上与现有解算器比较速度与精度。

Result: 提出的解算器在三种情形下都比现有方法更快，精度相当；在包含鱼眼图像的基准测试中表现良好。

Conclusion: 作者给出了一个统一框架，能够为三类径向畸变配置生成更快速且稳定的极小解算器；在标准基准上比现有同类方法更快，精度相当，适用于鱼眼等强畸变场景。

Abstract: Homographies are among the most prevalent transformations occurring in
geometric computer vision and projective geometry, and homography estimation is
consequently a crucial step in a wide assortment of computer vision tasks. When
working with real images, which are often afflicted with geometric distortions
caused by the camera lens, it may be necessary to determine both the homography
and the lens distortion-particularly the radial component, called radial
distortion-simultaneously to obtain anything resembling useful estimates. When
considering a homography with radial distortion between two images, there are
three conceptually distinct configurations for the radial distortion; (i)
distortion in only one image, (ii) identical distortion in the two images, and
(iii) independent distortion in the two images. While these cases have been
addressed separately in the past, the present paper provides a novel and
unified approach to solve all three cases. We demonstrate how the proposed
approach can be used to construct new fast, stable, and accurate minimal
solvers for radially distorted homographies. In all three cases, our proposed
solvers are faster than the existing state-of-the-art solvers while maintaining
similar accuracy. The solvers are tested on well-established benchmarks
including images taken with fisheye cameras. The source code for our solvers
will be made available in the event our paper is accepted for publication.

</details>


### [14] [GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability](https://arxiv.org/abs/2508.21197)
*Zhenghao He,Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: 提出GCAV，通过对比学习对齐不同层的概念表示并用注意力融合，生成跨层一致的全局CAV，降低TCAV分数方差并保持概念相关性，提升概念定位与对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统CAV在不同层独立计算会产生不一致，导致跨层比较不可靠，需要一种能整合跨层信息并保持语义一致的概念表示。

Method: 通过对比学习对齐各层的概念嵌入，学习跨层语义一致的表示；然后用基于注意力的融合机制将对齐后的层级表示加权融合成单一全局CAV；最后用TGCAV框架将TCAV应用于GCAV评估概念敏感度和定位。

Result: 实验表明GCAV显著降低TCAV评分方差、提高概念定位精度，并在多种网络上增强了对抗扰动下的鲁棒性，验证了跨层信息整合带来的解释性与稳定性提升。

Conclusion: GCAV能有效缓解不同层间CAV的不一致性，提供更稳定可靠的概念归因，增强概念定位并提高对抗扰动下的鲁棒性，使得对深度模型中人类定义概念的解释更加一致和全面。

Abstract: Concept Activation Vectors (CAVs) provide a powerful approach for
interpreting deep neural networks by quantifying their sensitivity to
human-defined concepts. However, when computed independently at different
layers, CAVs often exhibit inconsistencies, making cross-layer comparisons
unreliable. To address this issue, we propose the Global Concept Activation
Vector (GCAV), a novel framework that unifies CAVs into a single, semantically
consistent representation. Our method leverages contrastive learning to align
concept representations across layers and employs an attention-based fusion
mechanism to construct a globally integrated CAV. By doing so, our method
significantly reduces the variance in TCAV scores while preserving concept
relevance, ensuring more stable and reliable concept attributions. To evaluate
the effectiveness of GCAV, we introduce Testing with Global Concept Activation
Vectors (TGCAV) as a method to apply TCAV to GCAV-based representations. We
conduct extensive experiments on multiple deep neural networks, demonstrating
that our method effectively mitigates concept inconsistency across layers,
enhances concept localization, and improves robustness against adversarial
perturbations. By integrating cross-layer information into a coherent
framework, our method offers a more comprehensive and interpretable
understanding of how deep learning models encode human-defined concepts. Code
and models are available at https://github.com/Zhenghao-He/GCAV.

</details>


### [15] [Generalizable Object Re-Identification via Visual In-Context Prompting](https://arxiv.org/abs/2508.21222)
*Zhizhong Huang,Xiaoming Liu*

Main category: cs.CV

TL;DR: VICP uses LLMs to generate semantic prompts from few-shot examples and dynamic visual prompts with a VFM to enable zero-shot ReID across novel categories, validated on ShopID10K and other benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current ReID models are domain-specific and need labeled data for new categories; self-supervised methods fail to capture identity-sensitive features. Propose framework to generalize to unseen categories without retraining.

Method: Few-shot positive/negative pairs fed to LLMs via task-specific prompting produce semantic identity rules; these rules guide a VFM (e.g., DINO) through dynamic visual prompts to extract identity-discriminative embeddings; evaluation uses ShopID10K and other ReID datasets.

Result: VICP combines LLMs to infer semantic identity rules from few-shot positive/negative examples and guides a vision foundation model via dynamic visual prompts to extract ID-discriminative features; achieves better performance on new benchmarks; introduces ShopID10K dataset.

Conclusion: VICP allows models trained on seen categories to generalize to unseen ones without parameter adaptation by aligning LLM semantic rules with VFM priors via dynamic visual prompts, outperforming baselines on unseen-category ReID.

Abstract: Current object re-identification (ReID) methods train domain-specific models
(e.g., for persons or vehicles), which lack generalization and demand costly
labeled data for new categories. While self-supervised learning reduces
annotation needs by learning instance-wise invariance, it struggles to capture
\textit{identity-sensitive} features critical for ReID. This paper proposes
Visual In-Context Prompting~(VICP), a novel framework where models trained on
seen categories can directly generalize to unseen novel categories using only
\textit{in-context examples} as prompts, without requiring parameter
adaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infer
semantic identity rules from few-shot positive/negative pairs through
task-specific prompting, which then guides a VFM (\eg, DINO) to extract
ID-discriminative features via \textit{dynamic visual prompts}. By aligning
LLM-derived semantic concepts with the VFM's pre-trained prior, VICP enables
generalization to novel categories, eliminating the need for dataset-specific
retraining. To support evaluation, we introduce ShopID10K, a dataset of 10K
object instances from e-commerce platforms, featuring multi-view images and
cross-domain testing. Experiments on ShopID10K and diverse ReID benchmarks
demonstrate that VICP outperforms baselines by a clear margin on unseen
categories. Code is available at https://github.com/Hzzone/VICP.

</details>


### [16] [Lightweight MRI-Based Automated Segmentation of Pancreatic Cancer with Auto3DSeg](https://arxiv.org/abs/2508.21227)
*Keshav Jha,William Sharp,Dominic LaBella*

Main category: cs.CV

TL;DR: 利用SegResNet的Auto3DSeg在两类MRI胰腺肿瘤数据上进行分割实验，数据量小且序列不同导致性能有限（任务1中等，任务2较差），需要更多标准化大规模MRI数据与改进方法以提高稳健性。


<details>
  <summary>Details</summary>
Motivation: 解决胰腺肿瘤自动分割的挑战——高解剖变异性与标注稀缺性——以期提高诊断、治疗计划和疗效评估的一致性和效率。

Method: 在Auto3DSeg框架下使用SegResNet，进行5折交叉验证并在关注的解剖学ROI上应用STAPLE集成。训练数据为两组MRI：任务1（91例T1动脉期增强）与任务2（50例T2 MR‑Linac），使用标准分割指标评估。

Result: 任务1: DSC 0.56, 5mm DSC 0.73, HD95 41.1mm, MASD 26.0mm, RMSE 5164mm。任务2: DSC 0.33, 5mm DSC 0.50, HD95 20.1mm, MASD 7.2mm, RMSE 17203mm。总体显示任务2性能更差且指标波动大。

Conclusion: 尽管在小样本和序列差异下表现有限，研究展示了自动分割在胰腺肿瘤勾画上的可行性，但距离临床可用仍需更大、标准化的数据集与更鲁棒的方法。

Abstract: Accurate delineation of pancreatic tumors is critical for diagnosis,
treatment planning, and outcome assessment, yet automated segmentation remains
challenging due to anatomical variability and limited dataset availability. In
this study, SegResNet models, as part of the Auto3DSeg architecture, were
trained and evaluated on two MRI-based pancreatic tumor segmentation tasks as
part of the 2025 PANTHER Challenge. Algorithm methodology included 5-fold
cross-validation with STAPLE ensembling after focusing on an anatomically
relevant region-of-interest. The Pancreatic Tumor Segmentation on Diagnostic
MRI task 1 training set included 91 T1-weighted arterial contrast-enhanced MRI
with expert annotated pancreas and tumor labels. The Pancreatic Tumor
Segmentation on MR-Linac task 2 training set used 50 T2-weighted MR-Linac cases
with expert annotated pancreas and tumor labels. Algorithm-automated
segmentation performance of pancreatic tumor was assessed using Dice Similarity
Coefficient (DSC), 5 mm DSC, 95th percentile Hausdorff Distance (HD95), Mean
Average Surface Distance (MASD), and Root Mean Square Error (RMSE). For Task 1,
the algorithm achieved a DSC of 0.56, 5 mm DSC of 0.73, HD95 of 41.1 mm, MASD
of 26.0 mm, and RMSE of 5164 mm. For Task 2, performance decreased, with a DSC
of 0.33, 5 mm DSC of 0.50, HD95 of 20.1 mm, MASD of 7.2 mm, and RMSE of 17,203
mm. These findings illustrate the challenges of MRI-based pancreatic tumor
segmentation with small datasets, highlighting variability introduced by
different MRI sequences. Despite modest performance, the results demonstrate
potential for automated delineation and emphasize the need for larger,
standardized MRI datasets to improve model robustness and clinical utility.

</details>


### [17] [Reverse Imaging for Wide-spectrum Generalization of Cardiac MRI Segmentation](https://arxiv.org/abs/2508.21254)
*Yidong Zhao,Peter Kellman,Hui Xue,Tongyun Yang,Yi Zhang,Yuchi Han,Orlando Simonetti,Qian Tao*

Main category: cs.CV

TL;DR: Reverse Imaging infers spin properties (T1,T2,PD) from MR images using a learned diffusion prior and physics-based forward model, enabling synthesis of arbitrary sequence contrasts for robust segmentation generalization


<details>
  <summary>Details</summary>
Motivation: Pretrained segmentation models fail across MRI sequences due to contrast variation, but underlying spin properties (proton density, T1, T2) are shared—use them as latent variables to enable generalization via image synthesis

Method: Physics-driven inverse modeling + diffusion spin prior

Result: Learned diffusion model of spin-property priors from mSASHA dataset; solve regularized nonlinear inverse problem to estimate spin maps from images; synthesize new sequence images for augmentation; achieves highly accurate segmentation across diverse contrasts/protocols

Conclusion: Physics-informed inversion with learned spin priors enables interpretable latent estimation and flexible image synthesis, solving cross-sequence generalization in cardiac MRI segmentation.

Abstract: Pretrained segmentation models for cardiac magnetic resonance imaging (MRI)
struggle to generalize across different imaging sequences due to significant
variations in image contrast. These variations arise from changes in imaging
protocols, yet the same fundamental spin properties, including proton density,
T1, and T2 values, govern all acquired images. With this core principle, we
introduce Reverse Imaging, a novel physics-driven method for cardiac MRI data
augmentation and domain adaptation to fundamentally solve the generalization
problem. Our method reversely infers the underlying spin properties from
observed cardiac MRI images, by solving ill-posed nonlinear inverse problems
regularized by the prior distribution of spin properties. We acquire this "spin
prior" by learning a generative diffusion model from the multiparametric
SAturation-recovery single-SHot acquisition sequence (mSASHA) dataset, which
offers joint cardiac T1 and T2 maps. Our method enables approximate but
meaningful spin-property estimates from MR images, which provide an
interpretable "latent variable" that lead to highly flexible image synthesis of
arbitrary novel sequences. We show that Reverse Imaging enables highly accurate
segmentation across vastly different image contrasts and imaging protocols,
realizing wide-spectrum generalization of cardiac MRI segmentation.

</details>


### [18] [PHD: Personalized 3D Human Body Fitting with Point Diffusion](https://arxiv.org/abs/2508.21257)
*Hsuan-I Ho,Chen Guo,Po-Chen Wu,Ivan Shugurov,Chengcheng Tang,Abhay Mittal,Sizhe An,Manuel Kaufmann,Linguang Zhang*

Main category: cs.CV

TL;DR: PHD通过先进行个性化体型校准，再用体型条件化的Point Diffusion Transformer先验和蒸馏采样损失迭代拟合姿势，减少对2D约束的过度依赖，从而提升骨盆对齐及绝对姿势准确性，且仅需合成数据训练，可作为插件提升现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统HMR为通用性优化，忽略了个体体型对3D姿势恢复的影响；过度依赖2D约束导致3D不一致性，需引入个性化体型信息以提高3D与绝对位姿准确性。

Method: 管线先验校准用户体型，训练体型条件的Point Diffusion Transformer作为3D姿势先验，通过Point Distillation Sampling损失在姿势拟合过程中迭代优化。

Result: PHD提出了一种用于个性化3D人体网格恢复和身体拟合的新方法，通过利用用户特定的体型信息提高视频中姿势估计的准确性。该方法首先进行用户体型校准，然后基于体型进行个性化姿势拟合，避免了传统仅依赖2D约束导致的3D误差。提出了基于体型的3D姿势先验（Point Diffusion Transformer）和Point Distillation Sampling损失，用于引导迭代姿势拟合。该方法在骨盆对齐姿势和绝对姿势准确性上均有提升，只需合成数据训练，且可作为插件与现有3D姿势估计器集成。

Conclusion: 基于体型的3D姿势先验可显著提高视频中3D姿势与绝对位置的恢复精度，并具有数据效率高、易集成等优点。

Abstract: We introduce PHD, a novel approach for personalized 3D human mesh recovery
(HMR) and body fitting that leverages user-specific shape information to
improve pose estimation accuracy from videos. Traditional HMR methods are
designed to be user-agnostic and optimized for generalization. While these
methods often refine poses using constraints derived from the 2D image to
improve alignment, this process compromises 3D accuracy by failing to jointly
account for person-specific body shapes and the plausibility of 3D poses. In
contrast, our pipeline decouples this process by first calibrating the user's
body shape and then employing a personalized pose fitting process conditioned
on that shape. To achieve this, we develop a body shape-conditioned 3D pose
prior, implemented as a Point Diffusion Transformer, which iteratively guides
the pose fitting via a Point Distillation Sampling loss. This learned 3D pose
prior effectively mitigates errors arising from an over-reliance on 2D
constraints. Consequently, our approach improves not only pelvis-aligned pose
accuracy but also absolute pose accuracy -- an important metric often
overlooked by prior work. Furthermore, our method is highly data-efficient,
requiring only synthetic data for training, and serves as a versatile
plug-and-play module that can be seamlessly integrated with existing 3D pose
estimators to enhance their performance. Project page:
https://phd-pose.github.io/

</details>


### [19] [Efficient Diffusion-Based 3D Human Pose Estimation with Hierarchical Temporal Pruning](https://arxiv.org/abs/2508.21363)
*Yuquan Bi,Hongsong Wang,Xinli Shi,Zhipeng Gui,Jie Gui,Yuan Yan Tang*

Main category: cs.CV

TL;DR: 通过自适应时间图选择关键帧并在空间语义上聚类剪枝，HTP在Human3.6M和MPI-INF-3DHP数据集上显著减少训练/推理计算量并加速推理，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 动机是扩散模型虽然能生成高保真3D姿态，但其迭代和多假设特性带来高计算代价，需通过智能剪枝保持关键动态信息的同时降低计算量。

Method: 方法包括三步：1）时间相关增强剪枝（TCEP）通过自适应时间图分析帧间运动相关性选出重要帧；2）稀疏聚焦的时间多头自注意力（SFT MHSA）利用帧级稀疏性降低注意力计算；3）基于掩码的姿态Token剪枝（MGPTP）通过聚类在语义级别进行细粒度剪枝。

Result: 提出了一种高效的基于扩散模型的3D人体姿态估计框架，包含层次化时间剪枝（HTP）策略，通过帧级和语义级的动态剪枝来降低计算量并保持运动动态信息。

Conclusion: HTP在保证或提升精度的前提下，大幅降低了扩散模型在3D人体姿态估计任务中的计算开销和推理时间，适合资源受限场景。

Abstract: Diffusion models have demonstrated strong capabilities in generating
high-fidelity 3D human poses, yet their iterative nature and multi-hypothesis
requirements incur substantial computational cost. In this paper, we propose an
Efficient Diffusion-Based 3D Human Pose Estimation framework with a
Hierarchical Temporal Pruning (HTP) strategy, which dynamically prunes
redundant pose tokens across both frame and semantic levels while preserving
critical motion dynamics. HTP operates in a staged, top-down manner: (1)
Temporal Correlation-Enhanced Pruning (TCEP) identifies essential frames by
analyzing inter-frame motion correlations through adaptive temporal graph
construction; (2) Sparse-Focused Temporal MHSA (SFT MHSA) leverages the
resulting frame-level sparsity to reduce attention computation, focusing on
motion-relevant tokens; and (3) Mask-Guided Pose Token Pruner (MGPTP) performs
fine-grained semantic pruning via clustering, retaining only the most
informative pose tokens. Experiments on Human3.6M and MPI-INF-3DHP show that
HTP reduces training MACs by 38.5\%, inference MACs by 56.8\%, and improves
inference speed by an average of 81.1\% compared to prior diffusion-based
methods, while achieving state-of-the-art performance.

</details>


### [20] [Print2Volume: Generating Synthetic OCT-based 3D Fingerprint Volume from 2D Fingerprint Image](https://arxiv.org/abs/2508.21371)
*Qingran Miao,Haixia Wang,Haohao Sun,Yilong Zhang*

Main category: cs.CV

TL;DR: Print2Volume通过2D风格迁移、3D结构扩展和3D GAN真实感渲染，从2D指纹生成高质量OCT三维指纹，构建42万合成样本，并将EER显著降至2.50%。


<details>
  <summary>Details</summary>
Motivation: OCT指纹数据采集成本高且耗时，导致大规模公开数据集稀缺，阻碍基于深度学习的指纹识别算法发展，故需要合成大规模高质量的OCT三维指纹数据。

Method: 三阶段方法：1) 2D风格迁移将二值指纹转为类似OCT Z向均投影的灰度图；2) 3D结构扩展网络将2D图像扩展为合理的3D解剖体积；3) 基于3D GAN的OCT真实感生成器为体积添加纹理、斑点噪声等真实成像特性。

Result: 构建了42万样本的合成数据集；通过在合成数据上预训练并在小规模真实数据上微调，ZJUT-EIFD基准上的EER从15.62%降至2.50%，表明合成数据显著提升识别性能。

Conclusion: 本文提出了Print2Volume框架，能够从2D指纹图像合成逼真的OCT三维指纹体积，缓解真实OCT数据稀缺问题。

Abstract: Optical Coherence Tomography (OCT) enables the acquisition of
high-resolution, three-dimensional fingerprint data, capturing rich subsurface
structures for robust biometric recognition. However, the high cost and
time-consuming nature of OCT data acquisition have led to a scarcity of
large-scale public datasets, significantly hindering the development of
advanced algorithms, particularly data-hungry deep learning models. To address
this critical bottleneck, this paper introduces Print2Volume, a novel framework
for generating realistic, synthetic OCT-based 3D fingerprints from 2D
fingerprint image. Our framework operates in three sequential stages: (1) a 2D
style transfer module that converts a binary fingerprint into a grayscale
images mimicking the style of a Z-direction mean-projected OCT scan; (2) a 3D
Structure Expansion Network that extrapolates the 2D im-age into a plausible 3D
anatomical volume; and (3) an OCT Realism Refiner, based on a 3D GAN, that
renders the structural volume with authentic textures, speckle noise, and other
imaging characteristics. Using Print2Volume, we generated a large-scale
synthetic dataset of 420,000 samples. Quantitative experiments demonstrate the
high quality of our synthetic data and its significant impact on recognition
performance. By pre-training a recognition model on our synthetic data and
fine-tuning it on a small real-world dataset, we achieved a remarkable
reduction in the Equal Error Rate (EER) from 15.62% to 2.50% on the ZJUT-EIFD
benchmark, proving the effectiveness of our approach in overcoming data
scarcity.

</details>


### [21] [GLENDA: Gynecologic Laparoscopy Endometriosis Dataset](https://arxiv.org/abs/2508.21398)
*Andreas Leibetseder,Sabrina Kletz,Klaus Schoeffmann,Simon Keckstein,Jörg Keckstein*

Main category: cs.CV

TL;DR: 发布了GLENDA数据集——首个面向子宫内膜异位症的宫腔镜/腹腔镜图像区域标注数据集，旨在促进基于视觉的自动化分析


<details>
  <summary>Details</summary>
Motivation: Manual review of laparoscopic videos is time-consuming; lack of annotated medical data limits ML development

Method: Dataset publication and annotation for ML in gynecologic laparoscopy

Result: Released GLENDA, the first region-annotated laparoscopic image dataset for endometriosis, created with medical experts

Conclusion: GLENDA填补了术中图像数据稀缺的空白，将推动计算机视觉和机器学习在妇科腹腔镜中对子宫内膜异位症检测和研究的发展。

Abstract: Gynecologic laparoscopy as a type of minimally invasive surgery (MIS) is
performed via a live feed of a patient's abdomen surveying the insertion and
handling of various instruments for conducting treatment. Adopting this kind of
surgical intervention not only facilitates a great variety of treatments, the
possibility of recording said video streams is as well essential for numerous
post-surgical activities, such as treatment planning, case documentation and
education. Nonetheless, the process of manually analyzing surgical recordings,
as it is carried out in current practice, usually proves tediously
time-consuming. In order to improve upon this situation, more sophisticated
computer vision as well as machine learning approaches are actively developed.
Since most of such approaches heavily rely on sample data, which especially in
the medical field is only sparsely available, with this work we publish the
Gynecologic Laparoscopy ENdometriosis DAtaset (GLENDA) - an image dataset
containing region-based annotations of a common medical condition named
endometriosis, i.e. the dislocation of uterine-like tissue. The dataset is the
first of its kind and it has been created in collaboration with leading medical
experts in the field.

</details>


### [22] [Identifying Surgical Instruments in Laparoscopy Using Deep Learning Instance Segmentation](https://arxiv.org/abs/2508.21399)
*Sabrina Kletz,Klaus Schoeffmann,Jenny Benois-Pineau,Heinrich Husslein*

Main category: cs.CV

TL;DR: Using a region-based fully convolutional network, the authors achieve reliable instrument localization and binary segmentation in laparoscopic gynecology videos with limited data, but multi-class recognition of instrument types remains difficult


<details>
  <summary>Details</summary>
Motivation: automatic content indexing of surgical videos for archive search; need to segment and recognize instruments despite special video content

Method: region-based fully convolutional network for instance-aware segmentation and recognition

Result: high accuracy for instrument localization and binary segmentation with moderate training data; poor performance for instrument-type recognition due to high inter-instrument similarity

Conclusion: Instance-aware segmentation works well even with limited training data, but distinguishing instrument types is challenging because instruments look very similar

Abstract: Recorded videos from surgeries have become an increasingly important
information source for the field of medical endoscopy, since the recorded
footage shows every single detail of the surgery. However, while video
recording is straightforward these days, automatic content indexing - the basis
for content-based search in a medical video archive - is still a great
challenge due to the very special video content. In this work, we investigate
segmentation and recognition of surgical instruments in videos recorded from
laparoscopic gynecology. More precisely, we evaluate the achievable performance
of segmenting surgical instruments from their background by using a
region-based fully convolutional network for instance-aware (1) instrument
segmentation as well as (2) instrument recognition. While the first part
addresses only binary segmentation of instances (i.e., distinguishing between
instrument or background) we also investigate multi-class instrument
recognition (i.e., identifying the type of instrument). Our evaluation results
show that even with a moderately low number of training examples, we are able
to localize and segment instrument regions with a pretty high accuracy.
However, the results also reveal that determining the particular instrument is
still very challenging, due to the inherently high similarity of surgical
instruments.

</details>


### [23] [SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing](https://arxiv.org/abs/2508.21402)
*Jakub Straka,Ivan Gruber*

Main category: cs.CV

TL;DR: Paper adapts DINO for satellite images (SatDINO), adds GSD encoding and adaptive view sampling, shows superior performance over MAE-based baselines across datasets; provides ablations and releases code.


<details>
  <summary>Details</summary>
Motivation: Investigate DINO (contrastive self-supervised method) for pretraining on remote sensing imagery to leverage large unlabeled satellite image datasets.

Method: Use contrastive self-supervised learning (DINO) adapted to satellite data with modifications: GSD encoding, adaptive view sampling; extensive experiments and ablation studies across datasets.

Result: Introduce SatDINO tailored for satellite imagery; satdino outperforms MAE-based methods and is competitive on multiple benchmarks; includes ablation study and novel enhancements (GSD encoding, adaptive view sampling); code/models released.

Conclusion: SatDINO is an effective self-supervised pretraining approach for remote sensing, improving representation quality over MAE baselines; proposed enhancements further boost performance and are modular.

Abstract: Self-supervised learning has emerged as a powerful tool for remote sensing,
where large amounts of unlabeled data are available. In this work, we
investigate the use of DINO, a contrastive self-supervised method, for
pretraining on remote sensing imagery. We introduce SatDINO, a model tailored
for representation learning in satellite imagery. Through extensive experiments
on multiple datasets in multiple testing setups, we demonstrate that SatDINO
outperforms other state-of-the-art methods based on much more common masked
autoencoders (MAE) and achieves competitive results in multiple benchmarks.
  We also provide a rigorous ablation study evaluating SatDINO's individual
components. Finally, we propose a few novel enhancements, such as a new way to
incorporate ground sample distance (GSD) encoding and adaptive view sampling.
These enhancements can be used independently on our SatDINO model. Our code and
trained models are available at: https://github.com/strakaj/SatDINO.

</details>


### [24] [Standardized Multi-Layer Tissue Maps for Enhanced Artificial Intelligence Integration and Search in Large-Scale Whole Slide Image Archives](https://arxiv.org/abs/2508.21418)
*Gernot Fiala,Markus Plass,Robert Harb,Peter Regitnig,Kristijan Skok,Wael Al Zoughbi,Carmen Zerner,Paul Torke,Michaela Kargl,Heimo Müller,Tomas Brazdil,Matej Gallo,Jaroslav Kubín,Roman Stoklasa,Rudolf Nenutil,Norman Zerbe,Andreas Holzinger,Petr Holub*

Main category: cs.CV

TL;DR: 为WSI提出三层次（来源/组织类型/病理改变）的标准化2D组织索引图与领域配置方法，解决大规模WSI内容描述与检索的标准化问题，便于AI数据集构建与下游分析。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏WSI内容元数据标准，导致需人工检查以组建AI训练/验证队列，不适用于数百万级对象的大规模集合。

Method: 通过为每张WSI生成细粒度的组织图（分为来源、组织类型、病理改变三层），采用统一语法/语义实现目录间互操作，并展示在WSI目录、机器学习和图结构表示中的应用示例。

Result: 为WSI集合增强了可检索的组织地图，支持跨目录互操作并在ML与图基表示中展示了实用性，示例证明能提升自动化筛选与分析效率。

Conclusion: 提出了一个用于WSI的通用2D索引地图生成框架与领域配置机制，利于大规模图像集的内容检索与筛选。

Abstract: A Whole Slide Image (WSI) is a high-resolution digital image created by
scanning an entire glass slide containing a biological specimen, such as tissue
sections or cell samples, at multiple magnifications. These images can be
viewed, analyzed, shared digitally, and are used today for Artificial
Intelligence (AI) algorithm development. WSIs are used in a variety of fields,
including pathology for diagnosing diseases and oncology for cancer research.
They are also utilized in neurology, veterinary medicine, hematology,
microbiology, dermatology, pharmacology, toxicology, immunology, and forensic
science.
  When assembling cohorts for the training or validation of an AI algorithm, it
is essential to know what is present on such a WSI. However, there is currently
no standard for this metadata, so such selection has mainly been done through
manual inspection, which is not suitable for large collections with several
million objects.
  We propose a general framework to generate a 2D index map for WSI and a
profiling mechanism for specific application domains. We demonstrate this
approach in the field of clinical pathology, using common syntax and semantics
to achieve interoperability between different catalogs.
  Our approach augments each WSI collection with a detailed tissue map that
provides fine-grained information about the WSI content. The tissue map is
organized into three layers: source, tissue type, and pathological alterations,
with each layer assigning segments of the WSI to specific classes.
  We illustrate the advantages and applicability of the proposed standard
through specific examples in WSI catalogs, Machine Learning (ML), and
graph-based WSI representations.

</details>


### [25] [Unsupervised Incremental Learning Using Confidence-Based Pseudo-Labels](https://arxiv.org/abs/2508.21424)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 提出一种用置信度伪标签替代人工标签的无监督类增量学习方法ICPL，能在无标签增量数据上有效运行，性能接近监督方法并优于现有class-iNCD方法。


<details>
  <summary>Details</summary>
Motivation: 解决增量学习场景中，新类别到来但增量数据无标签的问题，减少对人工标注的依赖，使类增量学习在现实世界更实用。

Method: 使用模型预测生成伪标签，依据置信度筛选可靠样本，将伪标签与现有CIL框架整合，评估不同阈值与策略对性能的影响，并在多数据集与细粒度场景验证。

Result: 提出ICPL（基于置信度的伪标签的无监督增量学习），将置信度筛选的伪标签整合到多种CIL方法中，在CIFAR100、ImageNet100及细粒度数据集上表现优良，比现有class-iNCD方法提高约5%最终精度，并在计算复杂度上适合资源受限环境。

Conclusion: ICPL为无标签环境下的类增量学习提供了可行方案，兼顾性能与计算效率，拓展了CIL在真实场景中的应用。

Abstract: Deep learning models have achieved state-of-the-art performance in many
computer vision tasks. However, in real-world scenarios, novel classes that
were unseen during training often emerge, requiring models to acquire new
knowledge incrementally. Class-Incremental Learning (CIL) methods enable a
model to learn novel classes while retaining knowledge of previous classes.
However, these methods make the strong assumption that the incremental dataset
is fully labeled, which is unrealistic in practice. In this work, we propose an
unsupervised Incremental Learning method using Confidence-based Pseudo-labels
(ICPL), which replaces human annotations with pseudo-labels, enabling
incremental learning from unlabeled datasets. We integrate these pseudo-labels
into various CIL methods with confidence-based selection and evaluate
performance degradation on CIFAR100 and ImageNet100. Then, we compare our
approach to popular Class Incremental Novel Category Discovery (class-iNCD)
methods addressing similar challenges. Additionally, we apply our method to
fine-grained datasets to demonstrate its real-world practicality and measure
its computational complexity to validate its suitability for
resource-constrained environments. ICPL achieves competitive results compared
to supervised methods and outperforms state-of-the-art class-iNCD methods by
more than 5% in final accuracy.

</details>


### [26] [MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation](https://arxiv.org/abs/2508.21435)
*Francisco Caetano,Christiaan Viviers,Peter H. H. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: MedShift: a Flow Matching + Schrodinger Bridges based class-conditional generative model for multi-domain unpaired translation between synthetic and real skull X-rays; introduces X-DigiSkull dataset; good performance and inference flexibility


<details>
  <summary>Details</summary>
Motivation: Bridge domain gaps between synthetic and real skull X-rays (attenuation, noise, soft tissue) enabling better generalization of models trained on synthetic data

Method: Flow Matching and Schrodinger Bridges for unpaired translation

Result: MedShift enables high-fidelity, class-conditional unpaired translation across multiple domains via shared latent space; flexible at inference to trade perceptual vs structural fidelity; strong performance despite smaller size than diffusion models

Conclusion: MedShift provides a scalable, generalizable solution for cross-domain adaptation in medical X-ray imaging, outperforming larger diffusion models while allowing tunable inference for perceptual vs structural priorities.

Abstract: Synthetic medical data offers a scalable solution for training robust models,
but significant domain gaps limit its generalizability to real-world clinical
settings. This paper addresses the challenge of cross-domain translation
between synthetic and real X-ray images of the head, focusing on bridging
discrepancies in attenuation behavior, noise characteristics, and soft tissue
representation. We propose MedShift, a unified class-conditional generative
model based on Flow Matching and Schrodinger Bridges, which enables
high-fidelity, unpaired image translation across multiple domains. Unlike prior
approaches that require domain-specific training or rely on paired data,
MedShift learns a shared domain-agnostic latent space and supports seamless
translation between any pair of domains seen during training. We introduce
X-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays
under varying radiation doses, to benchmark domain translation models.
Experimental results demonstrate that, despite its smaller model size compared
to diffusion-based approaches, MedShift offers strong performance and remains
flexible at inference time, as it can be tuned to prioritize either perceptual
fidelity or structural consistency, making it a scalable and generalizable
solution for domain adaptation in medical imaging. The code and dataset are
available at https://caetas.github.io/medshift.html

</details>


### [27] [Trees as Gaussians: Large-Scale Individual Tree Mapping](https://arxiv.org/abs/2508.21437)
*Dimitri Gominski,Martin Brandt,Xiaoye Tong,Siyu Liu,Maurice Mugabowindekwe,Sizhuo Li,Florian Reiner,Andrew Davies,Rasmus Fensholt*

Main category: cs.CV

TL;DR: 基于PlanetScope影像与大规模机载激光雷达点训练的深度学习框架，通过高斯核模拟树冠中心，实现全球尺度3米分辨率单株大树检测，性能优于现有树覆盖产品，适配未来更好影像。


<details>
  <summary>Details</summary>
Motivation: 现有全球产品多为二值树覆盖或冠层高度，未明确识别单株树，限制了大尺度单株树监测与应用；需要高分辨率、可扩展的方法用于全球范围内检测单株大树。

Method: 使用深度学习模型在PlanetScope 3米影像上训练，目标标签由可伸缩高斯核模拟的树冠中心生成；训练数据来自自动从机载激光雷达提取的数十亿点；输出包括树冠中心点和二值树覆盖图；通过与机载激光雷达和现有产品比较评估，并使用人工标注微调。

Result: 提出一种基于深度学习的全球尺度3米分辨率PlanetScope影像大树检测方法，通过可伸缩的高斯核模拟树冠以提取树冠中心并生成二值树覆盖图；使用从机载激光雷达自动提取的数十亿点进行训练，使模型能在林内与林外识别单个大树；与现有树覆盖产品和机载激光雷达对比，在分数覆盖预测上达到R^2=0.81并在不同生物群系中表现均衡；微调人工标注可进一步提升检测。

Conclusion: 方法证明可在全球尺度高分辨率影像中稳定检测单株大树，产出树冠中心与二值覆盖图，且可通过人工标注微调提升性能，为高分辨率树监测提供可扩展框架。

Abstract: Trees are key components of the terrestrial biosphere, playing vital roles in
ecosystem function, climate regulation, and the bioeconomy. However,
large-scale monitoring of individual trees remains limited by inadequate
modelling. Available global products have focused on binary tree cover or
canopy height, which do not explicitely identify trees at individual level. In
this study, we present a deep learning approach for detecting large individual
trees in 3-m resolution PlanetScope imagery at a global scale. We simulate tree
crowns with Gaussian kernels of scalable size, allowing the extraction of crown
centers and the generation of binary tree cover maps. Training is based on
billions of points automatically extracted from airborne lidar data, enabling
the model to successfully identify trees both inside and outside forests. We
compare against existing tree cover maps and airborne lidar with
state-of-the-art performance (fractional cover R$^2 = 0.81$ against aerial
lidar), report balanced detection metrics across biomes, and demonstrate how
detection can be further improved through fine-tuning with manual labels. Our
method offers a scalable framework for global, high-resolution tree monitoring,
and is adaptable to future satellite missions offering improved imagery.

</details>


### [28] [Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content](https://arxiv.org/abs/2508.21444)
*Jiayu Yang,Weijian Su,Songqian Zhang,Yuqi Han,Jinli Suo,Qiang Zhang*

Main category: cs.CV

TL;DR: 提出一种层级锚点式3D Gaussian Splatting方法，辅以混合变形/生成与双向自适应掩码，显著提升动态图像的训练效率并提高或保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 将高保真实时渲染的3D Gaussian Splatting扩展到动态场景面临两个主要挑战：大量密集高斯导致数据量庞大，以及为每一帧训练耗时长，影响流式/实时应用需求。论文旨在解决训练效率和可扩展性问题。

Method: 方法包括：(1) 将高斯球以尺度层级组织在锚点结构中，粗层描述低分辨率结构，细层按需被激活以提供细节；(2) 提出混合变形与生成（deformation + spawning）策略：通过高斯变形建模帧间小范围运动，遇到大范围运动则触发高斯生成来表示；(3) 引入双向自适应掩码机制，去除静态区域并优先训练信息量大的视点，从而提升训练效率。

Result: 作者在大量实验中表明，该方法在保持或提升视觉质量的同时，相较于最先进方法显著减少训练时间。具体包括更高保真渲染与更快的训练收敛（原文宣称在多种数据集上均有优势）。

Conclusion: 该工作提出了一种针对动态图像的可扩展3D Gaussian Splatting框架（文中称为\M），旨在通过层级化、高效的高斯组织和混合变形/生成策略，加速训练并降低计算开销，从而在保持或提升视觉质量的同时大幅缩短训练时间。

Abstract: 3D Gaussian Splatting (3DGS) enables high-fidelity real-time rendering, a key
requirement for immersive applications. However, the extension of 3DGS to
dynamic scenes remains limitations on the substantial data volume of dense
Gaussians and the prolonged training time required for each frame. This paper
presents \M, a scalable Gaussian Splatting framework designed for efficient
training in streaming tasks. Specifically, Gaussian spheres are hierarchically
organized by scale within an anchor-based structure. Coarser-level Gaussians
represent the low-resolution structure of the scene, while finer-level
Gaussians, responsible for detailed high-fidelity rendering, are selectively
activated by the coarser-level Gaussians. To further reduce computational
overhead, we introduce a hybrid deformation and spawning strategy that models
motion of inter-frame through Gaussian deformation and triggers Gaussian
spawning to characterize wide-range motion. Additionally, a bidirectional
adaptive masking mechanism enhances training efficiency by removing static
regions and prioritizing informative viewpoints. Extensive experiments
demonstrate that \M~ achieves superior visual quality while significantly
reducing training time compared to state-of-the-art methods.

</details>


### [29] [One More Glance with Sharp Eyes: Rethinking Lightweight Captioning as a Practical Visual Specialist](https://arxiv.org/abs/2508.21451)
*Junha Song,Yongsik Jo,So Yeon Min,Quanting Xie,Taehwan Kim,Yonatan Bisk,Jaegul Choo*

Main category: cs.CV

TL;DR: A 125M-language-model-based visual specialist can match large MLLMs for captioning; Sharp-Eyed Refinement+DeepLens improves visual grounding and reduces errors


<details>
  <summary>Details</summary>
Motivation: Deploy image captioning on local devices is hard due to heavy MLLMs; explore lightweight specialist for on-device use

Method: Paper analysis

Result: 125M-parameter specialist achieves comparable performance to large generalists; proposed Sharp-Eyed Refinement with DeepLens improves grounding and reduces visual blindness

Conclusion: Lightweight specialists with refined visual representation can be effective for on-device captioning, mitigating visual blindness via region-focused refinement.

Abstract: Image captioning is fundamental for applications like video instruction
systems and exploration robots, yet deploying such models on local devices is
challenging due to the high computational demands of multimodal large language
models (MLLMs). To address this, we first explore lightweight captioning by
implementing a specialist based on a 125M-parameter language model, 56 times
smaller than LLaMA-7B, and evaluating its performance on both single-sentence
and detailed captioning tasks. Surprisingly, we find that our model can achieve
performance comparable to large multimodal generalists, suggesting its
potential to serve as a strong visual specialist for on-device applications.
While promising, our model also exhibits a limitation: like other MLLMs, it
suffers from visual blindness, occasionally resulting in semantic captioning
errors. We carry out toy experiments and investigate the underlying causes,
where we observe that the problems arise from ineffective attention mechanisms
and limited visual representations. To alleviate them, we develop a novel
captioning framework, Sharp-Eyed Refinement, which enhances caption quality
through improved visual grounding. At its core, our DeepLens extracts detailed
visual representations by concentrating on informative regions identified
during the initial glance. Our experiments confirm both the advantages of our
specialist over prior small captioning models and large generalists and the
effectiveness of our framework.

</details>


### [30] [Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification](https://arxiv.org/abs/2508.21458)
*Kaouther Mouheb,Marawan Elbatel,Janne Papma,Geert Jan Biessels,Jurgen Claassen,Huub Middelkoop,Barbara van Munster,Wiesje van der Flier,Inez Ramakers,Stefan Klein,Esther E. Bron*

Main category: cs.CV

TL;DR: 对联邦学习下的基础模型微调进行系统基准测试，发现分类头、微调策略与聚合方法是关键设计因素；在实践中可优先采用合适的分类头、冻结编码器以节省资源，并选用先进聚合以提升性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像任务中表现优异，但在隐私敏感的临床场景中，数据分散使得传统集中式微调不可行；将基础模型与联邦学习结合能兼顾性能与隐私，但对如何在联邦设置中设计微调与聚合策略缺乏系统性基准研究。

Method: 在多队列大规模脑MRI数据集上，以基础模型为骨干，比较不同分类头（可能包括线性、MLP、池化或任务特定结构）、微调策略（冻结编码器vs全量微调）、以及聚合方法（FedAvg与更先进的方法，如带权重或个性化聚合）在联邦学习设置下的性能与效率指标。采用跨站点联邦训练模拟真实医疗数据分布差异，使用标准分类指标（如AUC、准确率）和计算通信/计算开销评估方法优劣。

Result: 实验表明：1) 分类头架构对诊断性能影响最大，特定结构（如更复杂的任务适配层）能显著提升结果；2) 在带隐私与通信约束的联邦环境下，冻结基础模型编码器并仅微调分类头可在多数情形下达到与全量微调可比的性能，且通信与计算成本更低；3) 使用先进聚合方法（考虑客户端不均衡、数据异质性或自适应权重）优于标准FedAvg，能进一步提升泛化性能与鲁棒性。

Conclusion: 本研究通过对比不同分类头结构、微调策略与聚合方法，系统评估了在联邦学习框架下微调基础模型用于脑MRI痴呆诊断的设计选择，得出分类头结构对性能影响显著、冻结编码器与全量微调表现相近、以及先进聚合方法优于标准联邦平均的结论。

Abstract: While foundation models (FMs) offer strong potential for AI-based dementia
diagnosis, their integration into federated learning (FL) systems remains
underexplored. In this benchmarking study, we systematically evaluate the
impact of key design choices: classification head architecture, fine-tuning
strategy, and aggregation method, on the performance and efficiency of
federated FM tuning using brain MRI data. Using a large multi-cohort dataset,
we find that the architecture of the classification head substantially
influences performance, freezing the FM encoder achieves comparable results to
full fine-tuning, and advanced aggregation methods outperform standard
federated averaging. Our results offer practical insights for deploying FMs in
decentralized clinical settings and highlight trade-offs that should guide
future method development.

</details>


### [31] [Multi-Method Ensemble for Out-of-Distribution Detection](https://arxiv.org/abs/2508.21463)
*Lucas Rakotoarivony*

Main category: cs.CV

TL;DR: 本文提出将多种现有OOD检测方法融合为一个统一评分函数（MME），结合特征截断与多种打分函数以增强对不同类型OOD样本的鲁棒性。在大/小规模基准（近域与远域OOD）上均显著超越最新基线；在ImageNet-1K上用BiT模型将平均FPR95降至27.57%，较最佳基线提升约6%。


<details>
  <summary>Details</summary>
Motivation: 现有方法多聚焦单一技术或某类OOD数据，缺少将多种成熟方法组合以提高对不同OOD类型通用性的研究。作者观察到不同方法互补性，旨在通过融合提升鲁棒性与整体性能。

Method: 理论分析证明特征截断和打分函数可组合，基于此提出Multi-Method Ensemble (MME)评分：先对特征进行截断（保留重要维度/去除噪声），然后计算并聚合多种已有打分函数的输出（例如最大类概率、能量、Mahalanobis等），形成统一得分用于判定OOD。

Result: 在多个大/小规模数据集（覆盖近OOD与远OOD）上广泛实验，MME普遍优于最新方法。以BiT模型在ImageNet-1K上为例，平均FPR95为27.57%，比最优基线提升约6%。

Conclusion: 将多种特征截断与打分函数有效组合并聚合多个评分可以显著提升OOD检测性能；MME作为统一评分函数在多种基准上优于单一方法，尤其在复杂的ImageNet-1K基准上有明显改进。

Abstract: Detecting out-of-distribution (OOD) samples is essential for neural networks
operating in open-world settings, particularly in safety-critical applications.
Existing methods have improved OOD detection by leveraging two main techniques:
feature truncation, which increases the separation between in-distribution (ID)
and OOD samples, and scoring functions, which assign scores to distinguish
between ID and OOD data. However, most approaches either focus on a single
family of techniques or evaluate their effectiveness on a specific type of OOD
dataset, overlooking the potential of combining multiple existing solutions.
Motivated by this observation, we theoretically and empirically demonstrate
that state-of-the-art feature truncation and scoring functions can be
effectively combined. Moreover, we show that aggregating multiple scoring
functions enhances robustness against various types of OOD samples. Based on
these insights, we propose the Multi-Method Ensemble (MME) score, which unifies
state-of-the-art OOD detectors into a single, more effective scoring function.
Extensive experiments on both large-scale and small-scale benchmarks, covering
near-OOD and far-OOD scenarios, show that MME significantly outperforms recent
state-of-the-art methods across all benchmarks. Notably, using the BiT model,
our method achieves an average FPR95 of 27.57% on the challenging ImageNet-1K
benchmark, improving performance by 6% over the best existing baseline.

</details>


### [32] [Adversarial Patch Attack for Ship Detection via Localized Augmentation](https://arxiv.org/abs/2508.21472)
*Chun Liu,Panpan Ding,Zheng Zheng,Hailong Wang,Bingqian Zhu,Tao Xu,Zhigang Han,Jiayao Wang*

Main category: cs.CV

TL;DR: 仅对目标区域进行增强，减少背景干扰，从而提升对抗补丁攻击的成功率与可迁移性。


<details>
  <summary>Details</summary>
Motivation: 常规基于变换的数据增强虽能提升对抗样本的可迁移性，但过度增强背景或非目标区域会引入误报；因此提出仅对目标区域进行增强以减少干扰并提高攻击效果。

Method: 在图像中定位目标区域，仅对该区域应用数据增强（如旋转、缩放、颜色变换等），并在训练对抗补丁时使用该局部增强以优化损失函数。

Result: proposed localized augmentation focusing on target regions to improve adversarial patch attacks in ship detection; experiments on HRSC2016 show increased success and transferability.

Conclusion: 局部增强能有效减少背景引入的干扰，使得损失更专注于补丁影响，显著提升攻击成功率和迁移能力。

Abstract: Current ship detection techniques based on remote sensing imagery primarily
rely on the object detection capabilities of deep neural networks (DNNs).
However, DNNs are vulnerable to adversarial patch attacks, which can lead to
misclassification by the detection model or complete evasion of the targets.
Numerous studies have demonstrated that data transformation-based methods can
improve the transferability of adversarial examples. However, excessive
augmentation of image backgrounds or irrelevant regions may introduce
unnecessary interference, resulting in false detections of the object detection
model. These errors are not caused by the adversarial patches themselves but
rather by the over-augmentation of background and non-target areas. This paper
proposes a localized augmentation method that applies augmentation only to the
target regions, avoiding any influence on non-target areas. By reducing
background interference, this approach enables the loss function to focus more
directly on the impact of the adversarial patch on the detection model, thereby
improving the attack success rate. Experiments conducted on the HRSC2016
dataset demonstrate that the proposed method effectively increases the success
rate of adversarial patch attacks and enhances their transferability.

</details>


### [33] [ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding](https://arxiv.org/abs/2508.21496)
*Hao Lu,Jiahao Wang,Yaolun Zhang,Ruohui Wang,Xuanyu Zheng,Yepeng Tang,Dahua Lin,Lewei Lu*

Main category: cs.CV

TL;DR: 提出ELV-Halluc长视频幻觉基准并定义“语义聚合幻觉”(SAH)，发现SAH随语义复杂度和快速变化增加，通过位置编码和DPO训练及8K对抗数据集减轻SAH，显著降低27.7% SAH比率。


<details>
  <summary>Details</summary>
Motivation: 现有视频幻觉研究主要针对短视频，未区分帧级语义正确但事件级聚合错误的幻觉类型。长视频多事件语义更复杂，易产生语义聚合幻觉，需单独构建基准并研究其成因与缓解手段。

Method: 构建长视频幻觉基准ELV-Halluc，设计SAH辨别指标与测试集；分析模型在不同语义复杂性与变化速率下的表现；采用位置编码策略调整时序表征；使用DPO (direct preference optimization) 在8K对抗数据对模型进行微调，提高语义区分能力。

Result: The paper introduces ELV-Halluc, the first benchmark targeting hallucination in long-video Video-MLLMs, focusing on a new type called Semantic Aggregation Hallucination (SAH). It provides experiments showing SAH increases with semantic complexity and rapid semantic changes, and proposes mitigation methods including positional encoding adjustments and DPO training using an 8K adversarial pair dataset, achieving significant reduction in SAH.

Conclusion: ELV-Halluc证实并量化了长视频中特有的SAH问题，提出有效缓解策略（位置编码 + DPO + 对抗数据），显著改善模型在长视频的语义聚合准确性。

Abstract: Video multimodal large language models (Video-MLLMs) have achieved remarkable
progress in video understanding. However, they remain vulnerable to
hallucination-producing content inconsistent with or unrelated to video inputs.
Previous video hallucination benchmarks primarily focus on short-videos. They
attribute hallucinations to factors such as strong language priors, missing
frames, or vision-language biases introduced by the visual encoder. While these
causes indeed account for most hallucinations in short videos, they still
oversimplify the cause of hallucinations. Sometimes, models generate incorrect
outputs but with correct frame-level semantics. We refer to this type of
hallucination as Semantic Aggregation Hallucination (SAH), which arises during
the process of aggregating frame-level semantics into event-level semantic
groups. Given that SAH becomes particularly critical in long videos due to
increased semantic complexity across multiple events, it is essential to
separate and thoroughly investigate the causes of this type of hallucination.
To address the above issues, we introduce ELV-Halluc, the first benchmark
dedicated to long-video hallucination, enabling a systematic investigation of
SAH. Our experiments confirm the existence of SAH and show that it increases
with semantic complexity. Additionally, we find that models are more prone to
SAH on rapidly changing semantics. Moreover, we discuss potential approaches to
mitigate SAH. We demonstrate that positional encoding strategy contributes to
alleviating SAH, and further adopt DPO strategy to enhance the model's ability
to distinguish semantics within and across events. To support this, we curate a
dataset of 8K adversarial data pairs and achieve improvements on both
ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.

</details>


### [34] [Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation](https://arxiv.org/abs/2508.21529)
*Ronan Docherty,Antonis Vamvakeros,Samuel J. Cooper*

Main category: cs.CV

TL;DR: 训练一个卷积上采样器将低分辨率基础模型特征参考图像还原为高分辨率特征，能高效改善显微图像的分割效果，节省标注和训练时间。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型（通常为ViT）提供了丰富语义特征但以patch为单位，难以表示显微图像中的细小结构且在大尺寸图像上计算开销高。通过学习将低分辨率特征与原始图像细节对齐，可以在保留基础模型语义的前提下恢复细粒度信息并提高计算效率。

Method: 在训练阶段，使用输入图像和对应的低分辨率基础模型特征对一个卷积上采样网络进行训练，使其学习将粗糙的patch特征与图像细节对齐并恢复细粒度语义描述。训练完成后，直接将该上采样器应用于不同显微图像（植物细胞、锂离子电池正极、有机晶体等）的基础模型特征，无需对上游基础模型或下游分割器再次训练。随后利用交互式分割工具在上采样特征上进行快速、少标注的高质量分割。

Result: 上采样后的特征在多种显微图像数据集上表现出更丰富的语义细节，能够分离如发丝状裂纹等难分割相位。基于这些特征的交互式分割比训练或微调传统卷积网络所需标注和时间显著更少，同时产出更高质量的分割结果。

Conclusion: 该论文提出了一种基于卷积神经网络的上采样器，用于将低分辨率（大patch）基础模型特征在参考输入图像的条件下还原为高分辨率特征，从而改善微观图像和大尺寸图像的特征表达与分割性能。

Abstract: Feature foundation models - usually vision transformers - offer rich semantic
descriptors of images, useful for downstream tasks such as (interactive)
segmentation and object detection. For computational efficiency these
descriptors are often patch-based, and so struggle to represent the fine
features often present in micrographs; they also struggle with the large image
sizes present in materials and biological image analysis. In this work, we
train a convolutional neural network to upsample low-resolution (i.e, large
patch size) foundation model features with reference to the input image. We
apply this upsampler network (without any further training) to efficiently
featurise and then segment a variety of microscopy images, including plant
cells, a lithium-ion battery cathode and organic crystals. The richness of
these upsampled features admits separation of hard to segment phases, like
hairline cracks. We demonstrate that interactive segmentation with these deep
features produces high-quality segmentations far faster and with far fewer
labels than training or finetuning a more traditional convolutional network.

</details>


### [35] [HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones](https://arxiv.org/abs/2508.21539)
*Hao Ruan,Jinliang Lin,Yingxin Lai,Zhiming Luo,Shaozi Li*

Main category: cs.CV

TL;DR: HCCM通过区域-全局对比与匹配学习以及动量对比蒸馏，提高无人机视觉语言模型的细粒度语义理解和鲁棒性，检索与泛化表现优异


<details>
  <summary>Details</summary>
Motivation: 解决无人机场景中大视野和复杂组合语义对视觉-语言理解的挑战，提升细粒度对齐和组合推理能力

Method: RG-ITC:局部视觉区域与全局文本互对比；RG-ITM:在全局表征下评估局部语义一致性；MCD:动量对比与蒸馏以增强对不完整/歧义文本的鲁棒性

Result: 提出HCCM框架，包含RG-ITC和RG-ITM两项机制，并引入MCD以提高鲁棒性；在GeoText-1652和ERA数据集上取得优异检索与零样本泛化性能

Conclusion: HCCM无需精确场景分割与严格包含约束，能更稳健地捕捉局部到全局的跨模态语义，提升无人机引导下的目标匹配与导航任务表现

Abstract: Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such
as target matching and navigation. However, the wide field of view and complex
compositional semantics in drone scenarios pose challenges for vision-language
understanding. Mainstream Vision-Language Models (VLMs) emphasize global
alignment while lacking fine-grained semantics, and existing hierarchical
methods depend on precise entity partitioning and strict containment, limiting
effectiveness in dynamic environments. To address this, we propose the
Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM)
framework with two components: (1) Region-Global Image-Text Contrastive
Learning (RG-ITC), which avoids precise scene partitioning and captures
hierarchical local-to-global semantics by contrasting local visual regions with
global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),
which dispenses with rigid constraints and instead evaluates local semantic
consistency within global cross-modal representations, enhancing compositional
reasoning. Moreover, drone text descriptions are often incomplete or ambiguous,
destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation
(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM
achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text
retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot
generalization with 39.93% mean recall (mR), outperforming fine-tuned
baselines.

</details>


### [36] [Complete Gaussian Splats from a Single Image with Denoising Diffusion Models](https://arxiv.org/abs/2508.21542)
*Ziwei Liao,Mohamed Sayed,Steven L. Waslander,Sara Vicente,Daniyar Turmukhambetov,Michael Firman*

Main category: cs.CV

TL;DR: 本文提出一种基于潜在扩散模型的方法，用于从单张图像生成包含遮挡部分的完整3D场景高斯斑点表示。通过变分自编码重构器在无监督的2D图像数据上学习潜在空间，并在该潜在空间上训练扩散模型，实现多样化和高质量的360度渲染重建。


<details>
  <summary>Details</summary>
Motivation: 传统高斯斑点重建需大量视角观测，且对遮挡与未观测区域通常采用回归预测单一模式，导致模糊或不合理的结果。为了解决这个多模态、不确定性的补全问题，作者提出生成式建模以学习可能的3D解释分布，从而生成多样且合理的补全结果。

Method: 方法包括：1) 用变分自编码重构器(VAR)在仅有的2D图像上自监督学习高斯斑点表示的潜在空间；2) 在该潜在空间上训练条件扩散模型，通过给定单张图像生成对应的高斯斑点分布样本；3) 在推理阶段以单张图像为条件输入，采样扩散模型生成多样化的高质量完整3D高斯斑点场并渲染360度视图。

Result: 在有限观测（单视图）条件下，所提方法可以生成忠实于输入并具有多样性的3D重建样本，能有效补全遮挡区域并生成高质量的360度渲染，比回归基线在可视与不可视区域表现更好。

Conclusion: 提出的方法能从单张输入图像生成逼真且多样化的高斯斑点3D重建，能够补全被遮挡或未观测到的表面，生成高质量的360度渲染结果，克服了回归方法产生模糊和不可行解的缺点。

Abstract: Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.

</details>


### [37] [EZ-Sort: Efficient Pairwise Comparison via Zero-Shot CLIP-Based Pre-Ordering and Human-in-the-Loop Sorting](https://arxiv.org/abs/2508.21550)
*Yujin Park,Haejun Chung,Ikbeom Jang*

Main category: cs.CV

TL;DR: Use CLIP to pre-order and auto-resolve easy comparisons, then apply uncertainty-driven MergeSort with Elo scores to drastically reduce human pairwise annotations while preserving reliability.


<details>
  <summary>Details</summary>
Motivation: Pairwise comparisons are reliable but expensive (O(n^2)); prior active sorting reduced burden to O(n log n); aim to further cut human cost by leveraging CLIP pre-ordering and automating easy comparisons.

Method: EZ-Sort: CLIP-assisted pairwise sorting with automated easy comparisons

Result: EZ-Sort uses CLIP zero-shot pre-ordering, initializes bucket-aware Elo, and runs uncertainty-guided human-in-the-loop MergeSort; across FGNET, DHCI, EyePACS achieved 90.5% reduction vs exhaustive and 19.8% vs prior work at n=100, while maintaining or improving inter-rater reliability.

Conclusion: Combining CLIP priors with uncertainty-aware sampling gives an efficient scalable pairwise ranking method that cuts annotation costs substantially without harming reliability.

Abstract: Pairwise comparison is often favored over absolute rating or ordinal
classification in subjective or difficult annotation tasks due to its improved
reliability. However, exhaustive comparisons require a massive number of
annotations (O(n^2)). Recent work has greatly reduced the annotation burden
(O(n log n)) by actively sampling pairwise comparisons using a sorting
algorithm. We further improve annotation efficiency by (1) roughly pre-ordering
items using the Contrastive Language-Image Pre-training (CLIP) model
hierarchically without training, and (2) replacing easy, obvious human
comparisons with automated comparisons. The proposed EZ-Sort first produces a
CLIP-based zero-shot pre-ordering, then initializes bucket-aware Elo scores,
and finally runs an uncertainty-guided human-in-the-loop MergeSort. Validation
was conducted using various datasets: face-age estimation (FGNET), historical
image chronology (DHCI), and retinal image quality assessment (EyePACS). It
showed that EZ-Sort reduced human annotation cost by 90.5% compared to
exhaustive pairwise comparisons and by 19.8% compared to prior work (when n =
100), while improving or maintaining inter-rater reliability. These results
demonstrate that combining CLIP-based priors with uncertainty-aware sampling
yields an efficient and scalable solution for pairwise ranking.

</details>


### [38] [ECHO: Ego-Centric modeling of Human-Object interactions](https://arxiv.org/abs/2508.21556)
*Ilya A. Petrov,Vladimir Guzov,Riccardo Marin,Emre Aksan,Xu Chen,Daniel Cremers,Thabo Beeler,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: 提出ECHO框架：从仅有头部和手腕轨迹恢复佩戴者的人体骨架、物体运动和接触信息。采用Diffusion Transformer与三元扩散过程，在头部中心坐标系下建模，支持灵活输入并用“conveyor-based”推理处理任意长度序列，在多项评估中优于先前方法。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备普及，从极简传感（仅头部和手腕）恢复人-物交互对AR/VR与行为理解很重要，但未被充分研究。

Method: 设计Diffusion Transformer与三变量扩散过程，联合生成/恢复人体运动、物体轨迹和接触信息；使用头部中心化坐标系；提出conveyor-based推理通过随帧位置增加扩散时间戳来处理任意长度序列。

Result: 大量实验显示在多个评估指标上超越现有方法，尤其在灵活输入配置和长序列处理方面表现突出。

Conclusion: 在最小观测（头部+手腕）下，ECHO能联合重建人体姿态、物体轨迹和接触序列，具有较强鲁棒性与灵活性，并在基准上达到了最优或领先性能。

Abstract: Modeling human-object interactions (HOI) from an egocentric perspective is a
largely unexplored yet important problem due to the increasing adoption of
wearable devices, such as smart glasses and watches. We investigate how much
information about interaction can be recovered from only head and wrists
tracking. Our answer is ECHO (Ego-Centric modeling of Human-Object
interactions), which, for the first time, proposes a unified framework to
recover three modalities: human pose, object motion, and contact from such
minimal observation. ECHO employs a Diffusion Transformer architecture and a
unique three-variate diffusion process, which jointly models human motion,
object trajectory, and contact sequence, allowing for flexible input
configurations. Our method operates in a head-centric canonical space,
enhancing robustness to global orientation. We propose a conveyor-based
inference, which progressively increases the diffusion timestamp with the frame
position, allowing us to process sequences of any length. Through extensive
evaluation, we demonstrate that ECHO outperforms existing methods that do not
offer the same flexibility, setting a state-of-the-art in egocentric HOI
reconstruction.

</details>


### [39] [How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images](https://arxiv.org/abs/2508.21565)
*Juneyoung Ro,Namwoo Kim,Yoonjin Yoon*

Main category: cs.CV

TL;DR: 研究表明通过合成的Chain-of-Thought监督微调，通用VLM能显著提升在城市空间推理任务（尤其否定与反事实问题）上的表现，提出了将合成VQA数据用于领域适配的实用方案。


<details>
  <summary>Details</summary>
Motivation: 探索通用预训练VLM是否以及如何能迁移到需要细粒度空间推理的城市场景，并检验合成CoT监督数据在领域自适应中的作用。

Method: 比较评估三种现成VLM（BLIP-2、InstructBLIP、LLaVA-1.5），构建基于分割、深度与检测预测的合成城市VQA数据集，并使用LLM生成的Chain-of-Thought答案作为逐步推理监督进行微调；评估包括零样本与微调后性能比较。

Result: 零样本时模型表现尚可；使用合成CoT数据微调后，整体性能大幅提升，对复杂类型问题（如否定、反事实）提升更明显，表明合成逐步推理监督对域适应有效。

Conclusion: 本文证明，在城市场景下，通用VLM通过合成的CoT监督数据进行微调后，其空间推理能力显著提升，尤其在否定与反事实问题上效果明显。

Abstract: Effectively understanding urban scenes requires fine-grained spatial
reasoning about objects, layouts, and depth cues. However, how well current
vision-language models (VLMs), pretrained on general scenes, transfer these
abilities to urban domain remains underexplored. To address this gap, we
conduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,
and LLaVA-1.5-evaluating both zero-shot performance and the effects of
fine-tuning with a synthetic VQA dataset specific to urban scenes. We construct
such dataset from segmentation, depth, and object detection predictions of
street-view images, pairing each question with LLM-generated Chain-of-Thought
(CoT) answers for step-by-step reasoning supervision. Results show that while
VLMs perform reasonably well in zero-shot settings, fine-tuning with our
synthetic CoT-supervised dataset substantially boosts performance, especially
for challenging question types such as negation and counterfactuals. This study
introduces urban spatial reasoning as a new challenge for VLMs and demonstrates
synthetic dataset construction as a practical path for adapting general-purpose
models to specialized domains.

</details>


### [40] [Temporal Flow Matching for Learning Spatio-Temporal Trajectories in 4D Longitudinal Medical Imaging](https://arxiv.org/abs/2508.21580)
*Nico Albert Disch,Yannick Kirchhoff,Robin Peretzke,Maximilian Rokuss,Saikat Roy,Constantin Ulrich,David Zimmerer,Klaus Maier-Hein*

Main category: cs.CV

TL;DR: TFM提出了一种能学习时间分布并兼容多先前时点与不规则采样的生成式轨迹方法，支持3D体积，表现优于现有时空方法，成为4D医学图像预测的新基线。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在解决现有深度学习方法在医学成像时间动态建模方面的局限性，尤其是大多数方法仅处理单一时间上下文或专注于分类/回归，无法进行精细的空间预测，同时受制于单时间点、特定疾病或技术限制。

Method: 引入Temporal Flow Matching，一种生成式轨迹学习方法，估计时间演化的分布，通过设计包含可退化为最近图像预测的机制，并扩展到3D体积、支持多先前扫描与不规则时间间隔。

Result: 提出了Temporal Flow Matching (TFM)，一种统一的生成式轨迹方法，能够学习潜在时间分布，支持退化为最近图像预测(LCI)的特例，且支持3D体积、多次先前扫描和不规则采样。在三个公开纵向数据集上的大量基准测试中，TFM超越了自然图像的时空方法，建立了新的4D医学图像预测的基线。

Conclusion: TFM为纵向医学成像的空间细节预测提供了强有力且通用的生成框架，其退化为LCI的特性和对多时点、不规则采样与3D支持，使其成为鲁棒的基线方案。

Abstract: Understanding temporal dynamics in medical imaging is crucial for
applications such as disease progression modeling, treatment planning and
anatomical development tracking. However, most deep learning methods either
consider only single temporal contexts, or focus on tasks like classification
or regression, limiting their ability for fine-grained spatial predictions.
While some approaches have been explored, they are often limited to single
timepoints, specific diseases or have other technical restrictions. To address
this fundamental gap, we introduce Temporal Flow Matching (TFM), a unified
generative trajectory method that (i) aims to learn the underlying temporal
distribution, (ii) by design can fall back to a nearest image predictor, i.e.
predicting the last context image (LCI), as a special case, and (iii) supports
$3D$ volumes, multiple prior scans, and irregular sampling. Extensive
benchmarks on three public longitudinal datasets show that TFM consistently
surpasses spatio-temporal methods from natural imaging, establishing a new
state-of-the-art and robust baseline for $4D$ medical image prediction.

</details>


### [41] [Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer](https://arxiv.org/abs/2508.21581)
*Daniël Boeke,Cedrik Blommestijn,Rebecca N. Wray,Kalina Chupetlovska,Shangqi Gao,Zeyu Gao,Regina G. H. Beets-Tan,Mireia Crispin-Ortuzar,James O. Jones,Wilson Silva,Ines P. Machado*

Main category: cs.CV

TL;DR: 将预训练编码器与Cox生存模型结合的多模态中间融合可提高ccRCC复发风险预测，病理WSI信息最关键，放射CT在融合中带来增量价值，但需更复杂融合策略与更大数据集进一步提升。


<details>
  <summary>Details</summary>
Motivation: Leibovich评分虽广泛用于远处复发风险分层，但分辨率有限且忽略影像信息；研究旨评估将术前CT与术后病理WSI多模态整合以改善个体化复发风险预测。

Method: 采用带预训练编码器的模块化深度学习框架，结合Cox生存建模，比较单模态（WSI或CT）、后期融合和中间融合策略；使用真实世界ccRCC队列进行训练和评估，模型包括TITAN-CONCH与ResNet-18等。

Result: WSI模型持续优于仅CT模型；中间融合模型表现最佳（TITAN-CONCH+ResNet-18接近调整后的Leibovich评分）；简单嵌入拼接显示放射学通过融合贡献增量价值；随机平局处理显示离散化可能夸大基线与学习模型差距。

Conclusion: 研究表明在ccRCC复发风险预测中，病理WSI比放射CT提供更强的预后信息，且多模态中间融合可进一步提升预测性能；当前最佳深度学习模型接近经调整的Leibovich评分但仍存在差距，离散化评分可能高估个体化模型表现。

Abstract: Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) is
essential for guiding postoperative surveillance and treatment. The Leibovich
score remains widely used for stratifying distant recurrence risk but offers
limited patient-level resolution and excludes imaging information. This study
evaluates multimodal recurrence prediction by integrating preoperative computed
tomography (CT) and postoperative histopathology whole-slide images (WSIs). A
modular deep learning framework with pretrained encoders and Cox-based survival
modeling was tested across unimodal, late fusion, and intermediate fusion
setups. In a real-world ccRCC cohort, WSI-based models consistently
outperformed CT-only models, underscoring the prognostic strength of pathology.
Intermediate fusion further improved performance, with the best model
(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Random
tie-breaking narrowed the gap between the clinical baseline and learned models,
suggesting discretization may overstate individualized performance. Using
simple embedding concatenation, radiology added value primarily through fusion.
These findings demonstrate the feasibility of foundation model-based multimodal
integration for personalized ccRCC risk prediction. Future work should explore
more expressive fusion strategies, larger multimodal datasets, and
general-purpose CT encoders to better match pathology modeling capacity.

</details>


### [42] [Unfolding Framework with Complex-Valued Deformable Attention for High-Quality Computer-Generated Hologram Generation](https://arxiv.org/abs/2508.21657)
*Haomiao Zhang,Zhangyuan Li,Yanling Piao,Zhi Li,Xiaodong Wang,Miao Cao,Xiongfei Su,Qiang Song,Xin Yuan*

Main category: cs.CV

TL;DR: 本文提出的DUN将物理建模与深度学习结合，通过ABPM扩大工作距离、通过复值可变形自注意力去噪器捕捉全局特征，实现了超越ASM和传统CNN方法的高质量全息重建（PSNR>35 dB）。


<details>
  <summary>Details</summary>
Motivation: 传统CGH方法受限于黑盒端到端网络不可解释性、CNN局部感受野限制及ASM基方法近场范围限制，亟需一种既遵循物理模型又能捕捉长程上下文的可解释、高性能方法。

Method: 采用深度展开框架，将物理约束与数据驱动模块结合。ABPM用于替代传统ASM传播，保留带宽并支持更远的工作距离；PCD为复值网络，集成可变形自注意力以增强全局特征捕获。整体通过迭代梯度下降步骤展开并进行端到端训练。

Result: 在模拟和真实数据上均取得最先进的重建效果，PCD模块使得重建图像PSNR超过35 dB，DUN较现有方法在稳定性和远场工作距离上有显著提升。

Conclusion: 提出的深度展开网络（DUN）通过将梯度下降分解为自适应带宽保持模块（ABPM）和相位域复值去噪器（PCD），在可解释性和灵活性上优于端到端黑盒方法，能够扩大工作距离并捕捉全局特征，从而实现更高质量的全息重建。

Abstract: Computer-generated holography (CGH) has gained wide attention with deep
learning-based algorithms. However, due to its nonlinear and ill-posed nature,
challenges remain in achieving accurate and stable reconstruction.
Specifically, ($i$) the widely used end-to-end networks treat the
reconstruction model as a black box, ignoring underlying physical
relationships, which reduces interpretability and flexibility. ($ii$) CNN-based
CGH algorithms have limited receptive fields, hindering their ability to
capture long-range dependencies and global context. ($iii$) Angular spectrum
method (ASM)-based models are constrained to finite near-fields.In this paper,
we propose a Deep Unfolding Network (DUN) that decomposes gradient descent into
two modules: an adaptive bandwidth-preserving model (ABPM) and a phase-domain
complex-valued denoiser (PCD), providing more flexibility. ABPM allows for
wider working distances compared to ASM-based methods. At the same time, PCD
leverages its complex-valued deformable self-attention module to capture global
features and enhance performance, achieving a PSNR over 35 dB. Experiments on
simulated and real data show state-of-the-art results.

</details>


### [43] [Towards Interactive Lesion Segmentation in Whole-Body PET/CT with Promptable Models](https://arxiv.org/abs/2508.21680)
*Maximilian Rokuss,Yannick Kirchhoff,Fabian Isensee,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: 在nnU-Net基础上加入可交互提示（前景/背景点击），使用欧氏距离变换(EDT)作为提示编码，在线模拟用户交互与定制采样策略，集成模型在多中心、多示踪剂PET/CT上显著提升分割性能，公开代码。


<details>
  <summary>Details</summary>
Motivation: 临床中全自动病灶分割受示踪剂异质性、生理摄取与多中心差异影响，需人机协同的交互式方法以便临床人员高效修正自动分割结果。

Method: 在winning autoPET III nnU-Net基础上，增加前景/背景点击输入通道，比较EDT与高斯核提示编码，提出在线模拟交互与自定义点采样策略，训练多模型并集成，使用有/无外部数据的训练集进行交叉验证评估。

Result: Promptable nnU-Net for interactive PET/CT lesion segmentation using EDT encodings and online simulated clicks; ensemble reduces FPs/FNs vs baseline.

Conclusion: EDT作为空间提示编码优于高斯核；在线交互模拟与专门采样策略提升模型对真实用户提示的鲁棒性；集成EDT模型显著降低假阳性和假阴性，适合临床人机协同分割流程。

Abstract: Whole-body PET/CT is a cornerstone of oncological imaging, yet accurate
lesion segmentation remains challenging due to tracer heterogeneity,
physiological uptake, and multi-center variability. While fully automated
methods have advanced substantially, clinical practice benefits from approaches
that keep humans in the loop to efficiently refine predicted masks. The
autoPET/CT IV challenge addresses this need by introducing interactive
segmentation tasks based on simulated user prompts. In this work, we present
our submission to Task 1. Building on the winning autoPET III nnU-Net pipeline,
we extend the framework with promptable capabilities by encoding user-provided
foreground and background clicks as additional input channels. We
systematically investigate representations for spatial prompts and demonstrate
that Euclidean Distance Transform (EDT) encodings consistently outperform
Gaussian kernels. Furthermore, we propose online simulation of user
interactions and a custom point sampling strategy to improve robustness under
realistic prompting conditions. Our ensemble of EDT-based models, trained with
and without external data, achieves the strongest cross-validation performance,
reducing both false positives and false negatives compared to baseline models.
These results highlight the potential of promptable models to enable efficient,
user-guided segmentation workflows in multi-tracer, multi-center PET/CT. Code
is publicly available at https://github.com/MIC-DKFZ/autoPET-interactive

</details>


### [44] [Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping](https://arxiv.org/abs/2508.21689)
*Fatih Erdoğan,Merve Rabia Barın,Fatma Güney*

Main category: cs.CV

TL;DR: 用摄像机几何初始投影+概率化置信度校正，选择性累积高置信时序信息，减少虚构道路要素，提高BEV地图生成精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于注意力或学习的投影方法泛化性差，容易在BEV中虚构不存在的道路元素。作者主张以确定性几何投影为起点，并通过概率置信度机制自适应校正以抑制错误投影并提高生成HD地图的可靠性。

Method: 1) 使用相机内外参进行初始几何投影到BEV；2) 设计概率投影机制，为每个投影位置计算置信度（confidence scores）；3) 基于置信度对映射进行场景自适应校正并过滤不相关要素；4) 在时序融合中利用置信度选择性积累可靠信息；5) 在nuScenes与Argoverse2上训练评估。

Result: Paper提出了一种基于几何投影并结合不确定性评估的BEV映射方法，通过摄像机参数初步投影到地面视图，再用概率投影机制和置信度评分对映射进行场景自适应校正与噪声过滤，从而提升高清地图矢量化的准确性。实验在nuScenes与Argoverse2的数据划分上显示优于SOTA，尤其在长距离感知与nuScenes上增益明显。

Conclusion: 将确定性几何映射与概率置信评分结合能有效抑制注意力类方法的泛化误差，提升HD地图生成的准确性与长期感知鲁棒性。

Abstract: Constructing high-definition (HD) maps from sensory input requires accurately
mapping the road elements in image space to the Bird's Eye View (BEV) space.
The precision of this mapping directly impacts the quality of the final
vectorized HD map. Existing HD mapping approaches outsource the projection to
standard mapping techniques, such as attention-based ones. However, these
methods struggle with accuracy due to generalization problems, often
hallucinating non-existent road elements. Our key idea is to start with a
geometric mapping based on camera parameters and adapt it to the scene to
extract relevant map information from camera images. To implement this, we
propose a novel probabilistic projection mechanism with confidence scores to
(i) refine the mapping to better align with the scene and (ii) filter out
irrelevant elements that should not influence HD map generation. In addition,
we improve temporal processing by using confidence scores to selectively
accumulate reliable information over time. Experiments on new splits of the
nuScenes and Argoverse2 datasets demonstrate improved performance over
state-of-the-art approaches, indicating better generalization. The improvements
are particularly pronounced on nuScenes and in the challenging long perception
range. Our code and model checkpoints are available at
https://github.com/Fatih-Erdogan/mapping-like-skeptic .

</details>


### [45] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 将词级OCR升级为行级OCR，使用seq2seq对整行识别，发布251页行级数据集，准确率提升5.4%、效率提升4倍。


<details>
  <summary>Details</summary>
Motivation: 现代OCR从字符到词的转变把瓶颈移至词分割，作者提出进一步提升到行级以减少分割错误并更好利用语言模型。

Method: 基于序列到序列模型直接对整行文本进行识别，避免逐词检测；构建并使用包含251页英文图像的行级标注数据集进行训练与评估。

Result: 在文档图像上实现了端到端准确率提升5.4%，效率提高约4倍；并发布了行级标注数据集。

Conclusion: 本文提出将词级OCR升级为行级OCR，以绕过词检测误差并利用更大的句子上下文，从而提高准确率和效率。

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


### [46] [FLORA: Efficient Synthetic Data Generation for Object Detection in Low-Data Regimes via finetuning Flux LoRA](https://arxiv.org/abs/2508.21712)
*Alvaro Patricio,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: FLORA: LoRA fine-tuned Flux diffusion enables efficient synthetic data generation on consumer GPUs; 500 images beat 5000 ODGEN images, offering major compute and data savings


<details>
  <summary>Details</summary>
Motivation: Reduce computational cost and data requirements for synthetic data generation for object detection by using LoRA with a smaller diffusion model (Flux 1.1 Dev) to enable generation on consumer GPUs

Method: Fine-tuning large diffusion models with LoRA on consumer GPUs

Result: Using FLORA (LoRA fine-tuned Flux 1.1 Dev), training detectors with 500 synthetic images outperforms ODGEN's 5000-image baseline, improving mAP@.50:.95 by up to 21.3% across seven datasets

Conclusion: Quality-focused LoRA fine-tuning of a smaller diffusion model can outperform brute-force large-scale generation, making advanced synthetic augmentation practical and accessible

Abstract: Recent advances in diffusion-based generative models have demonstrated
significant potential in augmenting scarce datasets for object detection tasks.
Nevertheless, most recent models rely on resource-intensive full fine-tuning of
large-scale diffusion models, requiring enterprise-grade GPUs (e.g., NVIDIA
V100) and thousands of synthetic images. To address these limitations, we
propose Flux LoRA Augmentation (FLORA), a lightweight synthetic data generation
pipeline. Our approach uses the Flux 1.1 Dev diffusion model, fine-tuned
exclusively through Low-Rank Adaptation (LoRA). This dramatically reduces
computational requirements, enabling synthetic dataset generation with a
consumer-grade GPU (e.g., NVIDIA RTX 4090). We empirically evaluate our
approach on seven diverse object detection datasets. Our results demonstrate
that training object detectors with just 500 synthetic images generated by our
approach yields superior detection performance compared to models trained on
5000 synthetic images from the ODGEN baseline, achieving improvements of up to
21.3% in mAP@.50:.95. This work demonstrates that it is possible to surpass
state-of-the-art performance with far greater efficiency, as FLORA achieves
superior results using only 10% of the data and a fraction of the computational
cost. This work demonstrates that a quality and efficiency-focused approach is
more effective than brute-force generation, making advanced synthetic data
creation more practical and accessible for real-world scenarios.

</details>


### [47] [Entropy-Based Non-Invasive Reliability Monitoring of Convolutional Neural Networks](https://arxiv.org/abs/2508.21715)
*Amirhossein Nazeri,Wael Hafez*

Main category: cs.CV

TL;DR: Monitor activation entropy of early CNN layers to detect adversarial inputs in real-time without modifying model; shows ~7% entropy shift and ~90% detection accuracy.


<details>
  <summary>Details</summary>
Motivation: avoid retraining or architecture change while detecting adversarial examples; leverage internal activation statistics

Method: entropy monitoring of CNN activations

Result: adversarial inputs shift activation entropy by ~7% in early conv layers; 90% detection accuracy with FPR/FNR <20%; complete separation between distributions

Conclusion: Activation entropy provides a practical, non-invasive self-diagnostic for CNNs to detect adversarial examples with high accuracy and low impact on clean performance.

Abstract: Convolutional Neural Networks (CNNs) have become the foundation of modern
computer vision, achieving unprecedented accuracy across diverse image
recognition tasks. While these networks excel on in-distribution data, they
remain vulnerable to adversarial perturbations imperceptible input
modifications that cause misclassification with high confidence. However,
existing detection methods either require expensive retraining, modify network
architecture, or degrade performance on clean inputs. Here we show that
adversarial perturbations create immediate, detectable entropy signatures in
CNN activations that can be monitored without any model modification. Using
parallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs
consistently shift activation entropy by 7% in early convolutional layers,
enabling 90% detection accuracy with false positives and false negative rates
below 20%. The complete separation between clean and adversarial entropy
distributions reveals that CNNs inherently encode distribution shifts in their
activation patterns. This work establishes that CNN reliability can be assessed
through activation entropy alone, enabling practical deployment of
self-diagnostic vision systems that detect adversarial inputs in real-time
without compromising original model performance.

</details>


### [48] [CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models](https://arxiv.org/abs/2508.21732)
*João Valente,Atabak Dehban,Rodrigo Ventura*

Main category: cs.CV

TL;DR: 作者提出CAD2DMD-SET合成数据工具与1,000张真实图像验证集DMDBench，使用合成数据对LVLMs进行LoRA微调，显著提高模型在复杂现实条件下的DMD读数能力，InternVL提升约200%。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在现实场景（杂乱背景、遮挡、极端视角、运动模糊等）下读取DMD数值仍表现欠佳，尤其对头戴摄像与AR应用非常重要，因此需要专门的数据和方法提升鲁棒性。

Method: 通过3D CAD建模、先进渲染与高保真图像合成生成带VQA标签的合成数据集，并构建1,000张真实图像的验证集DMDBench；对三种SOTA LVLMs进行基准测试，并用CAD2DMD-SET合成数据通过LoRA微调提升模型表现。

Result: 在ANLS指标下，使用CAD2DMD-SET微调后三款LVLM均显著提升；其中InternVL得分提升约200%，且在其他任务上未见降级，证明合成数据对提升DMD读数任务的有效性。

Conclusion: 该工作提出了一个基于3D CAD模型的合成数据生成工具CAD2DMD-SET，用于提升大视觉-语言模型(LVLMs)在数字测量设备(DMDs)读数任务上的性能。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
impressive capabilities across various multimodal tasks. They continue,
however, to struggle with trivial scenarios such as reading values from Digital
Measurement Devices (DMDs), particularly in real-world conditions involving
clutter, occlusions, extreme viewpoints, and motion blur; common in
head-mounted cameras and Augmented Reality (AR) applications. Motivated by
these limitations, this work introduces CAD2DMD-SET, a synthetic data
generation tool designed to support visual question answering (VQA) tasks
involving DMDs. By leveraging 3D CAD models, advanced rendering, and
high-fidelity image composition, our tool produces diverse, VQA-labelled
synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present
DMDBench, a curated validation set of 1,000 annotated real-world images
designed to evaluate model performance under practical constraints.
Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein
Similarity (ANLS) and further fine-tuning LoRA's of these models with
CAD2DMD-SET's generated dataset yielded substantial improvements, with InternVL
showcasing a score increase of 200% without degrading on other tasks. This
demonstrates that the CAD2DMD-SET training dataset substantially improves the
robustness and performance of LVLMs when operating under the previously stated
challenging conditions. The CAD2DMD-SET tool is expected to be released as
open-source once the final version of this manuscript is prepared, allowing the
community to add different measurement devices and generate their own datasets.

</details>


### [49] [Learning from Silence and Noise for Visual Sound Source Localization](https://arxiv.org/abs/2508.21761)
*Xavier Juanola,Giovana Morais,Magdalena Fuentes,Gloria Haro*

Main category: cs.CV

TL;DR: 通过在自监督训练中引入沉默和噪声并扩展数据与指标，本文提升了视觉声源定位模型对负音频场景的鲁棒性，同时保持正例性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉声源定位方法在低视听语义对应（如静音、噪声、场外声）场景表现差，且评估多集中在单一可见声源的正例，缺乏对负音频场景的训练与评测。

Method: 提出SSL-SaN自监督模型，通过在训练中加入沉默（silence）和噪声（noise）样本来改善模型对负音频的鲁棒性；设计新的度量用于衡量视听特征在正负对之间的对齐性与可分离性；并扩展IS3数据集为IS3+以包含负音频样本用于训练和评估。

Result: SSL-SaN在声源定位和跨模态检索任务上优于其他自监督模型，在正例表现提升的同时在包含负音频的场景中更稳定；提出的新指标能量化对齐与可分离性权衡；IS3+为负音频研究提供了更全面的数据支持。

Conclusion: 本文提出了针对视觉声源定位中负音频问题的自监督训练策略、评估指标与数据集扩展，有效提升在含沉默、噪声与场外声等低视听语义对应场景中的鲁棒性，并在正例任务中仍保持或提升性能。

Abstract: Visual sound source localization is a fundamental perception task that aims
to detect the location of sounding sources in a video given its audio. Despite
recent progress, we identify two shortcomings in current methods: 1) most
approaches perform poorly in cases with low audio-visual semantic
correspondence such as silence, noise, and offscreen sounds, i.e. in the
presence of negative audio; and 2) most prior evaluations are limited to
positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key
contributions. First, we propose a new training strategy that incorporates
silence and noise, which improves performance in positive cases, while being
more robust against negative sounds. Our resulting self-supervised model,
SSL-SaN, achieves state-of-the-art performance compared to other
self-supervised models, both in sound localization and cross-modal retrieval.
Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative
audio-visual pairs. Third, we present IS3+, an extended and improved version of
the IS3 synthetic dataset with negative audio.
  Our data, metrics and code are available on the
https://xavijuanola.github.io/SSL-SaN/.

</details>


### [50] [UItron: Foundational GUI Agent with Advanced Perception and Planning](https://arxiv.org/abs/2508.21767)
*Zhixiong Zeng,Jing Huang,Liming Zheng,Wenkang Han,Yufeng Zhong,Lei Chen,Longrong Yang,Yingjie Chu,Yuzhi He,Lin Ma*

Main category: cs.CV

TL;DR: UItron is an open-source foundational GUI agent model that combines data engineering, interactive infrastructure, supervised finetuning, and curriculum RL, with a million-step Chinese app dataset, achieving state-of-the-art results in GUI perception, grounding, and planning.


<details>
  <summary>Details</summary>
Motivation: Develop a robust foundational model for GUI agents to enable automated operations on mobile/PC devices, addressing data scarcity, interactive infrastructure, and limitations of foundation models.

Method: Collect large-scale operation trajectories (>1M steps) across top 100 Chinese apps; build offline and online evaluation environments; supervised finetuning on perception and planning tasks; curriculum reinforcement learning for complex reasoning and exploration; systemic data engineering strategies; interactive environment connecting Mobile and PC devices.

Result: UItron, an open-source model with enhanced GUI perception, grounding, and planning; a connected interactive environment for mobile and PC; supervised finetuning and curriculum reinforcement learning; over one million collected operation steps; improved performance especially in Chinese app scenarios.

Conclusion: UItron substantially advances GUI agents by integrating systemic data engineering, interactive infrastructure, and hybrid training (supervised + curriculum RL), notably improving performance in Chinese mobile app scenarios and enabling closer-to-real-world applications.

Abstract: GUI agent aims to enable automated operations on Mobile/PC devices, which is
an important task toward achieving artificial general intelligence. The rapid
advancement of VLMs accelerates the development of GUI agents, owing to their
powerful capabilities in visual understanding and task planning. However,
building a GUI agent remains a challenging task due to the scarcity of
operation trajectories, the availability of interactive infrastructure, and the
limitation of initial capabilities in foundation models. In this work, we
introduce UItron, an open-source foundational model for automatic GUI agents,
featuring advanced GUI perception, grounding, and planning capabilities. UItron
highlights the necessity of systemic data engineering and interactive
infrastructure as foundational components for advancing GUI agent development.
It not only systematically studies a series of data engineering strategies to
enhance training effects, but also establishes an interactive environment
connecting both Mobile and PC devices. In training, UItron adopts supervised
finetuning over perception and planning tasks in various GUI scenarios, and
then develop a curriculum reinforcement learning framework to enable complex
reasoning and exploration for online environments. As a result, UItron achieves
superior performance in benchmarks of GUI perception, grounding, and planning.
In particular, UItron highlights the interaction proficiency with top-tier
Chinese mobile APPs, as we identified a general lack of Chinese capabilities
even in state-of-the-art solutions. To this end, we manually collect over one
million steps of operation trajectories across the top 100 most popular apps,
and build the offline and online agent evaluation environments. Experimental
results demonstrate that UItron achieves significant progress in Chinese app
scenarios, propelling GUI agents one step closer to real-world application.

</details>


### [51] [Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations](https://arxiv.org/abs/2508.21769)
*Ha Min Son,Zhe Zhao,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: 提出CLIP-DCA：先增强基础模型的域感知表示，再通过与域特征解耦实现域不变分类，从而在更具挑战性的OOD评估上显著提升CLIP的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有DG评估可能被预训练数据覆盖，无法衡量在真正未见域上的泛化；标准追求域不变的策略可能误伤基础模型中有用的域感知信息，因而提出先增强域感知再解耦分类的思路以提升DG性能。

Method: 在CLIP编码器上增加独立的域头，并通过合成多样域数据来辨识并增强域感知表示；同时在分类分支对域特征进行解耦以实现域不变分类，训练目标结合域识别损失与解耦约束。

Result: The paper introduces CLIP-DCA, a method to improve domain generalization (DG) for foundation models (CLIP) by enhancing domain-aware representations and disentangling them from classification features. It evaluates DG using 33 datasets with OOD scoring and unlearning to simulate unseen domains, showing CLIP struggles on highly OOD data. CLIP-DCA uses a domain head and synthetic diverse domain data to boost performance, especially on more OOD datasets.

Conclusion: CLIP在高度OOD数据上性能显著下降；通过增强域感知并进行解耦的策略（CLIP-DCA），可在挑战性DG评估中取得明显提升，尤其对更OOD的数据效果更好。

Abstract: Evaluating domain generalization (DG) for foundational models like CLIP is
challenging, as web-scale pretraining data potentially covers many existing
benchmarks. Consequently, current DG evaluation may neither be sufficiently
challenging nor adequately test genuinely unseen data scenarios. To better
assess the performance of CLIP on DG in-the-wild, a scenario where CLIP
encounters challenging unseen data, we consider two approaches: (1) evaluating
on 33 diverse datasets with quantified out-of-distribution (OOD) scores after
fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'
some domains as an approximation. We observe that CLIP's performance
deteriorates significantly on more OOD datasets. To address this, we present
CLIP-DCA (Disentangling Classification from enhanced domain Aware
representations). Our approach is motivated by the observation that while
standard domain invariance losses aim to make representations domain-invariant,
this can be harmful to foundation models by forcing the discarding of
domain-aware representations beneficial for generalization. We instead
hypothesize that enhancing domain awareness is a prerequisite for effective
domain-invariant classification in foundation models. CLIP-DCA identifies and
enhances domain awareness within CLIP's encoders using a separate domain head
and synthetically generated diverse domain data. Simultaneously, it encourages
domain-invariant classification through disentanglement from the domain
features. CLIP-DCA shows significant improvements within this challenging
evaluation compared to existing methods, particularly on datasets that are more
OOD.

</details>


### [52] [What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos](https://arxiv.org/abs/2508.21770)
*Qiyue Sun,Qiming Huang,Yang Yang,Hongjun Wang,Jianbo Jiao*

Main category: cs.CV

TL;DR: 本文收集并构建了包含科幻、动画等非典型视频的新数据集，研究将这些“非典型”视频用于表征学习对开放世界任务（OOD检测、NCD、ZSAR）的影响。实验证明，即使是简单的训练策略，加入非典型样本也能一致提升三类任务的表现；增加非典型样本的类别多样性能进一步改善OOD检测；在NCD任务中，较小但语义多样的非典型集优于更大但更典型的数据集；在ZSAR中，语义多样性有助于对未见动作类的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注封闭集和常见典型数据，缺乏对开放世界中不寻常/非典型视频样本如何影响学习的研究。作者希望探究这些异常视频是否能帮助模型在开放世界场景中更好地检测异常、发现新类别并泛化到未见动作。

Method: 作者收集并构建非典型视频数据集（如科幻、动画等），将这些样本加入模型训练用于表征学习，并在三项开放世界任务（OOD检测、NCD、ZSAR）上进行系统化评估，比较不同规模与语义/类别多样性配置的影响。

Result: 实验表明：1) 将非典型视频加入训练能一致提升OOD检测、NCD、ZSAR的性能；2) 对于OOD检测，增加非典型样本的类别多样性进一步提升效果；3) 对于NCD，较小但语义多样的非典型集优于更大但更典型的数据集；4) 对于ZSAR，语义多样性有利于对未见动作的泛化。

Conclusion: 将非典型（不寻常）视频纳入训练能普遍提升开放世界视觉表征的泛化能力，并且语义/类别多样性是关键因素；作者同时提供了一个新的非典型视频数据集以促进后续研究。

Abstract: Humans usually show exceptional generalisation and discovery ability in the
open world, when being shown uncommon new concepts. Whereas most existing
studies in the literature focus on common typical data from closed sets,
open-world novel discovery is under-explored in videos. In this paper, we are
interested in asking: \textit{What if atypical unusual videos are exposed in
the learning process?} To this end, we collect a new video dataset consisting
of various types of unusual atypical data (\eg sci-fi, animation, \etc). To
study how such atypical data may benefit open-world learning, we feed them into
the model training process for representation learning. Focusing on three key
tasks in open-world learning: out-of-distribution (OOD) detection, novel
category discovery (NCD), and zero-shot action recognition (ZSAR), we found
that even straightforward learning approaches with atypical data consistently
improve performance across various settings. Furthermore, we found that
increasing the categorical diversity of the atypical samples further boosts OOD
detection performance. Additionally, in the NCD task, using a smaller yet more
semantically diverse set of atypical samples leads to better performance
compared to using a larger but more typical dataset. In the ZSAR setting, the
semantic diversity of atypical videos helps the model generalise better to
unseen action classes. These observations in our extensive experimental
evaluations reveal the benefits of atypical videos for visual representation
learning in the open world, together with the newly proposed dataset,
encouraging further studies in this direction.

</details>


### [53] [Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering](https://arxiv.org/abs/2508.21773)
*Nattapong Kurpukdee,Adrian G. Bors*

Main category: cs.CV

TL;DR: 本文提出无监督视频持续学习（uVCL）情景与非参数方法：用无监督视频Transformer提取特征，基于核密度估计（KDE）构建记忆簇，并通过新颖性检测动态扩展簇，同时利用迁移学习初始化，解决在无标签、无任务边界下的连续任务学习问题。实验证明在UCF101、HMDB51、Something-Something V2上效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习多为有监督，依赖标签与任务边界，且视频数据计算与内存成本高。研究旨在填补无监督视频持续学习的空白，提出现实场景和可行的非参数解决方案。

Method: 用无监督视频Transformer提取嵌入特征；对这些特征采用核密度估计建立非参数概率记忆簇；设计新颖性检测准则判断是否为新任务/类别并动态新增簇；采用从前任务迁移学习参数作为当前任务的初始状态；在无标签、无边界设置下增量学习并评估性能。

Result: 在UCF101、HMDB51、Something-Something V2三数据集上进行评估，结果表明所提方法在连续学习多个任务时显著提升性能，能动态扩展记忆簇并有效迁移知识，减缓遗忘。

Conclusion: 提出的方法在无标签无任务边界的连续视频学习场景下能有效捕获新知识并减缓遗忘，通过KDE记忆簇和新颖性检测动态扩展机制，以及任务间迁移初始化，显著提升在三个标准视频数据集上的表现。

Abstract: We propose a realistic scenario for the unsupervised video learning where
neither task boundaries nor labels are provided when learning a succession of
tasks. We also provide a non-parametric learning solution for the
under-explored problem of unsupervised video continual learning. Videos
represent a complex and rich spatio-temporal media information, widely used in
many applications, but which have not been sufficiently explored in
unsupervised continual learning. Prior studies have only focused on supervised
continual learning, relying on the knowledge of labels and task boundaries,
while having labeled data is costly and not practical. To address this gap, we
study the unsupervised video continual learning (uVCL). uVCL raises more
challenges due to the additional computational and memory requirements of
processing videos when compared to images. We introduce a general benchmark
experimental protocol for uVCL by considering the learning of unstructured
video data categories during each task. We propose to use the Kernel Density
Estimation (KDE) of deep embedded video features extracted by unsupervised
video transformer networks as a non-parametric probabilistic representation of
the data. We introduce a novelty detection criterion for the incoming new task
data, dynamically enabling the expansion of memory clusters, aiming to capture
new knowledge when learning a succession of tasks. We leverage the use of
transfer learning from the previous tasks as an initial state for the knowledge
transfer to the current learning task. We found that the proposed methodology
substantially enhances the performance of the model when successively learning
many tasks. We perform in-depth evaluations on three standard video action
recognition datasets, including UCF101, HMDB51, and Something-to-Something V2,
without using any labels or class boundaries.

</details>


### [54] [A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI](https://arxiv.org/abs/2508.21775)
*Omer Faruk Durugol,Maximilian Rokuss,Yannick Kirchhoff,Klaus H. Maier-Hein*

Main category: cs.CV

TL;DR: Use cascaded pre-training from anatomical model → CT pancreatic lesions → MRI, evaluate augmentation strategies, and build metric-aware heterogeneous ensembles to balance volumetric vs boundary performance; achieved state-of-the-art results on Task1 and strong Task2 results.


<details>
  <summary>Details</summary>
Motivation: Improve PDAC segmentation on MRI despite low contrast and limited annotated data by leveraging pre-training on related datasets and creating specialist ensembles.

Method: nnU-Net-based cascaded pre-training + metric-aware heterogeneous ensembling

Result: Achieved cross-validation Tumor Dice 0.661 (Task1) and 0.523 (Task2); Task1 MASD 5.46 mm and HD95 17.33 mm; found trade-off between aggressive augmentations (better volumetric accuracy) and default augmentations (better boundary precision).

Conclusion: Cascaded pre-training plus heterogeneous, metric-aware ensembling yields robust PDAC MRI segmentation under limited data; balancing augmentation strategies is key.

Abstract: Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is
critical for clinical workflows but is hindered by poor tumor-tissue contrast
and a scarcity of annotated data. This paper details our submission to the
PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and
therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the
nnU-Net framework and leverages a deep, multi-stage cascaded pre-training
strategy, starting from a general anatomical foundation model and sequentially
fine-tuning on CT pancreatic lesion datasets and the target MRI modalities.
Through extensive five-fold cross-validation, we systematically evaluated data
augmentation schemes and training schedules. Our analysis revealed a critical
trade-off, where aggressive data augmentation produced the highest volumetric
accuracy, while default augmentations yielded superior boundary precision
(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).
For our final submission, we exploited this finding by constructing custom,
heterogeneous ensembles of specialist models, essentially creating a mix of
experts. This metric-aware ensembling strategy proved highly effective,
achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523
for Task 2. Our work presents a robust methodology for developing specialized,
high-performance models in the context of limited data and complex medical
imaging tasks (Team MIC-DKFZ).

</details>


### [55] [Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight](https://arxiv.org/abs/2508.21777)
*Ugur Dinc,Jibak Sarkar,Philipp Schubert,Sabine Semrau,Thomas Weissmann,Andre Karius,Johann Brand,Bernd-Niklas Axer,Ahmed Gomaa,Pluvio Stephan,Ishita Sheth,Sogand Beirami,Annette Schwarz,Udo Gaipl,Benjamin Frey,Christoph Bert,Stefanie Corradini,Rainer Fietkau,Florian Putz*

Main category: cs.CV

TL;DR: GPT-5在放疗多选题表现优异并能生成总体合理的治疗建议，但存在细节性错误与低专家一致性，临床部署前需严格专家监管。


<details>
  <summary>Details</summary>
Motivation: 评估针对肿瘤学市场化的GPT-5在放射肿瘤学临床决策支持的能力与局限。

Method: 使用两套基准评估：1) ACR放射肿瘤学在训考试（TXIT，2021）300题多项选择；2) 60例真实放疗病例摘要，由GPT-5生成简洁治疗计划，四位董事会认证放疗医师评估正确性、全面性和幻觉，并计算Fleiss' kappa评估评分一致性。

Result: 在TXIT上GPT-5平均准确率92.8%，高于GPT-4和GPT-3.5；在60例病例中正确性均分3.24/4、全面性3.59/4，幻觉罕见但评审一致性低（Fleiss' kappa=0.083），错误多在需精确试验知识或细节适应的复杂情形。

Conclusion: GPT-5在放射肿瘤学多选题基准上明显优于先前模型，但在真实病例推荐中仍有改进空间；需要专家把关才能临床应用。

Abstract: Introduction: Large language models (LLM) have shown great potential in
clinical decision support. GPT-5 is a novel LLM system that has been
specifically marketed towards oncology use.
  Methods: Performance was assessed using two complementary benchmarks: (i) the
ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300
multiple-choice items, and (ii) a curated set of 60 authentic radiation
oncologic vignettes representing diverse disease sites and treatment
indications. For the vignette evaluation, GPT-5 was instructed to generate
concise therapeutic plans. Four board-certified radiation oncologists rated
correctness, comprehensiveness, and hallucinations. Inter-rater reliability was
quantified using Fleiss' \k{appa}.
  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,
outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were
most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's
treatment recommendations were rated highly for correctness (mean 3.24/4, 95%
CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).
Hallucinations were rare with no case reaching majority consensus for their
presence. Inter-rater agreement was low (Fleiss' \k{appa} 0.083 for
correctness), reflecting inherent variability in clinical judgment. Errors
clustered in complex scenarios requiring precise trial knowledge or detailed
clinical adaptation.
  Discussion: GPT-5 clearly outperformed prior model variants on the radiation
oncology multiple-choice benchmark. Although GPT-5 exhibited favorable
performance in generating real-world radiation oncology treatment
recommendations, correctness ratings indicate room for further improvement.
While hallucinations were infrequent, the presence of substantive errors
underscores that GPT-5-generated recommendations require rigorous expert
oversight before clinical implementation.

</details>


### [56] [TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank](https://arxiv.org/abs/2508.21795)
*Jiawei Liu,Jiahe Hou,Wei Wang,Jinsong Du,Yang Cong,Huijie Fan*

Main category: cs.CV

TL;DR: 本文提出TMUAD，一种三重记忆库框架，通过类级文本记忆、目标级图像记忆和补丁级图像记忆联合检测逻辑与结构异常，实现多层次相似检索与融合评分，在七个工业与医疗数据集上达SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖精心设计的图像特征提取器和记忆库来捕捉对象间逻辑关系，但缺乏对逻辑异常的有效建模；因此引入文本记忆以增强逻辑异常检测并与结构检测统一。

Method: 提出逻辑感知文本提取器构建类级文本记忆库；对分割后的目标提取特征构建目标级图像记忆库以保留完整轮廓；用视觉编码器提取补丁级特征构建补丁级记忆库用于结构异常检测；三者检索相似正常样本、计算多层次异常分数并融合为最终分数。

Result: 在七个公开工业和医疗数据集上，TMUAD在结构与逻辑异常检测任务中表现出领先的性能，源码与模型已开源。

Conclusion: TMUAD通过协同的三种记忆库有效统一结构与逻辑异常检测，显著提升检测性能并在多数据集上验证了其优越性。

Abstract: Anomaly detection, which aims to identify anomalies deviating from normal
patterns, is challenging due to the limited amount of normal data available.
Unlike most existing unified methods that rely on carefully designed image
feature extractors and memory banks to capture logical relationships between
objects, we introduce a text memory bank to enhance the detection of logical
anomalies. Specifically, we propose a Three-Memory framework for Unified
structural and logical Anomaly Detection (TMUAD). First, we build a class-level
text memory bank for logical anomaly detection by the proposed logic-aware text
extractor, which can capture rich logical descriptions of objects from input
images. Second, we construct an object-level image memory bank that preserves
complete object contours by extracting features from segmented objects. Third,
we employ visual encoders to extract patch-level image features for
constructing a patch-level memory bank for structural anomaly detection. These
three complementary memory banks are used to retrieve and compare normal images
that are most similar to the query image, compute anomaly scores at multiple
levels, and fuse them into a final anomaly score. By unifying structural and
logical anomaly detection through collaborative memory banks, TMUAD achieves
state-of-the-art performance across seven publicly available datasets involving
industrial and medical domains. The model and code are available at
https://github.com/SIA-IDE/TMUAD.

</details>


### [57] [VoCap: Video Object Captioning and Segmentation from Any Prompt](https://arxiv.org/abs/2508.21809)
*Jasper Uijlings,Xingyi Zhou,Xiuye Gu,Arsha Nagrani,Anurag Arnab,Alireza Fathi,David Ross,Cordelia Schmid*

Main category: cs.CV

TL;DR: VoCap: a promptable video model producing spatio-temporal masklets and object-centric captions trained on SAV-Caption (pseudo-labelled via VLM) plus other data, yielding SOTA on referring expression VOS and a new video object captioning benchmark


<details>
  <summary>Details</summary>
Motivation: Existing video understanding tasks are siloed; obtaining aligned dense masks and captions is expensive; leverage VLMs to generate pseudo-captions for large segmentation datasets to scale training

Method: Combine masked video object segmentation, referring expression segmentation and object captioning into one model

Result: VoCap achieves SOTA on referring expression VOS, competitive semi-supervised VOS, and new benchmark for video object captioning; created SAV-Caption dataset

Conclusion: Unified model and dataset enable joint mask+caption understanding in videos and advance multiple tasks simultaneously

Abstract: Understanding objects in videos in terms of fine-grained localization masks
and detailed semantic properties is a fundamental task in video understanding.
In this paper, we propose VoCap, a flexible video model that consumes a video
and a prompt of various modalities (text, box or mask), and produces a
spatio-temporal masklet with a corresponding object-centric caption. As such
our model addresses simultaneously the tasks of promptable video object
segmentation, referring expression segmentation, and object captioning. Since
obtaining data for this task is tedious and expensive, we propose to annotate
an existing large-scale segmentation dataset (SAV) with pseudo object captions.
We do so by preprocessing videos with their ground-truth masks to highlight the
object of interest and feed this to a large Vision Language Model (VLM). For an
unbiased evaluation, we collect manual annotations on the validation set. We
call the resulting dataset SAV-Caption. We train our VoCap model at scale on a
SAV-Caption together with a mix of other image and video datasets. Our model
yields state-of-the-art results on referring expression video object
segmentation, is competitive on semi-supervised video object segmentation, and
establishes a benchmark for video object captioning. Our dataset will be made
available at https://github.com/google-deepmind/vocap.

</details>


### [58] [The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning](https://arxiv.org/abs/2508.21816)
*Yiming Lin,Yuchen Niu,Shang Wang,Kaizhu Huang,Qiufeng Wang,Xiao-Bo Jin*

Main category: cs.CV

TL;DR: 将动词分类视为单正多标签问题，提出GE-VerbMLP结合GNN和对抗训练，并构建多标签评估基准，MAP提升显著。


<details>
  <summary>Details</summary>
Motivation: 传统将动词识别视为单标签问题忽视了视觉事件识别中的固有模糊性和动词类之间的语义重叠，导致模型评估和训练不合理。

Method: 提出GE-VerbMLP：使用图神经网络捕捉标签相关性，结合多层感知机进行动词预测，并采用对抗训练来优化决策边界。此外，提出将动词分类建模为SPMLL，并设计多标签评估基准。

Result: 通过在实际数据集上的实验，GE-VerbMLP在MAP上提升超过3%，同时在传统Top-1和Top-5准确率上仍保持竞争力。

Conclusion: 该论文主张将动词分类从单标签问题重构为单正多标签学习（SPMLL），并提出了一种结合图神经网络和对抗训练的GE-VerbMLP模型，同时构建了多标签评估基准。实验表明在MAP上有显著提升且在Top-1/Top-5上保持竞争力。

Abstract: Context recognition (SR) is a fundamental task in computer vision that aims
to extract structured semantic summaries from images by identifying key events
and their associated entities. Specifically, given an input image, the model
must first classify the main visual events (verb classification), then identify
the participating entities and their semantic roles (semantic role labeling),
and finally localize these entities in the image (semantic role localization).
Existing methods treat verb classification as a single-label problem, but we
show through a comprehensive analysis that this formulation fails to address
the inherent ambiguity in visual event recognition, as multiple verb categories
may reasonably describe the same image. This paper makes three key
contributions: First, we reveal through empirical analysis that verb
classification is inherently a multi-label problem due to the ubiquitous
semantic overlap between verb categories. Second, given the impracticality of
fully annotating large-scale datasets with multiple labels, we propose to
reformulate verb classification as a single positive multi-label learning
(SPMLL) problem - a novel perspective in SR research. Third, we design a
comprehensive multi-label evaluation benchmark for SR that is carefully
designed to fairly evaluate model performance in a multi-label setting. To
address the challenges of SPMLL, we futher develop the Graph Enhanced Verb
Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to
capture label correlations and adversarial training to optimize decision
boundaries. Extensive experiments on real-world datasets show that our approach
achieves more than 3\% MAP improvement while remaining competitive on
traditional top-1 and top-5 accuracy metrics.

</details>


### [59] [DriveQA: Passing the Driving Knowledge Test](https://arxiv.org/abs/2508.21824)
*Maolin Wei,Wanzhou Liu,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: DriveQA 是一个文本+视觉的开源驾驶知识基准，暴露了当前 LLM/MLLM 在数值推理、复杂路权和空间理解方面的不足；微调与预训练在多个类别和下游任务上能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 评估并提升 LLM/MLLM 在驾驶领域的知识理解与决策能力，尤其针对稀有但关键的边缘场景和全面的交通法规掌握。

Method: 构建详尽覆盖交通法规与情景的文本与视觉数据集（DriveQA、DriveQA-V），设计分类的问答任务与受控环境变量，评估多种 LLM/MLLM 基线，实施微调与预训练实验并在 nuScenes、BDD 等真实数据集上验证迁移效果。

Result: DriveQA 提供了一个覆盖交通法规与场景的文本与视觉基准，旨在评估 LLM/MLLM 的驾驶知识理解能力。实验发现：模型在基本交通规则上表现良好，但在数值推理、复杂通行权判断、标志变体和空间布局方面存在显著弱点；在 DriveQA 上微调能提升多类准确率，尤其是法规标志识别与路口决策；DriveQA-V 的受控变化揭示模型对光照、视角、距离和天气等环境因素的敏感性；在 DriveQA 上进行预训练可提升在 nuScenes、BDD 等真实数据集的下游驾驶任务表现，且模型能将文本与合成交通知识内化以在下游 QA 任务中有效泛化。

Conclusion: DriveQA 有助于发现并弥补 LLM/MLLM 在驾驶知识理解上的薄弱环节，通过微调和预训练能实质提升实际驾驶场景下的问答与感知性能。

Abstract: If a Large Language Model (LLM) were to take a driving knowledge test today,
would it pass? Beyond standard spatial and visual question-answering (QA) tasks
on current autonomous driving benchmarks, driving knowledge tests require a
complete understanding of all traffic rules, signage, and right-of-way
principles. To pass this test, human drivers must discern various edge cases
that rarely appear in real-world datasets. In this work, we present DriveQA, an
extensive open-source text and vision-based benchmark that exhaustively covers
traffic regulations and scenarios. Through our experiments using DriveQA, we
show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on
basic traffic rules but exhibit significant weaknesses in numerical reasoning
and complex right-of-way scenarios, traffic sign variations, and spatial
layouts, (2) fine-tuning on DriveQA improves accuracy across multiple
categories, particularly in regulatory sign recognition and intersection
decision-making, (3) controlled variations in DriveQA-V provide insights into
model sensitivity to environmental factors such as lighting, perspective,
distance, and weather conditions, and (4) pretraining on DriveQA enhances
downstream driving task performance, leading to improved results on real-world
datasets such as nuScenes and BDD, while also demonstrating that models can
internalize text and synthetic traffic knowledge to generalize effectively
across downstream QA tasks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [60] [ORCA: ORchestrating Causal Agent](https://arxiv.org/abs/2508.21304)
*Joanie Hayoun Chung,Chaemyung Lim,Sumin Lee,Sungbin Lim*

Main category: cs.DB

TL;DR: 提出 ORCA：一个可交互的 LLM 代理，用于在 RDBMS 中自动化因果分析全流程，兼顾自动化与专家监督，并在实验中显著提升了平均处理效应估计性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模与业务复杂性增加，关系型数据库中的因果分析工作流对非专家变得难以执行，导致重复性瓶颈阻碍及时且负责任的商业洞见。需要一种既能自动化又能保留专家监督的解决方案。

Method: 构建 LLM 驱动的智能代理，负责解读自然语言、在数据库中导航表结构、生成 SQL、进行数据预处理，并配置因果推断库的建模流程；引入人机交互以允许专家迭代控制自动化过程。

Result: 在基准和合成电商数据集上，ORCA 在表理解、查询生成和因果效应估计上表现出竞争力，特别是在估计平均处理效应上，相较于 GPT-4o mini 提升了 7 倍以上。

Conclusion: ORCA 提出了一种基于 LLM 的代理系统，能在关系型数据库中自动化因果推断工作流，同时保留人类专家的监督与交互，从而提升复杂业务场景下的数据分析效率与可控性。

Abstract: Causal inference is essential for decision-making science while the
complexity of the data analysis workflow, ranging from data wrangling to causal
analysis, increases substantially as the scale of data grows in complicated
business environments. Especially, the execution of the workflow in relational
databases by non-experts can result in repetitive bottlenecks which impede
timely and responsible business insights. To address this challenge, we propose
ORCA (Orchestrating Causal Agent), an LLM agentic system that can automate
routine workflows in RDBMS while preserving expert oversight via human-AI
interactions. ORCA orchestrates the full data analysis pipeline: interpreting
natural language queries, navigating tables from DB servers, generating proper
SQL codes, preprocessing data, and configuring modeling processes using causal
inference libraries. Domain experts still can control the automation through
iterative interactions with ORCA, enabling robust data-driven decision making
with less technical expertise in statistical computing. Empirical evaluations
on benchmark and synthetic e-commerce datasets demonstrate competitive
performance of ORCA in table understanding, query generation, and cause-effect
estimation -- achieving over $7\times$ improvement in estimating average
treatment compared to GPT-4o mini.

</details>


### [61] [Hilbert Forest in the SISAP 2025 Indexing Challenge](https://arxiv.org/abs/2508.21682)
*Yasunobu Imamura,Takeshi Shinohara,Naoya Higuchi,Kouichi Hirata,Tetsuji Kuboyama*

Main category: cs.DB

TL;DR: 本文介绍了Hilbert forest索引方法，基于快速Hilbert排序和多棵Hilbert树用于近似最近邻搜索，在SISAP 2025挑战赛的两个任务上表现优异，尤其在k-NN图构建任务中达到最快构建时间，符合回召要求，适合内存受限场景。


<details>
  <summary>Details</summary>
Motivation: 在严格内存限制下寻求高效的近似最近邻搜索和k-NN图构建方法，Hilbert order提供一种简单高效的线性化策略，适合在受限资源环境中使用。

Method: 提出Hilbert forest：利用快速Hilbert排序将高维点沿Hilbert曲线线性化，然后构建多棵Hilbert树进行近似最近邻搜索。实现中针对SISAP两个任务分别提交了索引/构建代码，并在16GB/8核资源限制下测试。

Result: 在PUBMED23上的Task 1表现具竞争力；在GOOAQ上的Task 2实现了最快构建时间并满足回召要求，证明了方法在实际资源受限条件下的有效性。

Conclusion: Hilbert forest在受限资源（16GB内存、8核CPU）下证明了其有效性：Task 1中表现具有竞争力，Task 2中达到最快构建时间并满足召回率，显示出基于Hilbert顺序的索引在严格内存限制下的实用性。

Abstract: We report our participation in the SISAP 2025 Indexing Challenge using a
novel indexing technique called the Hilbert forest. The method is based on the
fast Hilbert sort algorithm, which efficiently orders high-dimensional points
along a Hilbert space-filling curve, and constructs multiple Hilbert trees to
support approximate nearest neighbor search. We submitted implementations to
both Task 1 (approximate search on the PUBMED23 dataset) and Task 2 (k-nearest
neighbor graph construction on the GOOAQ dataset) under the official resource
constraints of 16 GB RAM and 8 CPU cores. The Hilbert forest demonstrated
competitive performance in Task 1 and achieved the fastest construction time in
Task 2 while satisfying the required recall levels. These results highlight the
practical effectiveness of Hilbert order-based indexing under strict memory
limitations.

</details>
