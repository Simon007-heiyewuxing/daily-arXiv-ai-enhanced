<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.DB](#cs.DB) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS：50件可从澳洲超市购买的真实物品，基于SfM重建的高质量纹理3D网格，面向机器人与视觉基准测试。


<details>
  <summary>Details</summary>
Motivation: 弥补现有数据集依赖合成模型或难以获取的专用物品的问题，提供易得且更贴近实际的日常超市物品供研究复现。

Method: 使用高分辨率多视角影像并结合Structure-from-Motion（SfM）与多视角立体重建，生成封闭的高质量纹理化3D网格；物品选自澳大利亚大型超市，涵盖10个类别、50个物品。

Result: 发布包含50个经高质量重建的纹理化水密网格的数据集，覆盖多种形状、尺寸与重量，适用于物体检测、位姿估计与机器人抓取等任务，并强调可获取性与实际应用性。

Conclusion: ASOS是一套针对机器人与计算机视觉的高可用性、低成本3D物体数据集，适合用于检测、位姿估计与抓取基准测试。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出MM-RAG：两分支（ResNet+Transformer图像编码、BERT文本检索）+跨模态交互+模态注意门控，端到端多任务训练，Top-1检索准确率提高9.6%。


<details>
  <summary>Details</summary>
Motivation: 灾后对房屋损毁的准确评估对理赔和资源规划至关重要，现有方法在结合视觉证据与文本信息进行检索和生成方面存在不足，因而提出MM-RAG以提高检索准确率和损毁分类性能，并能生成基于图文证据的理赔相关文本。

Method: 在经典RAG架构基础上，设计两分支编码器：视觉分支采用ResNet与Transformer提取图像特征，文本分支用BERT进行帖子及保单文本向量化并建立可检索索引；通过多头注意力的跨模态交互模块对齐图文语义；生成模块引入模态注意力门控以动态控制视觉证据与文本先验的权重；整体端到端训练，联合比较损失、检索损失与生成损失进行多任务优化。

Result: 所提方法在检索准确率和损毁严重度分类指标上取得优越性能，Top-1检索准确率提升约9.6%。此外，框架实现了图像理解与保单匹配的协同学习（具体数值、基线与数据集细节未在摘要中明确）。

Conclusion: 该工作提出了一种两分支的多模态检索增强生成（MM-RAG）框架，通过视觉分支（ResNet+Transformer）和文本分支（BERT检索器）提取跨模态特征，并引入跨模态交互模块与模态注意门控机制，结合检索、对比和生成损失进行端到端多任务训练，实现了对灾后房屋损毁评估的检索与生成任务优化。实验证明在检索准确率和损毁严重度分类上均有提升，Top-1检索准确率提高了9.6%。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 对历史文档图像做多种增强，使用Gemini 2.0 Flash转录，然后用Needleman-Wunsch样式对齐器融合输出，得到更稳定更准确的转录和置信度；在622条死亡记录上比单次转录提高约4%。


<details>
  <summary>Details</summary>
Motivation: 嘈杂、退化的历史文档使得单次OCR/LLM转录容易出错，无法可靠评估置信度；通过图像增强和结果融合可以抵消单次转录的随机性和噪声，提高鲁棒性与可校验性。

Method: 对每张图像生成多种增强（如填充、模糊、网格扭曲等），使用Gemini 2.0 Flash对每个变体进行一次或多次转录，然后用改进的Needleman-Wunsch风格对齐算法将多个转录结果对齐并融合为共识文本，同时计算置信度得分；评估使用新构建的622条宾夕法尼亚死亡记录数据集，并与单次转录基线比较。

Result: 在622条死亡记录数据集上，该方法相比单次转录基线提升了约4个百分点的转录准确率；实验还发现填充和模糊增强对提升总体准确率最有效，而网格扭曲增强有助于区分高/低置信度样本。

Conclusion: 该论文提出了基于集成的稳定化框架，通过对历史文档图像生成多种增强变体，用大型语言模型进行转录，并使用定制的Needleman-Wunsch风格对齐器融合输出，得到共识转录和置信度评分，从而在嘈杂历史文档的文本提取任务上提高了准确率。

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: MITS是首个专为智能交通监控设计的大规模多模态数据集（170k图+5M QA），通过微调显著提升了LMM在ITS任务上的效果并开源资源。


<details>
  <summary>Details</summary>
Motivation: 现有通用大规模多模态模型在智能交通监控场景表现有限，主要因缺乏专门的多模态ITS数据集与任务指令。为促进ITS应用与研究，需要构建大规模、标注丰富且包含任务指令的数据集。

Method: 构建包含170,400张真实监控图像的数据集，标注8类主类和24类子类；设计数据生成流水线生成图像描述与5M条指令式视觉QA，覆盖识别、计数、定位、背景分析与事件推理五类任务；在多个主流LMM（如LLaVA、Qwen-VL系列）上进行微调并评估性能提升。

Result: 微调后显著提升性能：LLaVA-1.5从0.494提升到0.905（+83.2%），LLaVA-1.6从0.678到0.921（+35.8%），Qwen2-VL从0.584到0.926（+58.6%），Qwen2.5-VL从0.732到0.930（+27.0%）。同时开源数据集、代码与模型。

Conclusion: 本文提出了首个面向智能交通监控的大规模多模态基准数据集MITS，通过收集真实监控图像并构建高质量文本标注与5百万指令式视觉问答，显著提升了主流大模型在ITS任务上的表现。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 尽管树状、可解释的决策分解能被VLM正确理解，但在视觉分类上并未优于标准零样本提示；通过生成性描述可部分提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究将明确的树结构推理引入视觉语言模型能否提升其在细粒度任务与层级标签空间上的性能与可解释性。

Method: 提出将分类分解为基于决策树的可解释决策序列，并在GTSRB和CIFAR-10上评估，同时尝试通过LLM生成的类名和图像描述增强树提示。

Result: 模型在理解树状知识上达98.2%准确率，但树式推理方法整体表现低于标准零样本提示；加入LLM生成的描述能提升树式和零样本方法的性能。

Conclusion: 结构化、基于树的推理在该任务中未能超越标准零样本提示，尽管模型能很好地理解树状知识。

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI 用可随机访问的自回归概率模型结合因果抽取与回写策略，在大规模视频数据上自动提取并利用中间结构，提升模型预测能力与可控性。


<details>
  <summary>Details</summary>
Motivation: 希望构建既能高度可控又能灵活通过提示进行泛化的世界模型，能从大规模视频数据中自动发现并利用有意义的中间表征来改善预测与理解。

Method: 三步循环：1) 概率预测：训练随机访问自回归概率图模型 Psi，学习任意子集条件分布；2) 结构抽取：对 Psi 进行零样本因果推断以提取低维中间结构（如光流、深度、分割等）；3) 集成：将提取出的结构作为新 token 类型回写训练，增强条件信号与预测目标，从而迭代提升Psi。

Result: 在1.4万亿 token 的互联网视频数据上训练的实例展示了多项应用：视频预测、光流、无监督深度估计、目标分割等，并用这些结构实现了预测性能的迭代提升。

Conclusion: PSI 提出了一种循环式框架，通过在数据上训练可随机访问的自回归概率图模型 Psi，结合因果推断抽取中间结构并回写训练，实现模型能力与可控性的双重提升。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 本文首次分析了联邦学习中视频的梯度反演泄露：特征提取器提高抗攻击性但不足以完全防护；超分辨率和参考帧能显著提升重建效果，视频泄露为现实威胁。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过只共享模型更新来保护隐私，但梯度反演攻击能从梯度中重建训练数据。尽管已有对图像/文本/表格数据的研究，视频数据的风险尚未被系统研究，因此需要评估视频在FL下的泄露可能性。

Method: 使用梯度反演攻击对两类视频分类方法（预训练特征提取器+简单帧处理）进行评估，并结合图像超分辨率提升恢复质量，测试攻击者拥有不同数量参考帧的场景。

Result: 实验显示使用预训练特征提取器比直接处理原始帧更能抵抗梯度反演攻击，但在分类器复杂度不足或攻击者利用超分辨率与参考帧时，仍能重建有用的视频帧。总体证明视频数据在FL中存在泄露风险。

Conclusion: 视频数据在联邦学习中的泄露是现实威胁，特征提取器可降低但不能完全消除泄露风险，且分类器复杂度不足时仍可被还原。

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出一种基于Faster R-CNN与YOLO协同伪标签交换的半监督检测框架，配合XGBoost/随机森林/SVM集成与元启发式超参优化，在SKU-110k上表现良好，能减少标注成本并适应零售环境变化。


<details>
  <summary>Details</summary>
Motivation: 动机是应对零售货架场景标注数据匮乏、物品密集重叠/遮挡严重以及商品与布局经常变化，减少人工标注成本并提升检测与识别的泛化能力。

Method: 方法包含：1) 检测器协同训练：Faster R-CNN（ResNet骨干）用于精确定位，YOLO（Darknet骨干）用于全局上下文，两者互换伪标签进行半监督学习；2) 分类器集成：XGBoost、随机森林、SVM基于不同特征表示进行融合以增强分类鲁棒性；3) 超参数优化：采用元启发式算法对模型超参进行搜索以提升精度与效率。

Result: 在SKU-110k数据集上实验表明该框架取得较强性能，展示了在自动化库存跟踪、商品监控与结账系统等实际零售场景的可扩展性与实用性。

Conclusion: 该论文提出的半监督协同训练框架在零售密集场景下具有实用性，通过两种互补检测器互换伪标签并结合分类器集成与元启发式超参优化，能够在标注有限、遮挡与重叠严重的条件下提升检测与识别性能，减小人工标注成本并适应频繁变化的商品与布局。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出无反向传播的Token Purging方法，通过提前剔除受域偏移影响的token实现高效、低内存的测试时自适应，显著提升3D点云分类在分布变化下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法通常依赖反向传播或更新模型参数，而在资源受限或需快速部署的场景中不适用。为此提出一种快速、低内存开销且无需反向传播的token级剔除策略，以应对点云分类中的域偏移。

Method: PG在token级别运行，无需进行迭代更新或反向传播。提出两种变体：PG-SP（利用源域统计信息）和PG-SF（完全无源版本，基于CLS-token驱动的自适应）。两者在进入Transformer注意力机制前，通过筛选/剔除不可靠token来减少域偏差影响。

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C数据集上，PG-SP比现有无反向传播方法平均高出10.3%准确率，PG-SF在无源自适应任务上创下新基线。此外，PG在速度上比基线快12.4倍、内存效率提升5.5倍。

Conclusion: 本文提出了一种名为Token Purging（PG）的无反向传播测试时自适应方法，通过在注意力层前剔除受域迁移影响严重的token来提升3D点云分类在分布变化下的鲁棒性。

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出一种直接建立地面-航拍局部对应并仅提升匹配点到BEV的跨视图定位方法，结合单目深度先验与尺度感知Procrustes对齐，兼顾精度与可解释性，适应真实部署。


<details>
  <summary>Details</summary>
Motivation: 整图透视到BEV的变换会造成高度信息压缩和透视畸变，降低与航拍图像对齐质量；仅提升匹配关键点到BEV可保留更多细节，提升对应与定位精度，同时利用现代单目深度预测器提供的可靠相对/绝对深度信息。

Method: 直接匹配地面与航拍图像的局部特征，利用单目深度先验将匹配关键点提升到BEV（仅对匹配点），并用尺度感知Procrustes方法从对应关系恢复相机位姿；当深度为相对值时，方法可选地恢复尺度。

Result: 在仅有弱相机位姿监督下，该方法学会了准确的局部特征对应，并在跨区域泛化、未知朝向等挑战性场景下表现出优越的定位性能；兼容多种相对深度模型且无需逐模型微调。

Conclusion: 该文提出了一种直接在地面图像与航拍图像间建立局部特征对应并仅将匹配点提升到BEV空间的细粒度跨视图定位方法，避免了整图BEV变换导致的信息损失，支持度量与相对深度并通过尺度感知的Procrustes对齐估计相机位姿。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 此研究通过训练深度学习模型在手机拍摄的儿童红眼反射图像上实现了90%准确率的视力筛查，推动了可访问、低成本的儿童视力早期筛查发展。


<details>
  <summary>Details</summary>
Motivation: 使传统需医院和专业设备的Bruckner测试通过移动设备普及，实现早期发现儿童视觉异常，降低获取专业筛查的门槛。

Method: 基于深度神经网络（未具体说明架构），在由眼科专家标注的儿童瞳孔红眼反射图像上进行训练与测试，同时分析数据采集条件以提供即时采集反馈。

Result: 在未见测试集上达到90%准确率，模型能在最佳采集条件下为用户提供即时反馈，从而提高数据质量和筛查可靠性。

Conclusion: 该工作展示了使用智能手机和深度学习在儿童红眼反射图像上进行视力筛查的可行性，为便捷、无专用设备的儿童视力早筛提供了有力证据。

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 通过辅助深度学习生成空间可变的深度token并结合全局条件token，DGFusion按深度自适应地进行跨模态注意力融合，提升了在复杂/恶劣场景下的语义与全景分割表现。


<details>
  <summary>Details</summary>
Motivation: 现有传感器融合方法在空间上均匀处理输入，不能适应传感器可靠性随场景深度变化而变化的问题，导致在恶劣条件下性能下降。作者希望利用深度信息使融合按空间自适应地调整。

Method: 将多模态分割建模为多任务学习，使用LiDAR作为输入并作为深度监督的地面真值。设计辅助深度分支学习深度感知特征，编码为空间可变的局部深度token并与全局条件token联合用于条件化的注意力跨模态融合；同时提出针对稀疏噪声LiDAR的鲁棒深度损失。

Result: 在MUSES和DELIVER两个具有挑战性的数据集上，DGFusion达到了最先进的全景与语义分割性能。代码与模型将公开。

Conclusion: 本文提出DGFusion，通过引入深度引导的局部深度token和全局条件token，实现空间可变的跨模态注意力融合，从而提升在恶劣条件下的语义与全景分割性能。

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 利用不同大小/形状/位置的图像patch与ResNet-18进行训练，patch-based策略在性能、可解释性和隐私保护上优于或不逊于全图方法。


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮需要早期、精确检测以提高治疗效果，但全图方法可能受背景与可识别人脸特征影响，且存在隐私风险；因此探索只用局部皮损区域的patch来提升性能并保护患者隐私。

Method: 论文从人脸图像中以不同尺寸、形状和位置提取多个patch，使用ResNet-18对patch进行训练与测试；通过多组对比实验评估局部信息对模型性能的影响，并与全图方法比较准确率和灵敏度。

Result: 实验表明若干基于patch的方法在准确率和灵敏度上优于或与全图方法持平；模型更关注临床相关区域，增强了鲁棒性与可解释性，同时避免暴露可识别面部特征，保护了隐私。

Conclusion: 该论文提出了一种基于图像patch的玫瑰痤疮（rosacea）自动检测策略，基于ResNet-18网络，通过局部图像信息提高检测性能并保护隐私。

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 本文提出利用基于红通道的一致性掩码+合成数据训练ResNet-18来实现隐私保护的酒渣鼻检测，在真实测试集上优于全脸基线，展示了合成数据与临床先验在隐私敏感场景下的潜力。


<details>
  <summary>Details</summary>
Motivation: 动机是酒渣鼻症状分散、标注数据稀缺且面部图像存在隐私问题，因此希望设计一个在保护隐私前提下仍能有效检测酒渣鼻的自动方法。

Method: 方法包括两步：1) 基于红通道强度在脸部图像中选择一致性高的区域，构建固定的“红润度引导”掩码，从而聚焦于诊断相关的中面部（如双颊、鼻子、额头），并排除可识别身份的特征；2) 在被掩码的合成图像上训练ResNet-18分类器进行酒渣鼻检测，并以真实世界测试数据进行评估。

Result: 在真实世界测试集上，使用红润度掩码并在合成数据上训练的ResNet-18在准确率、召回率和F1值上均显著优于直接在全脸图像上训练的基线模型，证明了方法的有效性。

Conclusion: 该论文提出了一种在合成数据上训练、并结合临床先验的隐私保护型酒渣鼻自动检测方法，实验证明在真实测试集上相比全脸基线模型有明显提升，从而表明合成数据与临床先验可用于构建准确且伦理的皮肤科AI系统。

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 可学习Wiener滤波器+复合损失（MSE+SSIM+感知）对ULW去烟效果最优，单项或去滤波器均会牺牲部分质量。


<details>
  <summary>Details</summary>
Motivation: 明确ULW中各组件（可学习Wiener滤波器与复合损失各项）对去烟任务性能的必要性与贡献，以指导简化或改进模型设计。

Method: 在公开配对腹腔镜图像数据集上，基于U-Net的骨干，系统构建多种消融变体：去掉可学习Wiener滤波器、仅使用MSE/SSIM/感知损失的单项损失、以及不同组合；使用SSIM、PSNR、MSE和CIEDE-2000进行定量评估，并辅以视觉对比。

Result: 删除可学习Wiener滤波器会降低PSNR/SSIM并增加色差(CIEDE-2000)；仅用MSE虽能降低MSE指标但在结构与感知上表现差；仅用SSIM或感知损失能保持更好视觉质量但数值MSE/PSNR不一定最好；复合损失与学习滤波器结合取得最佳综合性能。

Conclusion: ULW框架中各组件均对去烟性能有正贡献，学习型Wiener滤波器和复合损失中的SSIM与感知损失尤为关键；移除或单独使用某些项会显著降低重建质量与色彩保真。

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: 将Wav2Vec2的声学特征与Deformable DETR多尺度视觉特征融合（最佳为门控），在新建的ARDrone数据集上显著提升了尤其是小无人机的检测mAP。


<details>
  <summary>Details</summary>
Motivation: 单纯视觉在复杂环境（低光、遮挡、远距离）下对小型无人机检测性能受限，引入声学信号可提供互补信息以增强鲁棒性。

Method: 基于Deformable DETR的检测框架与Wav2Vec2的声学特征提取器，作者将声学嵌入与Deformable DETR的多分辨率特征通过四种融合策略（门控、线性层、MLP、交叉注意力）进行融合，并在新构建的ARDrone数据集及已有Drone-vs-Bird数据上训练与评估。

Result: 在ARDrone数据集上，门控融合策略表现最好：对小型无人机在IoU 0.5–0.9范围内mAP提升11.1%到15.3%；中型与大型无人机mAP也分别有3.27%到5.84%的提升；总体跨尺寸提升3.27%到15.3%。

Conclusion: 本文提出了将可见光图像与声音融合的多模态WAVE-DETR无人机检测器，在现实环境下显著提升对小型无人机的检测效果。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 通过在代理图像上生成监督信号，替代监督使配准网络能用异构输入训练而保证监督在相似性定义良好的域中计算，从而显著提升配准模型对伪影、视野差异和模态不一致的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习配准对输入图像特性（如伪影、视野不匹配、模态差异）敏感，需一种普适的训练范式以提高模型在真实临床多样性条件下的鲁棒性与泛化能力。

Method: 在代理图像上施加估计的空间变换来生成监督信号，使训练时监督计算发生在相似性度量良好定义的域中，从而允许在异构输入上进行训练。框架在三项应用场景评估：脑MR抗伪影配准、肺CT无掩码配准、多模态MR配准。

Result: 在三项任务中，替代监督对输入变化（不均匀场、视野不一致、模态差异）展现出强鲁棒性，同时在优质数据上保持高性能。

Conclusion: 提出了一种名为替代监督（surrogate supervision）的训练范式，通过在代理图像上应用估计的空间变换，将输入域与监督域解耦，从而在输入图像存在伪影、视野不一致或模态差异等情况下提升可形变配准网络的鲁棒性和泛化性。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 将自编码器与ViT结合，不仅提升牙齿分期分类性能（牙37:0.712→0.815，牙38:0.462→0.543），还通过潜在空间与重建分析揭示数据质量/类内变异是主要瓶颈，提供比单一注意力可解释性更全面的诊断。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在司法鉴定（牙齿年龄估计）中因黑箱性导致的信任与可用性问题，同时提升在表现不均衡的不同牙齿类别上的准确率和透明度。

Method: 将卷积自编码器(AE)与Vision Transformer(ViT)结合：AE用于学习无监督的潜在表示并通过重建与潜在空间度量提供诊断信息，ViT负责监督分类；比较基线ViT并分析AE重建误差、潜在空间聚类与注意力图。

Result: 在牙37上准确率从0.712提升至0.815，在牙38上从0.462提升至0.543；AE的潜在空间分析与重建表明牙38的高类内形态变异是主要性能瓶颈；强调仅依赖注意力图可能误导且不足以揭示数据问题。

Conclusion: 该框架有效提升了牙齿分期任务的分类性能并增强模型可解释性，但仍受限于数据层面问题（牙38类内形态差异大）。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: 提出SCoDA：用自监督教师和流形对齐的空间相似性损失改进源无域适配，结合EMA更新防止遗忘，在基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法依赖监督预训练并用L2归一化后基于余弦相似度进行特征对齐，丢失了潜在流形的几何信息；此外监督预训练可能不适用或受限。

Method: 用自监督学习（SSL）训练教师模型；学生通过实例级特征匹配与空间相似性损失（Space Similarity Loss）进行训练；教师参数通过学生参数的EMA更新以防灾难性遗忘。

Result: 在多个基准数据集上，SCoDA明显优于最先进的SFDA方法，证明自监督初始化与流形对齐策略有效。

Conclusion: 论文提出了SCoDA，用自监督初始化教师模型并在无源数据情形下通过几何流形对齐与实例级特征匹配训练学生模型，显著优于现有方法。

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出基于SAM2的零样本细胞追踪方法，免标注、跨数据集泛化好，在2D/3D时序显微镜视频上表现良好。


<details>
  <summary>Details</summary>
Motivation: 深度学习方法依赖人工标注数据且对新数据泛化能力差；显微镜数据多样性大，标注成本高，有必要探索无监督或零样本方法以提高通用性并降低标注需求。

Method: 将SAM2（用于通用图像/视频分割的基础模型）集成到追踪流水线中，构建零样本细胞分割与关联模块，利用时间一致性和几何信息进行轨迹建立与分裂判定，无需在显微镜数据上微调。

Result: 在多个2D和大尺度3D时序显微镜数据集上获得了具有竞争力的准确率，同时省去了数据集特定的适配或微调步骤。

Conclusion: 本文提出了一个无需监督、基于大模型SAM2的细胞追踪框架，实现了对2D和大规模3D时序显微镜数据的泛化追踪与有丝分裂事件检测。

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出一种基于深度信息将在线2D多摄像机跟踪拓展到3D的实用框架，通过点云重建、聚类与偏航精化恢复3D盒，并用局部ID一致性的在线关联实现全局ID分配，在AI City 2025数据集上获第3名。


<details>
  <summary>Details</summary>
Motivation: 在有标定与深度信息的多摄像头监控场景下，3D跟踪能提供更丰富的环境理解，但将已有的2D跟踪组件完全替换为3D版本代价高且不可行，因此提出一种以最小改动将现有2D系统扩展到3D的实用方法。

Method: 先使用现有的在线2D多摄像机跟踪系统得到2D跟踪结果；利用相机标定与深度信息将检测到的目标投影到3D点云空间；对投影得到的点云进行聚类以恢复目标的3D包围盒，并通过yaw（偏航）角精化进一步优化3D盒；引入基于目标局部ID一致性的在线数据关联策略，在跨摄像机和跨帧场景中赋予全局ID。

Result: 在2025 AI City Challenge 3D MTMC数据集上，所提框架取得了排行榜第三名的成绩，表明该方法在实际竞赛场景中具备竞争力。

Conclusion: 该论文提出了一种将任意在线2D多摄像机跟踪系统扩展到3D空间的可行方案，通过深度信息重建点云并聚类与偏航角精化恢复目标的3D包围盒，同时提出了一种利用局部ID一致性的在线数据关联机制来实现跨帧全局ID分配。实验在2025 AI City Challenge 3D MTMC数据集上取得了排行榜第3名，证明了方法的有效性。

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 将REC改为逐框真/假的视觉-语言验证，结合通用检测器与VLM，无需微调即可在多个REC基准上实现或超越有监督方法。


<details>
  <summary>Details</summary>
Motivation: 质疑现有依赖任务训练的REC方法，探讨是否通过合理工作流程和通用VLM在零样本情形下达到甚至超过有监督模型。

Method: 使用COCO-clean通用检测器（YOLO-World）生成候选框，针对每个区域用通用VLM回答True/False查询（逐框独立验证），无需REC特定微调；比较了验证式提示与选择式提示的效果，并在相同候选框下做受控实验。

Result: 在RefCOCO、RefCOCO+、RefCOCOg上，方法超越零样本GroundingDINO基线，并优于报告中的训练过的GroundingDINO与GroundingDINO+CRG；验证式提示明显优于选择式提示，且对开放VLM同样适用。

Conclusion: 工作流程设计比任务特定预训练更能推动零样本REC表现；将REC重构为逐框视觉-语言验证并结合通用检测器与VLM可在多个基准上取得优异效果。

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 提出一种基于随机投影的Copy-and-Paste数据增强（RPCP），通过裁取变换并滤波融合稀有虫损补丁，缓解像素级极度不平衡，显著提高虫损分割效果。


<details>
  <summary>Details</summary>
Motivation: 小面积虫损像素在标注中占比极低，导致分割模型更容易过拟合常见类别而忽视稀有类别，亟需增强稀有类样本的多样性和出现频率。

Method: 从标注训练图像中裁取稀有的虫损补丁，进行随机几何变换以模拟形变，粘贴到合适区域并避免与已有病损重叠；随后对粘贴区域施加随机投影滤波，以细化局部特征并与背景自然融合。

Result: 实验表明，RPCP显著提升了虫损类别的分割性能，同时对其他类别的精度保持稳定或略有提升。

Conclusion: 该论文提出的RPCP针对小目标像素极度不平衡问题，通过生成稀有损伤样本并融合到训练图像中，有效提升了受损类别的分割性能。

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 利用隐马尔可夫模型将稀疏且不确定的身份观测融入在线多目标跟踪，可有效减少长期跟踪中的身份切换并提升整体性能，适用于畜牧等具有间歇性身份来源的应用场景。


<details>
  <summary>Details</summary>
Motivation: 长时间视频分析（例如畜牧场动物行为监测）中，目标身份随时间容易发生交换，导致跟踪器性能随时间下降；同时在实际场景中可获得零散但不完全可靠的身份信息，如何将这些不确定身份信息有效融合进跟踪以提升长期MOT性能是本文核心动机。

Method: 将稀疏且带有不确定性的身份观测（例如喂食站的识别）作为HMM的观测输入，与现有在线跟踪器（如ByteTrack、FairMOT）产生的轨迹关联，通过HMM的状态转移建模身份随时间的演化并利用观测更新来修正身份和轨迹。框架兼容不同跟踪器且可处理观测不确定性。

Result: 在一个包含10分钟、并在喂食站有21次身份识别的猪群跟踪数据集上，所提HMM框架在ByteTrack基础上提升了F1分数；随着身份观测频率增加，性能进一步提升。方法在MOT17和MOT20基准上与ByteTrack和FairMOT结合也显示出鲁棒性和改进。代码与数据集已开源。

Conclusion: 本文提出了一种结合不确定身份信息与跟踪的隐马尔可夫模型（HMM）框架，旨在提升长时间多目标跟踪（MOT）中因身份切换导致的性能下降。

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 综述事件相机与帧相机融合在视频恢复与3D重建的最新进展，分类讨论时间/空间增强方法、融合策略与数据集，指出优势与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、低功耗和高时间分辨率，能弥补传统帧相机在快速运动、光照变化和高动态范围场景下的不足，因此融合两类传感器有望提升视觉恢复和3D重建性能。

Method: 系统性回顾了基于深度学习的方法，按时间增强（帧插值、运动去模糊）与空间增强（超分辨率、低光/HDR增强、伪影去除）两个维度分类，并讨论了多模态融合策略与架构设计。

Result: 整理并比较了近期主流方法、数据集与评估指标，指出融合方法在多项任务上优于单模态方法，但仍面临配准、噪声建模、数据稀疏与端到端训练等挑战。

Conclusion: 本文综述了事件相机与传统帧相机融合在视频恢复与3D重建任务中的进展，认为二者互补性显著，可显著提升在低光、快速运动和高动态范围等挑战场景下的视觉重建质量。

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: 提出ISTASTrack：首个Transformer型ANN-SNN混合跟踪器，利用基于ISTA的适配器与时序下采样注意力实现高效双向特征融合，在多数据集上实现SOTA且能效高。


<details>
  <summary>Details</summary>
Motivation: 现有ANN难以充分利用事件流的稀疏异步特性，混合ANN-SNN架构在感知任务中潜力大，但不同范式和模态特征融合困难且时序对齐有挑战，需设计模型化的适配机制提升融合效果与能效。

Method: 构建双分支架构：RGB分支为Vision Transformer提取空间上下文，事件分支为Spiking Transformer捕捉时空脉冲动态；提出基于稀疏表示理论的ISTA适配器，通过展开迭代收缩阈值算法实现双向特征交互，并在适配器中加入时序下采样注意力模块以将多步SNN特征与单步ANN特征对齐。

Result: 在FE240hz、VisEvent、COESOT和FELT等RGB-Event跟踪数据集上，ISTASTrack达到了最先进的跟踪性能，并在能效上表现优秀，验证了所提方法的有效性与实用性。

Conclusion: 本文提出了首个基于Transformer的ANN-SNN混合跟踪器ISTASTrack，通过ISTA适配器实现RGB图像与事件流在特征层的双向高效融合，解决了ANN与SNN范式及时序对齐问题，在多个RGB-Event跟踪基准上实现了SOTA性能并兼顾能效。

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出利用多重深度状态空间模型结合频率与局部边界感知的FLARE损失，解决类别不平衡问题，在11年多波段太阳图像数据集上提升了太阳耀斑等级预测的性能与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有太阳耀斑预测性能不足，且不同等级耀斑的样本分布极不平衡，导致模型难以兼顾罕见但重要的强耀斑预测与整体可靠性，因此需要一种既能建模时序动态又能应对类别不平衡并提升可靠性的方案。

Method: 方法由两部分构成：1) 使用多重深度状态空间模型（multiple deep state space models）对时间序列多波段太阳图像/特征进行建模，捕捉时序动态；2) 提出频率与局部边界感知的可靠性损失（FLARE loss），用于解决严重类别不平衡，提升预测置信度和可校准性。

Result: 在覆盖完整11年太阳周期的多波段太阳图像数据集上进行实验，提出的方法在Gandin-Murphy-Gerrity分数和True Skill Statistic（TSS）上均优于基线，表明在预测性能和可靠性方面均得到提升。

Conclusion: 该论文提出了基于多重深度状态空间模型的太阳耀斑等级预测方法，并设计了针对类别不平衡的FLARE损失，实验在覆盖完整11年太阳活动周期的多波段太阳图像数据集上进行，结果在Gandin-Murphy-Gerrity分数和真实技能统计量上优于基线方法，表明在性能和可靠性方面有提升。

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: 提出统一的轻量RGB-T编码器与局部自适应融合模块，通过RGB+伪热大规模预训练实现高效跨模态特征提取与融合，在保证精度的同时显著降低参数和计算，适合嵌入式实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T语义分割方法常用基于RGB预训练的双分支编码器加独立融合模块，导致热模态特征提取不足、跨模态融合次优且结构冗余影响实时性。为此需一个紧凑且在编码器内同时完成提取与融合的设计。

Method: 提出一个多层堆叠的RGB-T编码器，结合大规模RGB与伪热图预训练，压缩热模态分支并在编码器内统一执行特征提取与融合；引入RGB-T局部模块，基于自适应余弦相似度选择性增强跨模态一致与互补的局部特征。

Result: 在FMB、PST900和CART数据集上，TUNI以更少的参数和更低的计算量达到与最先进模型相当的性能；在Jetson Orin NX上推理速度达27 FPS，证明了其实时部署能力。

Conclusion: TUNI通过统一的RGB-T编码器和局部融合模块，有效整合了多模态特征提取与跨模态融合，在保证轻量与实时性的同时实现了与现有最先进方法相当的分割性能。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出一种从少量部分形状生成完整字体的新范式和模型，减少样本需求并展示部分设计元素对字符整体结构的影响。


<details>
  <summary>Details</summary>
Motivation: 传统few-shot字体生成需完整字符样本，获取成本高且不利于探索部件对字形的影响。提出few-part-shot以降低样本需求并研究部分设计细节如何影响整体字符结构。

Method: 引入few-part-shot范式，模型以部分设计元素（部分形状）为条件输入，使用生成网络（可能结合条件生成对抗网络或自回归/变分框架）学习从局部细节到全字形结构的映射，并利用结构一致性或重建损失保持字符整体连贯性。

Result: 模型能在仅有少数部分形状的情况下生成一致且风格统一的完整字符，提升字体制作效率，并揭示部分细节如何塑造字符整体结构（实验上在重建质量、风格一致性度量上优于基线）。

Conclusion: 本论文提出了一种基于少量部分笔画/部件样本生成完整字体的模型，能从部分形状推断出完整字符设计。

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [30] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 本文提出针对RISC-V超低功耗SoC（GAP9）优化并量化的实时VIO流水线，融合SuperPoint、PX4FLOW与ORB特征器并引入刚体运动约束，在实际SoC上验证后显示可在微/纳无人机平台上实现显著精度提升（ORB：RMSE最高降3.65x），且PX4FLOW在低速下运行效率优于ORB。


<details>
  <summary>Details</summary>
Motivation: 弥合高精度VIO（需强大计算资源）与适用于微控制器/超低功耗平台实现之间的差距，使得微/纳级无人机能够在受限能耗与算力下实现实时且精确的位姿估计。

Method: 将SuperPoint、PX4FLOW和ORB三种特征器分别进行量化与算子优化，部署到RISC-V并行低功耗SoC（GAP9）；结合IMU并采用刚体运动约束进行状态估计；评估计算复杂度、运行时与量化后跟踪精度，并在真实硬件上完成端到端实现与RMSE评估。

Result: 在GAP9 SoC上实现并验证了量化与优化后的VIO流水线；使用ORB的优化版本平均RMSE相比基线最高降低3.65倍；PX4FLOW在低速（<24像素/帧）下以更低运行时达到与ORB相当的跟踪精度；整体证明了在超低功耗SoC上可实现实时且精确的VIO。

Conclusion: 提出了一套针对微/纳无人机的低功耗、实时视觉惯性里程计（VIO）流水线，结合SuperPoint、PX4FLOW与ORB三种特征检测/跟踪方法，并在RISC-V低功耗并行SoC（GAP9）上完成量化优化与实现，利用刚体运动模型在平面运动中降低估计误差。实验在真实SoC平台上验证，量化后仍保持精度，基于ORB的优化流水线相比基线平均RMSE最高可降至3.65倍；在移动速度低于24像素/帧时，PX4FLOW在运行时比ORB更优且保持近似精度。

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出MLANet：结合分层预训练骨干、多级注意力和半监督可微渲染端到端训练，从单张野外人脸图像恢复高质量3D人脸模型，并在两大基准上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 受限于缺乏带标注的真实3D人脸数据与复杂的野外成像条件，作者希望通过设计更强的特征提取与注意力机制结合半监督策略，提高单张野外人脸图像的3D重建质量。

Method: 采用预训练的分层骨干网络，在不同特征提取阶段引入多级注意力模块，预测3DMM参数、纹理、姿态和光照；使用可微渲染器和公开数据的3DMM参数进行半监督端到端训练，并进行了消融和对比实验。

Result: 在AFLW2000-3D和MICC Florence上的定量和定性评估显示，MLANet在3D重建与对齐任务中取得了优异表现；消融实验验证了多层次注意力与半监督训练的有效性。

Conclusion: 本文提出的MLANet通过多层次注意力机制和半监督训练，实现了从单张自然图像重建细节丰富的3D人脸模型，实验证明在AFLW2000-3D和MICC Florence上优于或接近现有方法。

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: 提出 LaV-CoT：首个语言感知的视觉 CoT 框架，通过多阶段可解释推理与多方面奖励优化，用自动化生成多语种 CoT 数据，并结合 SFT+GRPO 训练，显著提升 mVQA 性能并在真实业务中验证。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖文本 CoT，缺乏对多语言多模态推理的支持，限制了实际部署；因此需要一个支持多语言、多模态且可解释的 CoT 框架。

Method: LaV-CoT 包含四阶段推理：文本摘要+边界框、语言识别、面向对象的空间描述、逐步逻辑推理；并通过自动化数据构建循环生成/校正/精炼多语言 CoT 注释；训练采用 SFT + 语言感知的组相对策略优化（GRPO），以语言一致性、结构准确性和语义对齐作为可验证奖励。

Result: 在 MMMB、Multilingual MMBench、MTVQA 等数据集上，LaV-CoT 相较同等规模开源基线最高提升约 9.5% 准确率；比 2× 大小模型高约 2.6%；并优于 GPT-4o-0513 与 Gemini-2.5-flash 等专有模型；线上 A/B 测试验证了工业有效性。

Conclusion: LaV-CoT 提出了一种新的面向多语言的视觉链式思维（CoT）框架，通过可解释的多阶段推理流水线与多方面奖励优化策略，提高了多语言多模态问答的性能。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出一种训练-free的LLM驱动文本嵌入颜色混合方法，通过消歧颜色描述并在CIELAB空间调整嵌入，提高T2I模型的颜色准确性，且无需参考图像或额外训练。


<details>
  <summary>Details</summary>
Motivation: 动机是当前扩散模型在处理细微或复合颜色描述（如Tiffany blue、lime green、hot pink）时常出现与人类意图不一致的问题，现有方法依赖注意力操作、参考图像或微调，无法系统性解决颜色描述的歧义性。

Method: 方法包括两步：1）利用LLM解析并消解模糊或复合颜色词，将提示词转为明确的颜色术语；2）根据这些颜色术语在CIELAB色空间中的空间关系，对对应的文本嵌入进行调整与混合，从而在生成过程中引导颜色表现，无需额外训练或参考图像。

Result: 实验显示，该框架在不损害图像质量的前提下，显著提升了颜色对齐度，使生成结果更符合文本语义与用户的颜色意图。

Conclusion: 该论文提出了一个无需训练的框架，通过使用大语言模型（LLM）消歧颜色描述并在文本嵌入空间进行颜色混合操作，提高文本到图像生成中的颜色一致性。

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: 提出面向UAV航拍的数学推理基准AVI-Math，评测14个VLM并发现其在该域表现不足，CoT提示和微调可部分缓解问题，强调需发展更强的域特定推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在无人机航拍的数学推理场景（如精确距离/面积计算、轨迹估算、空间分析等）未被充分测试，且现有多模态基准多集中于简单计数或常见视觉问答，缺乏对域特定数学推理能力的考察。

Method: 构建了包含3,773个高质量车辆相关数学问题的数据集，覆盖6个数学科目和20个主题，数据采自不同高度与角度的UAV图像；对14个主流视觉-语言模型进行了系统评测；进行细致错误分析；尝试Chain-of-Thought提示和微调方法以提升模型推理能力。

Result: 数据集包含3773个问题，覆盖多高度和视角。14个主流VLM在AVI-Math上总体表现较差，显示出显著的数学推理短板；Chain-of-Thought提示与微调在若干任务上带来改进但仍有限。作者给出详细失败模式与未来改进方向。

Conclusion: 本论文提出了AVI-Math，一个首个面向无人机航拍图像的多模态数学推理基准，用以填补现有VLM在该领域评估的空白。作者通过实证评测指出当前14个主流VLM在该任务上表现薄弱，并展示了CoT提示与微调在一定程度上能改善表现。论文结论强调需要针对UAV场景的模型能力提升与更可信的推理方法。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: 提出BEVTraj——一种基于BEV、无需HD地图且端到端的轨迹预测框架，使用可变形注意力和SGCP模块，在准确性与灵活性上接近或匹配HD地图方法。


<details>
  <summary>Details</summary>
Motivation: 预建HD地图受限于区域且对动态场景变化不适应；本地地图构建模块只能识别预定义元素且可能引入错误，都会影响预测性能，因此需要无地图依赖的实时方法。

Method: 在BEV空间直接利用实时传感器生成的密集BEV特征，使用可变形注意力提取相关上下文信息，并设计一个稀疏目标候选提案(SGCP)模块以实现端到端预测，避免后处理。

Result: 实验表明BEVTraj在各种场景下性能可比拟于最先进的基于HD地图的方法，同时消除了对预建地图的依赖；并已开源代码。

Conclusion: BEVTraj能在无HD地图情形下实现与基于HD地图模型相当的轨迹预测性能，提供更灵活的实时感知能力。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 通过多视角弱监督与一致性损失，结合半自动多视角数据标注，本文在遮挡严重的多人解析任务上获得了显著改进（最高4.2%相对提升）。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多人重叠/遮挡时表现差，直观想法是不同视角下重叠可能被分离，因此引入多视角信息可为训练提供额外的可区分信号以改善分割与关联。

Method: 在训练时利用多视角RGB+D数据与3D骨架信息，采用半自动标注生成实例分割掩码，并通过弱监督（基于实例）和多视角一致性损失约束模型，使得不同视角下的人体部件分割结果保持一致。

Result: 在遮挡场景中，相较于基线模型，提出的方法在人体解析任务上最多可带来约4.20%相对提升。

Conclusion: 本文提出通过训练阶段引入多视角信息和弱监督实例一致性损失，能在遮挡严重的多人解析场景中提升模型性能，从而缓解重叠人体分割错误。

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0为韩英双语的开源VLM，强化了多图像理解与布局感知OCR，采用四阶段训练并发布14B和1.7B两个版本，14B在基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升VARCO-VISION-14B的能力，尤其是对复杂视觉输入（如文档、图表、表格）的理解和空间定位能力，并扩展到韩英双语场景，同时提供轻量级版本便于设备部署。

Method: 采用四阶段课程化训练，结合内存高效技术、偏好优化提升安全性，支持多图像输入和布局感知OCR。

Result: 在多项基准测试中显示出强劲的空间定位能力和双语性能，14B模型在OpenCompass VLM排行榜同规模模型中排名第8；同时发布1.7B的部署优化版本。

Conclusion: VARCO-VISION-2.0在多模态对齐、空间定位OCR和双语理解方面都有显著改进，同时提供14B和1.7B两种规模以兼顾性能与部署需求。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出一种将MobileNetV3-Small与ShuffleNetV2融合并采用MSECorrLoss的轻量级FIQA方法，在VQualA上实现高相关性指标并满足效率约束，适合实际部署。


<details>
  <summary>Details</summary>
Motivation: 现有通用无参考图像质量评估方法难以捕捉面部特定退化，且现有优秀FIQA模型计算开销大，不利于实际部署。作者旨在设计一个既轻量又能与人类主观评价对齐的FIQA模型，适用于复杂的真实场景。

Method: 模型由MobileNetV3-Small和ShuffleNetV2组成，分别输出质量分数后通过简单平均进行融合；训练时采用MSECorrLoss，即MSE损失加上皮尔逊相关系数的正则项，以提高与人类感知的一致性。

Result: 在VQualA基准测试中，模型达到了SRCC=0.9829和PLCC=0.9894，同时满足竞赛设定的效率限制，证明在准确性和计算成本之间取得良好平衡。

Conclusion: 该论文提出了一种轻量级的面部图像质量评估（FIQA）方法，通过两个紧凑的卷积神经网络（MobileNetV3-Small和ShuffleNetV2）进行预测级别融合，并使用结合MSE与皮尔逊相关正则化的相关感知损失（MSECorrLoss）。实验表明在VQualA基准上取得了高SRCC和PLCC，且满足效率约束。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: RCOD通过隐域分组、退化感知采样和视觉提示注入，为单步扩散超分模型提供推理阶段可控的真实感-保真度权衡，提升了恢复效果并保持高效。


<details>
  <summary>Details</summary>
Motivation: 现有单步扩散超分方法因仅在单一时间步上训练或蒸馏，缺乏在推理时灵活调节保真度与真实感的能力，而多步方法能通过采样步数调节该权衡。为在保留单步效率的前提下引入可控性，提出RCOD。

Method: 提出了隐域分组策略（在噪声预测阶段实现显式的保真-真实感控制），退化感知采样（与分组正则化对齐以增强控制能力），以及视觉提示注入模块（用退化感知视觉token替代文本提示，提升恢复精度和语义一致性）。训练改动小且使用原始训练数据。

Result: 在多项定量指标和主观视觉质量评估上均优于最先进的单步扩散方法，能在推理阶段灵活控制真实感强度，同时保持计算效率。代码将公开。

Conclusion: RCOD在单步扩散（OSD）框架下，通过隐空间分组、退化感知采样和视觉提示注入，实现了在推理阶段灵活控制真实感与保真度的权衡，兼顾了恢复质量与计算效率，优于现有OSD方法。

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL利用预训练源模型的梯度信息进行伪标签精炼，并结合余弦相似度对比学习，在无源数据下显著提升视盘/视杯跨域分割效果。


<details>
  <summary>Details</summary>
Motivation: 由于不同成像协议导致的域差异，源域训练的分割模型在目标域上性能显著下降，且出于隐私或法规源数据不可用，设计一种只利用源模型和无标签目标数据的自适应方法非常必要。

Method: 方法包含两阶段：(1) 梯度引导的特征提取与伪标签精炼：利用预训练源模型的梯度信息提取类特异性显著特征，结合不确定性度量迭代优化原始伪标签；(2) 基于余弦相似度的对比学习：对梯度提取的特征施加对比损失，增强类间可分性，细化边界。

Result: 在多组跨域视网膜基金图像数据集上，Grad-CL优于现有无监督和源无关域自适应方法，在分割精度和边界划定上取得一致提升。

Conclusion: Grad-CL在源数据不可用的情况下，通过梯度引导的伪标签精炼与基于余弦相似度的对比学习，有效提升了视网膜图像中视盘与视杯分割的跨域性能。

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge+学习退火构成的FVQ解决了VQ训练的不稳定性，保证了全码本使用并提升重构与生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有VQ训练存在直通估计偏差、落后一帧更新和稀疏梯度等问题，导致重构效果欠佳和码本使用率低，需一种稳健且可扩展的方法提高码本利用与训练稳定性。

Method: 提出VQBridge，一种基于映射函数（map function）的投影器，采用压缩-处理-恢复流水线来优化码向量训练；结合学习退火与VQBridge形成FullVQ(FVQ)，并在多种码本规模、通道数和训练时长下进行实验验证。

Result: 在多组实验中，FVQ实现100%码本使用率（最高达262k码本），达到或超过最先进的重建性能，随码本增大、通道增多或训练延长持续提升，并在与LlamaGen整合的图像生成任务中优于VAR和DiT（分别提升0.5和0.2 rFID）。

Conclusion: 提出VQBridge和学习退火结合能显著提升向量量化（VQ）训练稳定性与码本使用率，实现100%码本覆盖并改进重构和生成性能。

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock通过按层冻结利用ViT层级收敛顺序，实现从像素到潜在预测的渐进训练，能加速MAE并在大模型上获得更好的视频表征性能。


<details>
  <summary>Details</summary>
Motivation: 加速视频Masked Autoencoder训练并设计稳定的潜在预测方法，解决训练效率和表示坍缩问题。

Method: 观察ViT在视频MAE训练中浅层先收敛、深层后收敛，基于此按预定计划逐步冻结层，结合该计划在潜在预测上进行训练，避免坍缩问题。

Result: 在4DS感知套件上，对高达4B参数模型表现优于非潜在的掩码预测方法，同时训练更高效。

Conclusion: LayerLock通过逐层冻结实现从像素到潜在预测的平滑过渡，能加速训练并避免潜在表示坍缩，为大规模模型提供了可扩展的自监督视频表征学习方法。

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 外观嵌入能改善视觉效果但不等于更准的几何重建；凸样条在紧凑性和安全性上优于高斯样条，嵌入主要降低显式模型复杂度而非几何误差。


<details>
  <summary>Details</summary>
Motivation: 空间机器人任务（如交互与避碰）对几何精度要求高，现有Novel View Synthesis方法常使用外观嵌入提升光度一致性，但其对几何重建是否有帮助尚不明确；因此需要系统比较隐式与显式方法及嵌入的作用。

Method: 在SPEED+数据集上，对比了隐式方法与显式方法的Novel View Synthesis性能，具体评估了K-Planes、Gaussian Splatting和Convex Splatting三种显式方法，并分析了使用外观嵌入与否对光度和几何指标的影响。

Result: 实验表明：1) 外观嵌入显著提升光度保真度，但对几何误差提升有限或无影响；2) 嵌入能减少显式方法所需的渲染基元数量，从而提高表示效率；3) 凸样条相比高斯样条生成更紧凑、少杂乱的重建，有利于安全关键应用。

Conclusion: 在太空基于视觉的3D重建中，外观嵌入（appearance embeddings）能提升光度再现但并不显著改善几何精度；嵌入的主要作用是降低显式表示所需的基元数量，而非提高几何质量。凸样条（Convex Splatting）在表示紧凑性和减少杂乱方面优于高斯样条（Gaussian Splatting），更适合安全关键的空间机器人应用。

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA通过多样化操控、像素级多任务和反向交叉注意力，减少对生成器特定伪迹依赖，显著提升对未知生成模型的检测泛化。


<details>
  <summary>Details</summary>
Motivation: 现有检测器依赖生成器特定的风格/压缩伪迹，导致面对未见生成模型时泛化能力差。需要弱化域偏差并增强语义对齐以提升通用性。

Method: 提出GAMMA训练框架：1) 使用inpainting-based manipulation和语义保持扰动生成多样化训练样本；2) 采用双分割头+分类头的多任务监督实现像素级源归属；3) 引入反向交叉注意力机制，让分割头指导分类分支纠偏表示。

Result: 在GenImage基准上达到SOTA泛化性能，准确率提升约5.8%，且对新发布生成模型如GPT-4o保持较强鲁棒性。

Conclusion: GAMMA通过引入多样化的图像操控策略、像素级多任务监督以及反向交叉注意力机制，有效减弱对生成模型特定伪迹的依赖，提升了对未见生成器的泛化检测性能。

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 评估 3 种 SRR 方法在 140 例胎儿脑 MRI 上的表现：NeSVoR 重建最稳定，SRR 方法会影响体积测量但不显著改变 VM 的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 快速多视角 2D 切片采集易受胎儿运动影响，导致分辨率低且可能有运动伪影，传统切片不能全面反映 3D 解剖结构；SRR 方法可整合多视图信息生成高分辨率 3D 体积，有望改进定量分析与诊断，但不同 SRR 方法在病理情况与下游任务中的影响尚不明确。

Method: 对 140 例胎儿脑 MRI（包括健康对照和脑室增大病理例）分别用三种 SRR 方法（NiftyMIC、SVRTK、NeSVoR）生成高分辨率重建。对每个重建结果使用 BoUNTi 算法分割出九个主要脑结构并计算体积；评估视觉质量、重建成功率、体积测量一致性及分类诊断性能。

Result: NeSVoR 的重建成功率最高且最稳定（>90%），三种方法在体积分割输出上存在显著差异，但在脑室增大诊断分类任务（如 VM）上的性能差异不显著。

Conclusion: NeSVoR 在重建成功率和稳定性上优于 NiftyMIC 和 SVRTK，尤其在病理病例中表现突出；尽管不同 SRR 方法导致体积估计存在显著差异，但对自动诊断（如脑室增大 VM）的分类性能影响有限，表明诊断任务对 SRR 引入的体积偏差具有一定鲁棒性。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: MCR通过掩码膨胀与重塑的训练一致性约束，有效抑制掩码幻觉与形状偏差，提升对象移除的上下文一致性和修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在目标移除任务中易生成与上下文不相关的内容（掩码幻觉）或仅按掩码形状填充对象（掩码形状偏差），降低修复质量与真实感。需要一种训练策略加强模型对掩码扰动的不变性并促进对周围语义信息的利用。

Method: 在训练过程中，MCR为每个样本构造三条分支：原始掩码、膨胀掩码和重塑掩码。膨胀分支扩大掩码区域以促使模型更多地参考周围上下文，重塑分支改变掩码形状以打破掩码形状先验。通过在损失函数中加入各分支输出的一致性约束（例如像素或特征级L1/L2损失），联合训练扩展了模型对不同掩码变体的稳健性。

Result: 实验表明，MCR能显著降低掩码内的无关生成与形状复制现象，在多个客观指标（如FID/LPIPS/PSNR）和主观人类评估上均有提升，且在不同掩码尺寸与复杂场景中均表现稳健。

Conclusion: 本文提出的Mask Consistency Regularization (MCR)通过训练阶段对掩码进行膨胀和重塑两种扰动，并强制各分支输出与原始掩码输出一致，从而减少掩码幻觉和掩码形状偏差，提升目标移除任务的修复一致性与上下文连贯性。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出MagicMirror：分类+340K人工标注数据+VLM评估器+GRPO策略，构建MagicBench评估T2I图像瑕疵，发现顶级模型仍普遍有明显物理瑕疵。


<details>
  <summary>Details</summary>
Motivation: 当前T2I在指令遵循与美学上进步显著，但生成图像普遍存在解剖学与结构性等物理瑕疵，缺乏系统化、细粒度的评估基准，阻碍瑕疵检测与改进。

Method: 梳理瑕疵分类体系并手工标注340K生成图像构成MagicData340K；基于此训练Vision-Language模型MagicAssessor用于细粒度评估与打标签；为解决类别不平衡和奖励投机，提出数据采样策略和多层次奖励的Group Relative Policy Optimization (GRPO)；最终用MagicAssessor构建自动化评测基准MagicBench对主流T2I模型进行评估。

Result: 构建了首个340K人工标注的细粒度瑕疵数据集；训练出可输出详细评估与标签的评估器MagicAssessor；提出针对不平衡和奖励投机的GRPO训练机制；基于MagicAssessor的MagicBench评估表明即便顶级模型（如GPT-image-1）仍存在大量显著瑕疵。

Conclusion: 本文提出了MagicMirror框架，通过构建细粒度瑕疵分类、人工标注的MagicData340K数据集、训练视觉-语言评估器MagicAssessor并设计GRPO训练策略与抽样方法，形成自动化基准MagicBench，用于评估T2I生成的物理瑕疵。评估显示即使是顶级模型仍普遍存在显著瑕疵，表明瑕疵减少是T2I未来的重要方向。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip融合手势与口型并用分层对比学习对齐多模态信息，有效提升手语翻译性能。


<details>
  <summary>Details</summary>
Motivation: 现有SLT多数侧重手势（手部信号），忽视了口型等非手部线索，而口型在区分视觉相似手势和提供语言信息方面十分重要。

Method: 提出将手部空间特征与口型（唇部运动）特征融合，并设计层次化对比学习目标，包含手语-口型对齐和视觉-文本对齐，实现语义一致性。

Result: 在PHOENIX14T和How2Sign两大基准上进行实验；在PHOENIX14T无Gloss设置中，SignClip使BLEU-4从24.32提升到24.71，ROUGE从46.57提升到48.38，优于SpaMo等先前方法。

Conclusion: SignClip通过融合手势和口型信息，并采用分层对比学习实现多层次对齐，从而提升手语翻译准确性。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 论文评估了闭源与开源VLMs在文本篡改检测上的表现，发现闭源领先、开源在追赶，且图像篡改专用模型泛化性差，尤其在仿真ID卡等现实滥用场景中检测难度大。


<details>
  <summary>Details</summary>
Motivation: 尽管已有工作展示了VLM在图像篡改检测的有效性，但文本篡改检测这一重要方向被忽视。作者希望填补该空白，评估VLMs在文本篡改检测上的能力与局限，并分析开源/闭源模型差距及专用模型的泛化问题。

Method: 作者对比分析了闭源和开源VLMs在多个文本篡改数据集上的表现，同时将一些专门用于图像篡改检测的VLMs用于文本篡改检测任务，涵盖自然场景文本和仿真ID卡两类数据集，以评估模型的泛化能力。

Result: 实验表明：1) 闭源模型（如GPT-4o）整体性能优于当前开源模型，但开源模型在持续缩小差距；2) 图像篡改检测专用VLMs在文本篡改检测上表现欠佳，泛化能力不足；3) 在‘自然场景文本’与‘仿真ID卡’两类测试集中，模型在仿真ID卡（更具挑战性和真实滥用场景）上的表现更差，表明现实滥用风险下检测更困难。

Conclusion: 该论文表明文本篡改检测在使用大型视觉语言模型（VLMs/LVLMs）方面仍有明显差距，尤其是开放源代码模型尚落后于闭源模型如GPT-4o；为图像篡改专门训练的VLM在文本篡改检测上泛化能力不足。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: 组合点云、RGB与文本的多模态提示与协同调制，提高了零样本3D异常检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有ZS-3D方法大多仅基于点云，忽视RGB和文本先验的语义信息，限制了无标签场景下的检测能力。多模态信息可提供互补线索以提高鲁棒性与精度。

Method: 提出Multimodal Prompt Learning Mechanism (MPLM)引入去对象化解耦文本提示与多模态对比损失增强模态内表征与模态间协同；提出Collaborative Modulation Mechanism (CMM)联合调制图像引导与点云引导分支以融合互补表征。

Result: 大量实验表明MCL-AD在零样本3D异常检测上达到了新的最先进（SOTA）性能。

Conclusion: MCL-AD通过多模态协作学习在无监督下提升3D异常检测性能，验证了在点云、RGB图像与文本语义联合建模下的有效性。

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出通过层深递增的DropPath概率来控制有效Lipschitz常数，从而在保持干净精度的同时提升对抗鲁棒性并减少计算量，CIFAR-10/ViT-Tiny实验验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和视觉Transformer虽然性能优秀，但对对抗扰动高度脆弱；现有防御方法要么计算开销大，要么缺乏形式化保证。借助Lipschitz常数与对抗鲁棒性的联系，通过控制有效Lipschitz常数来提升鲁棒性并降低计算成本是有潜力的方向。

Method: 设计了一个深度依赖的DropPath调度策略，使得更深层具有更高的丢弃概率，从而控制模型的有效Lipschitz常数；在训练过程中使用该调度进行随机深度训练，评估其对干净精度、对抗鲁棒性和计算量（FLOPs）的影响，并与基线和线性DropPath对比。

Result: 在CIFAR-10上的ViT-Tiny实验表明：自定义的深度依赖DropPath调度在不显著损失干净精度的情况下，提升了对抗攻击（FGSM、PGD-20、AutoAttack）下的鲁棒性；与基线和线性DropPath相比，还显著降低了FLOPs。

Conclusion: 本文提出的基于Lipschitz引导的深度相关随机深度（DropPath）方法，通过随层数增加的丢弃概率来控制网络的有效Lipschitz常数，从而正则化深层并提升对抗鲁棒性，同时保持干净样本精度并降低计算量。实验在CIFAR-10上的ViT-Tiny上验证了该方法在FGSM、PGD-20和AutoAttack下的鲁棒性提升，并显著减少了FLOPs，相比基线和线性DropPath调度表现更佳。

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 论文用能量地图结合GIS信息和出生-死亡随机优化，能在仿真中准确、可扩展地定位城市街道设施，代码将开源。


<details>
  <summary>Details</summary>
Motivation: 提高城市街道设施（如路灯）在复杂城市环境中的精确地理定位，便于市政和私营方高效监测和维护公共基础设施。

Method: 将目标位置信息以地图化的能量分布表示，结合GIS图层和道路等约束，构建空间似然；使用随机出生-死亡（stochastic birth-and-death）优化算法在能量地图上搜索最可能的对象配置。

Result: 在基于都柏林市中心路灯地理数据驱动的逼真仿真中进行评估，结果表明该方法具有可扩展性并能实现较高的定位准确性。

Conclusion: 该论文提出了一个基于能量地图的概率框架，用以在复杂城市环境中精确定位街道设施，能够整合外部地理信息提高定位精度，并通过随机出生-死亡优化算法推断最优资产配置。

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 通过在空间维对 tokens 聚类并只计算簇代表，ClusCa 无需训练即可显著加速扩散变换器（如 4.96x），并在保持或提升生成质量的同时将 token 数量减少 >90%。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法利用了时间维的相似性但忽略了空间维相似性，作者希望通过空间聚类进一步减少重复计算，提升推理效率。

Method: 在每个时间步对 token 做空间聚类，每个簇只计算一个代表 token 并将其信息传播到簇内其他 token，从而大幅减少需计算的 token 数量；可直接应用于已有扩散变换器，无需额外训练。

Result: 在 DiT、FLUX、HunyuanVideo 上的实验表明，ClusCa 在文本到图像和文本到视频生成任务上有效：例如在 FLUX 上实现 4.96x 加速，ImageReward 达到 99.49%，比原模型提高 0.51%。另外能将 token 数量减少超过 90%。

Conclusion: ClusCa 提出了一种基于空间聚类的特征缓存方法，显著减少了扩散变换器在迭代去噪过程中的计算量，在多种模型和任务上实现了显著加速且保持甚至提升了生成质量。

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: 提出I-Segmenter：首个全整数ViT分割框架，通过λ-ShiftGELU、去归一化和改用最近邻上采样实现端到端整数运算，在几乎不牺牲实用性的前提下显著压缩模型和加速推理。


<details>
  <summary>Details</summary>
Motivation: Enable deployment of ViT-based semantic segmentation on resource-constrained devices by reducing memory and compute via quantization while addressing fragility of ViTs under low-precision.

Method: Based on Segmenter, the authors convert all floating-point ops to integer equivalents, introduce λ-ShiftGELU activation, remove L2 normalization, replace bilinear upsampling with nearest neighbor, and apply quantization (including one-shot PTQ).

Result: Achieves average accuracy drop of ~5.1% vs FP32, model size reduction up to 3.8x, up to 1.2x faster inference on optimized runtimes, and competitive performance even with single-image one-shot PTQ.

Conclusion: I-Segmenter demonstrates that fully integer-only ViT segmentation is feasible, trading modest accuracy loss for substantial memory and runtime gains, and remains practical even with minimal calibration.

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: 提出针对OCT散斑噪声的伽马分布扩散去噪模型GARD，结合低噪声引导项与DDIM加速，在定量与定性上均优于对比方法。


<details>
  <summary>Details</summary>
Motivation: OCT图像受散斑噪声严重影响，现有方法难以兼顾噪声抑制与细节保留；散斑噪声非高斯分布，需设计与其统计特性相匹配的生成/去噪模型。

Method: 提出Denoising Diffusion Gamma Model以适配散斑噪声的统计特性，加入基于预处理得到的低噪声图像的Noise-Reduced Fidelity Term以引导去噪并避免高频噪声重引入；并将DDIM加速推理框架适配至伽马模型以提升推理效率。

Result: 在配对噪声/低噪声OCT B-scan数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统和现有深度学习去噪方法，定性结果显示边缘更清晰、解剖细节保存更好，同时借助DDIM适配获得更快的推理速度。

Conclusion: GARD通过引入基于伽马分布的扩散去噪模型以及噪声-降低的保真项，有效地在去除OCT图像散斑噪声的同时保持解剖细节，实验显示在PSNR、SSIM和MSE上均优于现有方法。

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 提出GLAM：基于几何引导的全局+局部跨视图对齐，用于多视图乳腺X线影像的VLM预训练，显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有从自然图像迁移的VLM忽视乳腺影像的域特性，特别是多视图（双侧/对侧）之间的几何与局部对应关系，导致模型无法有效利用视图间的几何上下文和细粒度病变信息。

Method: GLAM结合全局和局部对齐策略，采用视觉-视觉与视觉-语言的对比学习，并利用乳腺摄影成像的先验几何关系进行局部跨视图对齐。在EMBED数据集上进行预训练。

Result: 在EMBED上预训练后，GLAM在多个数据集和不同设置下均优于基线方法，表明引入几何引导的全局与局部对齐可以提升多视图乳腺影像的表征能力与下游任务性能。

Conclusion: 该文提出了一种针对乳腺X线照片多视图特征对齐的预训练方法GLAM，通过几何引导的全局与局部对齐提升视觉-语言基础模型在乳腺影像上的表现。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 本综述系统回顾了视觉定位在通用视觉-语言模型中的作用、方法学体系、应用与评估，分析了定位与多模态推理的联系，总结挑战并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉定位能把语言描述精确映射到视觉区域，是实现可解释、可控的视觉-语言系统的基础。具备定位能力的模型能扩展到更多应用场景（如指代理解、细粒度问答、实体指代生成、环境控制等），因此需要系统回顾和总结研究进展、评价基准与挑战。

Method: 文章通过文献综述方式，系统梳理现代通用视觉语言模型中与视觉定位相关的核心组件：例如定位模块设计（检测器、稠密对齐、跨模态注意力）、训练范式（对比学习、指向性标注、合成数据）、任务与评估（指代表达理解、视觉问答、定位引导生成）以及多模态链式思维与推理方法。作者对代表性工作进行分类比较并分析其优缺点。

Result: 综述总结出：1) 当前主流VLM在定位能力上已取得显著进展，但仍受限于训练数据质量、跨模态对齐精度与评估不足；2) 多模态链式思维与显式定位结合有望提升推理透明性与性能；3) 未来可通过丰富标注、合成数据、改进对齐机制与评估基准来推动研究。

Conclusion: 本文综述了视觉定位（visual grounding）在通用视觉语言模型中的重要性，认为视觉定位是实现细粒度视觉-语言理解与生成的关键能力，并指出当前工作主要围绕模型架构、训练数据与策略、任务设计与评估展开。作者总结了视觉定位与多模态链式思维、推理的紧密关系，强调结合定位机制可提升模型在指代表达理解、细粒度问答和多模态生成等任务的性能。最后，论文指出了若干挑战并提出未来研究方向。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 提出一种黑盒视觉攻击（Attention Attack），利用自动caption扰动cross-attention，配套新评估指标，在TEDBench++上显著破坏文本驱动的图像编辑而不易被察觉。


<details>
  <summary>Details</summary>
Motivation: 当前文本驱动图像编辑方法对对抗性攻击敏感，而大多数攻击依赖于对编辑提示或模型内部细节的访问；本工作旨在设计一种无需访问编辑提示或模型细节的黑盒式视觉攻击，通过破坏跨模态对齐来降低编辑效果的可靠性。

Method: 使用自动生成的图像caption作为代理提示，针对编辑模型中的cross-attention机制进行优化以扰乱注意力分布，使编辑结果与原始意图脱钩；提出两种新的评估指标：Caption Similarity（衡量语义一致性）和semantic IoU（通过分割掩码衡量空间布局破坏）。在TEDBench++基准上进行大规模实验以验证方法有效性。

Result: 在TEDBench++基准测试中，Attention Attack在保持视觉上不易察觉的前提下显著降低了编辑性能，导致语义一致性和空间布局的显著下降，且相比现有度量显示出更能反映免疫化失败的现象。

Conclusion: 本文提出了一种针对基于文本的图像编辑方法的视觉攻击——Attention Attack，通过利用源图像的自动生成描述作为代理编辑提示，扰动文本提示与图像视觉表示之间的cross-attention，从而破坏编辑对齐性，在不需了解编辑方法或编辑提示的情况下有效攻击。

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 本文将知识蒸馏应用于图像压缩，展示了在多种模型大小和质量/码率设置下，小模型可在降低计算与能耗的同时维持或提升重建性能，适合资源受限场景，代码开源。


<details>
  <summary>Details</summary>
Motivation: 当前基于神经网络的图像压缩在性能上超越传统编解码器，但计算资源需求高，不适合资源受限平台与实时应用，因此希望通过模型压缩手段降低资源消耗以实现部署。

Method: 在训练过程中使用知识蒸馏，将大型高性能教师模型的中间表征或输出作为目标，训练较小的学生压缩网络，以减少参数量和计算复杂度，同时保留重建图像质量；评估不同架构尺寸、码率/质量点并测量处理时间和能耗。

Result: 实验表明，经过知识蒸馏的小模型在相同码率下能实现与大模型接近甚至更好的重建质量，并显著减少推理延迟与能耗；研究还讨论了新出现的超参数设置及未来可探索的教师选择和损失函数。

Conclusion: 知识蒸馏可显著降低图像压缩模型的计算和能耗需求，同时在多种模型尺寸和质量/码率权衡下保持或提升性能。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 提出一种训练免费、基于可见-热图像序关系的自监督固有图分解方法，实现密集监督并在室外场景上优于近期学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法受限于真实世界大量标注数据稀缺，尤其室外场景；作者利用热成像提供的一种可扩展、自动的序监督信号，避免合成数据或稀疏人工注释。

Method: 基于热成像中未被表面反射的光被吸收并以热的形式探测的原则，构建可见-热图像间的序关系约束；使用这些稠密的序监督信号训练一个优化中的神经网络（无外部训练集）来分解图像的遮光与反射率。

Result: 在已知反射率和遮光的定量评估（自然/人工光照）以及多样室外场景的定性实验中，该方法优于近期基于学习的方法，展示了利用可见-热序监督构建真实世界监督集的可行性。

Conclusion: 该论文提出一种无需训练数据、仅利用可见光与热成像对进行固有图分解的新方法，通过将可见与热图像的亮度序关系与遮光/反射率序关系关联，实现了密集自监督优化并恢复遮光与反射率。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: TL;DR：本文提出了新的CVQE方法分类、统一基准和复杂度-性能分析，为一致评估与模型选择提供了系统基础。


<details>
  <summary>Details</summary>
Motivation: 动机：现有CVQE综述缺乏系统分类、未能将方法与具体编码标准和伪影类型关联，架构范式比较不足且基准测试不统一，阻碍了模型选择和领域发展。

Method: 方法：1）构建一种新的分类体系，按架构范式、编码标准（如H.264/H.265/H.266）和压缩域特征利用方式对方法进行分类；2）提出统一的基准测试框架，整合现代压缩协议与标准测试序列以进行公平的多指标评估；3）对现有方法在重建性能与计算复杂度之间的权衡进行系统分析并指出未来研究方向。

Result: 结果：建立了全面的分类体系与统一基准框架，比较了主流方法的性能与复杂度权衡，识别出若干在不同编码类型与资源约束下的优秀策略，并提出了未来研究方向建议。

Conclusion: 论文总结：本文提出了针对压缩视频质量增强（CVQE）领域的系统性综述，填补了现有综述在方法分类、与视频编码标准和伪影类型的关联、架构范式比较以及基准测试体系方面的不足。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: 提出MM SAM-adapter，通过adapter将融合的多模态特征注入SAM的RGB特征，从而在保留RGB泛化能力同时有选择地利用辅助模态，实现在多基准上达到更强鲁棒性的语义分割。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割在光照差、遮挡和恶劣天气等条件下脆弱，多模态传感器（LiDAR、红外等）可提供互补信息以增强鲁棒性，需要一种能在保留SAM强泛化能力同时融合多模态信息的方法。

Method: 设计一个adapter网络，将融合后的多模态特征注入SAM的RGB特征，保持原始RGB特征的泛化能力，仅在辅助模态有益时选择性利用它们，从而实现高效平衡的多模态信息融合。

Result: 在DeLiVER、FMB和MUSES三个基准上达到SOTA；在将DeLiVER和FMB划分为RGB-easy和RGB-hard后，MM SAM-adapter在两类子集上均优于竞争方法，表明其在良好与恶劣条件下均有效。

Conclusion: MM SAM-adapter有效增强了SAM在多模态语义分割任务中的性能，在多个基准上达到或超过SOTA，且在RGB易/难子集上均有优势。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: 提出InfGen，用one-step生成器替换VAE解码器，从固定潜在生成任意分辨率图像，显著降低高分辨率生成时间（4K<10s）。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型计算成本随分辨率二次增长，导致4K图像生成非常慢，需一种在不改动扩散模型的前提下实现任意分辨率快速生成的方法。

Method: 将扩散模型输出的固定潜在视为内容表示，训练一个one-step生成器（替代VAE解码器）将该潜在直接解码为任意分辨率图像，无需重训扩散模型。

Result: InfGen能将多个模型扩展到任意高分辨率，并把生成4K图像时间缩短到10秒以内。

Conclusion: InfGen提出用一阶段生成器替换VAE解码器，从固定尺寸潜在表示生成任意分辨率图像，实现高分辨率生成加速。

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 将时序自监督学习扩展到3D脑MRI并处理可变长度时序输入，聚合3161例数据预训练，在多数阿尔茨海默预测任务上优于监督方法并具有良好泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在阿尔茨海默预测上受限于标注数据稀缺、跨数据集泛化性差以及对输入扫描数量与时间间隔的不灵活，因而需要一种能利用大量未标注时序影像并对可变临床采集条件鲁棒的方法。

Method: 采用时序顺序预测和对比学习为主的自监督预训练策略，聚合四个公开数据集（3,161名患者）进行预训练；设计用于可变数量扫描和不同时隔的输入处理模块以及空间特征增强模块；在下游任务上（诊断分类、转归检测、未来转化预测）进行微调与评估。

Result: 基于时序顺序预测与对比学习的自监督模型在七个下游任务中有六个任务超过了监督学习基线，显示出对不同任务、不同输入图像数量及时间间隔的适应性和泛化能力。

Conclusion: 本文提出将三种先进的时序自监督学习方法适配到3D脑MRI分析，并引入处理可变长度输入和学习鲁棒空间特征的新扩展，实验证明在多个阿尔茨海默预测任务上优于监督学习。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [65] [Space-Time Tradeoffs for Spatial Conjunctive Queries](https://arxiv.org/abs/2509.10050)
*Aryan Esmailpour,Xiao Hu,Stavros Sintos*

Main category: cs.DB

TL;DR: 论文证明了空间-时间下界并构造了在范围空性与计数查询上时间-空间最优的索引，适用于k-星、k-路径及通过广义超树分解扩展的任意连接查询，从而能在关系数据的空间查询中显著加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么查询时间近似线性，要么空间与查询结果数同级，均在实用性上存在严重不足；目标是设计同时在时间和空间上都高效的索引以支持常见空间查询（范围空性、范围计数、最近邻）在连接查询结果上的快速执行。

Method: 通过证明下界Ω(N + N^k / T^k)（k-星）和Ω(N + N^2 / T^{2/(k-1)})（k-路径），并结合构造性算法利用分解技术（如广义超树分解）设计索引结构，匹配下界以获得时间-空间最优性；最后将索引嵌入已有关系算法实现加速。

Result: 给出针对k-星、k-路径的时间-空间下界，并构造出匹配这些下界的索引；扩展到层次与任意连接查询；展示了在关系算法（如聚类、临时图分析）中的加速效果。

Conclusion: 本文建立了空间-时间权衡的下界，并为k-星、k-路径及层次查询构造了在范围空性与计数查询上最优的索引，进而通过广义超树分解扩展到任意连接查询，最后展示了这些索引在关系算法中的加速效果。

Abstract: Given a conjunctive query and a database instance, we aim to develop an index
that can efficiently answer spatial queries on the results of a conjunctive
query. We are interested in some commonly used spatial queries, such as range
emptiness, range count, and nearest neighbor queries. These queries have
essential applications in data analytics, such as filtering relational data
based on attribute ranges and temporal graph analysis for counting graph
structures like stars, paths, and cliques. Furthermore, this line of research
can accelerate relational algorithms that incorporate spatial queries in their
workflow, such as relational clustering. Known approaches either have to spend
$\tilde{O}(N)$ query time or use space as large as the number of query results,
which are inefficient or unrealistic to employ in practice. Hence, we aim to
construct an index that answers spatial conjunctive queries in both time- and
space-efficient ways.
  In this paper, we establish lower bounds on the tradeoff between answering
time and space usage. For $k$-star (resp. $k$-path) queries, we show that any
index for range emptiness, range counting or nearest neighbor queries with $T$
answering time requires $\Omega\left(N+\frac{N^k}{T^k}\right)$ (resp.
$\Omega\left(N+\frac{N^2}{T^{2/(k-1)}}\right)$) space. Then, we construct
optimal indexes for answering range emptiness and range counting problems over
$k$-star and $k$-path queries. Extending this result, we build an index for
hierarchical queries. By resorting to the generalized hypertree decomposition,
we can extend our index to arbitrary conjunctive queries for supporting spatial
conjunctive queries. Finally, we show how our new indexes can be used to
improve the running time of known algorithms in the relational setting.

</details>


### [66] [Semi-interval Comparison Constraints in Query Containment and Their Impact on Certain Answer Computation](https://arxiv.org/abs/2509.10138)
*Foto N. Afrati,Matthew Damigos*

Main category: cs.DB

TL;DR: 研究CQAC查询的包含性与使用视图答复问题：总体复杂性高（Π2^p-完全），但当包含查询限制为半区间比较时变为NP可解；最大包含重写能精确计算确定答案，并在部分情形下多项式时间可解。


<details>
  <summary>Details</summary>
Motivation: 理解带算术比较的连接查询的包含性与确定答案计算的复杂性，以便在数据库查询重写和基于视图的查询回答中找到可行且高效的算法与可识别的易解子类。

Method: 复杂性分析与分类证明：通过构造性归约证明Π2^p-完全性，定义半区间比较类并证明在此类下问题落入NP；在视图回答问题上，形式化最大包含重写并证明其正确性与完备性，给出多项式可解子类。

Result: - 一般CQAC包含性为Π2^p-完全。 - 若包含查询使用半区间算术比较，则包含性问题可归入NP（从而更易求解）。 - 存在简单情形仍为Π2^p-完全。 - 最大包含重写（并的CQAC形式）能精确给出所有确定答案。 - 在一些情形下可多项式时间计算确定答案。

Conclusion: 论文得出：CQAC（带算术比较的连接查询）包含性问题在一般情况下是Π2^p-完全的，但当包含查询的算术比较属于半区间（semi-interval）类时，可降至NP可解；此外存在简单情形仍为Π2^p-完全。关于使用任意CQAC视图回答带半区间比较的CQAC查询，论文证明最大包含重写（以CQAC并的形式）能精确计算所有确定答案，并识别出可在多项式时间内通过最大包含重写计算确定答案的情形。

Abstract: We consider conjunctive queries with arithmetic comparisons (CQAC) and
investigate the computational complexity of the problem: Given two CQAC
queries, $Q$ and $Q'$, is $Q'$ contained in $Q$? We know that, for CQAC
queries, the problem of testing containment is $\Pi_2 ^p$ -complete. However,
there are broad classes of queries with semi-interval arithmetic comparisons in
the containing query that render the problem solvable in NP. In all cases
examined the contained query is allowed to be any CQAC. Interestingly, we also
prove that there are simple cases where the problem remains $\Pi_2 ^p$
-complete.
  We also investigate the complexity of computing certain answers in the
framework of answering CQAC queries with semi-interval comparisons using any
CQAC views. We prove that maximally contained rewritings in the language of
union of CQACs always compute exactly all certain answers. We find cases where
we can compute certain answers in polynomial time using maximally contained
rewritings.

</details>
